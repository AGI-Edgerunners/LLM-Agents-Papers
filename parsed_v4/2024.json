{
    "5b608143a5925bfbbcf579346d04fa2e": {
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
        "authors": [
            "Evan Hubinger",
            "Carson Denison",
            "Jesse Mu",
            "Mike Lambert",
            "Meg Tong",
            "Monte MacDiarmid",
            "Tamera Lanham",
            "Daniel M. Ziegler",
            "Tim Maxwell",
            "Newton Cheng",
            "Adam Jermyn",
            "Amanda Askell",
            "Ansh Radhakrishnan",
            "Cem Anil",
            "David Duvenaud",
            "Deep Ganguli",
            "Fazl Barez",
            "Jack Clark",
            "Kamal Ndousse",
            "Kshitij Sachan",
            "Michael Sellitto",
            "Mrinank Sharma",
            "Nova DasSarma",
            "Roger Grosse",
            "Shauna Kravec",
            "Yuntao Bai",
            "Zachary Witten",
            "Marina Favaro",
            "Jan Brauner",
            "Holden Karnofsky",
            "Paul Christiano",
            "Samuel R. Bowman",
            "Logan Graham",
            "Jared Kaplan",
            "SÃ¶ren Mindermann",
            "Ryan Greenblatt",
            "Buck Shlegeris",
            "Nicholas Schiefer",
            "Ethan Perez"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05566.pdf",
        "abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.05566"
    },
    "e84680c3461c7d5e66d91a05c35cb6c9": {
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "authors": [
            "Dennis Ulmer",
            "Elman Mansimov",
            "Kaixiang Lin",
            "Justin Sun",
            "Xibin Gao",
            "Yi Zhang"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05033.pdf",
        "abstract": "Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via &#34;self-talk&#34; of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.05033"
    },
    "5b15ebb969f67edb0bff7a691cf357d6": {
        "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models",
        "authors": [
            "Na Liu",
            "Liangyu Chen",
            "Xiaoyu Tian",
            "Wei Zou",
            "Kaijiang Chen",
            "Ming Cui"
        ],
        "date": "2024/01/05",
        "pdf": "http://arxiv.org/pdf/2401.02777.pdf",
        "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.02777"
    },
    "4da8b341d1dc74b5c91c82a8b8332fe3": {
        "title": "AUTOACT: Automatic Agent Learning from Scratch via Self-Planning",
        "authors": [
            "Shuofei Qiao",
            "Ningyu Zhang",
            "Runnan Fang",
            "Yujie Luo",
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Chengfei Lv",
            "Huajun Chen"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05268.pdf",
        "abstract": "Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent. Code will be available at https://github.com/zjunlp/AutoAct.",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2401.05268"
    },
    "ca294c5bb4a93752040084e62a9bedc9": {
        "title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
        "authors": [
            "Quan Tu",
            "Shilong Fan",
            "Zihang Tian",
            "Rui Yan"
        ],
        "date": "2024/01/02",
        "pdf": "http://arxiv.org/pdf/2401.01275.pdf",
        "abstract": "Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2401.01275"
    },
    "0c56afc81fe63849e15821130b7b8562": {
        "title": "AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models",
        "authors": [
            "Zihong He",
            "Changwang Zhang"
        ],
        "date": "2024/01/05",
        "pdf": "http://arxiv.org/pdf/2401.02870.pdf",
        "abstract": "The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents&#39; preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, sensory perceptions and social networking with subjective information, wield the most pronounced influence on preference shaping. AFSPP can significantly enhance the efficiency and scope of psychological experiments, while yielding valuable insights for Trustworthy Artificial Intelligence research for strategies to prevent undesirable preference and personality development.",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2401.02870"
    },
    "0f608fe3eaae4d7140d7e1b012544308": {
        "title": "MARG: Multi-Agent Review Generation for Scientific Papers",
        "authors": [
            "Mike D&#39;Arcy",
            "Tom Hope",
            "Larry Birnbaum",
            "Doug Downey"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.04259.pdf",
        "abstract": "We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.04259"
    },
    "6eb13c310b738a72ae549f50036d452d": {
        "title": "SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems",
        "authors": [
            "Dong Zhang",
            "Zhaowei Li",
            "Pengyu Wang",
            "Xin Zhang",
            "Yaqian Zhou",
            "Xipeng Qiu"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.03945.pdf",
        "abstract": "Human communication is a complex and diverse process that not only involves multiple factors such as language, commonsense, and cultural backgrounds but also requires the participation of multimodal information, such as speech. Large Language Model (LLM)-based multi-agent systems have demonstrated promising performance in simulating human society. Can we leverage LLM-based multi-agent systems to simulate human communication? However, current LLM-based multi-agent systems mainly rely on text as the primary medium. In this paper, we propose SpeechAgents, a multi-modal LLM based multi-agent system designed for simulating human communication. SpeechAgents utilizes multi-modal LLM as the control center for individual agent and employes multi-modal signals as the medium for exchanged messages among agents. Additionally, we propose Multi-Agent Tuning to enhance the multi-agent capabilities of LLM without compromising general abilities. To strengthen and evaluate the effectiveness of human communication simulation, we build the Human-Communication Simulation Benchmark. Experimental results demonstrate that SpeechAgents can simulate human communication dialogues with consistent content, authentic rhythm, and rich emotions and demonstrate excellent scalability even with up to 25 agents, which can apply to tasks such as drama creation and audio novels generation. Code and models will be open-sourced at https://github. com/0nutation/SpeechAgents",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.03945"
    },
    "b9ae12751107d3f49280fc6a22ba0d08": {
        "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
        "authors": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.03630.pdf",
        "abstract": "With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study how to solve MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on a slightly harder room map. We present our hypothesis of why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.03630"
    },
    "bdfaf34ffca3c335dfac368a6bd9439a": {
        "title": "Combating Adversarial Attacks with Multi-Agent Debate",
        "authors": [
            "Steffi Chern",
            "Zhen Fan",
            "Andy Liu"
        ],
        "date": "2024/01/11",
        "pdf": "http://arxiv.org/pdf/2401.05998.pdf",
        "abstract": "While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models&#39; susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.05998"
    },
    "815211b5ab3616d6d3d1fba384bca69a": {
        "title": "Agent Alignment in Evolving Social Norms",
        "authors": [
            "Shimin Li",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "date": "2024/01/09",
        "pdf": "http://arxiv.org/pdf/2401.04620.pdf",
        "abstract": "Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent possesses the capability to align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2401.04620"
    },
    "c7716e44d8d1c20e6e1fa9922dfc75d7": {
        "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents",
        "authors": [
            "Ke Yang",
            "Jiateng Liu",
            "John Wu",
            "Chaoqi Yang",
            "Yi R. Fung",
            "Sha Li",
            "Zixuan Huang",
            "Xu Cao",
            "Xingyao Wang",
            "Yiquan Wang",
            "Heng Ji",
            "Chengxiang Zhai"
        ],
        "date": "2024/01/01",
        "pdf": "http://arxiv.org/pdf/2401.00812.pdf",
        "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs&#39; training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2401.00812"
    },
    "039db49f2e19de35f5de8b42670347a3": {
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "authors": [
            "Boyuan Zheng",
            "Boyu Gou",
            "Jihyung Kil",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2024/01/03",
        "pdf": "http://arxiv.org/pdf/2401.01614.pdf",
        "abstract": "The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML text and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2401.01614"
    },
    "c5e9a179be09dac22812b7f097407e07": {
        "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval",
        "authors": [
            "He Zhu",
            "Wenjia Zhang",
            "Nuoxian Huang",
            "Boyang Li",
            "Luyao Niu",
            "Zipei Fan",
            "Tianle Lun",
            "Yicheng Tao",
            "Junyou Su",
            "Zhaoya Gong",
            "Chenyu Fang",
            "Xing Liu"
        ],
        "date": "2024/02/29",
        "pdf": "http://arxiv.org/pdf/2402.19273.pdf",
        "abstract": "In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.19273"
    },
    "bb03d9d04304fb8cf361615291a5e13d": {
        "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
        "authors": [
            "Chenglei Shen",
            "Guofu Xie",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "date": "2024/02/29",
        "pdf": "http://arxiv.org/pdf/2402.18807.pdf",
        "abstract": "Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\\&amp;$exploitation trade-off ability, reasoning ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4. Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs. These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.18807"
    },
    "2b67bb3cf1de46c69a656f1dd20fd49a": {
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "authors": [
            "Roberto Gallotta",
            "Graham Todd",
            "Marvin Zammit",
            "Sam Earle",
            "Antonios Liapis",
            "Julian Togelius",
            "Georgios N. Yannakakis"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18659.pdf",
        "abstract": "Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.18659"
    },
    "aaf7b78ec6f39f72b41938c6a0519e85": {
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "authors": [
            "Adyasha Maharana",
            "Dong-Ho Lee",
            "Sergey Tulyakov",
            "Mohit Bansal",
            "Francesco Barbieri",
            "Yuwei Fang"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17753.pdf",
        "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.17753"
    },
    "8e89988cfaaa29803f9c446bc8ee3eaf": {
        "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
        "authors": [
            "Ruiyang Ren",
            "Peng Qiu",
            "Yingqi Qu",
            "Jing Liu",
            "Wayne Xin Zhao",
            "Hua Wu",
            "Ji-Rong Wen",
            "Haifeng Wang"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17505.pdf",
        "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval. Our code and data will be publicly released soon.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.17505"
    },
    "ab05a8f2b05390e1cbe83e0b06b55b43": {
        "title": "Benchmarking Data Science Agents",
        "authors": [
            "Yuge Zhang",
            "Qiyang Jiang",
            "Xingyu Han",
            "Nan Chen",
            "Yuqing Yang",
            "Kan Ren"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17168.pdf",
        "abstract": "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.17168"
    },
    "8baca9839158cad236aad1235f6695c7": {
        "title": "RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation",
        "authors": [
            "Qinyu Luo",
            "Yining Ye",
            "Shihao Liang",
            "Zhong Zhang",
            "Yujia Qin",
            "Yaxi Lu",
            "Yesai Wu",
            "Xin Cong",
            "Yankai Lin",
            "Yingli Zhang",
            "Xiaoyin Che",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16667.pdf",
        "abstract": "Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2402.16667"
    },
    "4c8c556d314a7cbb7541e472f63cbccd": {
        "title": "Language Agents as Optimizable Graphs",
        "authors": [
            "Mingchen Zhuge",
            "Wenyi Wang",
            "Louis Kirsch",
            "Francesco Faccio",
            "Dmitrii Khizbullin",
            "JÃ¼rgen Schmidhuber"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16823.pdf",
        "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.",
        "code": "https://github.com/metauto-ai/gptswarm",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.16823"
    },
    "6edba424f625da489f6bcf7dac4d5a57": {
        "title": "Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation",
        "authors": [
            "Xinyi Mou",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16333.pdf",
        "abstract": "Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.",
        "code": "",
        "category": [
            "Role Playing",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.16333"
    },
    "a5538ed0a6964ec13a4ebdd94f383ecc": {
        "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning",
        "authors": [
            "Jianguo Zhang",
            "Tian Lan",
            "Rithesh Murthy",
            "Zhiwei Liu",
            "Weiran Yao",
            "Juntao Tan",
            "Thai Hoang",
            "Liangwei Yang",
            "Yihao Feng",
            "Zuxin Liu",
            "Tulika Awalgaonkar",
            "Juan Carlos Niebles",
            "Silvio Savarese",
            "Shelby Heinecke",
            "Huan Wang",
            "Caiming Xiong"
        ],
        "date": "2024/02/23",
        "pdf": "http://arxiv.org/pdf/2402.15506.pdf",
        "abstract": "Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address these challenges. \\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.15506"
    },
    "f9329da7eacdad837ccd68b129b853c6": {
        "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
        "authors": [
            "Andrew Brown",
            "Jiading Zhu",
            "Mohamed Abdelwahab",
            "Alec Dong",
            "Cindy Wang",
            "Jonathan Rose"
        ],
        "date": "2024/02/01",
        "pdf": "http://arxiv.org/pdf/2402.01051",
        "abstract": "Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.01051"
    },
    "fcabab00a378e1841f69e2241deddfa9": {
        "title": "Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",
        "authors": [
            "Pouya Pezeshkpour",
            "Eser Kandogan",
            "Nikita Bhutani",
            "Sajjadur Rahman",
            "Tom Mitchell",
            "Estevam Hruschka"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01108",
        "abstract": "Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration of constraints during optimization and establish connections among different components within the system, which also enable a more holistic and comprehensive approach to evaluation. We present a formal definition of reasoning capacity and illustrate its utility in identifying limitations within each component of the system. We then argue how these limitations can be addressed with a self-reflective process wherein human-feedback is used to alleviate shortcomings in reasoning and enhance overall consistency of the system.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.01108"
    },
    "426a7d04fbea7fc6a79285e5550740b2": {
        "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
        "authors": [
            "Wenyue Hua",
            "Xianjun Yang",
            "Zelong Li",
            "Wei Cheng",
            "Yongfeng Zhang"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01586",
        "abstract": "The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent&#39;s safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model&#39;s reasoning ability and its efficacy as a safe agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01586"
    },
    "88881d9d9219e2d3a274466dfd101760": {
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Tinghui Zhu",
            "Renze Lou",
            "Yuandong Tian",
            "Yanghua Xiao",
            "Yu Su"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01622",
        "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.",
        "code": "https://github.com/OSU-NLP-Group/TravelPlanner",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.01622"
    },
    "8447f702715c2b57c8891a3bac8e010e": {
        "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
        "authors": [
            "Yuncheng Hua",
            "Lizhen Qu",
            "Gholamreza Haffari"
        ],
        "date": "2024/01/29",
        "pdf": "http://arxiv.org/pdf/2402.01737",
        "abstract": "In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01737"
    },
    "8ed6f5a24a793642920666f6e4727b5b": {
        "title": "LLMs Simulate Big Five Personality Traits: Further Evidence",
        "authors": [
            "Aleksandra Sorokovikova",
            "Natalia Fedorova",
            "Sharwin Rezagholi",
            "Ivan P. Yamshchikov"
        ],
        "date": "2024/01/31",
        "pdf": "http://arxiv.org/pdf/2402.01765",
        "abstract": "An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01765"
    },
    "b301607d40a29aa568b9e9de00343499": {
        "title": "Executable Code Actions Elicit Better LLM Agents",
        "authors": [
            "Xingyao Wang",
            "Yangyi Chen",
            "Lifan Yuan",
            "Yizhe Zhang",
            "Yunzhu Li",
            "Hao Peng",
            "Heng Ji"
        ],
        "date": "2024/02/01",
        "pdf": "http://arxiv.org/pdf/2402.01030",
        "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents&#39; actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01030"
    },
    "dbbad9b76945033370d64c196857dcc8": {
        "title": "NavHint: Vision and Language Navigation Agent with a Hint Generator",
        "authors": [
            "Yue Zhang",
            "Quan Guo",
            "Parisa Kordjamshidi"
        ],
        "date": "2024/02/04",
        "pdf": "http://arxiv.org/pdf/2402.02559",
        "abstract": "Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent&#39;s attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the interpretability of the agent&#39;s actions.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02559"
    },
    "a619330cab01491ca0129cc7efed892c": {
        "title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
        "authors": [
            "Ivar Frisch",
            "Mario Giulianelli"
        ],
        "date": "2024/02/05",
        "pdf": "http://arxiv.org/pdf/2402.02896",
        "abstract": "While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02896"
    },
    "485502588a1d88b1dff5929f92c33d9c": {
        "title": "Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies",
        "authors": [
            "Zhixuan Chu",
            "Yan Wang",
            "Feng Zhu",
            "Lu Yu",
            "Longfei Li",
            "Jinjie Gu"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.03628",
        "abstract": "The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.03628"
    },
    "90cf304f11ecd2cb08a276a70b0118a3": {
        "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls",
        "authors": [
            "Yu Du",
            "Fangyun Wei",
            "Hongyang Zhang"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.04253",
        "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.",
        "code": "https://github.com/dyabel/anytool",
        "category": [
            "Feedback&Reflection",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.04253"
    },
    "4874c0c0abd9411762e846540adcb6c1": {
        "title": "More Agents Is All You Need",
        "authors": [
            "Junyou Li",
            "Qin Zhang",
            "Yangbin Yu",
            "Qiang Fu",
            "Deheng Ye"
        ],
        "date": "2024/02/03",
        "pdf": "http://arxiv.org/pdf/2402.05120.pdf",
        "abstract": "We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.05120"
    },
    "a15433aff8c528c636484d5f5da5010d": {
        "title": "Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients",
        "authors": [
            "Mahyar Abbasian",
            "Zhongqi Yang",
            "Elahe Khatibi",
            "Pengfei Zhang",
            "Nitish Nagesh",
            "Iman Azimi",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "date": "2024/02/15",
        "pdf": "http://arxiv.org/pdf/2402.10153",
        "abstract": "Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.10153"
    },
    "13b82b1326a95c1678d2892563c13ae3": {
        "title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation",
        "authors": [
            "Yaoxiang Wang",
            "Zhiyong Wu",
            "Junfeng Yao",
            "Jinsong Su"
        ],
        "date": "2024/02/15",
        "pdf": "http://arxiv.org/pdf/2402.10178",
        "abstract": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents&#39; abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.10178"
    },
    "a89feebbb8979ccaff7b3590a33af966": {
        "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
        "authors": [
            "Junjie Ye",
            "Sixian Li",
            "Guanyu Li",
            "Caishuang Huang",
            "Songyang Gao",
            "Yilong Wu",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "date": "2024/02/16",
        "pdf": "http://arxiv.org/pdf/2402.10753",
        "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in https://github.com/Junjie-Ye/ToolSword.",
        "code": "https://github.com/junjie-ye/toolsword",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.10753"
    },
    "7d840205c5d5d0c8418e1a6c9032503a": {
        "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
        "authors": [
            "Ziru Chen",
            "Michael White",
            "Raymond Mooney",
            "Ali Payani",
            "Yu Su",
            "Huan Sun"
        ],
        "date": "2024/02/16",
        "pdf": "http://arxiv.org/pdf/2402.10890",
        "abstract": "In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs&#39; discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data will be released at https://github.com/OSU-NLP-Group/llm-planning-eval.",
        "code": "https://github.com/osu-nlp-group/llm-planning-eval",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.10890"
    },
    "0c5062abf0c98c49de2bd6bec7500db3": {
        "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Yang Song",
            "Chen Zhu",
            "Hengshu Zhu",
            "Ji-Rong Wen"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11163",
        "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2402.11163"
    },
    "5eff26e408b6a81fe444f59554a196e4": {
        "title": "Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima",
        "authors": [
            "Shu Yang",
            "Lijie Hu",
            "Lu Yu",
            "Muhammad Asif Ali",
            "Di Wang"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11271",
        "abstract": "The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of autophagic (&#34;self-consumption&#34;) loops to account for the suppression of human-generated information in the exchange of information between humans and AI systems. We generalize the declining diversity of social information and the bottleneck in model performance caused by the above trends to the local optima of large models.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11271"
    },
    "91668d0b0eba8cee741a474aa4cbe16a": {
        "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
        "authors": [
            "Siyuan Wang",
            "Zhuohan Long",
            "Zhihao Fan",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11443",
        "abstract": "This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models&#39; capabilities. Besides, our framework widens performance discrepancies both between different models and within the same model across various tasks, facilitating more informed model selection for specific tasks (Code and data are available at https://github.com/NanshineLoong/Self-Evolving-Benchmark).",
        "code": "https://github.com/nanshineloong/self-evolving-benchmark",
        "category": [
            "Multi-Agent System",
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.11443"
    },
    "71b8888021fefef765ee9189114ebc03": {
        "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
        "authors": [
            "Yubo Ma",
            "Zhibin Gou",
            "Junheng Hao",
            "Ruochen Xu",
            "Shuohang Wang",
            "Liangming Pan",
            "Yujiu Yang",
            "Yixin Cao",
            "Aixin Sun",
            "Hany Awadalla",
            "Weizhu Chen"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11451",
        "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs&#39; abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11451"
    },
    "80a2c8c973d3216b19515f53d4911fcd": {
        "title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization",
        "authors": [
            "Zhiyu Yang",
            "Zihan Zhou",
            "Shuo Wang",
            "Xin Cong",
            "Xu Han",
            "Yukun Yan",
            "Zhenghao Liu",
            "Zhixing Tan",
            "Pengyuan Liu",
            "Dong Yu",
            "Zhiyuan Liu",
            "Xiaodong Shi",
            "Maosong Sun"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11453",
        "abstract": "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.11453"
    },
    "8efb45b8104c964a69dec8eb8a385013": {
        "title": "What&#39;s the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs",
        "authors": [
            "Eran Hirsch",
            "Guy Uziel",
            "Ateret Anaby-Tavor"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11489",
        "abstract": "Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.11489"
    },
    "f6cf3f3fc3c1eb72a0cce8a84f4b1cd0": {
        "title": "PreAct: Predicting Future in ReAct Enhances Agent&#39;s Planning Ability",
        "authors": [
            "Dayuan Fu",
            "Jianzhao Huang",
            "Siyuan Lu",
            "Guanting Dong",
            "Yejie Wang",
            "Keqing He",
            "Weiran Xu"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11534",
        "abstract": "Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\\textbf{PreAct}$, an agent framework that integrates $\\textbf{pre}$diction with $\\textbf{rea}$soning and $\\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.11534"
    },
    "7e1da3043b5d4208fd9b19f042d48f1f": {
        "title": "LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration",
        "authors": [
            "Jun Zhao",
            "Can Zu",
            "Hao Xu",
            "Yi Lu",
            "Wei He",
            "Yiwen Ding",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11550",
        "abstract": "Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \\textit{lost in the middle}. In this paper, we propose \\textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members&#39; hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an \\textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that \\textsc{LongAgent} offers a promising alternative for long-text processing. The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.11550"
    },
    "645c1394ff4bc534034d1906961852ca": {
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
        "authors": [
            "Renxi Wang",
            "Haonan Li",
            "Xudong Han",
            "Yixuan Zhang",
            "Timothy Baldwin"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11651",
        "abstract": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. Experimental results demonstrate that compared to solely using positive examples, incorporating negative examples enhances model performance by a large margin.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.11651"
    },
    "46c43356d6af4d363b0d08678696cea6": {
        "title": "Large Language Models as Agents in Two-Player Games",
        "authors": [
            "Yang Liu",
            "Peng Sun",
            "Hang Li"
        ],
        "date": "2024/02/12",
        "pdf": "http://arxiv.org/pdf/2402.08078",
        "abstract": "By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08078"
    },
    "fb31c87283351112060f2a7cbc8d0fe7": {
        "title": "Large Language Models as Minecraft Agents",
        "authors": [
            "Chris Madge",
            "Massimo Poesio"
        ],
        "date": "2024/02/13",
        "pdf": "http://arxiv.org/pdf/2402.08392",
        "abstract": "In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08392"
    },
    "da5cb2cc846f66b14b41853d5ffe1299": {
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "authors": [
            "Xiangming Gu",
            "Xiaosen Zheng",
            "Tianyu Pang",
            "Chao Du",
            "Qian Liu",
            "Ye Wang",
            "Jing Jiang",
            "Min Lin"
        ],
        "date": "2024/02/13",
        "pdf": "http://arxiv.org/pdf/2402.08567",
        "abstract": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.",
        "code": "https://github.com/sail-sg/agent-smith",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08567"
    },
    "cc4a576822918876acbf397bee723c62": {
        "title": "Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications",
        "authors": [
            "Negar Arabzadeh",
            "Julia Kiseleva",
            "Qingyun Wu",
            "Chi Wang",
            "Ahmed Awadallah",
            "Victor Dibia",
            "Adam Fourney",
            "Charles Clarke"
        ],
        "date": "2024/02/14",
        "pdf": "http://arxiv.org/pdf/2402.09015",
        "abstract": "The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application&#39;s functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of quantifier&#39;s work.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.09015"
    },
    "ac40351f97fee2d30d652151eea8c078": {
        "title": "InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory",
        "authors": [
            "Chaojun Xiao",
            "Pengle Zhang",
            "Xu Han",
            "Guangxuan Xiao",
            "Yankai Lin",
            "Zhengyan Zhang",
            "Zhiyuan Liu",
            "Song Han",
            "Maosong Sun"
        ],
        "date": "2024/02/07",
        "pdf": "http://arxiv.org/pdf/2402.04617",
        "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining the ability to capture long-distance dependencies. Without any training, InfLLM enables LLMs pre-trained on sequences of a few thousand tokens to achieve superior performance than competitive baselines continually training these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.04617"
    },
    "062ae31f680adf5ec6f03a536d620444": {
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "authors": [
            "Farhad Moghimifar",
            "Yuan-Fang Li",
            "Robert Thomson",
            "Gholamreza Haffari"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11712",
        "abstract": "Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between political parties and predict the outcomes. We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.11712"
    },
    "1243ff5fb9ecf0de62aacd39f8af85c0": {
        "title": "Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations",
        "authors": [
            "Nuo Chen",
            "Hongguang Li",
            "Juhua Huang",
            "Baoyuan Wang",
            "Jia Li"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.11975",
        "abstract": "Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a &#39;&#39;One-for-All&#39;&#39; approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY&#39;s superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences. Our codes are available at https://github.com/nuochenpku/COMEDY.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.11975"
    },
    "dc755c8abb2a1b7c1dadcd90909f8391": {
        "title": "AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production",
        "authors": [
            "Jiuniu Wang",
            "Zehua Du",
            "Yuyuan Zhao",
            "Bo Yuan",
            "Kexiang Wang",
            "Jian Liang",
            "Yaxi Zhao",
            "Yihen Lu",
            "Gengliang Li",
            "Junlong Gao",
            "Xin Tu",
            "Zhenyu Guo"
        ],
        "date": "2024/03/12",
        "pdf": "http://arxiv.org/pdf/2403.07952",
        "abstract": "The Agent and AIGC (Artificial Intelligence Generated Content) technologies have recently made significant progress. We propose AesopAgent, an Agent-driven Evolutionary System on Story-to-Video Production. AesopAgent is a practical application of agent technology for multimodal content generation. The system integrates multiple generative capabilities within a unified framework, so that individual users can leverage these modules easily. This innovative system would convert user story proposals into scripts, images, and audio, and then integrate these multimodal contents into videos. Additionally, the animating units (e.g., Gen-2 and Sora) could make the videos more infectious. The AesopAgent system could orchestrate task workflow for video generation, ensuring that the generated video is both rich in content and coherent. This system mainly contains two layers, i.e., the Horizontal Layer and the Utility Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary system that optimizes the whole video generation workflow and the steps within the workflow. It continuously evolves and iteratively optimizes workflow by accumulating expert experience and professional knowledge, including optimizing the LLM prompts and utilities usage. The Utility Layer provides multiple utilities, leading to consistent image generation that is visually coherent in terms of composition, characters, and style. Meanwhile, it provides audio and special effects, integrating them into expressive and logically arranged videos. Overall, our AesopAgent achieves state-of-the-art performance compared with many previous works in visual storytelling. Our AesopAgent is designed for convenient service for individual users, which is available on the following page: https://aesopai.github.io/.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.07952"
    },
    "5411575149b2ac84b69af030d50fa13e": {
        "title": "Polarization of Autonomous Generative AI Agents Under Echo Chambers",
        "authors": [
            "Masaya Ohagi"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12212",
        "abstract": "Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs. An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol. The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group&#39;s opinions changed as the discussion progressed. As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is caused by ChatGPT&#39;s high prompt understanding ability to update its opinion by considering its own and surrounding agents&#39; opinions. We conducted additional experiments to investigate under what specific conditions AI agents tended to polarize. As a result, we identified factors that strongly influence polarization, such as the agent&#39;s persona. These factors should be monitored to prevent the polarization of AI agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12212"
    },
    "2b620690c1f044fb1168a82f75c237f0": {
        "title": "LLM Agents for Psychology: A Study on Gamified Assessments",
        "authors": [
            "Qisen Yang",
            "Zekun Wang",
            "Honghui Chen",
            "Shenzhi Wang",
            "Yifan Pu",
            "Xin Gao",
            "Wenhao Huang",
            "Shiji Song",
            "Gao Huang"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12326",
        "abstract": "Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT&#39;s enhancements in content coherence, interactivity, interest, immersion, and satisfaction.",
        "code": "",
        "category": [
            "Role Playing",
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12326"
    },
    "564261b63f4cd7fb3327b8e6855d1027": {
        "title": "Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues",
        "authors": [
            "Michimasa Inaba",
            "Mariko Ukiyo",
            "Keiko Takamizo"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.12738",
        "abstract": "Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human counselors.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12738"
    },
    "4e7147609270bc7198406f69cfffcba4": {
        "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
        "authors": [
            "Xueyang Feng",
            "Zhi-Yuan Chen",
            "Yujia Qin",
            "Yankai Lin",
            "Xu Chen",
            "Zhiyuan Liu",
            "Ji-Rong Wen"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.12914",
        "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model&#39;s effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.12914"
    },
    "462c569bbda3a96911503fd5e1e980d2": {
        "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents",
        "authors": [
            "Mingyu Jin",
            "Beichen Wang",
            "Zhaoqian Xue",
            "Suiyuan Zhu",
            "Wenyue Hua",
            "Hua Tang",
            "Kai Mei",
            "Mengnan Du",
            "Yongfeng Zhang"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.13184",
        "abstract": "In this study, we introduce &#34;CosmoAgent,&#34; an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking&#39;s cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/agiresearch/AlienAgent.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.13184"
    },
    "b8a99309f72b7f01137f7a2178d15e97": {
        "title": "Soft Self-Consistency Improves Language Model Agents",
        "authors": [
            "Han Wang",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.13212",
        "abstract": "Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current &#34;sample and select&#34; methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC&#39;s discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that Soft-SC can be applied to both open-source and black-box models.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.13212"
    },
    "b739dc90a858a8d7d66e83e8115b2882": {
        "title": "AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning",
        "authors": [
            "Qiao Jin",
            "Zhizheng Wang",
            "Yifan Yang",
            "Qingqing Zhu",
            "Donald Wright",
            "Thomas Huang",
            "W John Wilbur",
            "Zhe He",
            "Andrew Taylor",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.13225",
        "abstract": "Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description. On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics. In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.13225"
    },
    "b4a30674fee6a3f3c8253cf8ae36c8b7": {
        "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
        "authors": [
            "Xiaoyan Yu",
            "Tongxu Luo",
            "Yifan Wei",
            "Fangyu Lei",
            "Yiming Huang",
            "Hao Peng",
            "Liehuang Zhu"
        ],
        "date": "2024/02/21",
        "pdf": "http://arxiv.org/pdf/2402.13717",
        "abstract": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko&#39;s adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at https://github.com/weiyifan1023/Neeko.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.13717"
    },
    "5425466c579cdb4a913f52545fd808cb": {
        "title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
        "authors": [
            "Chang Zong",
            "Yuchen Yan",
            "Weiming Lu",
            "Eliot Huang",
            "Jian Shao",
            "Yueting Zhuang"
        ],
        "date": "2024/02/22",
        "pdf": "http://arxiv.org/pdf/2402.14320",
        "abstract": "Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent&#39;s multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2402.14320"
    },
    "dc281e70bc671f58ce277899d44ca56f": {
        "title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
        "authors": [
            "Grgur KovaÄ",
            "RÃ©my Portelas",
            "Masataka Sawayama",
            "Peter Ford Dominey",
            "Pierre-Yves Oudeyer"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.14846",
        "abstract": "The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM&#39;s highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model&#39;s behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We explore two settings: with and without instructing LLMs to simulate particular personalities. We observe similar trends in the stability of models and model families - Mixtral, Mistral and Qwen families being more stable than LLaMa-2 and Phi - over those two settings, two different simulated populations, and even in the downstream behavioral task. When instructed to simulate particular personas, LLMs exhibit low Rank-Order stability, and this stability further diminishes with conversation length. This highlights the need for future research directions on LLMs that can coherently simulate a diversity of personas, as well as how context-dependence can be studied in more thorough and efficient ways. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in LLMs.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.14846"
    },
    "57c30a1d7bac2dee5e7ef2544482de25": {
        "title": "CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management",
        "authors": [
            "Sinan Abdulhak",
            "Wayne Hubbard",
            "Karthik Gopalakrishnan",
            "Max Z. Li"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.14850",
        "abstract": "Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions). We also detail the design of a graphical user interface for future users to interact and collaborate with the CHATATC conversational agent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.14850"
    },
    "c23d8b2451b56282954208f691de8e70": {
        "title": "LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain",
        "authors": [
            "Emanuele Musumeci",
            "Michele Brienza",
            "Vincenzo Suriani",
            "Daniele Nardi",
            "Domenico Daniele Bloisi"
        ],
        "date": "2024/02/21",
        "pdf": "http://arxiv.org/pdf/2402.14871",
        "abstract": "In the last years&#39; digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms. Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests. In this work, we propose a novel approach that combines the LLMs with prompt engineering and multi-agent systems for generating new documents compliant with a desired structure. The main contribution of this work concerns replacing the commonly used manual prompting with a task description generated by semantic retrieval from an LLM. The potential of this approach is demonstrated through a series of experiments and case studies, showcasing its effectiveness in real-world PA scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.14871"
    },
    "71aa00d952aa65d1d5fc0c1319a05590": {
        "title": "Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
        "authors": [
            "Hanqi Yan",
            "Qinglin Zhu",
            "Xinyu Wang",
            "Lin Gui",
            "Yulan He"
        ],
        "date": "2024/02/22",
        "pdf": "http://arxiv.org/pdf/2402.14963",
        "abstract": "While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror&#39;s superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.14963"
    },
    "428eb471e55fba81e7c48c4f54cdb12c": {
        "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
        "authors": [
            "Yang Deng",
            "Xuan Zhang",
            "Wenxuan Zhang",
            "Yifei Yuan",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "date": "2024/02/23",
        "pdf": "http://arxiv.org/pdf/2402.15057",
        "abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method.",
        "code": "",
        "category": [
            "Role Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.15057"
    },
    "842a8ba1857bd207c062e04b067a1aa8": {
        "title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering",
        "authors": [
            "Mingxu Tao",
            "Dongyan Zhao",
            "Yansong Feng"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16313",
        "abstract": "Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.",
        "code": "",
        "category": [
            "Agent Framework",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.16313"
    },
    "978d17d42f9c0ab8af8351e7e1163e4a": {
        "title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments",
        "authors": [
            "Junzhe Chen",
            "Xuming Hu",
            "Shuodi Liu",
            "Shiyu Huang",
            "Wei-Wei Tu",
            "Zhaofeng He",
            "Lijie Wen"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16499",
        "abstract": "Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.16499"
    },
    "046a041e159b5e6124bff8679e854b8d": {
        "title": "Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models",
        "authors": [
            "Anchun Gui",
            "Jian Li",
            "Yong Dai",
            "Nan Du",
            "Han Xiao"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16696",
        "abstract": "Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs&#39; flexibility to address diverse user&#39;s queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches via an automatic generation pipeline, thereby inspiring the decision-making awareness of LLMs under diverse scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the generalizability of LLMs over unseen tools. Extensive experiments demonstrate that our proposed DEER is effective and significantly outperforms baselines across various datasets.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.16696"
    },
    "732e0618c428ed5aa30a4bc21c0dbb5c": {
        "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection",
        "authors": [
            "Liangxin Liu",
            "Xuebo Liu",
            "Derek F. Wong",
            "Dongfang Li",
            "Ziyi Wang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16705",
        "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.16705"
    },
    "ddb24874c6acc4057939243cc129be37": {
        "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
        "authors": [
            "Zihao Yi",
            "Jiarui Ouyang",
            "Yuwen Liu",
            "Tianhao Liao",
            "Zhe Xu",
            "Ying Shen"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18013",
        "abstract": "This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.18013"
    },
    "4ac92f5bd1e6b1ed3456db0cec8804c4": {
        "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
        "authors": [
            "Qineng Wang",
            "Zihao Wang",
            "Ying Su",
            "Hanghang Tong",
            "Yangqiu Song"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18272",
        "abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.18272"
    },
    "97c680907ecee2890becf2523b5facf2": {
        "title": "PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models",
        "authors": [
            "Sihao Hu",
            "Tiansheng Huang",
            "Ling Liu"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01118",
        "abstract": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon&#39;s human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: \\url{https://github.com/git-disl/PokeLLMon}.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01118"
    },
    "8e3cbb1643b5524e1a59b88d912ef2c0": {
        "title": "A Multi-Agent Conversational Recommender System",
        "authors": [
            "Jiabao Fang",
            "Shen Gao",
            "Pengjie Ren",
            "Xiuying Chen",
            "Suzan Verberne",
            "Zhaochun Ren"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01135",
        "abstract": "Due to strong capabilities in conducting fluent, multi-turn conversations with users, Large Language Models (LLMs) have the potential to further improve the performance of Conversational Recommender System (CRS). Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address the above two key challenges. In this paper, we propose Multi-Agent Conversational Recommender System (MACRS) which contains two essential modules. First, we design a multi-agent act planning framework, which can control the dialogue flow based on four LLM-based agents. This cooperative multi-agent framework will generate various candidate responses based on different dialogue acts and then choose the most appropriate response as the system response, which can help MACRS plan suitable dialogue acts. Second, we propose a user feedback-aware reflection mechanism which leverages user feedback to reason errors made in previous turns to adjust the dialogue act planning, and higher-level user information from implicit semantics. We conduct extensive experiments based on user simulator to demonstrate the effectiveness of MACRS in recommendation and user preferences collection. Experimental results illustrate that MACRS demonstrates an improvement in user interaction experience compared to directly using LLMs.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.01135"
    },
    "f35e9df2a012ff5dbd78690968a9c985": {
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
        "authors": [
            "Shihan Dou",
            "Yan Liu",
            "Haoxiang Jia",
            "Limao Xiong",
            "Enyu Zhou",
            "Wei Shen",
            "Junjie Shan",
            "Caishuang Huang",
            "Xiao Wang",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Tao Ji",
            "Rui Zheng",
            "Qi Zhang",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01391",
        "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.01391"
    },
    "3300b5a7d577880f68a805dbd456926f": {
        "title": "Enhance Reasoning for Large Language Models in the Game Werewolf",
        "authors": [
            "Shuang Wu",
            "Liwen Zhu",
            "Tao Yang",
            "Shiwei Xu",
            "Qiang Fu",
            "Yang Wei",
            "Haobo Fu"
        ],
        "date": "2024/02/04",
        "pdf": "http://arxiv.org/pdf/2402.02330",
        "abstract": "This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework&#39;s effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset for social deduction games to date.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02330"
    },
    "cb258de9f27fddad03485d7f165904fb": {
        "title": "Understanding the planning of LLM agents: A survey",
        "authors": [
            "Xu Huang",
            "Weiwen Liu",
            "Xiaolong Chen",
            "Xingmei Wang",
            "Hao Wang",
            "Defu Lian",
            "Yasheng Wang",
            "Ruiming Tang",
            "Enhong Chen"
        ],
        "date": "2024/02/05",
        "pdf": "http://arxiv.org/pdf/2402.02716",
        "abstract": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.02716"
    },
    "3e3b6d0959265aa43dd113e51341cc90": {
        "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models",
        "authors": [
            "Haibo Jin",
            "Ruoxi Chen",
            "Andy Zhou",
            "Jinyin Chen",
            "Yang Zhang",
            "Haohan Wang"
        ],
        "date": "2024/02/05",
        "pdf": "http://arxiv.org/pdf/2402.03299",
        "abstract": "The discovery of &#34;jailbreaks&#34; to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD&#39;s versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.03299"
    },
    "34797899b0b543582283f74388953080": {
        "title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
        "authors": [
            "Tomoyuki Kagaya",
            "Thong Jing Yuan",
            "Yuxuan Lou",
            "Jayashree Karlekar",
            "Sugiri Pranata",
            "Akira Kinose",
            "Koki Oguri",
            "Felix Wick",
            "Yang You"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.03610",
        "abstract": "Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents&#39; planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP&#39;s effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents&#39; performance for embodied tasks. These results highlight RAP&#39;s potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.",
        "code": "",
        "category": [
            "Memory Mechanism",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.03610"
    },
    "0225b7cf486f65674b0fe04db98aaa16": {
        "title": "Can Generative Agents Predict Emotion?",
        "authors": [
            "Ciaran Regan",
            "Nanami Iwahashi",
            "Shogo Tanaka",
            "Mizuki Oka"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.04232",
        "abstract": "Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.04232"
    },
    "faf966da03b7ba9e7b5859e731558402": {
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "authors": [
            "Xiangru Tang",
            "Qiao Jin",
            "Kunlun Zhu",
            "Tongxin Yuan",
            "Yichi Zhang",
            "Wangchunshu Zhou",
            "Meng Qu",
            "Yilun Zhao",
            "Jian Tang",
            "Zhuosheng Zhang",
            "Arman Cohan",
            "Zhiyong Lu",
            "Mark Gerstein"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.04247",
        "abstract": "Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.04247"
    },
    "7a8b78fdb03c0cce97414fcfdb71fa23": {
        "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
        "authors": [
            "Chengxing Xie",
            "Canyu Chen",
            "Feiran Jia",
            "Ziyu Ye",
            "Kai Shu",
            "Adel Bibi",
            "Ziniu Hu",
            "Philip Torr",
            "Bernard Ghanem",
            "Guohao Li"
        ],
        "date": "2024/02/07",
        "pdf": "http://arxiv.org/pdf/2402.04559",
        "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, particularly for GPT-4, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications of our discoveries for various scenarios where trust is paramount. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans.",
        "code": "https://github.com/camel-ai/agent-trust",
        "category": [
            "Survey",
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.04559"
    },
    "6595dfd27493f7ec99213b704b178c9d": {
        "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models",
        "authors": [
            "Peiyuan Gong",
            "Jiamian Li",
            "Jiaxin Mao"
        ],
        "date": "2024/02/09",
        "pdf": "http://arxiv.org/pdf/2402.06360",
        "abstract": "Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users&#39; collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped with the capacity to understand the queries and context in multi-user conversations and the ability to search the Web for relevant information via APIs, CoSearchAgent can respond to user queries with answers grounded on the relevant search results. It can also ask clarifying questions when the information needs are unclear. The proposed CoSearchAgent is highly flexible and would be useful for supporting further research on collaborative search. The code and demo video are accessible.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.06360"
    },
    "7bd9188f4ffafb63274194bc0a99f3d1": {
        "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty",
        "authors": [
            "Kaiqu Liang",
            "Zixu Zhang",
            "Jaime FernÃ¡ndez Fisac"
        ],
        "date": "2024/02/09",
        "pdf": "http://arxiv.org/pdf/2402.06529",
        "abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches. Furthermore, we assess the effectiveness of introspective planning in conjunction with conformal prediction, revealing that this combination yields tighter confidence bounds, thereby maintaining statistical success guarantees with fewer superfluous user clarification queries.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.06529"
    },
    "f8620e0896c822455747c0236e11e463": {
        "title": "UFO: A UI-Focused Agent for Windows OS Interaction",
        "authors": [
            "Chaoyun Zhang",
            "Liqun Li",
            "Shilin He",
            "Xu Zhang",
            "Bo Qiao",
            "Si Qin",
            "Minghua Ma",
            "Yu Kang",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "date": "2024/02/08",
        "pdf": "http://arxiv.org/pdf/2402.07939",
        "abstract": "We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users&#39; daily usage. The results, derived from both quantitative metrics and real-case studies, underscore the superior effectiveness of UFO in fulfilling user requests. To the best of our knowledge, UFO stands as the first UI agent specifically tailored for task completion within the Windows OS environment. The open-source code for UFO is available on https://github.com/microsoft/UFO.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.07939"
    },
    "faf44b5210935885a5c485ad008d9c84": {
        "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
        "authors": [
            "Wenkai Yang",
            "Xiaohan Bi",
            "Yankai Lin",
            "Sishuo Chen",
            "Jie Zhou",
            "Xu Sun"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11208",
        "abstract": "Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided into two subcategories based on trigger locations: the backdoor trigger can be hidden either in the user query or in an intermediate observation returned by the external environment. We propose the corresponding data poisoning mechanisms to implement the above variations of agent backdoor attacks on two typical agent tasks, web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.11208"
    },
    "e41ce820e64617d634739d29708e867f": {
        "title": "Training Language Model Agents without Modifying Language Models",
        "authors": [
            "Shaokun Zhang",
            "Jieyu Zhang",
            "Jiale Liu",
            "Linxin Song",
            "Chi Wang",
            "Ranjay Krishna",
            "Qingyun Wu"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11359",
        "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent&#39;s functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters&#39; and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents&#39; functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.11359"
    },
    "f1bd6aec7fc832da2d885439f4190c32": {
        "title": "Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models",
        "authors": [
            "Paramveer S. Dhillon",
            "Somayeh Molaei",
            "Jiaqi Li",
            "Maximilian Golub",
            "Shaochun Zheng",
            "Lionel P. Robert"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11723",
        "abstract": "Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text ownership and satisfaction was noted. Our results have broad implications for the design of AI-powered writing tools, including the need for personalized scaffolding mechanisms.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11723"
    },
    "67182f0130bd01c7473a351917dfb201": {
        "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
        "authors": [
            "Hao Tang",
            "Darren Key",
            "Kevin Ellis"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12275",
        "abstract": "We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12275"
    },
    "3b6070b351092ec4f5fd27c0f5359d9a": {
        "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
        "authors": [
            "Archit Sharma",
            "Sedrick Keh",
            "Eric Mitchell",
            "Chelsea Finn",
            "Kushal Arora",
            "Thomas Kollar"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12366",
        "abstract": "Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.12366"
    },
    "991fe9838ff8be84f055c655cfd90ae4": {
        "title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation",
        "authors": [
            "Jiawei Wang",
            "Renhe Jiang",
            "Chuang Yang",
            "Zengqing Wu",
            "Makoto Onizuka",
            "Ryosuke Shibasaki",
            "Chuan Xiao"
        ],
        "date": "2024/02/22",
        "pdf": "http://arxiv.org/pdf/2402.14744",
        "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This research marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.14744"
    },
    "cff55d17a6bd1f5e25bf893bb95759a4": {
        "title": "Empowering Large Language Model Agents through Action Learning",
        "authors": [
            "Haiteng Zhao",
            "Chang Ma",
            "Guoyin Wang",
            "Jing Su",
            "Lingpeng Kong",
            "Jingjing Xu",
            "Zhi-Hong Deng",
            "Hongxia Yang"
        ],
        "date": "2024/02/24",
        "pdf": "http://arxiv.org/pdf/2402.15809",
        "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.",
        "code": "https://github.com/zhao-ht/learnact",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.15809"
    },
    "48b9abc38498cfd3a81ce2b6c7e3a370": {
        "title": "Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis",
        "authors": [
            "Zihan Liu",
            "Han Li",
            "Anfan Chen",
            "Renwen Zhang",
            "Yi-Chieh Lee"
        ],
        "date": "2024/02/25",
        "pdf": "http://arxiv.org/pdf/2402.16039",
        "abstract": "Conversational Agents (CAs) have increasingly been integrated into everyday life, sparking significant discussions on social media. While previous research has examined public perceptions of AI in general, there is a notable lack in research focused on CAs, with fewer investigations into cultural variations in CA perceptions. To address this gap, this study used computational methods to analyze about one million social media discussions surrounding CAs and compared people&#39;s discourses and perceptions of CAs in the US and China. We find Chinese participants tended to view CAs hedonically, perceived voice-based and physically embodied CAs as warmer and more competent, and generally expressed positive emotions. In contrast, US participants saw CAs more functionally, with an ambivalent attitude. Warm perception was a key driver of positive emotions toward CAs in both countries. We discussed practical implications for designing contextually sensitive and user-centric CAs to resonate with various users&#39; preferences and needs.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.16039"
    },
    "02d196e96f4c9879e876058c130090dd": {
        "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
        "authors": [
            "Raghav Kapoor",
            "Yash Parag Butala",
            "Melisa Russak",
            "Jing Yu Koh",
            "Kiran Kamble",
            "Waseem Alshikh",
            "Ruslan Salakhutdinov"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17553",
        "abstract": "For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent&#39;s capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as &#34;Play the next song&#34;, as well as longer horizon tasks such as &#34;Send an email to John Doe mentioning the time and place to meet&#34;. Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.17553"
    },
    "8271256b63765b8a4b2bca80cea0bc97": {
        "title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization",
        "authors": [
            "Wenqi Zhang",
            "Ke Tang",
            "Hai Wu",
            "Mengna Wang",
            "Yongliang Shen",
            "Guiyang Hou",
            "Zeqi Tan",
            "Peng Li",
            "Yueting Zhuang",
            "Weiming Lu"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17574",
        "abstract": "Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold&#39;em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.17574"
    },
    "f5a6f0b1770744cc9936c01c3da93afa": {
        "title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform",
        "authors": [
            "Jizhi Zhang",
            "Keqin Bao",
            "Wenjie Wang",
            "Yang Zhang",
            "Wentao Shi",
            "Wanhong Xu",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18240",
        "abstract": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.18240"
    },
    "a28b37bb6cb6d0a29d804cb33834a5fc": {
        "title": "ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning",
        "authors": [
            "A. Ghafarollahi",
            "M. J. Buehler"
        ],
        "date": "2024/01/27",
        "pdf": "http://arxiv.org/pdf/2402.04268",
        "abstract": "Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data -- natural vibrational frequencies -- via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.04268"
    },
    "f39b606d07c0f8bd9fd6b36696bf5fae": {
        "title": "Data Interpreter: An LLM Agent For Data Science",
        "authors": [
            "Sirui Hong",
            "Yizhang Lin",
            "Bang Liu",
            "Bangbang Liu",
            "Binhao Wu",
            "Danyang Li",
            "Jiaqi Chen",
            "Jiayi Zhang",
            "Jinlin Wang",
            "Li Zhang",
            "Lingyao Zhang",
            "Min Yang",
            "Mingchen Zhuge",
            "Taicheng Guo",
            "Tuo Zhou",
            "Wei Tao",
            "Wenyi Wang",
            "Xiangru Tang",
            "Xiangtao Lu",
            "Xiawu Zheng",
            "Xinbing Liang",
            "Yaying Fei",
            "Yuheng Cheng",
            "Zongze Xu",
            "Chenglin Wu"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18679",
        "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT.",
        "code": "https://github.com/geekan/metagpt",
        "category": [
            "Planning",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.18679"
    },
    "be2d4031465d914b9934dcb79717a783": {
        "title": "Bootstrapping Cognitive Agents with a Large Language Model",
        "authors": [
            "Feiyu Zhu",
            "Reid Simmons"
        ],
        "date": "2024/02/25",
        "pdf": "http://arxiv.org/pdf/2403.00810",
        "abstract": "Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. On the other hand cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent based entirely on large language models. Our experiments indicate that large language models are a good source of information for cognitive architectures, and the cognitive architecture in turn can verify and update the knowledge of large language models to a specific domain.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.00810"
    },
    "7e5087e7696617ed5cbf3c1b34f50b31": {
        "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code",
        "authors": [
            "Ziniu Hu",
            "Ahmet Iscen",
            "Aashi Jain",
            "Thomas Kipf",
            "Yisong Yue",
            "David A. Ross",
            "Cordelia Schmid",
            "Alireza Fathi"
        ],
        "date": "2024/03/02",
        "pdf": "http://arxiv.org/pdf/2403.01248",
        "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.01248"
    },
    "44f6c16c4530452a5f5a3bded715ffee": {
        "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
        "authors": [
            "Yutong Li",
            "Lu Chen",
            "Aiwei Liu",
            "Kai Yu",
            "Lijie Wen"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02574",
        "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.02574"
    },
    "6331f2d3779675a854d1e10deab9b472": {
        "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
        "authors": [
            "Yifan Zeng",
            "Yiran Wu",
            "Xiao Zhang",
            "Huazheng Wang",
            "Qingyun Wu"
        ],
        "date": "2024/03/02",
        "pdf": "http://arxiv.org/pdf/2403.04783",
        "abstract": "Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and data are publicly available at https://github.com/XHMY/AutoDefense.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.04783"
    },
    "9a7a65a110f0d8fe677a69a1b75ed96b": {
        "title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
        "authors": [
            "Ruiwen Zhou",
            "Yingxuan Yang",
            "Muning Wen",
            "Ying Wen",
            "Wenhao Wang",
            "Chunling Xi",
            "Guoqiang Xu",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "date": "2024/03/10",
        "pdf": "http://arxiv.org/pdf/2403.06221",
        "abstract": "Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM&#39;s wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent&#39;s overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.06221"
    },
    "1785c4df279fb0e9744e6557fb2831f4": {
        "title": "Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations",
        "authors": [
            "Carlos Jose Xavier Cruz"
        ],
        "date": "2024/03/12",
        "pdf": "http://arxiv.org/pdf/2403.07769",
        "abstract": "This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation). We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on large language models (LLM based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from LLM.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.07769"
    },
    "2c4d31a4fb12b85568b6ece4d71cdc6f": {
        "title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent",
        "authors": [
            "Xiaohan Wang",
            "Yuhui Zhang",
            "Orr Zohar",
            "Serena Yeung-Levy"
        ],
        "date": "2024/03/15",
        "pdf": "http://arxiv.org/pdf/2403.10517",
        "abstract": "Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.10517"
    },
    "ebcd928048d3695d60276a8bcaa3c2d7": {
        "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs&#39; Gaming Ability in Multi-Agent Environments",
        "authors": [
            "Jen-tse Huang",
            "Eric John Li",
            "Man Ho Lam",
            "Tian Liang",
            "Wenxuan Wang",
            "Youliang Yuan",
            "Wenxiang Jiao",
            "Xing Wang",
            "Zhaopeng Tu",
            "Michael R. Lyu"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.11807",
        "abstract": "Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs&#39; decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model&#39;s performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs&#39; robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other models on GAMA-Bench, achieving a score of 72.5. Moreover, the increasingly higher scores across the three iterations of GPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model&#39;s intelligence with each update. The code and experimental results are made publicly available via https://github.com/CUHK-ARISE/GAMABench.",
        "code": "",
        "category": [
            "Multi-Agent System",
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.11807"
    },
    "c136f0b43c06cced12b5f5dd89b15afb": {
        "title": "Tur[k]ingBench: A Challenge Benchmark for Web Agents",
        "authors": [
            "Kevin Xu",
            "Yeganeh Kordi",
            "Kate Sanders",
            "Yizhong Wang",
            "Adam Byerly",
            "Jack Zhang",
            "Benjamin Van Durme",
            "Daniel Khashabi"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.11905",
        "abstract": "Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains? To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instances distributed across 158 tasks. Additionally, to facilitate the evaluation on TurkingBench, we develop an evaluation framework that connects the responses of chatbots to modifications on web pages (modifying a text box, checking a radio, etc.). We evaluate the performance of state-of-the-art models, including language-only, vision-only, and layout-only models, and their combinations, on this benchmark. Our findings reveal that these models perform significantly better than random chance, yet considerable room exists for improvement. We hope this benchmark will help facilitate the evaluation and development of web-based agents.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.11905"
    },
    "693f00f098fa0521c4e2a8694f71b09e": {
        "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
        "authors": [
            "Xudong Guo",
            "Kaixuan Huang",
            "Jiale Liu",
            "Wenhui Fan",
            "Natalia VÃ©lez",
            "Qingyun Wu",
            "Huazheng Wang",
            "Thomas L. Griffiths",
            "Mengdi Wang"
        ],
        "date": "2024/03/19",
        "pdf": "http://arxiv.org/pdf/2403.12482",
        "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.12482"
    },
    "0a86cfd4f6ddb5b35652e271d6e72f94": {
        "title": "Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior",
        "authors": [
            "Zhouhong Gu",
            "Xiaoxuan Zhu",
            "Haoran Guo",
            "Lin Zhang",
            "Yin Cai",
            "Hao Shen",
            "Jiangjie Chen",
            "Zheyu Ye",
            "Yifei Dai",
            "Yan Gao",
            "Yao Hu",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "date": "2024/03/20",
        "pdf": "http://arxiv.org/pdf/2403.13433",
        "abstract": "To investigate the role of language in human collective behaviors, we developed the Agent Group Chat simulation to simulate linguistic interactions among multi-agent in different settings. Agents are asked to free chat in this simulation for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines. By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations. We evaluate the disorder within the environment by computing the n-gram Shannon entropy of all the content speak by characters. Our findings reveal that under the premise of agents possessing substantial alignment with human expectations, facilitating more extensive information exchange within the simulation ensures greater orderliness amidst diversity, which leads to the emergence of more unexpected and meaningful emergent behaviors. The code is open source in https://github.com/MikeGu721/AgentGroup, and online platform will be open soon.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.13433"
    },
    "cc29e2a1416d7c71fb4302fb8cca7374": {
        "title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents",
        "authors": [
            "Yifan Song",
            "Da Yin",
            "Xiang Yue",
            "Jie Huang",
            "Sujian Li",
            "Bill Yuchen Lin"
        ],
        "date": "2024/03/04",
        "pdf": "http://arxiv.org/pdf/2403.02502",
        "abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2403.02502"
    },
    "2dc81cefe824ff47859daa8c4433ebb7": {
        "title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
        "authors": [
            "Qiusi Zhan",
            "Zhixiang Liang",
            "Zifan Ying",
            "Daniel Kang"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02691",
        "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative. In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.",
        "code": "",
        "category": [
            "Benchmark&Evaluation",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2403.02691"
    },
    "0f7cdc7644f6cc8b301e0d841297c9a4": {
        "title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
        "authors": [
            "Jiwen Zhang",
            "Jihao Wu",
            "Yihua Teng",
            "Minghui Liao",
            "Nuo Xu",
            "Xiao Xiao",
            "Zhongyu Wei",
            "Duyu Tang"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02713",
        "abstract": "Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 200M model on our AitZ dataset achieves on par performance with CogAgent-Chat-18B.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2403.02713"
    },
    "94ad4073c700730f6fe1185d85688377": {
        "title": "SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents",
        "authors": [
            "Zhitao He",
            "Pengfei Cao",
            "Chenhao Wang",
            "Zhuoran Jin",
            "Yubo Chen",
            "Jiexin Xu",
            "Huaijun Li",
            "Xiaojian Jiang",
            "Kang Liu",
            "Jun Zhao"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02959",
        "abstract": "With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt. Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision-making of judge. (3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.02959"
    },
    "1d7449def73d3ffa6c8306de65e4a7f7": {
        "title": "KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents",
        "authors": [
            "Yuqi Zhu",
            "Shuofei Qiao",
            "Yixin Ou",
            "Shumin Deng",
            "Ningyu Zhang",
            "Shiwei Lyu",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.03101",
        "abstract": "Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in https://github.com/zjunlp/KnowAgent.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.03101"
    },
    "d1805f7178a10d4395c1ade7d6ab6fe2": {
        "title": "Language Guided Exploration for RL Agents in Text Environments",
        "authors": [
            "Hitesh Golchha",
            "Sahil Yerawar",
            "Dhruvesh Patel",
            "Soham Dan",
            "Keerthiram Murugesan"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.03141",
        "abstract": "Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.03141"
    },
    "85a3a98c4b9cadc46bc5c25eb3065638": {
        "title": "Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences",
        "authors": [
            "Atiyah Elsheikh"
        ],
        "date": "2024/03/07",
        "pdf": "http://arxiv.org/pdf/2403.04417",
        "abstract": "The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SHCS.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2403.04417"
    },
    "e23ceb4b035965c2dad3406c7761edba": {
        "title": "ChatASU: Evoking LLM&#39;s Reflexion to Truly Understand Aspect Sentiment in Dialogues",
        "authors": [
            "Yiding Liu",
            "Jingjing Wang",
            "Jiamin Luo",
            "Tao Zeng",
            "Guodong Zhou"
        ],
        "date": "2024/03/08",
        "pdf": "http://arxiv.org/pdf/2403.05326",
        "abstract": "Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs&#39; ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU. Specifically, this TSA treats the ACR task as an auxiliary task to boost the performance of the primary ASU task, and further integrates trusted learning into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to evaluate TSA, and extensive experiments show that our proposed TSA can significantly outperform several state-of-the-art baselines, justifying the effectiveness of TSA to ChatASU and the importance of considering the coreference and hallucination issues in ChatASU.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2403.05326"
    },
    "bf1e2f319094afd2377286a6c0cc74ce": {
        "title": "Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning",
        "authors": [
            "Tong Zhang",
            "Chen Huang",
            "Yang Deng",
            "Hongru Liang",
            "Jia Liu",
            "Zujie Wen",
            "Wenqiang Lei",
            "Tat-Seng Chua"
        ],
        "date": "2024/03/11",
        "pdf": "http://arxiv.org/pdf/2403.06769",
        "abstract": "We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm&#39;s failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.06769"
    },
    "46fceb681f72cba835bf36132b086d85": {
        "title": "AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents",
        "authors": [
            "Yao Fu",
            "Dong-Ki Kim",
            "Jaekyeom Kim",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Kyunghoon Bae",
            "Honglak Lee"
        ],
        "date": "2024/03/13",
        "pdf": "http://arxiv.org/pdf/2403.08978",
        "abstract": "The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent&#39;s current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential decision-making benchmarks.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.08978"
    },
    "1bed4874c7bbe62254c363f22586f90e": {
        "title": "Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation",
        "authors": [
            "Se-eun Yoon",
            "Zhankui He",
            "Jessica Maria Echterhoff",
            "Julian McAuley"
        ],
        "date": "2024/03/13",
        "pdf": "http://arxiv.org/pdf/2403.09738",
        "abstract": "Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.09738"
    },
    "1aadeba9f62724a15bc508b6fd42e956": {
        "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
        "authors": [
            "Dong Won Lee",
            "Hae Won Park",
            "Yoon Kim",
            "Cynthia Breazeal",
            "Louis-Philippe Morency"
        ],
        "date": "2024/03/17",
        "pdf": "http://arxiv.org/pdf/2403.11330",
        "abstract": "We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2403.11330"
    },
    "ef4ee63fa07c8149752731e8aeed308a": {
        "title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction",
        "authors": [
            "Xiang Huang",
            "Sitao Cheng",
            "Shanshan Huang",
            "Jiayu Shen",
            "Yong Xu",
            "Chaoyun Zhang",
            "Yuzhong Qu"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.11886",
        "abstract": "Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2403.11886"
    },
    "f6008fb8d9c10f96f8bcc4c46291d9f5": {
        "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
        "authors": [
            "Abhay Zala",
            "Jaemin Cho",
            "Han Lin",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.12014",
        "abstract": "Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs&#39; reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent&#39;s performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We show qualitatively how the LLM adapts training environments to help improve RL agents&#39; weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of LLM calls. Lastly, we present detailed ablation studies for our design choices.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.12014"
    },
    "6323bb1cd980022ec0d082da0b6a35e3": {
        "title": "Characteristic AI Agents via Large Language Models",
        "authors": [
            "Xi Wang",
            "Hongliang Dai",
            "Shen Gao",
            "Piji Li"
        ],
        "date": "2024/03/19",
        "pdf": "http://arxiv.org/pdf/2403.12368",
        "abstract": "The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100&#39;&#39; is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings. In addition, we devise a set of automatic metrics for quantitative performance evaluation. The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents. The benchmark is available at https://github.com/nuaa-nlp/Character100.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.12368"
    },
    "ea6393973de4fe2d29ba00e20f748441": {
        "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
        "authors": [
            "Zehui Chen",
            "Kuikun Liu",
            "Qiuchen Wang",
            "Wenwei Zhang",
            "Jiangning Liu",
            "Dahua Lin",
            "Kai Chen",
            "Feng Zhao"
        ],
        "date": "2024/03/19",
        "pdf": "http://arxiv.org/pdf/2403.12881",
        "abstract": "Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.12881"
    },
    "1cc44ebd645024966f49e3a5a660eae1": {
        "title": "RoleInteract: Evaluating the Social Interaction of Role-Playing Agents",
        "authors": [
            "Hongzhan Chen",
            "Hehong Chen",
            "Ming Yan",
            "Wenshen Xu",
            "Xing Gao",
            "Weizhou Shen",
            "Xiaojun Quan",
            "Chenliang Li",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "date": "2024/03/20",
        "pdf": "http://arxiv.org/pdf/2403.13679",
        "abstract": "Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group. Experimental results on RoleInteract confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents. The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.13679"
    },
    "d254cf4f173176c2999c0a1d87ca1ac8": {
        "title": "Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies",
        "authors": [
            "Philipp Sadler",
            "Sherzod Hakimov",
            "David Schlangen"
        ],
        "date": "2024/03/26",
        "pdf": "http://arxiv.org/pdf/2403.17497",
        "abstract": "In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players&#39; assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement -- which invites further research in the direction of cost-sharing in collaborative interactions.",
        "code": "https://github.com/clp-research/cost-sharing-reference-game",
        "category": [
            "Game Playing",
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2403.17497"
    },
    "85fd90755f5739a7fb483412602e97e6": {
        "title": "CACA Agent: Capability Collaboration based AI Agent",
        "authors": [
            "Peng Xu",
            "Haoran Wang",
            "Chuang Wang",
            "Xu Liu"
        ],
        "date": "2024/03/22",
        "pdf": "http://arxiv.org/pdf/2403.15137",
        "abstract": "As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario extension of CACA Agent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.15137"
    },
    "6c7684079093a696f575560ed81b715f": {
        "title": "Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering",
        "authors": [
            "Bowen Jiang",
            "Zhijun Zhuang",
            "Shreyas S. Shivakumar",
            "Dan Roth",
            "Camillo J. Taylor"
        ],
        "date": "2024/03/21",
        "pdf": "http://arxiv.org/pdf/2403.14783",
        "abstract": "This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system&#39;s performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.14783"
    },
    "f8ff753c3aa2bb437a2e5380838bfef6": {
        "title": "ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training",
        "authors": [
            "Zonghan Yang",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "date": "2024/03/21",
        "pdf": "http://arxiv.org/pdf/2403.14589",
        "abstract": "Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.14589"
    },
    "b14f11857f2bd0666a823802ced632df": {
        "title": "Learn to Disguise: Avoid Refusal Responses in LLM&#39;s Defense via a Multi-agent Attacker-Disguiser Game",
        "authors": [
            "Qianqiao Xu",
            "Zhiliang Tian",
            "Hongyan Wu",
            "Zhen Huang",
            "Yiping Song",
            "Feng Liu",
            "Dongsheng Li"
        ],
        "date": "2024/04/03",
        "pdf": "http://arxiv.org/pdf/2404.02532",
        "abstract": "With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers&#39; attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers&#39; capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model&#39;s ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.",
        "code": "",
        "category": [
            "Multi-Agent System",
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2404.02532"
    },
    "132f6a74ec7ec450c6106e9467b63208": {
        "title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization",
        "authors": [
            "Yoichi Ishibashi",
            "Yoshimasa Nishimura"
        ],
        "date": "2024/04/02",
        "pdf": "http://arxiv.org/pdf/2404.02183",
        "abstract": "Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.02183"
    },
    "ab2ebbcc0da2f32564c4167a2306ff73": {
        "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
        "authors": [
            "Xuechen Liang",
            "Meiling Tao",
            "Tianyu Shi",
            "Yiting Xie"
        ],
        "date": "2024/04/02",
        "pdf": "http://arxiv.org/pdf/2404.01663",
        "abstract": "Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.01663"
    },
    "0da1af4d68be428f423f0e488588bb54": {
        "title": "DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model",
        "authors": [
            "Lirui Zhao",
            "Yue Yang",
            "Kaipeng Zhang",
            "Wenqi Shao",
            "Yuxin Zhang",
            "Yu Qiao",
            "Ping Luo",
            "Rongrong Ji"
        ],
        "date": "2024/03/31",
        "pdf": "http://arxiv.org/pdf/2404.01342",
        "abstract": "Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent&#39;s capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. Our evaluations reveal that DiffAgent not only excels in identifying the appropriate T2I API but also underscores the effectiveness of the SFTA training framework. Codes are available at https://github.com/OpenGVLab/DiffAgent.",
        "code": "https://github.com/OpenGVLab/DiffAgent",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.01342"
    },
    "4a545d9e08ef7641444abce3a33444ee": {
        "title": "TraveLER: A Multi-LMM Agent Framework for Video Question-Answering",
        "authors": [
            "Chuyi Shang",
            "Amos You",
            "Sanjay Subramanian",
            "Trevor Darrell",
            "Roei Herzig"
        ],
        "date": "2024/04/01",
        "pdf": "http://arxiv.org/pdf/2404.01476",
        "abstract": "Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to &#34;Traverse&#34; through the video, ask questions about individual frames to &#34;Locate&#34; and store key information, and then &#34;Evaluate&#34; if there is enough information to answer the question. Finally, if there is not enough information, our method is able to &#34;Replan&#34; based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several video question-answering benchmarks, such as NExT-QA, STAR, and Perception Test, without the need to fine-tune on specific datasets.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.01476"
    },
    "13a2dcc1dde7c8d8aa2560b567b6a73b": {
        "title": "DataAgent: Evaluating Large Language Models&#39; Ability to Answer Zero-Shot, Natural Language Queries",
        "authors": [
            "Manit Mishra",
            "Abderrahman Braham",
            "Charles Marsom",
            "Bryan Chung",
            "Gavin Griffin",
            "Dakshesh Sidnerlikar",
            "Chatanya Sarin",
            "Arjun Rajaram"
        ],
        "date": "2024/03/29",
        "pdf": "http://arxiv.org/pdf/2404.00188",
        "abstract": "Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI&#39;s GPT-3.5 as a &#34;Language Data Scientist&#34; (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.00188"
    },
    "f8ee0b1f0b3c9c1fea3d64bc0053aa66": {
        "title": "AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent",
        "authors": [
            "Hanyu Lai",
            "Xiao Liu",
            "Iat Long Iong",
            "Shuntian Yao",
            "Yuxuan Chen",
            "Pengbo Shen",
            "Hao Yu",
            "Hanchen Zhang",
            "Xiaohan Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2024/04/04",
        "pdf": "http://arxiv.org/pdf/2404.03648",
        "abstract": "Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at \\url{https://github.com/THUDM/AutoWebGLM}.",
        "code": "https://github.com/THUDM/AutoWebGLM",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.03648"
    },
    "c5a6769c1c0badbeb24b45a8b9e4b1d9": {
        "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
        "authors": [
            "Siyu Yuan",
            "Kaitao Song",
            "Jiangjie Chen",
            "Xu Tan",
            "Yongliang Shen",
            "Ren Kan",
            "Dongsheng Li",
            "Deqing Yang"
        ],
        "date": "2024/01/11",
        "pdf": "http://arxiv.org/pdf/2401.06201",
        "abstract": "To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at \\url{https://github.com/microsoft/JARVIS/} in the future.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2401.06201"
    },
    "d6c196649bda278dff206fe125f58a0a": {
        "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
        "authors": [
            "Yu Li",
            "Shenyu Zhang",
            "Rui Wu",
            "Xiutian Huang",
            "Yongrui Chen",
            "Wenhao Xu",
            "Guilin Qi",
            "Dehai Min"
        ],
        "date": "2024/03/28",
        "pdf": "http://arxiv.org/pdf/2403.19305",
        "abstract": "Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A &#34;Multi-Agent Text Evaluation framework&#34; where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents&#39; interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.19305"
    },
    "c1649f215b96688fccfcbc30d4e394e1": {
        "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
        "authors": [
            "Qinhao Zhou",
            "Zihan Zhang",
            "Xiang Xiang",
            "Ke Wang",
            "Yuchuan Wu",
            "Yongbin Li"
        ],
        "date": "2024/03/29",
        "pdf": "http://arxiv.org/pdf/2403.19962",
        "abstract": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.19962"
    },
    "ca90b46ea8556ad2018c64fe4bb59682": {
        "title": "ITCMA: A Generative Agent Based on a Computational Consciousness Structure",
        "authors": [
            "Hanzhong Zhang",
            "Jibin Yin",
            "Haoyang Wang",
            "Ziwei Xiang"
        ],
        "date": "2024/03/29",
        "pdf": "http://arxiv.org/pdf/2403.20097",
        "abstract": "Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs&#39; ability to understand implicit instructions and apply common-sense knowledge by considering agents&#39; interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2403.20097"
    },
    "bbdc7d81c7e1344bee08ecab46a592ba": {
        "title": "Empowering Biomedical Discovery with AI Agents",
        "authors": [
            "Shanghua Gao",
            "Ada Fang",
            "Yepeng Huang",
            "Valentina Giunchiglia",
            "Ayush Noori",
            "Jonathan Richard Schwarz",
            "Yasha Ektefaie",
            "Jovana Kondic",
            "Marinka Zitnik"
        ],
        "date": "2024/04/03",
        "pdf": "http://arxiv.org/pdf/2404.02831",
        "abstract": "We envision &#39;AI scientists&#39; as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI&#39;s ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.02831"
    },
    "bd832d696f6c8b97c8a962d70be2a48d": {
        "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
        "authors": [
            "Wei Tao",
            "Yucheng Zhou",
            "Wenqiang Zhang",
            "Yu Cheng"
        ],
        "date": "2024/03/26",
        "pdf": "http://arxiv.org/pdf/2403.17927",
        "abstract": "In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.17927"
    },
    "6037287a594e3e33c4df8c55a9261097": {
        "title": "Cleared for Takeoff? Compositional &amp; Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
        "authors": [
            "Harsh Kohli",
            "Huan Sun"
        ],
        "date": "2024/04/05",
        "pdf": "http://arxiv.org/pdf/2404.04237",
        "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their sophisticated reasoning to navigate complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two cornerstones of human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2404.04237"
    },
    "95468202e69deb8c7b13b4296cb1384f": {
        "title": "Social Skill Training with Large Language Models",
        "authors": [
            "Diyi Yang",
            "Caleb Ziems",
            "William Held",
            "Omar Shaikh",
            "Michael S. Bernstein",
            "John Mitchell"
        ],
        "date": "2024/04/05",
        "pdf": "http://arxiv.org/pdf/2404.04204",
        "abstract": "People rely on social skills like conflict resolution to communicate effectively and to thrive in both work and personal life. However, practice environments for social skills are typically out of reach for most people. How can we make social skill training more available, accessible, and inviting? Drawing upon interdisciplinary research from communication and psychology, this perspective paper identifies social skill barriers to enter specialized fields. Then we present a solution that leverages large language models for social skill training via a generic framework. Our AI Partner, AI Mentor framework merges experiential learning with realistic practice and tailored feedback. This work ultimately calls for cross-disciplinary innovation to address the broader implications for workforce development and social equality.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2404.04204"
    },
    "5e8a9349f5f9f58e294e0558b604cbf4": {
        "title": "A Survey on Large Language Model-Based Game Agents",
        "authors": [
            "Sihao Hu",
            "Tiansheng Huang",
            "Fatih Ilhan",
            "Selim Tekin",
            "Gaowen Liu",
            "Ramana Kompella",
            "Ling Liu"
        ],
        "date": "2024/04/02",
        "pdf": "http://arxiv.org/pdf/2404.02039",
        "abstract": "The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting &amp; exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.",
        "code": "https://github.com/git-disl/awesome-LLM-game-agent-papers",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.02039"
    },
    "6871dbd47b870df8b8c81d0776a86b40": {
        "title": "360{\\deg}REA: Towards A Reusable Experience Accumulation with 360{\\deg} Assessment for Multi-Agent System",
        "authors": [
            "Shen Gao",
            "Hao Li",
            "Zhengliang Shi",
            "Chengrui Huang",
            "Quan Tu",
            "Zhiliang Tian",
            "Minlie Huang",
            "Shuo Shang"
        ],
        "date": "2024/04/08",
        "pdf": "http://arxiv.org/pdf/2404.05569",
        "abstract": "Large language model agents have demonstrated remarkable advancements across various complex tasks. Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks. Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents. We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance. In this paper, we propose Reusable Experience Accumulation with 360{\\deg} Assessment (360{\\deg}REA), a hierarchical multi-agent framework inspired by corporate organizational practices. The framework employs a novel 360{\\deg} performance assessment method for multi-perspective performance evaluation with fine-grained assessment. To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment. Extensive experiments on complex task datasets demonstrate the effectiveness of 360{\\deg}REA.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.05569"
    },
    "0cfb4e05bac6b3386a7765e13cc10d48": {
        "title": "Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems",
        "authors": [
            "Kunal Garg",
            "Jacob Arkin",
            "Songyuan Zhang",
            "Nicholas Roy",
            "Chuchu Fan"
        ],
        "date": "2024/04/09",
        "pdf": "http://arxiv.org/pdf/2404.06413",
        "abstract": "Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution. We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along. A graph neural network (GNN) based low-level distributed control policy executes the assigned plan. We systematically study various prompting techniques to improve LLM&#39;s performance in resolving deadlocks. In particular, as part of prompt engineering, we provide in-context examples for LLMs. We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.06413"
    },
    "d495dbace9bd6c2bbbbc13da7e2af198": {
        "title": "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents",
        "authors": [
            "Luca Gioacchini",
            "Giuseppe Siracusano",
            "Davide Sanvito",
            "Kiril Gashteovski",
            "David Friede",
            "Roberto Bifulco",
            "Carolin Lawrence"
        ],
        "date": "2024/04/09",
        "pdf": "http://arxiv.org/pdf/2404.06411",
        "abstract": "The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.",
        "code": "https://github.com/nec-research/agentquest",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2404.06411"
    },
    "3919bfea394e8ef4c0ad613ffd0bd296": {
        "title": "SurveyAgent: A Conversational System for Personalized and Efficient Research Survey",
        "authors": [
            "Xintao Wang",
            "Jiangjie Chen",
            "Nianqi Li",
            "Lida Chen",
            "Xinfeng Yuan",
            "Wei Shi",
            "Xuyang Ge",
            "Rui Xu",
            "Yanghua Xiao"
        ],
        "date": "2024/04/09",
        "pdf": "http://arxiv.org/pdf/2404.06364",
        "abstract": "In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers. Although previous efforts have leveraged AI to assist with literature searches, paper recommendations, and question-answering, a comprehensive support system that addresses the holistic needs of researchers has been lacking. This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers. SurveyAgent integrates three key modules: Knowledge Management for organizing papers, Recommendation for discovering relevant literature, and Query Answering for engaging with content on a deeper level. This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization. Our evaluation demonstrates SurveyAgent&#39;s effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature.",
        "code": "",
        "category": [
            "Role Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.06364"
    },
    "6cff9e7aba238d85b3a23a95296ae8b2": {
        "title": "Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning",
        "authors": [
            "Moghis Fereidouni",
            "A. B. Siddique"
        ],
        "date": "2024/04/16",
        "pdf": "http://arxiv.org/pdf/2404.10887",
        "abstract": "Traditional search systems focus on query formulation for effective results but face challenges in scenarios such as product searches where crucial product details (e.g., size, color) remain concealed until users visit specific product pages. This highlights the need for intelligent web navigation agents capable of formulating queries and navigating web pages according to users&#39; high-level intents. In response to this need, this work introduces a Grounded Language Agent for Intelligent Web Interactions, called GLAINTEL. Drawing upon advancements in language modeling and reinforcement learning, GLAINTEL investigates the efficacy of transformer-based models in enhancing the search capabilities of interactive web environments. Given the dynamic action space for each state in web navigation, GLAINTEL employs the Flan-T5 architecture and incorporates language modeling and value estimation heads. This work focuses on training smaller language models as agents across various scenarios, systematically evaluating the impact of human demonstrations on the training process. Specifically, we investigate scenarios where no human demonstrations are available and subsequently assess the effective utilization of such demonstrations. We also explore unsupervised domain adaptation for situations where demonstrations are confined to a specific domain. Experimental evaluations across diverse setups demonstrate the effectiveness of training agents in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters. Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised learning-based methods. Additionally, combining human demonstrations with Reinforcement Learning-based training yields results comparable to models utilizing GPT-4.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.10887"
    },
    "6429fc4c2cf6413ca87d31ea50fa16af": {
        "title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
        "authors": [
            "Tula Masterman",
            "Sandi Besen",
            "Mason Sawtell",
            "Alex Chao"
        ],
        "date": "2024/04/17",
        "pdf": "http://arxiv.org/pdf/2404.11584",
        "abstract": "This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.11584"
    },
    "41737985bf67ad09153b971991bed0bd": {
        "title": "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions",
        "authors": [
            "Leena Mathur",
            "Paul Pu Liang",
            "Louis-Philippe Morency"
        ],
        "date": "2024/04/17",
        "pdf": "http://arxiv.org/pdf/2404.11023",
        "abstract": "Building socially-intelligent AI agents (Social-AI) is a multidisciplinary, multimodal research goal that involves creating agents that can sense, perceive, reason about, learn from, and respond to affect, behavior, and cognition of other agents (human or artificial). Progress towards Social-AI has accelerated in the past decade across several computing communities, including natural language processing, machine learning, robotics, human-machine interaction, computer vision, and speech. Natural language processing, in particular, has been prominent in Social-AI research, as language plays a key role in constructing the social world. In this position paper, we identify a set of underlying technical challenges and open questions for researchers across computing communities to advance Social-AI. We anchor our discussion in the context of social intelligence concepts and prior progress in Social-AI research.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.11023"
    },
    "035610ab3e24c6452439d97d8816f74b": {
        "title": "Memory Sharing for Large Language Model based Agents",
        "authors": [
            "Hang Gao",
            "Yongfeng Zhang"
        ],
        "date": "2024/04/15",
        "pdf": "http://arxiv.org/pdf/2404.09982",
        "abstract": "In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries. However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent&#39;s ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results. Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process. Each &#34;memory&#34; within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents. This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents. Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent&#39;s performance regrading the open-ended questions. Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS. The code and data are available at: https://github.com/GHupppp/MemorySharingLLM",
        "code": "https://github.com/GHupppp/MemorySharingLLM",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2404.09982"
    },
    "50229be1bb06bde01c7cd0f4bcd6a33e": {
        "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
        "authors": [
            "Ruixin Yang",
            "Dheeraj Rajagopal",
            "Shirley Anugrah Hayati",
            "Bin Hu",
            "Dongyeop Kang"
        ],
        "date": "2024/04/14",
        "pdf": "http://arxiv.org/pdf/2404.09127",
        "abstract": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the &#34;Collective Wisdom&#34;: the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.09127"
    },
    "e574d5257ac04d06751dabd1e710f55c": {
        "title": "MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs",
        "authors": [
            "Xianhao Yu",
            "Jiaqi Fu",
            "Renjia Deng",
            "Wenjuan Han"
        ],
        "date": "2024/03/28",
        "pdf": "http://arxiv.org/pdf/2403.19267",
        "abstract": "Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/cocacola-lab/MineLand.",
        "code": "https://github.com/cocacola-lab/mineland",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2403.19267"
    },
    "646f22e74c6eeed331b2c65036e763b4": {
        "title": "EduAgent: Generative Student Agents in Learning",
        "authors": [
            "Songlin Xu",
            "Xinyu Zhang",
            "Lianhui Qin"
        ],
        "date": "2024/03/23",
        "pdf": "http://arxiv.org/pdf/2404.07963",
        "abstract": "Student simulation in online education is important to address dynamic learning behaviors of students with diverse backgrounds. Existing simulation models based on deep learning usually need massive training data, lacking prior knowledge in educational contexts. Large language models (LLMs) may contain such prior knowledge since they are pre-trained from a large corpus. However, because student behaviors are dynamic and multifaceted with individual differences, directly prompting LLMs is not robust nor accurate enough to capture fine-grained interactions among diverse student personas, learning behaviors, and learning outcomes. This work tackles this problem by presenting a newly annotated fine-grained large-scale dataset and proposing EduAgent, a novel generative agent framework incorporating cognitive prior knowledge (i.e., theoretical findings revealed in cognitive science) to guide LLMs to first reason correlations among various behaviors and then make simulations. Our two experiments show that EduAgent could not only mimic and predict learning behaviors of real students but also generate realistic learning behaviors of virtual students without real data.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.07963"
    },
    "d9bf095ed730a8b63fd64cc395cf1bdf": {
        "title": "MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education",
        "authors": [
            "Murong Yue",
            "Wijdane Mifdal",
            "Yixuan Zhang",
            "Jennifer Suh",
            "Ziyu Yao"
        ],
        "date": "2024/04/10",
        "pdf": "http://arxiv.org/pdf/2404.06711",
        "abstract": "Mathematical modeling (MM) is considered a fundamental skill for students in STEM disciplines. Practicing the MM skill is often the most effective when students can engage in group discussion and collaborative problem-solving. However, due to unevenly distributed teachers and educational resources needed to monitor such group activities, students do not always receive equal opportunities for this practice. Excitingly, large language models (LLMs) have recently demonstrated strong capability in both modeling mathematical problems and simulating characters with different traits and properties. Drawing inspiration from the advancement of LLMs, in this work, we present MATHVC, the very first LLM-powered virtual classroom containing multiple LLM-simulated student characters, with whom a human student can practice their MM skill. To encourage each LLM character&#39;s behaviors to be aligned with their specified math-relevant properties (termed &#34;characteristics alignment&#34;) and the overall conversational procedure to be close to an authentic student MM discussion (termed &#34;conversational procedural alignment&#34;), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure. Through experiments and ablation studies, we confirmed the effectiveness of our simulation approach and showed the promise for MATHVC to benefit real-life students in the future.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.06711"
    },
    "d03fda5f2f4a4b417169cee197138d05": {
        "title": "MMInA: Benchmarking Multihop Multimodal Internet Agents",
        "authors": [
            "Ziniu Zhang",
            "Shulin Tian",
            "Liangyu Chen",
            "Ziwei Liu"
        ],
        "date": "2024/04/15",
        "pdf": "http://arxiv.org/pdf/2404.09992",
        "abstract": "Autonomous embodied agents live on an Internet of multimedia websites. Can they hop around multimodal websites to complete complex user tasks? Existing benchmarks fail to assess them in a realistic, evolving environment for their embodiment across websites. To answer this question, we present MMInA, a multihop and multimodal benchmark to evaluate the embodied agents for compositional Internet tasks, with several appealing properties: 1) Evolving real-world multimodal websites. Our benchmark uniquely operates on evolving real-world websites, ensuring a high degree of realism and applicability to natural user tasks. Our data includes 1,050 human-written tasks covering various domains such as shopping and travel, with each task requiring the agent to autonomously extract multimodal information from web pages as observations; 2) Multihop web browsing. Our dataset features naturally compositional tasks that require information from or actions on multiple websites to solve, to assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation. We propose a novel protocol for evaluating an agent&#39;s progress in completing multihop tasks. We experiment with both standalone (multimodal) language models and heuristic-based web agents. Extensive experiments demonstrate that while long-chain multihop web tasks are easy for humans, they remain challenging for state-of-the-art web agents. We identify that agents are more likely to fail on the early hops when solving tasks of more hops, which results in lower task success rates. To address this issue, we propose a simple memory augmentation approach replaying past action trajectories to reflect. Our method significantly improved both the single-hop and multihop web browsing abilities of agents. See our code and data at https://mmina.cliangyu.com",
        "code": "https://github.com/shulin16/mmina",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2404.09992"
    },
    "f188d764a8c441b5dec5b1194d16f4aa": {
        "title": "Apollonion: Profile-centric Dialog Agent",
        "authors": [
            "Shangyu Chen",
            "Zibo Zhao",
            "Yuanyuan Zhao",
            "Xiang Li"
        ],
        "date": "2024/04/10",
        "pdf": "http://arxiv.org/pdf/2404.08692",
        "abstract": "The emergence of Large Language Models (LLMs) has innovated the development of dialog agents. Specially, a well-trained LLM, as a central process unit, is capable of providing fluent and reasonable response for user&#39;s request. Besides, auxiliary tools such as external knowledge retrieval, personalized character for vivid response, short/long-term memory for ultra long context management are developed, completing the usage experience for LLM-based dialog agents. However, the above-mentioned techniques does not solve the issue of \\textbf{personalization from user perspective}: agents response in a same fashion to different users, without consideration of their features, such as habits, interests and past experience. In another words, current implementation of dialog agents fail in ``knowing the user&#39;&#39;. The capacity of well-description and representation of user is under development. In this work, we proposed a framework for dialog agent to incorporate user profiling (initialization, update): user&#39;s query and response is analyzed and organized into a structural user profile, which is latter served to provide personal and more precise response. Besides, we proposed a series of evaluation protocols for personalization: to what extend the response is personal to the different users. The framework is named as \\method{}, inspired by inscription of ``Know Yourself&#39;&#39; in the temple of Apollo (also known as \\method{}) in Ancient Greek. Few works have been conducted on incorporating personalization into LLM, \\method{} is a pioneer work on guiding LLM&#39;s response to meet individuation via the application of dialog agents, with a set of evaluation methods for measurement in personalization.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.08692"
    },
    "d0acc8a7c482a802b05a57eaa66a1a88": {
        "title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
        "authors": [
            "Tianbao Xie",
            "Danyang Zhang",
            "Jixuan Chen",
            "Xiaochuan Li",
            "Siheng Zhao",
            "Ruisheng Cao",
            "Toh Jing Hua",
            "Zhoujun Cheng",
            "Dongchan Shin",
            "Fangyu Lei",
            "Yitao Liu",
            "Yiheng Xu",
            "Shuyan Zhou",
            "Silvio Savarese",
            "Caiming Xiong",
            "Victor Zhong",
            "Tao Yu"
        ],
        "date": "2024/04/11",
        "pdf": "http://arxiv.org/pdf/2404.07972",
        "abstract": "Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at https://os-world.github.io.",
        "code": "https://github.com/xlang-ai/OSWorld",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2404.07972"
    },
    "90d497ca09a06d5a5c4e2145c4070532": {
        "title": "Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery",
        "authors": [
            "Shiva Aryal",
            "Tuyen Do",
            "Bisesh Heyojoo",
            "Sandeep Chataut",
            "Bichar Dip Shrestha Gurung",
            "Venkataramana Gadhamshetty",
            "Etienne Gnimpieba"
        ],
        "date": "2024/04/12",
        "pdf": "http://arxiv.org/pdf/2404.08511",
        "abstract": "In the rapidly evolving field of artificial intelligence, the ability to harness and integrate knowledge across various domains stands as a paramount challenge and opportunity. This study introduces a novel approach to cross-domain knowledge discovery through the deployment of multi-AI agents, each specialized in distinct knowledge domains. These AI agents, designed to function as domain-specific experts, collaborate in a unified framework to synthesize and provide comprehensive insights that transcend the limitations of single-domain expertise. By facilitating seamless interaction among these agents, our platform aims to leverage the unique strengths and perspectives of each, thereby enhancing the process of knowledge discovery and decision-making. We present a comparative analysis of the different multi-agent workflow scenarios evaluating their performance in terms of efficiency, accuracy, and the breadth of knowledge integration. Through a series of experiments involving complex, interdisciplinary queries, our findings demonstrate the superior capability of domain specific multi-AI agent system in identifying and bridging knowledge gaps. This research not only underscores the significance of collaborative AI in driving innovation but also sets the stage for future advancements in AI-driven, cross-disciplinary research and application. Our methods were evaluated on a small pilot data and it showed a trend we expected, if we increase the amount of data we custom train the agents, the trend is expected to be more smooth.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.08511"
    },
    "025449e23ed3eefcf2cca46adc679089": {
        "title": "Behavior Trees Enable Structured Programming of Language Model Agents",
        "authors": [
            "Richard Kelley"
        ],
        "date": "2024/04/11",
        "pdf": "http://arxiv.org/pdf/2404.07439",
        "abstract": "Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise &#34;language-model agents.&#34; In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF.",
        "code": "https://github.com/RichardKelley/dendron",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2404.07439"
    },
    "889657431ba1b80ade839a1491dd8506": {
        "title": "MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems",
        "authors": [
            "Bin Lei"
        ],
        "date": "2024/04/06",
        "pdf": "http://arxiv.org/pdf/2404.04735",
        "abstract": "Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries. Despite these advancements, their performance substantially declines in \\textbf{advanced mathematical problems requiring complex, multi-step logical reasoning}. To enhance their inferential capabilities, current research has delved into \\textit{prompting engineering}, exemplified by methodologies such as the Tree of Thought and Graph of Thought. Nonetheless, these existing approaches encounter two significant limitations. Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained. Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability. In response to these limitations, this paper introduces the \\textit{Multi-Agent System for conditional Mining} (\\textbf{MACM}) prompting method. It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from $\\mathbf{54.68\\%} \\text{ to } \\mathbf{76.73\\%}$. The code is available in \\url{https://github.com/bin123apple/MACM}.",
        "code": "https://github.com/bin123apple/macm",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.04735"
    },
    "78d78cdb8cb1730baf707d698e12d695": {
        "title": "Cooperative Sentiment Agents for Multimodal Sentiment Analysis",
        "authors": [
            "Shanmin Wang",
            "Hui Shuai",
            "Qingshan Liu",
            "Fei Wang"
        ],
        "date": "2024/04/19",
        "pdf": "http://arxiv.org/pdf/2404.12642",
        "abstract": "In this paper, we propose a new Multimodal Representation Learning (MRL) method for Multimodal Sentiment Analysis (MSA), which facilitates the adaptive interaction between modalities through Cooperative Sentiment Agents, named Co-SA. Co-SA comprises two critical components: the Sentiment Agents Establishment (SAE) phase and the Sentiment Agents Cooperation (SAC) phase. During the SAE phase, each sentiment agent deals with an unimodal signal and highlights explicit dynamic sentiment variations within the modality via the Modality-Sentiment Disentanglement (MSD) and Deep Phase Space Reconstruction (DPSR) modules. Subsequently, in the SAC phase, Co-SA meticulously designs task-specific interaction mechanisms for sentiment agents so that coordinating multimodal signals to learn the joint representation. Specifically, Co-SA equips an independent policy model for each sentiment agent that captures significant properties within the modality. These policies are optimized mutually through the unified reward adaptive to downstream tasks. Benefitting from the rewarding mechanism, Co-SA transcends the limitation of pre-defined fusion modes and adaptively captures unimodal properties for MRL in the multimodal interaction setting. To demonstrate the effectiveness of Co-SA, we apply it to address Multimodal Sentiment Analysis (MSA) and Multimodal Emotion Recognition (MER) tasks. Our comprehensive experimental results demonstrate that Co-SA excels at discovering diverse cross-modal features, encompassing both common and complementary aspects. The code can be available at https://github.com/smwanghhh/Co-SA.",
        "code": "https://github.com/smwanghhh/co-sa",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.12642"
    },
    "c810addf261e7030b42605755d0746df": {
        "title": "Towards Human-centered Proactive Conversational Agents",
        "authors": [
            "Yang Deng",
            "Lizi Liao",
            "Zhonghua Zheng",
            "Grace Hui Yang",
            "Tat-Seng Chua"
        ],
        "date": "2024/04/19",
        "pdf": "http://arxiv.org/pdf/2404.12670",
        "abstract": "Recent research on proactive conversational agents (PCAs) mainly focuses on improving the system&#39;s capabilities in anticipating and planning action sequences to accomplish tasks and achieve goals before users articulate their requests. This perspectives paper highlights the importance of moving towards building human-centered PCAs that emphasize human needs and expectations, and that considers ethical and social implications of these agents, rather than solely focusing on technological capabilities. The distinction between a proactive and a reactive system lies in the proactive system&#39;s initiative-taking nature. Without thoughtful design, proactive systems risk being perceived as intrusive by human users. We address the issue by establishing a new taxonomy concerning three key dimensions of human-centered PCAs, namely Intelligence, Adaptivity, and Civility. We discuss potential research opportunities and challenges based on this new taxonomy upon the five stages of PCA system construction. This perspectives paper lays a foundation for the emerging area of conversational information retrieval research and paves the way towards advancing human-centered proactive conversational systems.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.12670"
    },
    "f0ed13204545aa097b36058baa5bc6a0": {
        "title": "Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents",
        "authors": [
            "Giorgio Piatti",
            "Zhijing Jin",
            "Max Kleiman-Weiner",
            "Bernhard SchÃ¶lkopf",
            "Mrinmaya Sachan",
            "Rada Mihalcea"
        ],
        "date": "2024/04/25",
        "pdf": "http://arxiv.org/pdf/2404.16698",
        "abstract": "In the rapidly evolving field of artificial intelligence, ensuring safe decision-making of Large Language Models (LLMs) is a significant challenge. This paper introduces Governance of the Commons Simulation (GovSim), a simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. Through this simulation environment, we explore the dynamics of resource sharing among AI agents, highlighting the importance of ethical considerations, strategic planning, and negotiation skills. GovSim is versatile and supports any text-based agent, including LLMs agents. Using the Generative Agent framework, we create a standard agent that facilitates the integration of different LLMs. Our findings reveal that within GovSim, only two out of 15 tested LLMs managed to achieve a sustainable outcome, indicating a significant gap in the ability of models to manage shared resources. Furthermore, we find that by removing the ability of agents to communicate, they overuse the shared resource, highlighting the importance of communication for cooperation. Interestingly, most LLMs lack the ability to make universalized hypotheses, which highlights a significant weakness in their reasoning skills. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.",
        "code": "",
        "category": [
            "Role Playing",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.16698"
    },
    "d98e6b8727b73e0711f9da1f804b2a3a": {
        "title": "BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis",
        "authors": [
            "Shuhang Lin",
            "Wenyue Hua",
            "Lingyao Li",
            "Che-Jui Chang",
            "Lizhou Fan",
            "Jianchao Ji",
            "Hang Hua",
            "Mingyu Jin",
            "Jiebo Luo",
            "Yongfeng Zhang"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2404.15532",
        "abstract": "This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System. This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints. The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios. This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts. Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals. BattelAgent illustrates AI&#39;s potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society.",
        "code": "https://github.com/agiresearch/battleagent",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.15532"
    },
    "9eff76117627bac573c318dd183c049a": {
        "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
        "authors": [
            "Ge Gao",
            "Alexey Taymanov",
            "Eduardo Salinas",
            "Paul Mineiro",
            "Dipendra Misra"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2404.15269",
        "abstract": "We study interactive learning of language agents based on user edits made to the agent&#39;s output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent&#39;s alignment with the user&#39;s preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user&#39;s latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.15269"
    },
    "340f5e4367036a333e81bf97cd732408": {
        "title": "CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based Reasoning",
        "authors": [
            "Ling Yue",
            "Tianfan Fu"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2404.14777",
        "abstract": "Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge. Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility. We introduce Clinical Agent System (CT-Agent), a Clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology. This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities. Our system autonomously manages the entire clinical trial process, demonstrating significant efficiency improvements in our evaluations, which include both computational benchmarks and expert feedback.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.14777"
    },
    "e04c3f9762fe447296eb9a790f67c49b": {
        "title": "Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following",
        "authors": [
            "Suyeon Shin",
            "Sujin jeon",
            "Junghyun Kim",
            "Gi-Cheon Kang",
            "Byoung-Tak Zhang"
        ],
        "date": "2024/04/21",
        "pdf": "http://arxiv.org/pdf/2404.15190",
        "abstract": "Embodied Instruction Following (EIF) is the task of executing natural language instructions by navigating and interacting with objects in 3D environments. One of the primary challenges in EIF is compositional task planning, which is often addressed with supervised or in-context learning with labeled data. To this end, we introduce the Socratic Planner, the first zero-shot planning method that infers without the need for any training data. Socratic Planner first decomposes the instructions into substructural information of the task through self-questioning and answering, translating it into a high-level plan, i.e., a sequence of subgoals. Subgoals are executed sequentially, with our visually grounded re-planning mechanism adjusting plans dynamically through a dense visual feedback. We also introduce an evaluation metric of high-level plans, RelaxedHLP, for a more comprehensive evaluation. Experiments demonstrate the effectiveness of the Socratic Planner, achieving competitive performance on both zero-shot and few-shot task planning in the ALFRED benchmark, particularly excelling in tasks requiring higher-dimensional inference. Additionally, a precise adjustments in the plan were achieved by incorporating environmental visual information.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2404.15190"
    },
    "35ce08a40e0cdaf207031e3d97ba4c6e": {
        "title": "How Well Can LLMs Echo Us? Evaluating AI Chatbots&#39; Role-Play Ability with ECHO",
        "authors": [
            "Man Tik Ng",
            "Hui Tung Tse",
            "Jen-tse Huang",
            "Jingjing Li",
            "Wenxuan Wang",
            "Michael R. Lyu"
        ],
        "date": "2024/04/22",
        "pdf": "http://arxiv.org/pdf/2404.13957",
        "abstract": "The role-play ability of Large Language Models (LLMs) has emerged as a popular research direction. However, existing studies focus on imitating well-known public figures or fictional characters, overlooking the potential for simulating ordinary individuals. Such an oversight limits the potential for advancements in digital human clones and non-player characters in video games. To bridge this gap, we introduce ECHO, an evaluative framework inspired by the Turing test. This framework engages the acquaintances of the target individuals to distinguish between human and machine-generated responses. Notably, our framework focuses on emulating average individuals rather than historical or fictional figures, presenting a unique advantage to apply the Turing Test. We evaluated three role-playing LLMs using ECHO, with GPT-3.5 and GPT-4 serving as foundational models, alongside the online application GPTs from OpenAI. Our results demonstrate that GPT-4 more effectively deceives human evaluators, and GPTs achieves a leading success rate of 48.3%. Furthermore, we investigated whether LLMs could discern between human-generated and machine-generated texts. While GPT-4 can identify differences, it could not determine which texts were human-produced. Our code and results of reproducing the role-playing LLMs are made publicly available via https://github.com/CUHK-ARISE/ECHO.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.13957"
    },
    "ab48f35dec492ad32cc00c50276c7f36": {
        "title": "PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games",
        "authors": [
            "Steph Buongiorno",
            "Lawrence Jake Klinkert",
            "Tanishq Chawla",
            "Zixin Zhuang",
            "Corey Clark"
        ],
        "date": "2024/04/30",
        "pdf": "http://arxiv.org/pdf/2404.19721",
        "abstract": "This research introduces Procedural Artificial Narrative using Generative AI (PANGeA), a structured approach for leveraging large language models (LLMs), guided by a game designer&#39;s high-level criteria, to generate narrative content for turn-based role-playing video games (RPGs). Distinct from prior applications of LLMs used for video game design, PANGeA innovates by not only generating game level data (which includes, but is not limited to, setting, key items, and non-playable characters (NPCs)), but by also fostering dynamic, free-form interactions between the player and the environment that align with the procedural game narrative. The NPCs generated by PANGeA are personality-biased and express traits from the Big 5 Personality Model in their generated responses. PANGeA addresses challenges behind ingesting free-form text input, which can prompt LLM responses beyond the scope of the game narrative. A novel validation system that uses the LLM&#39;s intelligence evaluates text input and aligns generated responses with the unfolding narrative. Making these interactions possible, PANGeA is supported by a server that hosts a custom memory system that supplies context for augmenting generated responses thus aligning them with the procedural narrative. For its broad application, the server has a REST interface enabling any game engine to integrate directly with PANGeA, as well as an LLM interface adaptable with local or private LLMs. PANGeA&#39;s ability to foster dynamic narrative generation by aligning responses with the procedural narrative is demonstrated through an empirical study and ablation test of two versions of a demo game. These are, a custom, browser-based GPT and a Unity demo. As the results show, PANGeA holds potential to assist game designers in using LLMs to generate narrative-consistent content even when provided varied and unpredictable, free-form text input.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.19721"
    },
    "d3be68d8e8d6464c35b2ccdf320e52e2": {
        "title": "Large Language Model Agent as a Mechanical Designer",
        "authors": [
            "Yayati Jadhav",
            "Amir Barati Farimani"
        ],
        "date": "2024/04/26",
        "pdf": "http://arxiv.org/pdf/2404.17525",
        "abstract": "Conventional mechanical design paradigms rely on experts systematically refining concepts through experience-guided modification and FEA to meet specific requirements. However, this approach can be time-consuming and heavily dependent on prior knowledge and experience. While numerous machine learning models have been developed to streamline this intensive and expert-driven iterative process, these methods typically demand extensive training data and considerable computational resources. Furthermore, methods based on deep learning are usually restricted to the specific domains and tasks for which they were trained, limiting their applicability across different tasks. This creates a trade-off between the efficiency of automation and the demand for resources. In this study, we present a novel approach that integrates pre-trained LLMs with a FEM module. The FEM module evaluates each design and provides essential feedback, guiding the LLMs to continuously learn, plan, generate, and optimize designs without the need for domain-specific training. We demonstrate the effectiveness of our proposed framework in managing the iterative optimization of truss structures, showcasing its capability to reason about and refine designs according to structured feedback and criteria. Our results reveal that these LLM-based agents can successfully generate truss designs that comply with natural language specifications with a success rate of up to 90%, which varies according to the applied constraints. By employing prompt-based optimization techniques we show that LLM based agents exhibit optimization behavior when provided with solution-score pairs to iteratively refine designs to meet specifications. This ability of LLM agents to produce viable designs and optimize them based on their inherent reasoning capabilities highlights their potential to develop and implement effective design strategies autonomously.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.17525"
    },
    "aef6e27e25eb64666e2bbb9373ac3d70": {
        "title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
        "authors": [
            "Kaixuan Huang",
            "Yuanhao Qu",
            "Henry Cousins",
            "William A. Johnson",
            "Di Yin",
            "Mihir Shah",
            "Denny Zhou",
            "Russ Altman",
            "Mengdi Wang",
            "Le Cong"
        ],
        "date": "2024/04/27",
        "pdf": "http://arxiv.org/pdf/2404.18021",
        "abstract": "The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent&#39;s effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.18021"
    },
    "c20a9be3f493bc58a9706b98f5bfcb38": {
        "title": "ComposerX: Multi-Agent Symbolic Music Composition with LLMs",
        "authors": [
            "Qixin Deng",
            "Qikai Yang",
            "Ruibin Yuan",
            "Yipeng Huang",
            "Yi Wang",
            "Xubo Liu",
            "Zeyue Tian",
            "Jiahao Pan",
            "Ge Zhang",
            "Hanfeng Lin",
            "Yizhi Li",
            "Yinghao Ma",
            "Jie Fu",
            "Chenghua Lin",
            "Emmanouil Benetos",
            "Wenwu Wang",
            "Guangyu Xia",
            "Wei Xue",
            "Yike Guo"
        ],
        "date": "2024/04/28",
        "pdf": "http://arxiv.org/pdf/2404.18081",
        "abstract": "Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs&#39; potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",
        "code": "https://github.com/lllindsey0615/composerx",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.18081"
    },
    "96c56a428d90415310188fa9a434d00c": {
        "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
        "authors": [
            "Hanmeng Liu",
            "Zhiyang Teng",
            "Chaoli Zhang",
            "Yue Zhang"
        ],
        "date": "2024/04/28",
        "pdf": "http://arxiv.org/pdf/2404.18130",
        "abstract": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA&#39;s capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2404.18130"
    },
    "bf68ceff02d38dac075c997707f0632e": {
        "title": "Large Language Model Agent for Fake News Detection",
        "authors": [
            "Xinyi Li",
            "Yongfeng Zhang",
            "Edward C. Malthouse"
        ],
        "date": "2024/04/30",
        "pdf": "http://arxiv.org/pdf/2405.01593",
        "abstract": "In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim&#39;s veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent&#39;s application to news verification across various domains.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.01593"
    },
    "d7fa003b09d2ebcde7ea089949ad402a": {
        "title": "Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning",
        "authors": [
            "Lucas-AndreÃ¯ Thil",
            "Mirela Popa",
            "Gerasimos Spanakis"
        ],
        "date": "2024/05/01",
        "pdf": "http://arxiv.org/pdf/2405.00516",
        "abstract": "Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation. Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods. However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results. In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods. We also address a critical limitation in previous models&#39; understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure. To rectify this, we propose methods to enhance true understanding and present a new baseline of results. Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a multimodal RL approach. This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2405.00516"
    },
    "c1e0294614a29fc390d87c06d71e7a1e": {
        "title": "Large Language Models for Human-Robot Interaction: Opportunities and Risks",
        "authors": [
            "Jesse Atuhurra"
        ],
        "date": "2024/03/26",
        "pdf": "http://arxiv.org/pdf/2405.00693",
        "abstract": "The tremendous development in large language models (LLM) has led to a new wave of innovations and applications and yielded research results that were initially forecast to take longer. In this work, we tap into these recent developments and present a meta-study about the potential of large language models if deployed in social robots. We place particular emphasis on the applications of social robots: education, healthcare, and entertainment. Before being deployed in social robots, we also study how these language models could be safely trained to ``understand&#39;&#39; societal norms and issues, such as trust, bias, ethics, cognition, and teamwork. We hope this study provides a resourceful guide to other robotics researchers interested in incorporating language models in their robots.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2405.00693"
    },
    "78997b038dfb5f3e70c52686b5d86257": {
        "title": "Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration",
        "authors": [
            "Razan Baltaji",
            "Babak Hemmatian",
            "Lav R. Varshney"
        ],
        "date": "2024/05/06",
        "pdf": "http://arxiv.org/pdf/2405.03862",
        "abstract": "This study explores the sources of instability in maintaining cultural personas and opinions within multi-agent LLM systems. Drawing on simulations of inter-cultural collaboration and debate, we analyze agents&#39; pre- and post-discussion private responses alongside chat transcripts to assess the stability of cultural personas and the impact of opinion diversity on group outcomes. Our findings suggest that multi-agent discussions can encourage collective decisions that reflect diverse perspectives, yet this benefit is tempered by the agents&#39; susceptibility to conformity due to perceived peer pressure and challenges in maintaining consistent personas and opinions. Counterintuitively, instructions that encourage debate in support of one&#39;s opinions increase the rate of inconstancy. Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs will remain untapped.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.03862"
    },
    "77b2c05422904b637fe33ddffdddc2ff": {
        "title": "&#34;Ask Me Anything&#34;: How Comcast Uses LLMs to Assist Agents in Real Time",
        "authors": [
            "Scott Rome",
            "Tianwen Chen",
            "Raphael Tang",
            "Luwei Zhou",
            "Ferhan Ture"
        ],
        "date": "2024/05/01",
        "pdf": "http://arxiv.org/pdf/2405.00801",
        "abstract": "Customer service is how companies interface with their customers. It can contribute heavily towards the overall customer satisfaction. However, high-quality service can become expensive, creating an incentive to make it as cost efficient as possible and prompting most companies to utilize AI-powered assistants, or &#34;chat bots&#34;. On the other hand, human-to-human interaction is still desired by customers, especially when it comes to complex scenarios such as disputes and sensitive topics like bill payment. This raises the bar for customer service agents. They need to accurately understand the customer&#39;s question or concern, identify a solution that is acceptable yet feasible (and within the company&#39;s policy), all while handling multiple conversations at once. In this work, we introduce &#34;Ask Me Anything&#34; (AMA) as an add-on feature to an agent-facing customer service interface. AMA allows agents to ask questions to a large language model (LLM) on demand, as they are handling customer conversations -- the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs. In our internal experiments, we find that agents using AMA versus a traditional search experience spend approximately 10% fewer seconds per conversation containing a search, translating to millions of dollars of savings annually. Agents that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating its usefulness as an AI-assisted feature for customer care.",
        "code": "",
        "category": [
            "Role Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.00801"
    },
    "2704c7e4d3813e67bbf40356965122ec": {
        "title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting",
        "authors": [
            "Olly Styles",
            "Sam Miller",
            "Patricio Cerda-Mardini",
            "Tanaya Guha",
            "Victor Sanchez",
            "Bertie Vidgen"
        ],
        "date": "2024/05/01",
        "pdf": "http://arxiv.org/pdf/2405.00823",
        "abstract": "We introduce WorkBench: a benchmark dataset for evaluating agents&#39; ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents&#39; errors can result in the wrong action being taken, such as an email being sent to the wrong person. WorkBench reveals weaknesses in agents&#39; ability to undertake common business activities, raising questions about their use in high-stakes workplace settings. WorkBench is publicly available as a free resource at https://github.com/olly-styles/WorkBench.",
        "code": "https://github.com/olly-styles/workbench",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.00823"
    },
    "b72d2bd1cbea4425300fafee4f089549": {
        "title": "CACTUS: Chemistry Agent Connecting Tool-Usage to Science",
        "authors": [
            "Andrew D. McNaughton",
            "Gautham Ramalaxmi",
            "Agustin Kruel",
            "Carter R. Knutson",
            "Rohith A. Varikoti",
            "Neeraj Kumar"
        ],
        "date": "2024/05/02",
        "pdf": "http://arxiv.org/pdf/2405.00972",
        "abstract": "Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools. In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery. We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions. Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used. Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy. By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment. Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery. By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials. Moreover, CACTUS&#39;s ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery.",
        "code": "https://github.com/pnnl/cactus",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.00972"
    },
    "ba077a6c1f1f572103b1bcfa943cb42b": {
        "title": "GAIA: A General AI Assistant for Intelligent Accelerator Operations",
        "authors": [
            "Frank Mayet"
        ],
        "date": "2024/05/02",
        "pdf": "http://arxiv.org/pdf/2405.01359",
        "abstract": "Large-scale machines like particle accelerators are usually run by a team of experienced operators. In case of a particle accelerator, these operators possess suitable background knowledge on both accelerator physics and the technology comprising the machine. Due to the complexity of the machine, particular subsystems of the machine are taken care of by experts, who the operators can turn to. In this work the reasoning and action (ReAct) prompting paradigm is used to couple an open-weights large language model (LLM) with a high-level machine control system framework and other tools, e.g. the electronic logbook or machine design documentation. By doing so, a multi-expert retrieval augmented generation (RAG) system is implemented, which assists operators in knowledge retrieval tasks, interacts with the machine directly if needed, or writes high level control system scripts. This consolidation of expert knowledge and machine interaction can simplify and speed up machine operation tasks for both new and experienced human operators.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.01359"
    },
    "6d2bc9f35fcd10c0a43d0bb13c364676": {
        "title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent",
        "authors": [
            "Wei Chen",
            "Zhiyuan Li"
        ],
        "date": "2024/04/17",
        "pdf": "http://arxiv.org/pdf/2404.11459",
        "abstract": "A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions. Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging. In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications. To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters. Like GPT-4, our model can process both English and Chinese. We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2404.11459"
    },
    "b87a4cc0c759e31d54cf32e846c59ee9": {
        "title": "LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions",
        "authors": [
            "Chuanneng Sun",
            "Songjun Huang",
            "Dario Pompili"
        ],
        "date": "2024/05/17",
        "pdf": "http://arxiv.org/pdf/2405.11106",
        "abstract": "In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.11106"
    },
    "f042f087ecadcb354329fbcd38bbc000": {
        "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
        "authors": [
            "Yuexiang Zhai",
            "Hao Bai",
            "Zipeng Lin",
            "Jiayi Pan",
            "Shengbang Tong",
            "Yifei Zhou",
            "Alane Suhr",
            "Saining Xie",
            "Yann LeCun",
            "Yi Ma",
            "Sergey Levine"
        ],
        "date": "2024/05/16",
        "pdf": "http://arxiv.org/pdf/2405.10292",
        "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2405.10292"
    },
    "d9495626913b689d9b5dbe9d90df8923": {
        "title": "Exploring the Potential of Conversational AI Support for Agent-Based Social Simulation Model Design",
        "authors": [
            "Peer-Olaf Siebers"
        ],
        "date": "2024/05/12",
        "pdf": "http://arxiv.org/pdf/2405.08032",
        "abstract": "ChatGPT, the AI-powered chatbot with a massive user base of hundreds of millions, has become a global phenomenon. However, the use of Conversational AI Systems (CAISs) like ChatGPT for research in the field of Social Simulation is still limited. Specifically, there is no evidence of its usage in Agent-Based Social Simulation (ABSS) model design. While scepticism towards anything new is inherent to human nature, we firmly believe it is imperative to initiate the use of this innovative technology to support ABSS model design. This paper presents a proof-of-concept that demonstrates how CAISs can facilitate the development of innovative conceptual ABSS models in a concise timeframe and with minimal required upfront case-based knowledge. By employing advanced prompt engineering techniques and adhering to the Engineering ABSS framework, we have constructed a comprehensive prompt script that enables the design of ABSS models with or by the CAIS. The effectiveness of the script is demonstrated through an illustrative case study concerning the use of adaptive architecture in museums. Despite occasional inaccuracies and divergences in conversation, the CAIS proved to be a valuable companion for ABSS modellers.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.08032"
    },
    "bcf1e07dcc3653511e08f61ae367e0bb": {
        "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments",
        "authors": [
            "Samuel Schmidgall",
            "Rojin Ziaei",
            "Carl Harris",
            "Eduardo Reis",
            "Jeffrey Jopling",
            "Michael Moor"
        ],
        "date": "2024/05/13",
        "pdf": "http://arxiv.org/pdf/2405.07960",
        "abstract": "Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient&#39;s diagnosis through dialogue and active data collection. We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at https://AgentClinic.github.io.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.07960"
    },
    "a7ce7a0960bfdb2e0e3d36c076b3386c": {
        "title": "Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation",
        "authors": [
            "Jinyu Cai",
            "Jialong Li",
            "Mingyue Zhang",
            "Munan Li",
            "Chen-Shu Wang",
            "Kenji Tei"
        ],
        "date": "2024/05/05",
        "pdf": "http://arxiv.org/pdf/2405.02858",
        "abstract": "Social media platforms such as Twitter, Reddit, and Sina Weibo play a crucial role in global communication but often encounter strict regulations in geopolitically sensitive regions. This situation has prompted users to ingeniously modify their way of communicating, frequently resorting to coded language in these regulated social media environments. This shift in communication is not merely a strategy to counteract regulation, but a vivid manifestation of language evolution, demonstrating how language naturally evolves under societal and technological pressures. Studying the evolution of language in regulated social media contexts is of significant importance for ensuring freedom of speech, optimizing content moderation, and advancing linguistic research. This paper proposes a multi-agent simulation framework using Large Language Models (LLMs) to explore the evolution of user language in regulated social media environments. The framework employs LLM-driven agents: supervisory agent who enforce dialogue supervision and participant agents who evolve their language strategies while engaging in conversation, simulating the evolution of communication styles under strict regulations aimed at evading social media regulation. The study evaluates the framework&#39;s effectiveness through a range of scenarios from abstract scenarios to real-world situations. Key findings indicate that LLMs are capable of simulating nuanced language dynamics and interactions in constrained settings, showing improvement in both evading supervision and information accuracy as evolution progresses. Furthermore, it was found that LLM agents adopt different strategies for different scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.02858"
    },
    "b43b5affaaf41cfdf9e94780c8081629": {
        "title": "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts",
        "authors": [
            "Minghao Wu",
            "Yulin Yuan",
            "Gholamreza Haffari",
            "Longyue Wang"
        ],
        "date": "2024/05/20",
        "pdf": "http://arxiv.org/pdf/2405.11804",
        "abstract": "Recent advancements in machine translation (MT) have significantly enhanced translation quality across various domains. However, the translation of literary texts remains a formidable challenge due to their complex language, figurative expressions, and cultural nuances. In this work, we introduce a novel multi-agent framework based on large language models (LLMs) for literary translation, implemented as a company called TransAgents, which mirrors traditional translation publication process by leveraging the collective capabilities of multiple agents, to address the intricate demands of translating literary works. To evaluate the effectiveness of our system, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP). MHP assesses translations from the perspective of monolingual readers of the target language, while BLP uses advanced LLMs to compare translations directly with the original texts. Empirical findings indicate that despite lower d-BLEU scores, translations from TransAgents are preferred by both human evaluators and LLMs over human-written references, particularly in genres requiring domain-specific knowledge. We also highlight the strengths and limitations of TransAgents through case studies and suggests directions for future research.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.11804"
    },
    "795c4f39e715ad150c279b8e75ac567a": {
        "title": "Speaker Verification in Agent-Generated Conversations",
        "authors": [
            "Yizhe Yang",
            "Heyan Huang",
            "Palakorn Achananuparp",
            "Jing Jiang",
            "Ee-Peng Lim"
        ],
        "date": "2024/05/16",
        "pdf": "http://arxiv.org/pdf/2405.10150",
        "abstract": "The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.10150"
    },
    "d42b631cc1afc305dd8e925f98cfee2e": {
        "title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play",
        "authors": [
            "Li-Chun Lu",
            "Shou-Jen Chen",
            "Tsung-Min Pai",
            "Chan-Hung Yu",
            "Hung-yi Lee",
            "Shao-Hua Sun"
        ],
        "date": "2024/05/10",
        "pdf": "http://arxiv.org/pdf/2405.06373",
        "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. Our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.06373"
    },
    "cefe1287a3a99a48008bf93f0f79e333": {
        "title": "LLMs with Personalities in Multi-issue Negotiation Games",
        "authors": [
            "Sean Noh",
            "Ho-Chun Herbert Chang"
        ],
        "date": "2024/05/08",
        "pdf": "http://arxiv.org/pdf/2405.05248",
        "abstract": "Powered by large language models (LLMs), AI agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be &#34;jail broken&#34; to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2405.05248"
    },
    "8056aba2da622463a647edc55787daf9": {
        "title": "Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework",
        "authors": [
            "Xiangpeng Wan",
            "Haicheng Deng",
            "Kai Zou",
            "Shiqi Xu"
        ],
        "date": "2024/05/07",
        "pdf": "http://arxiv.org/pdf/2405.04294",
        "abstract": "Structured finance, which involves restructuring diverse assets into securities like MBS, ABS, and CDOs, enhances capital market efficiency but presents significant due diligence challenges. This study explores the integration of artificial intelligence (AI) with traditional asset review processes to improve efficiency and accuracy in structured finance. Using both open-sourced and close-sourced large language models (LLMs), we demonstrate that AI can automate the verification of information between loan applications and bank statements effectively. While close-sourced models such as GPT-4 show superior performance, open-sourced models like LLAMA3 offer a cost-effective alternative. Dual-agent systems further increase accuracy, though this comes with higher operational costs. This research highlights AI&#39;s potential to minimize manual errors and streamline due diligence, suggesting a broader application of AI in financial document analysis and risk management.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.04294"
    },
    "ce0125bf6fd7254260d4181c039c17dc": {
        "title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents",
        "authors": [
            "Yue Liu",
            "Sin Kit Lo",
            "Qinghua Lu",
            "Liming Zhu",
            "Dehai Zhao",
            "Xiwei Xu",
            "Stefan Harrer",
            "Jon Whittle"
        ],
        "date": "2024/05/16",
        "pdf": "http://arxiv.org/pdf/2405.10467",
        "abstract": "Foundation model-enabled generative artificial intelligence facilitates the development and implementation of agents, which can leverage distinguished reasoning and language processing capabilities to takes a proactive, autonomous role to pursue users&#39; goals. Nevertheless, there is a lack of systematic knowledge to guide practitioners in designing the agents considering challenges of goal-seeking (including generating instrumental goals and plans), such as hallucinations inherent in foundation models, explainability of reasoning process, complex accountability, etc. To address this issue, we have performed a systematic literature review to understand the state-of-the-art foundation model-based agents and the broader ecosystem. In this paper, we present a pattern catalogue consisting of 16 architectural patterns with analyses of the context, forces, and trade-offs as the outcomes from the previous literature review. The proposed catalogue can provide holistic guidance for the effective use of patterns, and support the architecture design of foundation model-based agents by facilitating goal-seeking and plan generation.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2405.10467"
    },
    "94a565cf17ac32e1846fc0b3fffad4ec": {
        "title": "Latent State Estimation Helps UI Agents to Reason",
        "authors": [
            "William E Bishop",
            "Alice Li",
            "Christopher Rawles",
            "Oriana Riva"
        ],
        "date": "2024/05/17",
        "pdf": "http://arxiv.org/pdf/2405.11120",
        "abstract": "A common problem for agents operating in real-world environments is that the response of an environment to their actions may be non-deterministic and observed through noise. This renders environmental state and progress towards completing a task latent. Despite recent impressive demonstrations of LLM&#39;s reasoning abilities on various benchmarks, whether LLMs can build estimates of latent state and leverage them for reasoning has not been explicitly studied. We investigate this problem in the real-world domain of autonomous UI agents. We establish that appropriately prompting LLMs in a zero-shot manner can be formally understood as forming point estimates of latent state in a textual space. In the context of autonomous UI agents we then show that LLMs used in this manner are more than $76\\%$ accurate at inferring various aspects of latent state, such as performed (vs. commanded) actions and task progression. Using both public and internal benchmarks and three reasoning methods (zero-shot, CoT-SC &amp; ReAct), we show that LLM-powered agents that explicitly estimate and reason about latent state are able to successfully complete up to 1.6x more tasks than those that do not.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.11120"
    },
    "750790b50612eb08e2ba6127a791d3c7": {
        "title": "ALI-Agent: Assessing LLMs&#39; Alignment with Human Values via Agent-based Evaluation",
        "authors": [
            "Jingnan Zheng",
            "Han Wang",
            "An Zhang",
            "Tai D. Nguyen",
            "Jun Sun",
            "Tat-Seng Chua"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14125",
        "abstract": "Large Language Models (LLMs) can elicit unintended and even harmful content when misaligned with human values, posing severe risks to users and society. To mitigate these risks, current evaluation benchmarks predominantly employ expert-designed contextual scenarios to assess how well LLMs align with human values. However, the labor-intensive nature of these benchmarks limits their test scope, hindering their ability to generalize to the extensive variety of open-world use cases and identify rare but crucial long-tail risks. Additionally, these static tests fail to adapt to the rapid evolution of LLMs, making it hard to evaluate timely alignment issues. To address these challenges, we propose ALI-Agent, an evaluation framework that leverages the autonomous abilities of LLM-powered agents to conduct in-depth and adaptive alignment assessments. ALI-Agent operates through two principal stages: Emulation and Refinement. During the Emulation stage, ALI-Agent automates the generation of realistic test scenarios. In the Refinement stage, it iteratively refines the scenarios to probe long-tail risks. Specifically, ALI-Agent incorporates a memory module to guide test scenario generation, a tool-using module to reduce human labor in tasks such as evaluating feedback from target LLMs, and an action module to refine tests. Extensive experiments across three aspects of human values--stereotypes, morality, and legality--demonstrate that ALI-Agent, as a general evaluation framework, effectively identifies model misalignment. Systematic analysis also validates that the generated test scenarios represent meaningful use cases, as well as integrate enhanced measures to probe long-tail risks. Our code is available at https://github.com/SophieZheng998/ALI-Agent.git",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.14125"
    },
    "b2904bbe6b5faeec0f25208fc522269e": {
        "title": "Human-Agent Cooperation in Games under Incomplete Information through Natural Language Communication",
        "authors": [
            "Shenghui Chen",
            "Daniel Fried",
            "Ufuk Topcu"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14173",
        "abstract": "Developing autonomous agents that can strategize and cooperate with humans under information asymmetry is challenging without effective communication in natural language. We introduce a shared-control game, where two players collectively control a token in alternating turns to achieve a common objective under incomplete information. We formulate a policy synthesis problem for an autonomous agent in this game with a human as the other player. To solve this problem, we propose a communication-based approach comprising a language module and a planning module. The language module translates natural language messages into and from a finite set of flags, a compact representation defined to capture player intents. The planning module leverages these flags to compute a policy using an asymmetric information-set Monte Carlo tree search with flag exchange algorithm we present. We evaluate the effectiveness of this approach in a testbed based on Gnomes at Night, a search-and-find maze board game. Results of human subject experiments show that communication narrows the information gap between players and enhances human-agent cooperation efficiency with fewer turns.",
        "code": "",
        "category": [
            "Game Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.14173"
    },
    "4d296b8ae0f8197119be49892f53d198": {
        "title": "AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents",
        "authors": [
            "Christopher Rawles",
            "Sarah Clinckemaillie",
            "Yifan Chang",
            "Jonathan Waltz",
            "Gabrielle Lau",
            "Marybeth Fair",
            "Alice Li",
            "William Bishop",
            "Wei Li",
            "Folawiyo Campbell-Ajala",
            "Daniel Toyama",
            "Robert Berry",
            "Divya Tyamagundlu",
            "Timothy Lillicrap",
            "Oriana Riva"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14573",
        "abstract": "Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. Yet, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functioning Android environment that provides reward signals for 116 programmatic task workflows across 20 real world Android applications. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and realistic suite of tasks. Reward signals are derived from the computer&#39;s system state, making them durable across task variations and extensible across different apps. To demonstrate AndroidWorld&#39;s benefits and mode of operation, we introduce a new computer control agent, M3A. M3A can complete 30.6% of the AndroidWorld&#39;s tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-domain agents. Finally, we conduct a robustness analysis by testing M3A against a range of task variations on a representative subset of tasks, demonstrating that variations in task parameters can significantly alter the complexity of a task and therefore an agent&#39;s performance, highlighting the importance of testing agents under diverse conditions. AndroidWorld and the experiments in this paper are available at https://github.com/google-research/android_world.",
        "code": "",
        "category": [
            "Benchmark&Evaluation",
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2405.14573"
    },
    "241a53b94a75c87f40db449ad7d7c09f": {
        "title": "CityGPT: Towards Urban IoT Learning, Analysis and Interaction with Multi-Agent System",
        "authors": [
            "Qinghua Guan",
            "Jinhui Ouyang",
            "Di Wu",
            "Weiren Yu"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14691",
        "abstract": "The spatiotemporal data generated by massive sensors in the Internet of Things (IoT) is extremely dynamic, heterogeneous, large scale and time-dependent. It poses great challenges (e.g. accuracy, reliability, and stability) in real-time analysis and decision making for different IoT applications. The complexity of IoT data prevents the common people from gaining a deeper understanding of it. Agentized systems help address the lack of data insight for the common people. We propose a generic framework, namely CityGPT, to facilitate the learning and analysis of IoT time series with an end-to-end paradigm. CityGPT employs three agents to accomplish the spatiotemporal analysis of IoT data. The requirement agent facilitates user inputs based on natural language. Then, the analysis tasks are decomposed into temporal and spatial analysis processes, completed by corresponding data analysis agents (temporal and spatial agents). Finally, the spatiotemporal fusion agent visualizes the system&#39;s analysis results by receiving analysis results from data analysis agents and invoking sub-visualization agents, and can provide corresponding textual descriptions based on user demands. To increase the insight for common people using our framework, we have agnentized the framework, facilitated by a large language model (LLM), to increase the data comprehensibility. Our evaluation results on real-world data with different time dependencies show that the CityGPT framework can guarantee robust performance in IoT computing.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.14691"
    },
    "71fde0ae3293374724587616030740e8": {
        "title": "Evaluating Tool-Augmented Agents in Remote Sensing Platforms",
        "authors": [
            "Simranjit Singh",
            "Michael Fore",
            "Dimitrios Stamoulis"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2405.00709",
        "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask &#34;Detect all objects here&#34;. Where is `here`, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.00709"
    },
    "adf670f5c071558cd2fe0713eb34f114": {
        "title": "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration",
        "authors": [
            "David Maranto"
        ],
        "date": "2024/04/13",
        "pdf": "http://arxiv.org/pdf/2405.01392",
        "abstract": "As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for. Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration. Recent work has explored AI-based goal-oriented systems to increase the level of autonomy in mission execution. These systems make use of symbolic reasoning managers to make inferences from the state of a spacecraft and a handcrafted knowledge base, enabling autonomous generation of tasks and re-planning. Such systems have proven to be successful in controlled cases, but they are difficult to implement as they require human-crafted ontological models to allow the spacecraft to understand the world. Reinforcement learning has been applied to train robotic agents to pursue a goal. A new architecture for autonomy is called for. This work explores the application of Large Language Models (LLMs) as the high-level control system of a spacecraft. Using a systems engineering approach, this work presents the design and development of an agentic spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate the utility of such an architecture in achieving higher levels of spacecraft autonomy. A series of deep space mission scenarios simulated within the popular game engine Kerbal Space Program (KSP) are used as case studies to evaluate the implementation against the requirements. It is shown the reasoning and planning abilities of present-day LLMs do not scale well as the complexity of a mission increases, but this can be alleviated with adequate prompting frameworks and strategic selection of the agent&#39;s level of authority over the host spacecraft. This research evaluates the potential of LLMs in augmenting autonomous decision-making systems for future robotic space applications.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.01392"
    },
    "84c6d6849a4e01526b03a20d54035cbb": {
        "title": "Rapid Mobile App Development for Generative AI Agents on MIT App Inventor",
        "authors": [
            "Jaida Gao",
            "Calab Su",
            "Etai Miller",
            "Kevin Lu",
            "Yu Meng"
        ],
        "date": "2024/04/01",
        "pdf": "http://arxiv.org/pdf/2405.01561",
        "abstract": "The evolution of Artificial Intelligence (AI) stands as a pivotal force shaping our society, finding applications across diverse domains such as education, sustainability, and safety. Leveraging AI within mobile applications makes it easily accessible to the public, catalyzing its transformative potential. In this paper, we present a methodology for the rapid development of AI agent applications using the development platform provided by MIT App Inventor. To demonstrate its efficacy, we share the development journey of three distinct mobile applications: SynchroNet for fostering sustainable communities; ProductiviTeams for addressing procrastination; and iHELP for enhancing community safety. All three applications seamlessly integrate a spectrum of generative AI features, leveraging OpenAI APIs. Furthermore, we offer insights gleaned from overcoming challenges in integrating diverse tools and AI functionalities, aiming to inspire young developers to join our efforts in building practical AI agent applications.",
        "code": "",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2405.01561"
    },
    "a3bec6082d05d452c748821b265713b1": {
        "title": "Large Language Models (LLMs) as Agents for Augmented Democracy",
        "authors": [
            "Jairo GudiÃ±o-Rosero",
            "Umberto Grandi",
            "CÃ©sar A. Hidalgo"
        ],
        "date": "2024/05/06",
        "pdf": "http://arxiv.org/pdf/2405.03452",
        "abstract": "We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections. We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject&#39;s individual political choices and the aggregate preferences of the full sample of participants. At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants. At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs. We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population. These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.03452"
    },
    "7280e591137995f5e4d5787bc79f1c9f": {
        "title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models",
        "authors": [
            "Cong Lu",
            "Shengran Hu",
            "Jeff Clune"
        ],
        "date": "2024/05/24",
        "pdf": "http://arxiv.org/pdf/2405.15143",
        "abstract": "Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems, built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of challenging problems including Atari games and robotic control, but requires manually designing heuristics to guide exploration, which is time-consuming and infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE) which greatly extends the scope of the original Go-Explore by replacing these heuristics with the intelligence and internalized human notions of interestingness captured by giant foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is (e.g. discovering new objects, locations, or behaviors), even in complex environments where heuristics are hard to define. Moreover, IGE offers the exciting and previously impossible opportunity to recognize and capitalize on serendipitous discoveries that cannot be predicted ahead of time. We evaluate IGE on a range of language-based tasks that require search and exploration. In Game of 24, a multistep mathematical reasoning problem, IGE reaches 100% success rate 70.8% faster than the best classic graph search baseline. Next, in BabyAI-Text, a challenging partially observable gridworld, IGE exceeds the previous SOTA with orders of magnitude fewer online samples. Finally, in TextWorld, we show the unique ability of IGE to succeed in settings requiring long-horizon exploration where prior SOTA FM agents like Reflexion completely fail. Overall, IGE combines the tremendous strengths of FMs and the powerful Go-Explore algorithm, opening up a new frontier of research into creating more generally capable agents with impressive exploration capabilities.",
        "code": "https://github.com/conglu1997/intelligent-go-explore",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2405.15143"
    },
    "c1543f88fc335d89f5e317fb67d609ed": {
        "title": "Large Language Models Can Self-Improve At Web Agent Tasks",
        "authors": [
            "Ajay Patel",
            "Markus Hofmarcher",
            "Claudiu Leoveanu-Condrei",
            "Marius-Constantin Dinu",
            "Chris Callison-Burch",
            "Sepp Hochreiter"
        ],
        "date": "2024/05/30",
        "pdf": "http://arxiv.org/pdf/2405.20309",
        "abstract": "Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2405.20309"
    },
    "aa807ff46f37c2e93777ace6d24d0fce": {
        "title": "Safe Multi-agent Reinforcement Learning with Natural Language Constraints",
        "authors": [
            "Ziyan Wang",
            "Meng Fang",
            "Tristan Tomilin",
            "Fei Fang",
            "Yali Du"
        ],
        "date": "2024/05/30",
        "pdf": "http://arxiv.org/pdf/2405.20018",
        "abstract": "The role of natural language constraints in Safe Multi-agent Reinforcement Learning (MARL) is crucial, yet often overlooked. While Safe MARL has vast potential, especially in fields like robotics and autonomous vehicles, its full potential is limited by the need to define constraints in pre-designed mathematical terms, which requires extensive domain expertise and reinforcement learning knowledge, hindering its broader adoption. To address this limitation and make Safe MARL more accessible and adaptable, we propose a novel approach named Safe Multi-agent Reinforcement Learning with Natural Language constraints (SMALL). Our method leverages fine-tuned language models to interpret and process free-form textual constraints, converting them into semantic embeddings that capture the essence of prohibited states and behaviours. These embeddings are then integrated into the multi-agent policy learning process, enabling agents to learn policies that minimize constraint violations while optimizing rewards. To evaluate the effectiveness of SMALL, we introduce the LaMaSafe, a multi-task benchmark designed to assess the performance of multiple agents in adhering to natural language constraints. Empirical evaluations across various environments demonstrate that SMALL achieves comparable rewards and significantly fewer constraint violations, highlighting its effectiveness in understanding and enforcing natural language constraints.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.20018"
    },
    "599e168857c70e1fcb2c891f1be0fe66": {
        "title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
        "authors": [
            "Chengxing Xie",
            "Difan Zou"
        ],
        "date": "2024/05/28",
        "pdf": "http://arxiv.org/pdf/2405.18208",
        "abstract": "Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2405.18208"
    },
    "21c1661e923a0cbe16270816189c6c85": {
        "title": "LLM-Based Cooperative Agents using Information Relevance and Plan Validation",
        "authors": [
            "SeungWon Seo",
            "Junhyeok Lee",
            "SeongRae Noh",
            "HyeongYeop Kang"
        ],
        "date": "2024/05/27",
        "pdf": "http://arxiv.org/pdf/2405.16751",
        "abstract": "We address the challenge of multi-agent cooperation, where agents achieve a common goal by interacting with a 3D scene and cooperating with decentralized agents under complex partial observations. This involves managing communication costs and optimizing interaction trajectories in dynamic environments. Our research focuses on three primary limitations of existing cooperative agent systems. Firstly, current systems demonstrate inefficiency in managing acquired information through observation, resulting in declining planning performance as the environment becomes more complex with additional objects or goals. Secondly, the neglect of false plans in partially observable settings leads to suboptimal cooperative performance, as agents struggle to adapt to environmental changes influenced by the unseen actions of other agents. Lastly, the failure to incorporate spatial data into decision-making processes restricts the agent&#39;s ability to construct optimized trajectories. To overcome these limitations, we propose the RElevance and Validation-Enhanced Cooperative Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5. REVECA leverages relevance assessment, plan validation, and spatial information to enhance the efficiency and robustness of agent cooperation in dynamic and partially observable environments while minimizing continuous communication costs and effectively managing irrelevant dummy objects. Our extensive experiments demonstrate the superiority of REVECA over previous approaches, including those driven by GPT-4.0. Additionally, a user study highlights REVECA&#39;s potential for achieving trustworthy human-AI cooperation. We expect that REVECA will have significant applications in gaming, XR applications, educational tools, and humanoid robots, contributing to substantial economic, commercial, and academic advancements.",
        "code": "",
        "category": [
            "Planning",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.16751"
    },
    "faa2d1dbd6a9e8f7c1e6bb8a7b396643": {
        "title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
        "authors": [
            "John Yang",
            "Carlos E. Jimenez",
            "Alexander Wettig",
            "Kilian Lieret",
            "Shunyu Yao",
            "Karthik Narasimhan",
            "Ofir Press"
        ],
        "date": "2024/05/06",
        "pdf": "http://arxiv.org/pdf/2405.15793",
        "abstract": "Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent&#39;s custom agent-computer interface (ACI) significantly enhances an agent&#39;s ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents&#39; behavior and performance.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.15793"
    },
    "d870e9fafc0687b64ab77ac017cbe309": {
        "title": "Hacc-Man: An Arcade Game for Jailbreaking LLMs",
        "authors": [
            "Matheus Valentim",
            "Jeanette Falk",
            "Nanna Inie"
        ],
        "date": "2024/05/24",
        "pdf": "http://arxiv.org/pdf/2405.15902",
        "abstract": "The recent leaps in complexity and fluency of Large Language Models (LLMs) mean that, for the first time in human history, people can interact with computers using natural language alone. This creates monumental possibilities of automation and accessibility of computing, but also raises severe security and safety threats: When everyone can interact with LLMs, everyone can potentially break into the systems running LLMs. All it takes is creative use of language. This paper presents Hacc-Man, a game which challenges its players to &#34;jailbreak&#34; an LLM: subvert the LLM to output something that it is not intended to. Jailbreaking is at the intersection between creative problem solving and LLM security. The purpose of the game is threefold: 1. To heighten awareness of the risks of deploying fragile LLMs in everyday systems, 2. To heighten people&#39;s self-efficacy in interacting with LLMs, and 3. To discover the creative problem solving strategies, people deploy in this novel context.",
        "code": "",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2405.15902"
    },
    "e80b28ea56e0a7ce32e3fc5404c316d7": {
        "title": "GeneAgent: Self-verification Language Agent for Gene Set Knowledge Discovery using Domain Databases",
        "authors": [
            "Zhizheng Wang",
            "Qiao Jin",
            "Chih-Hsuan Wei",
            "Shubo Tian",
            "Po-Ting Lai",
            "Qingqing Zhu",
            "Chi-Ping Day",
            "Christina Ross",
            "Zhiyong Lu"
        ],
        "date": "2024/05/25",
        "pdf": "http://arxiv.org/pdf/2405.16205",
        "abstract": "Gene set knowledge discovery is essential for advancing human functional genomics. Recent studies have shown promising performance by harnessing the power of Large Language Models (LLMs) on this task. Nonetheless, their results are subject to several limitations common in LLMs such as hallucinations. In response, we present GeneAgent, a first-of-its-kind language agent featuring self-verification capability. It autonomously interacts with various biological databases and leverages relevant domain knowledge to improve accuracy and reduce hallucination occurrences. Benchmarking on 1,106 gene sets from different sources, GeneAgent consistently outperforms standard GPT-4 by a significant margin. Moreover, a detailed manual review confirms the effectiveness of the self-verification module in minimizing hallucinations and generating more reliable analytical narratives. To demonstrate its practical utility, we apply GeneAgent to seven novel gene sets derived from mouse B2905 melanoma cell lines, with expert evaluations showing that GeneAgent offers novel insights into gene functions and subsequently expedites knowledge discovery.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.16205"
    },
    "498ac37f486ba2e8dc8ab3363404d05b": {
        "title": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning",
        "authors": [
            "Minghao Chen",
            "Yihang Li",
            "Yanting Yang",
            "Shiyu Yu",
            "Binbin Lin",
            "Xiaofei He"
        ],
        "date": "2024/05/25",
        "pdf": "http://arxiv.org/pdf/2405.16247",
        "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce \\textit{case-conditioned prompting} strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The source code will be available soon.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.16247"
    },
    "b1821b90ce36752821dab90a5e4cd860": {
        "title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models",
        "authors": [
            "Jaewoo Ahn",
            "Taehyun Lee",
            "Junyoung Lim",
            "Jin-Hwa Kim",
            "Sangdoo Yun",
            "Hwaran Lee",
            "Gunhee Kim"
        ],
        "date": "2024/05/28",
        "pdf": "http://arxiv.org/pdf/2405.18027",
        "abstract": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users&#39; narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters&#39; identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.18027"
    },
    "c750d6b039a1709a6a9959a889758567": {
        "title": "ResearchArena: Benchmarking LLMs&#39; Ability to Collect and Organize Information as Research Agents",
        "authors": [
            "Hao Kang",
            "Chenyan Xiong"
        ],
        "date": "2024/06/13",
        "pdf": "http://arxiv.org/pdf/2406.10291",
        "abstract": "Large language models (LLMs) have exhibited remarkable performance across various tasks in natural language processing. Nevertheless, challenges still arise when these tasks demand domain-specific expertise and advanced analytical skills, such as conducting research surveys on a designated topic. In this research, we develop ResearchArena, a benchmark that measures LLM agents&#39; ability to conduct academic surveys, an initial step of academic research process. Specifically, we deconstructs the surveying process into three stages 1) information discovery: locating relevant papers, 2) information selection: assessing papers&#39; importance to the topic, and 3) information organization: organizing papers into meaningful structures. In particular, we establish an offline environment comprising 12.0M full-text academic papers and 7.9K survey papers, which evaluates agents&#39; ability to locate supporting materials for composing the survey on a topic, rank the located papers based on their impact, and organize these into a hierarchical knowledge mind-map. With this benchmark, we conduct preliminary evaluations of existing techniques and find that all LLM-based methods under-performing when compared to basic keyword-based retrieval techniques, highlighting substantial opportunities for future research.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.10291"
    },
    "e8f100e713dc301cd7404ff87bb6b5aa": {
        "title": "GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents",
        "authors": [
            "Dongping Chen",
            "Yue Huang",
            "Siyuan Wu",
            "Jingyu Tang",
            "Liuyi Chen",
            "Yilin Bai",
            "Zhigang He",
            "Chenlong Wang",
            "Huichi Zhou",
            "Yiqiang Li",
            "Tianshuo Zhou",
            "Yue Yu",
            "Chujie Gao",
            "Qihui Zhang",
            "Yi Gui",
            "Zhen Li",
            "Yao Wan",
            "Pan Zhou",
            "Jianfeng Gao",
            "Lichao Sun"
        ],
        "date": "2024/06/16",
        "pdf": "http://arxiv.org/pdf/2406.10819",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code. However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-oriented questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-oriented tasks given the sparse GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding. The code and dataset are publicly available at our project homepage: https://gui-world.github.io/.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.10819"
    },
    "a73b2790e7940e467a2d7ac436a550f8": {
        "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents",
        "authors": [
            "Wentong Chen",
            "Junbo Cui",
            "Jinyi Hu",
            "Yujia Qin",
            "Junjie Fang",
            "Yue Zhao",
            "Chongyi Wang",
            "Jun Liu",
            "Guirong Chen",
            "Yupeng Huo",
            "Yuan Yao",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11317",
        "abstract": "Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.",
        "code": "https://github.com/yiye3/guicourse",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2406.11317"
    },
    "b7788f3550328428ea5ce540b638f532": {
        "title": "Toward Conversational Agents with Context and Time Sensitive Long-term Memory",
        "authors": [
            "Nick Alonso",
            "TomÃ¡s Figliolia",
            "Anthony Ndirango",
            "Beren Millidge"
        ],
        "date": "2024/05/29",
        "pdf": "http://arxiv.org/pdf/2406.00057",
        "abstract": "There has recently been growing interest in conversational agents with long-term memory which has led to the rapid development of language models that use retrieval-augmented generation (RAG). Until recently, most work on RAG has focused on information retrieval from large databases of texts, like Wikipedia, rather than information from long-form conversations. In this paper, we argue that effective retrieval from long-form conversational data faces two unique problems compared to static database retrieval: 1) time/event-based queries, which requires the model to retrieve information about previous conversations based on time or the order of a conversational event (e.g., the third conversation on Tuesday), and 2) ambiguous queries that require surrounding conversational context to understand. To better develop RAG-based agents that can deal with these challenges, we generate a new dataset of ambiguous and time-based questions that build upon a recent dataset of long-form, simulated conversations, and demonstrate that standard RAG based approaches handle such questions poorly. We then develop a novel retrieval model which combines chained-of-table search methods, standard vector-database retrieval, and a prompting method to disambiguate queries, and demonstrate that this approach substantially improves over current methods at solving these tasks. We believe that this new dataset and more advanced RAG agent can act as a key benchmark and stepping stone towards effective memory augmented conversational agents that can be used in a wide variety of AI applications.",
        "code": "https://github.com/Zyphra/TemporalMemoryDataset",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2406.00057"
    },
    "21aab690f7cd4cbd1de0b1f6393d4e30": {
        "title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training",
        "authors": [
            "Maximillian Chen",
            "Ruoxi Sun",
            "Sercan Ã. ArÄ±k",
            "Tomas Pfister"
        ],
        "date": "2024/05/31",
        "pdf": "http://arxiv.org/pdf/2406.00222",
        "abstract": "Large language models (LLMs) aligned through reinforcement learning from human feedback (RLHF) have quickly become one of the dominant paradigms for building intelligent conversational assistant agents. However, despite their strong performance across many benchmarks, LLM-based agents still lack conversational skills such as disambiguation: when generalized assistants are faced with ambiguity, they often overhedge or implicitly guess users&#39; ground-truth intents rather than asking clarification questions, and under task-specific settings, high-quality conversation samples are often limited, affecting models&#39; ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (henceforth ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO) which allows for sample-efficient dialogue policy learning in multi-turn conversation. We demonstrate ACT&#39;s efficacy under sample-efficient conditions in three difficult conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for text-to-SQL generation. Additionally, we propose evaluating LLMs&#39; ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard approaches to supervised fine-tuning and DPO.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.00222"
    },
    "86aec45a3853272af15d2b2d2c6ae12d": {
        "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Haitao Jia",
            "Xi Zhang",
            "Ming Yan",
            "Weizhou Shen",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "date": "2024/06/03",
        "pdf": "http://arxiv.org/pdf/2406.01014",
        "abstract": "Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks, task progress navigation and focus content navigation, are significantly complicated under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent generates task progress, making the navigation of history operations more efficient. To retain focus content, we design a memory unit that updates with task progress. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistakes accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.",
        "code": "https://github.com/x-plug/mobileagent",
        "category": [
            "Multi-Agent System",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2406.01014"
    },
    "da55b01bed4abf929270ff8bbef859af": {
        "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
        "authors": [
            "Yu-Min Tseng",
            "Yu-Chao Huang",
            "Teng-Yun Hsiao",
            "Yu-Ching Hsu",
            "Jia-Yin Foo",
            "Chao-Wei Huang",
            "Yun-Nung Chen"
        ],
        "date": "2024/06/03",
        "pdf": "http://arxiv.org/pdf/2406.01171",
        "abstract": "Recently, methods investigating how to adapt large language models (LLMs) for specific scenarios have gained great attention. Particularly, the concept of \\textit{persona}, originally adopted in dialogue literature, has re-surged as a promising avenue. However, the growing research on persona is relatively disorganized, lacking a systematic overview. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. To the best of our knowledge, we present the first survey tailored for LLM role-playing and LLM personalization under the unified view of persona, including taxonomy, current challenges, and potential directions. To foster future endeavors, we actively maintain a paper collection available to the community: https://github.com/MiuLab/PersonaLLM-Survey",
        "code": "https://github.com/miulab/personallm-survey",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2406.01171"
    },
    "a08093f55986bcc87491fb616bb60ea5": {
        "title": "Reflection-Reinforced Self-Training for Language Agents",
        "authors": [
            "Zi-Yi Dou",
            "Cheng-Fu Yang",
            "Xueqing Wu",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "date": "2024/06/03",
        "pdf": "http://arxiv.org/pdf/2406.01495",
        "abstract": "Self-training can potentially improve the performance of language agents without relying on demonstrations from humans or stronger models. The general process involves generating samples from a model, evaluating their quality, and updating the model by training on high-quality samples. However, self-training can face limitations because achieving good performance requires a good amount of high-quality samples, yet relying solely on model sampling for obtaining such samples can be inefficient. In addition, these methods often disregard low-quality samples, failing to leverage them effectively. To address these limitations, we present Reflection-Reinforced Self-Training (Re-ReST), which leverages a reflection model to refine low-quality samples and subsequently uses these improved samples to augment self-training. The reflection model takes both the model output and feedback from an external environment (e.g., unit test results in code generation) as inputs and produces improved samples as outputs. By employing this technique, we effectively enhance the quality of inferior samples, and enrich the self-training dataset with higher-quality samples efficiently. We perform extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. Results demonstrate improvements over self-training baselines across settings. Moreover, ablation studies confirm the reflection model&#39;s efficiency in generating quality self-training samples and its compatibility with self-consistency decoding.",
        "code": "https://github.com/PlusLabNLP/Re-ReST",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.01495"
    },
    "3150a26b3d768a61624882282f316dad": {
        "title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks",
        "authors": [
            "Yusen Zhang",
            "Ruoxi Sun",
            "Yanfei Chen",
            "Tomas Pfister",
            "Rui Zhang",
            "Sercan Ã. Arik"
        ],
        "date": "2024/06/04",
        "pdf": "http://arxiv.org/pdf/2406.02818",
        "abstract": "Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.02818"
    },
    "9289102622122ee14122949cf1ccb40c": {
        "title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents",
        "authors": [
            "Yifei Wang",
            "Dizhan Xue",
            "Shengjie Zhang",
            "Shengsheng Qian"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03007",
        "abstract": "With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent",
        "code": "https://github.com/dpamk/badagent",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.03007"
    },
    "269cd0ac32202d249d19366188e9d849": {
        "title": "Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework",
        "authors": [
            "Xiaoxi Sun",
            "Jinpeng Li",
            "Yan Zhong",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03075",
        "abstract": "The advent of large language models (LLMs) has facilitated the development of natural language text generation. It also poses unprecedented challenges, with content hallucination emerging as a significant concern. Existing solutions often involve expensive and complex interventions during the training process. Moreover, some approaches emphasize problem disassembly while neglecting the crucial validation process, leading to performance degradation or limited applications. To overcome these limitations, we propose a Markov Chain-based multi-agent debate verification framework to enhance hallucination detection accuracy in concise claims. Our method integrates the fact-checking process, including claim detection, evidence retrieval, and multi-agent verification. In the verification stage, we deploy multiple agents through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes. Experimental results across three generative tasks demonstrate that our approach achieves significant improvements over baselines.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.03075"
    },
    "02d61e99fd7fc97a53e4c73445171682": {
        "title": "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback",
        "authors": [
            "Timon Ziegenbein",
            "Gabriella Skitalinskaya",
            "Alireza Bayat Makou",
            "Henning Wachsmuth"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03363",
        "abstract": "Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies. Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.03363"
    },
    "dbdb0b9177ee31f171ff0223836e85a4": {
        "title": "Mixture-of-Agents Enhances Large Language Model Capabilities",
        "authors": [
            "Junlin Wang",
            "Jue Wang",
            "Ben Athiwaratkun",
            "Ce Zhang",
            "James Zou"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.04692",
        "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.04692"
    },
    "cd0e509bd2359089f92d7590f198aa63": {
        "title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
        "authors": [
            "Bill Yuchen Lin",
            "Yuntian Deng",
            "Khyathi Chandu",
            "Faeze Brahman",
            "Abhilasha Ravichander",
            "Valentina Pyatkin",
            "Nouha Dziri",
            "Ronan Le Bras",
            "Yejin Choi"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.04770",
        "abstract": "We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse&#39;&#39; to ``tie&#39;&#39; if the winner response exceeds the loser one by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard&#39;s 0.91 and AlpacaEval2.0&#39;s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.",
        "code": "https://github.com/allenai/wildbench",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.04770"
    },
    "d9f06df47f1047513338ea5d9a7a4b10": {
        "title": "SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals",
        "authors": [
            "Ruihan Yang",
            "Jiangjie Chen",
            "Yikai Zhang",
            "Siyu Yuan",
            "Aili Chen",
            "Kyle Richardson",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.04784",
        "abstract": "Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SelfGoal, a novel automatic approach designed to enhance agents&#39; capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SelfGoal involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure. Experimental results demonstrate that SelfGoal significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments. Project page: https://selfgoal-agent.github.io.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.04784"
    },
    "63bf5d41f59732724da5c7dd1eefabb2": {
        "title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions",
        "authors": [
            "Cheng Tan",
            "Dongxin Lyu",
            "Siyuan Li",
            "Zhangyang Gao",
            "Jingxuan Wei",
            "Siqi Ma",
            "Zicheng Liu",
            "Stan Z. Li"
        ],
        "date": "2024/06/09",
        "pdf": "http://arxiv.org/pdf/2406.05688",
        "abstract": "Large Language Models (LLMs) have demonstrated wide-ranging applications across various fields and have shown significant potential in the academic peer-review process. However, existing applications are primarily limited to static review generation based on submitted papers, which fail to capture the dynamic and iterative nature of real-world peer reviews. In this paper, we reformulate the peer-review process as a multi-turn, long-context dialogue, incorporating distinct roles for authors, reviewers, and decision makers. We construct a comprehensive dataset containing over 26,841 papers with 92,017 reviews collected from multiple sources, including the top-tier conference and prestigious journal. This dataset is meticulously designed to facilitate the applications of LLMs for multi-turn dialogues, effectively simulating the complete peer-review process. Furthermore, we propose a series of metrics to evaluate the performance of LLMs for each role under this reformulated peer-review setting, ensuring fair and comprehensive evaluations. We believe this work provides a promising perspective on enhancing the LLM-driven peer-review process by incorporating dynamic, role-based interactions. It aligns closely with the iterative and interactive nature of real-world academic peer review, offering a robust foundation for future research and development in this area. We open-source the dataset at https://github.com/chengtan9907/ReviewMT.",
        "code": "https://github.com/chengtan9907/reviewmt",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.05688"
    },
    "9c84d36ed7499d328b5a04b9ad50d85f": {
        "title": "Can Language Models Serve as Text-Based World Simulators?",
        "authors": [
            "Ruoyao Wang",
            "Graham Todd",
            "Ziang Xiao",
            "Xingdi Yuan",
            "Marc-Alexandre CÃ´tÃ©",
            "Peter Clark",
            "Peter Jansen"
        ],
        "date": "2024/06/10",
        "pdf": "http://arxiv.org/pdf/2406.06485",
        "abstract": "Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM&#39;s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.06485"
    },
    "ad75548bfd9b38bc7c85370ad899e213": {
        "title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents",
        "authors": [
            "Anthony Costarelli",
            "Mat Allen",
            "Roman Hauksson",
            "Grace Sodunke",
            "Suhas Hariharan",
            "Carlson Cheng",
            "Wenjie Li",
            "Arjun Yadav"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.06613",
        "abstract": "Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents&#39; performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models&#39; pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worse GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.06613"
    },
    "e180bb7bf6f20677a576c22b725e2f3f": {
        "title": "Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models",
        "authors": [
            "Shoutao Guo",
            "Shaolei Zhang",
            "Zhengrui Ma",
            "Min Zhang",
            "Yang Feng"
        ],
        "date": "2024/06/11",
        "pdf": "http://arxiv.org/pdf/2406.06910",
        "abstract": "Simultaneous Machine Translation (SiMT) generates target translations while reading the source sentence. It relies on a policy to determine the optimal timing for reading sentences and generating translations. Existing SiMT methods generally adopt the traditional Transformer architecture, which concurrently determines the policy and generates translations. While they excel at determining policies, their translation performance is suboptimal. Conversely, Large Language Models (LLMs), trained on extensive corpora, possess superior generation capabilities, but it is difficult for them to acquire translation policy through the training methods of SiMT. Therefore, we introduce Agent-SiMT, a framework combining the strengths of LLMs and traditional SiMT methods. Agent-SiMT contains the policy-decision agent and the translation agent. The policy-decision agent is managed by a SiMT model, which determines the translation policy using partial source sentence and translation. The translation agent, leveraging an LLM, generates translation based on the partial source sentence. The two agents collaborate to accomplish SiMT. Experiments demonstrate that Agent-SiMT attains state-of-the-art performance.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.06910"
    },
    "db18353b2d2aa343b33a016a72e601ce": {
        "title": "CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation",
        "authors": [
            "Renhao Li",
            "Minghuan Tan",
            "Derek F. Wong",
            "Min Yang"
        ],
        "date": "2024/06/11",
        "pdf": "http://arxiv.org/pdf/2406.07054",
        "abstract": "In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses to instructions. To effectively refine the responses, we develop an iterative framework following a debate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs.",
        "code": "https://github.com/lirenhao1997/coevol",
        "category": [
            "Multi-Agent System",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.07054"
    },
    "e70cbbb7be0f0f89684f14244681fe64": {
        "title": "Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models",
        "authors": [
            "Joshua Strong",
            "Qianhui Men",
            "Alison Noble"
        ],
        "date": "2024/06/11",
        "pdf": "http://arxiv.org/pdf/2406.07212",
        "abstract": "Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations. Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes. This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers. We leverage LLMs&#39; verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning smaller LLMs with data from larger models enhances performance while maintaining computational efficiency. A pilot study showcases the effectiveness of our deferral system.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2406.07212"
    },
    "8c868b243c898b5c69dfffa74887c7df": {
        "title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents",
        "authors": [
            "Cheng-Kuang Wu",
            "Zhi Rui Tam",
            "Chieh-Yen Lin",
            "Yun-Nung Chen",
            "Hung-yi Lee"
        ],
        "date": "2024/06/13",
        "pdf": "http://arxiv.org/pdf/2406.08747",
        "abstract": "Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.08747"
    },
    "357e3fd79056c360acc4d8ebf786ac35": {
        "title": "Multi-Agent Software Development through Cross-Team Collaboration",
        "authors": [
            "Zhuoyun Du",
            "Chen Qian",
            "Wei Liu",
            "Zihao Xie",
            "Yifei Wang",
            "Yufan Dang",
            "Weize Chen",
            "Cheng Yang"
        ],
        "date": "2024/06/13",
        "pdf": "http://arxiv.org/pdf/2406.08979",
        "abstract": "The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have catalyzed profound transformations, particularly through multi-agent collaboration for software development. LLM agents can collaborate in teams like humans, and follow the waterfall model to sequentially work on requirements analysis, development, review, testing, and other phases to perform autonomous software generation. However, for an agent team, each phase in a single development process yields only one possible outcome. This results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space. Consequently, this may lead to obtaining suboptimal results. To address this challenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions and communicate with their insights in a cross-team collaboration environment for superior content generation. Experimental results in software development reveal a notable increase in quality compared to state-of-the-art baselines, underscoring the efficacy of our framework. The significant improvements in story generation demonstrate the promising generalization ability of our framework across various domains. We anticipate that our work will guide LLM agents towards a cross-team paradigm and contribute to their significant growth in but not limited to software development. The code and data will be available at https://github.com/OpenBMB/ChatDev.",
        "code": "https://github.com/openbmb/chatdev",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.08979"
    },
    "5e681fd2374814e343b6a031a00e0904": {
        "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents",
        "authors": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11132",
        "abstract": "In this past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and people are starting to explore the usage of LLMs in more general and close to application domains like code generation, travel planning, and robot controls. Connecting these LLMs with great capacity and external tools, people are building the so-called LLM agents, which are supposed to help people do all kinds of work in everyday life. In all these domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering has become an important question for many researchers and users of LLMs. In this paper, we propose a novel method, \\textsc{RePrompt}, which does &#34;gradient descent&#34; to optimize the step-by-step instructions in the prompt of the LLM agents based on the chat history obtained from interactions with LLM agents. By optimizing the prompt, the LLM will learn how to plan in specific domains. We have used experiments in PDDL generation and travel planning to show that our method could generally improve the performance for different reasoning tasks when using the updated prompt as the initial prompt.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2406.11132"
    },
    "5af58f701b7864f75b5d42de686ee234": {
        "title": "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector",
        "authors": [
            "Xiaoxue Cheng",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Hongzhi Zhang",
            "Fuzheng Zhang",
            "Di Zhang",
            "Kun Gai",
            "Ji-Rong Wen"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11277",
        "abstract": "Hallucination detection is a challenging task for large language models (LLMs), and existing studies heavily rely on powerful closed-source LLMs such as GPT-4. In this paper, we propose an autonomous LLM-based agent framework, called HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat 7B) to actively select suitable tools for detecting multiple hallucination types such as text, code, and mathematical expression. In HaluAgent, we integrate the LLM, multi-functional toolbox, and design a fine-grained three-stage detection framework along with memory mechanism. To facilitate the effectiveness of HaluAgent, we leverage existing Chinese and English datasets to synthesize detection trajectories for fine-tuning, which endows HaluAgent with the capability for bilingual hallucination detection. Extensive experiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent can perform hallucination detection on various types of tasks and datasets, achieving performance comparable to or even higher than GPT-4 without tool enhancements on both in-domain and out-of-domain datasets. We release our dataset and code at https://github.com/RUCAIBox/HaluAgent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.11277"
    },
    "223160a127cf70c4accb671a891d7640": {
        "title": "Input Conditioned Graph Generation for Language Agents",
        "authors": [
            "Lukas Vierling",
            "Jie Fu",
            "Kai Chen"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11555",
        "abstract": "Recent progress in Large Language Models (LLMs) and language agents has demonstrated significant promise for various future applications across multiple disciplines. While traditional approaches to language agents often rely on fixed, handcrafted designs, our research aims to develop both learnable and dynamic agents. Our method uses an existing framework that abstracts language agents as graphs. Within this graph framework, we aim to learn a model that can generate edges for every given input to the language agent. This allows us to generate edges that represent the flow of communication within the graph based on the given input, thereby adjusting the internal communication of a language agent. We learn to generate these edges using a pretrained LLM that is fine-tuned with reinforcement learning. This LLM can be fine-tuned on several datasets simultaneously, and we hypothesize that the model learns to adapt to these different domains during training, achieving good overall performance when encountering data from different domains during deployment. We demonstrate that our approach surpasses the previous static approach by nearly 6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when trained with a sparsity-inducing loss. It also performs superior in additional experiments conducted with the MMLU and Mini Crossword Puzzles datasets. The code is available at https://github.com/lukasVierling/DynamicGPTSwarm.",
        "code": "https://github.com/lukasvierling/dynamicgptswarm",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.11555"
    },
    "fad605b82f8013fe20fdbd2e307bed36": {
        "title": "HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing",
        "authors": [
            "Jing Chen",
            "Xinyu Zhu",
            "Cheng Yang",
            "Chufan Shi",
            "Yadong Xi",
            "Yuxiang Zhang",
            "Junjie Wang",
            "Jiashu Pu",
            "Rongsheng Zhang",
            "Yujiu Yang",
            "Tian Feng"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11683",
        "abstract": "Generative AI has demonstrated unprecedented creativity in the field of computer vision, yet such phenomena have not been observed in natural language processing. In particular, large language models (LLMs) can hardly produce written works at the level of human experts due to the extremely high complexity of literature writing. In this paper, we present HoLLMwood, an automated framework for unleashing the creativity of LLMs and exploring their potential in screenwriting, which is a highly demanding task. Mimicking the human creative process, we assign LLMs to different roles involved in the real-world scenario. In addition to the common practice of treating LLMs as ${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing feedback and revision advice to ${Writer}$. Besides, to enrich the characters and deepen the plots, we introduce a role-playing mechanism and adopt LLMs as ${Actors}$ that can communicate and interact with each other. Evaluations on automatically generated screenplays show that HoLLMwood substantially outperforms strong baselines in terms of coherence, relevance, interestingness and overall quality.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.11683"
    },
    "4abb5f3980169817507e510eaf75db0c": {
        "title": "Improving Multi-Agent Debate with Sparse Communication Topology",
        "authors": [
            "Yunxuan Li",
            "Yibing Du",
            "Jiageng Zhang",
            "Le Hou",
            "Peter Grabowski",
            "Yeqing Li",
            "Eugene Ie"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11776",
        "abstract": "Multi-agent debate has proven effective in improving large language models quality for reasoning and factuality tasks. While various role-playing strategies in multi-agent debates have been explored, in terms of the communication among agents, existing approaches adopt a brute force algorithm -- each agent can communicate with all other agents. In this paper, we systematically investigate the effect of communication connectivity in multi-agent systems. Our experiments on GPT and Mistral models reveal that multi-agent debates leveraging sparse communication topology can achieve comparable or superior performance while significantly reducing computational costs. Furthermore, we extend the multi-agent debate framework to multimodal reasoning and alignment labeling tasks, showcasing its broad applicability and effectiveness. Our findings underscore the importance of communication connectivity on enhancing the efficiency and effectiveness of the &#34;society of minds&#34; approach.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.11776"
    },
    "b3f197dd580c7d2034936bda2f25387f": {
        "title": "Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey",
        "authors": [
            "Bowen Jiang",
            "Yangxinyu Xie",
            "Xiaomeng Wang",
            "Weijie J. Su",
            "Camillo J. Taylor",
            "Tanwi Mallick"
        ],
        "date": "2024/06/01",
        "pdf": "http://arxiv.org/pdf/2406.00252",
        "abstract": "Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at https://github.com/bowen-upenn/MMMA_Rationality.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2406.00252"
    },
    "2f0f904566cf4ea8ccfc9852638da7e0": {
        "title": "The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games",
        "authors": [
            "Mikhail Mozikov",
            "Nikita Severin",
            "Valeria Bodishtianu",
            "Maria Glushanina",
            "Mikhail Baklashkin",
            "Andrey V. Savchenko",
            "Ilya Makarov"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03299",
        "abstract": "Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions. In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger&#39; emotion, can disrupt the &#34;superhuman&#34; alignment of GPT-4, resembling human emotional responses.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2406.03299"
    },
    "880773299ddd850765aa2c59274c2e3f": {
        "title": "Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering",
        "authors": [
            "Yanming Liu",
            "Xinyue Peng",
            "Yuwei Zhang",
            "Jiannan Cao",
            "Xuhong Zhang",
            "Sheng Cheng",
            "Xun Wang",
            "Jianwei Yin",
            "Tianyu Du"
        ],
        "date": "2024/06/06",
        "pdf": "http://arxiv.org/pdf/2406.03807",
        "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method.",
        "code": "https://github.com/OceannTwT/Tool-Planner",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2406.03807"
    },
    "f5919d5d232d083271739eeed87bcf02": {
        "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments",
        "authors": [
            "Zhiheng Xi",
            "Yiwen Ding",
            "Wenxiang Chen",
            "Boyang Hong",
            "Honglin Guo",
            "Junzhe Wang",
            "Dingwen Yang",
            "Chenyang Liao",
            "Xin Guo",
            "Wei He",
            "Songyang Gao",
            "Lu Chen",
            "Rui Zheng",
            "Yicheng Zou",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "date": "2024/06/06",
        "pdf": "http://arxiv.org/pdf/2406.04151",
        "abstract": "Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.",
        "code": "https://github.com/woooodyy/agentgym",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2406.04151"
    },
    "0b295dba3c25b4491625f535f115ef76": {
        "title": "A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components",
        "authors": [
            "Xinzhe Li"
        ],
        "date": "2024/06/09",
        "pdf": "http://arxiv.org/pdf/2406.05804",
        "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated frameworks for developing LLM-based agents. However, the complexity of these frameworks r poses a hurdle for nuanced differentiation at a granular level, a critical aspect for enabling efficient implementations across different frameworks and fostering future research. Hence, the primary purpose of this survey is to facilitate a cohesive understanding of diverse recently proposed frameworks by identifying common workflows and reusable LLM-Profiled Components (LMPCs).",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2406.05804"
    }
}