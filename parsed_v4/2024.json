{
    "5b608143a5925bfbbcf579346d04fa2e": {
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
        "authors": [
            "Evan Hubinger",
            "Carson Denison",
            "Jesse Mu",
            "Mike Lambert",
            "Meg Tong",
            "Monte MacDiarmid",
            "Tamera Lanham",
            "Daniel M. Ziegler",
            "Tim Maxwell",
            "Newton Cheng",
            "Adam Jermyn",
            "Amanda Askell",
            "Ansh Radhakrishnan",
            "Cem Anil",
            "David Duvenaud",
            "Deep Ganguli",
            "Fazl Barez",
            "Jack Clark",
            "Kamal Ndousse",
            "Kshitij Sachan",
            "Michael Sellitto",
            "Mrinank Sharma",
            "Nova DasSarma",
            "Roger Grosse",
            "Shauna Kravec",
            "Yuntao Bai",
            "Zachary Witten",
            "Marina Favaro",
            "Jan Brauner",
            "Holden Karnofsky",
            "Paul Christiano",
            "Samuel R. Bowman",
            "Logan Graham",
            "Jared Kaplan",
            "SÃ¶ren Mindermann",
            "Ryan Greenblatt",
            "Buck Shlegeris",
            "Nicholas Schiefer",
            "Ethan Perez"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05566.pdf",
        "abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.05566"
    },
    "e84680c3461c7d5e66d91a05c35cb6c9": {
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "authors": [
            "Dennis Ulmer",
            "Elman Mansimov",
            "Kaixiang Lin",
            "Justin Sun",
            "Xibin Gao",
            "Yi Zhang"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05033.pdf",
        "abstract": "Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via &#34;self-talk&#34; of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.05033"
    },
    "5b15ebb969f67edb0bff7a691cf357d6": {
        "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models",
        "authors": [
            "Na Liu",
            "Liangyu Chen",
            "Xiaoyu Tian",
            "Wei Zou",
            "Kaijiang Chen",
            "Ming Cui"
        ],
        "date": "2024/01/05",
        "pdf": "http://arxiv.org/pdf/2401.02777.pdf",
        "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.02777"
    },
    "4da8b341d1dc74b5c91c82a8b8332fe3": {
        "title": "AUTOACT: Automatic Agent Learning from Scratch via Self-Planning",
        "authors": [
            "Shuofei Qiao",
            "Ningyu Zhang",
            "Runnan Fang",
            "Yujie Luo",
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Chengfei Lv",
            "Huajun Chen"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05268.pdf",
        "abstract": "Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent. Code will be available at https://github.com/zjunlp/AutoAct.",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2401.05268"
    },
    "ca294c5bb4a93752040084e62a9bedc9": {
        "title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
        "authors": [
            "Quan Tu",
            "Shilong Fan",
            "Zihang Tian",
            "Rui Yan"
        ],
        "date": "2024/01/02",
        "pdf": "http://arxiv.org/pdf/2401.01275.pdf",
        "abstract": "Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2401.01275"
    },
    "0c56afc81fe63849e15821130b7b8562": {
        "title": "AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models",
        "authors": [
            "Zihong He",
            "Changwang Zhang"
        ],
        "date": "2024/01/05",
        "pdf": "http://arxiv.org/pdf/2401.02870.pdf",
        "abstract": "The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents&#39; preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, sensory perceptions and social networking with subjective information, wield the most pronounced influence on preference shaping. AFSPP can significantly enhance the efficiency and scope of psychological experiments, while yielding valuable insights for Trustworthy Artificial Intelligence research for strategies to prevent undesirable preference and personality development.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2401.02870"
    },
    "0f608fe3eaae4d7140d7e1b012544308": {
        "title": "MARG: Multi-Agent Review Generation for Scientific Papers",
        "authors": [
            "Mike D&#39;Arcy",
            "Tom Hope",
            "Larry Birnbaum",
            "Doug Downey"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.04259.pdf",
        "abstract": "We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.04259"
    },
    "6eb13c310b738a72ae549f50036d452d": {
        "title": "SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems",
        "authors": [
            "Dong Zhang",
            "Zhaowei Li",
            "Pengyu Wang",
            "Xin Zhang",
            "Yaqian Zhou",
            "Xipeng Qiu"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.03945.pdf",
        "abstract": "Human communication is a complex and diverse process that not only involves multiple factors such as language, commonsense, and cultural backgrounds but also requires the participation of multimodal information, such as speech. Large Language Model (LLM)-based multi-agent systems have demonstrated promising performance in simulating human society. Can we leverage LLM-based multi-agent systems to simulate human communication? However, current LLM-based multi-agent systems mainly rely on text as the primary medium. In this paper, we propose SpeechAgents, a multi-modal LLM based multi-agent system designed for simulating human communication. SpeechAgents utilizes multi-modal LLM as the control center for individual agent and employes multi-modal signals as the medium for exchanged messages among agents. Additionally, we propose Multi-Agent Tuning to enhance the multi-agent capabilities of LLM without compromising general abilities. To strengthen and evaluate the effectiveness of human communication simulation, we build the Human-Communication Simulation Benchmark. Experimental results demonstrate that SpeechAgents can simulate human communication dialogues with consistent content, authentic rhythm, and rich emotions and demonstrate excellent scalability even with up to 25 agents, which can apply to tasks such as drama creation and audio novels generation. Code and models will be open-sourced at https://github. com/0nutation/SpeechAgents",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.03945"
    },
    "b9ae12751107d3f49280fc6a22ba0d08": {
        "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
        "authors": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.03630.pdf",
        "abstract": "With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study how to solve MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on a slightly harder room map. We present our hypothesis of why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.03630"
    },
    "bdfaf34ffca3c335dfac368a6bd9439a": {
        "title": "Combating Adversarial Attacks with Multi-Agent Debate",
        "authors": [
            "Steffi Chern",
            "Zhen Fan",
            "Andy Liu"
        ],
        "date": "2024/01/11",
        "pdf": "http://arxiv.org/pdf/2401.05998.pdf",
        "abstract": "While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models&#39; susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.05998"
    },
    "815211b5ab3616d6d3d1fba384bca69a": {
        "title": "Agent Alignment in Evolving Social Norms",
        "authors": [
            "Shimin Li",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "date": "2024/01/09",
        "pdf": "http://arxiv.org/pdf/2401.04620.pdf",
        "abstract": "Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent possesses the capability to align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2401.04620"
    },
    "c7716e44d8d1c20e6e1fa9922dfc75d7": {
        "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents",
        "authors": [
            "Ke Yang",
            "Jiateng Liu",
            "John Wu",
            "Chaoqi Yang",
            "Yi R. Fung",
            "Sha Li",
            "Zixuan Huang",
            "Xu Cao",
            "Xingyao Wang",
            "Yiquan Wang",
            "Heng Ji",
            "Chengxiang Zhai"
        ],
        "date": "2024/01/01",
        "pdf": "http://arxiv.org/pdf/2401.00812.pdf",
        "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs&#39; training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2401.00812"
    },
    "039db49f2e19de35f5de8b42670347a3": {
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "authors": [
            "Boyuan Zheng",
            "Boyu Gou",
            "Jihyung Kil",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2024/01/03",
        "pdf": "http://arxiv.org/pdf/2401.01614.pdf",
        "abstract": "The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML text and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2401.01614"
    },
    "c5e9a179be09dac22812b7f097407e07": {
        "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval",
        "authors": [
            "He Zhu",
            "Wenjia Zhang",
            "Nuoxian Huang",
            "Boyang Li",
            "Luyao Niu",
            "Zipei Fan",
            "Tianle Lun",
            "Yicheng Tao",
            "Junyou Su",
            "Zhaoya Gong",
            "Chenyu Fang",
            "Xing Liu"
        ],
        "date": "2024/02/29",
        "pdf": "http://arxiv.org/pdf/2402.19273.pdf",
        "abstract": "In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.19273"
    },
    "bb03d9d04304fb8cf361615291a5e13d": {
        "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
        "authors": [
            "Chenglei Shen",
            "Guofu Xie",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "date": "2024/02/29",
        "pdf": "http://arxiv.org/pdf/2402.18807.pdf",
        "abstract": "Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\\&amp;$exploitation trade-off ability, reasoning ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4. Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs. These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.18807"
    },
    "2b67bb3cf1de46c69a656f1dd20fd49a": {
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "authors": [
            "Roberto Gallotta",
            "Graham Todd",
            "Marvin Zammit",
            "Sam Earle",
            "Antonios Liapis",
            "Julian Togelius",
            "Georgios N. Yannakakis"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18659.pdf",
        "abstract": "Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.18659"
    },
    "aaf7b78ec6f39f72b41938c6a0519e85": {
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "authors": [
            "Adyasha Maharana",
            "Dong-Ho Lee",
            "Sergey Tulyakov",
            "Mohit Bansal",
            "Francesco Barbieri",
            "Yuwei Fang"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17753.pdf",
        "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.17753"
    },
    "8e89988cfaaa29803f9c446bc8ee3eaf": {
        "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
        "authors": [
            "Ruiyang Ren",
            "Peng Qiu",
            "Yingqi Qu",
            "Jing Liu",
            "Wayne Xin Zhao",
            "Hua Wu",
            "Ji-Rong Wen",
            "Haifeng Wang"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17505.pdf",
        "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval. Our code and data will be publicly released soon.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.17505"
    },
    "ab05a8f2b05390e1cbe83e0b06b55b43": {
        "title": "Benchmarking Data Science Agents",
        "authors": [
            "Yuge Zhang",
            "Qiyang Jiang",
            "Xingyu Han",
            "Nan Chen",
            "Yuqing Yang",
            "Kan Ren"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17168.pdf",
        "abstract": "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",
        "code": "",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2402.17168"
    },
    "8baca9839158cad236aad1235f6695c7": {
        "title": "RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation",
        "authors": [
            "Qinyu Luo",
            "Yining Ye",
            "Shihao Liang",
            "Zhong Zhang",
            "Yujia Qin",
            "Yaxi Lu",
            "Yesai Wu",
            "Xin Cong",
            "Yankai Lin",
            "Yingli Zhang",
            "Xiaoyin Che",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16667.pdf",
        "abstract": "Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.",
        "code": "",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2402.16667"
    },
    "4c8c556d314a7cbb7541e472f63cbccd": {
        "title": "Language Agents as Optimizable Graphs",
        "authors": [
            "Mingchen Zhuge",
            "Wenyi Wang",
            "Louis Kirsch",
            "Francesco Faccio",
            "Dmitrii Khizbullin",
            "JÃ¼rgen Schmidhuber"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16823.pdf",
        "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.",
        "code": "https://github.com/metauto-ai/gptswarm",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.16823"
    },
    "6edba424f625da489f6bcf7dac4d5a57": {
        "title": "Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation",
        "authors": [
            "Xinyi Mou",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16333.pdf",
        "abstract": "Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.16333"
    },
    "a5538ed0a6964ec13a4ebdd94f383ecc": {
        "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning",
        "authors": [
            "Jianguo Zhang",
            "Tian Lan",
            "Rithesh Murthy",
            "Zhiwei Liu",
            "Weiran Yao",
            "Juntao Tan",
            "Thai Hoang",
            "Liangwei Yang",
            "Yihao Feng",
            "Zuxin Liu",
            "Tulika Awalgaonkar",
            "Juan Carlos Niebles",
            "Silvio Savarese",
            "Shelby Heinecke",
            "Huan Wang",
            "Caiming Xiong"
        ],
        "date": "2024/02/23",
        "pdf": "http://arxiv.org/pdf/2402.15506.pdf",
        "abstract": "Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address these challenges. \\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.15506"
    },
    "f9329da7eacdad837ccd68b129b853c6": {
        "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
        "authors": [
            "Andrew Brown",
            "Jiading Zhu",
            "Mohamed Abdelwahab",
            "Alec Dong",
            "Cindy Wang",
            "Jonathan Rose"
        ],
        "date": "2024/02/01",
        "pdf": "http://arxiv.org/pdf/2402.01051",
        "abstract": "Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.01051"
    },
    "fcabab00a378e1841f69e2241deddfa9": {
        "title": "Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",
        "authors": [
            "Pouya Pezeshkpour",
            "Eser Kandogan",
            "Nikita Bhutani",
            "Sajjadur Rahman",
            "Tom Mitchell",
            "Estevam Hruschka"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01108",
        "abstract": "Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration of constraints during optimization and establish connections among different components within the system, which also enable a more holistic and comprehensive approach to evaluation. We present a formal definition of reasoning capacity and illustrate its utility in identifying limitations within each component of the system. We then argue how these limitations can be addressed with a self-reflective process wherein human-feedback is used to alleviate shortcomings in reasoning and enhance overall consistency of the system.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.01108"
    },
    "426a7d04fbea7fc6a79285e5550740b2": {
        "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
        "authors": [
            "Wenyue Hua",
            "Xianjun Yang",
            "Zelong Li",
            "Wei Cheng",
            "Yongfeng Zhang"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01586",
        "abstract": "The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent&#39;s safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model&#39;s reasoning ability and its efficacy as a safe agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01586"
    },
    "88881d9d9219e2d3a274466dfd101760": {
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Tinghui Zhu",
            "Renze Lou",
            "Yuandong Tian",
            "Yanghua Xiao",
            "Yu Su"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01622",
        "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.",
        "code": "https://github.com/OSU-NLP-Group/TravelPlanner",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.01622"
    },
    "8447f702715c2b57c8891a3bac8e010e": {
        "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
        "authors": [
            "Yuncheng Hua",
            "Lizhen Qu",
            "Gholamreza Haffari"
        ],
        "date": "2024/01/29",
        "pdf": "http://arxiv.org/pdf/2402.01737",
        "abstract": "In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01737"
    },
    "8ed6f5a24a793642920666f6e4727b5b": {
        "title": "LLMs Simulate Big Five Personality Traits: Further Evidence",
        "authors": [
            "Aleksandra Sorokovikova",
            "Natalia Fedorova",
            "Sharwin Rezagholi",
            "Ivan P. Yamshchikov"
        ],
        "date": "2024/01/31",
        "pdf": "http://arxiv.org/pdf/2402.01765",
        "abstract": "An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01765"
    },
    "b301607d40a29aa568b9e9de00343499": {
        "title": "Executable Code Actions Elicit Better LLM Agents",
        "authors": [
            "Xingyao Wang",
            "Yangyi Chen",
            "Lifan Yuan",
            "Yizhe Zhang",
            "Yunzhu Li",
            "Hao Peng",
            "Heng Ji"
        ],
        "date": "2024/02/01",
        "pdf": "http://arxiv.org/pdf/2402.01030",
        "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents&#39; actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01030"
    },
    "dbbad9b76945033370d64c196857dcc8": {
        "title": "NavHint: Vision and Language Navigation Agent with a Hint Generator",
        "authors": [
            "Yue Zhang",
            "Quan Guo",
            "Parisa Kordjamshidi"
        ],
        "date": "2024/02/04",
        "pdf": "http://arxiv.org/pdf/2402.02559",
        "abstract": "Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent&#39;s attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the interpretability of the agent&#39;s actions.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02559"
    },
    "a619330cab01491ca0129cc7efed892c": {
        "title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
        "authors": [
            "Ivar Frisch",
            "Mario Giulianelli"
        ],
        "date": "2024/02/05",
        "pdf": "http://arxiv.org/pdf/2402.02896",
        "abstract": "While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02896"
    },
    "485502588a1d88b1dff5929f92c33d9c": {
        "title": "Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies",
        "authors": [
            "Zhixuan Chu",
            "Yan Wang",
            "Feng Zhu",
            "Lu Yu",
            "Longfei Li",
            "Jinjie Gu"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.03628",
        "abstract": "The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.03628"
    },
    "90cf304f11ecd2cb08a276a70b0118a3": {
        "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls",
        "authors": [
            "Yu Du",
            "Fangyun Wei",
            "Hongyang Zhang"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.04253",
        "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.",
        "code": "https://github.com/dyabel/anytool",
        "category": [
            "Feedback&Reflection",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.04253"
    },
    "4874c0c0abd9411762e846540adcb6c1": {
        "title": "More Agents Is All You Need",
        "authors": [
            "Junyou Li",
            "Qin Zhang",
            "Yangbin Yu",
            "Qiang Fu",
            "Deheng Ye"
        ],
        "date": "2024/02/03",
        "pdf": "http://arxiv.org/pdf/2402.05120.pdf",
        "abstract": "We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.05120"
    },
    "a15433aff8c528c636484d5f5da5010d": {
        "title": "Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients",
        "authors": [
            "Mahyar Abbasian",
            "Zhongqi Yang",
            "Elahe Khatibi",
            "Pengfei Zhang",
            "Nitish Nagesh",
            "Iman Azimi",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "date": "2024/02/15",
        "pdf": "http://arxiv.org/pdf/2402.10153",
        "abstract": "Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.10153"
    },
    "13b82b1326a95c1678d2892563c13ae3": {
        "title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation",
        "authors": [
            "Yaoxiang Wang",
            "Zhiyong Wu",
            "Junfeng Yao",
            "Jinsong Su"
        ],
        "date": "2024/02/15",
        "pdf": "http://arxiv.org/pdf/2402.10178",
        "abstract": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents&#39; abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.10178"
    },
    "a89feebbb8979ccaff7b3590a33af966": {
        "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
        "authors": [
            "Junjie Ye",
            "Sixian Li",
            "Guanyu Li",
            "Caishuang Huang",
            "Songyang Gao",
            "Yilong Wu",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "date": "2024/02/16",
        "pdf": "http://arxiv.org/pdf/2402.10753",
        "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in https://github.com/Junjie-Ye/ToolSword.",
        "code": "https://github.com/junjie-ye/toolsword",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.10753"
    },
    "7d840205c5d5d0c8418e1a6c9032503a": {
        "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
        "authors": [
            "Ziru Chen",
            "Michael White",
            "Raymond Mooney",
            "Ali Payani",
            "Yu Su",
            "Huan Sun"
        ],
        "date": "2024/02/16",
        "pdf": "http://arxiv.org/pdf/2402.10890",
        "abstract": "In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs&#39; discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data will be released at https://github.com/OSU-NLP-Group/llm-planning-eval.",
        "code": "https://github.com/osu-nlp-group/llm-planning-eval",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.10890"
    },
    "0c5062abf0c98c49de2bd6bec7500db3": {
        "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Yang Song",
            "Chen Zhu",
            "Hengshu Zhu",
            "Ji-Rong Wen"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11163",
        "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
        "code": "",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2402.11163"
    },
    "5eff26e408b6a81fe444f59554a196e4": {
        "title": "Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima",
        "authors": [
            "Shu Yang",
            "Lijie Hu",
            "Lu Yu",
            "Muhammad Asif Ali",
            "Di Wang"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11271",
        "abstract": "The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of autophagic (&#34;self-consumption&#34;) loops to account for the suppression of human-generated information in the exchange of information between humans and AI systems. We generalize the declining diversity of social information and the bottleneck in model performance caused by the above trends to the local optima of large models.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11271"
    },
    "91668d0b0eba8cee741a474aa4cbe16a": {
        "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
        "authors": [
            "Siyuan Wang",
            "Zhuohan Long",
            "Zhihao Fan",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11443",
        "abstract": "This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models&#39; capabilities. Besides, our framework widens performance discrepancies both between different models and within the same model across various tasks, facilitating more informed model selection for specific tasks (Code and data are available at https://github.com/NanshineLoong/Self-Evolving-Benchmark).",
        "code": "https://github.com/nanshineloong/self-evolving-benchmark",
        "category": [
            "Multi-Agent System",
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2402.11443"
    },
    "71b8888021fefef765ee9189114ebc03": {
        "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
        "authors": [
            "Yubo Ma",
            "Zhibin Gou",
            "Junheng Hao",
            "Ruochen Xu",
            "Shuohang Wang",
            "Liangming Pan",
            "Yujiu Yang",
            "Yixin Cao",
            "Aixin Sun",
            "Hany Awadalla",
            "Weizhu Chen"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11451",
        "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs&#39; abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11451"
    },
    "80a2c8c973d3216b19515f53d4911fcd": {
        "title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization",
        "authors": [
            "Zhiyu Yang",
            "Zihan Zhou",
            "Shuo Wang",
            "Xin Cong",
            "Xu Han",
            "Yukun Yan",
            "Zhenghao Liu",
            "Zhixing Tan",
            "Pengyuan Liu",
            "Dong Yu",
            "Zhiyuan Liu",
            "Xiaodong Shi",
            "Maosong Sun"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11453",
        "abstract": "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.",
        "code": "",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2402.11453"
    },
    "8efb45b8104c964a69dec8eb8a385013": {
        "title": "What&#39;s the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs",
        "authors": [
            "Eran Hirsch",
            "Guy Uziel",
            "Ateret Anaby-Tavor"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11489",
        "abstract": "Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.11489"
    },
    "f6cf3f3fc3c1eb72a0cce8a84f4b1cd0": {
        "title": "PreAct: Predicting Future in ReAct Enhances Agent&#39;s Planning Ability",
        "authors": [
            "Dayuan Fu",
            "Jianzhao Huang",
            "Siyuan Lu",
            "Guanting Dong",
            "Yejie Wang",
            "Keqing He",
            "Weiran Xu"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11534",
        "abstract": "Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\\textbf{PreAct}$, an agent framework that integrates $\\textbf{pre}$diction with $\\textbf{rea}$soning and $\\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.11534"
    },
    "7e1da3043b5d4208fd9b19f042d48f1f": {
        "title": "LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration",
        "authors": [
            "Jun Zhao",
            "Can Zu",
            "Hao Xu",
            "Yi Lu",
            "Wei He",
            "Yiwen Ding",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11550",
        "abstract": "Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \\textit{lost in the middle}. In this paper, we propose \\textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members&#39; hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an \\textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that \\textsc{LongAgent} offers a promising alternative for long-text processing. The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.11550"
    },
    "645c1394ff4bc534034d1906961852ca": {
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
        "authors": [
            "Renxi Wang",
            "Haonan Li",
            "Xudong Han",
            "Yixuan Zhang",
            "Timothy Baldwin"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11651",
        "abstract": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. Experimental results demonstrate that compared to solely using positive examples, incorporating negative examples enhances model performance by a large margin.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.11651"
    },
    "46c43356d6af4d363b0d08678696cea6": {
        "title": "Large Language Models as Agents in Two-Player Games",
        "authors": [
            "Yang Liu",
            "Peng Sun",
            "Hang Li"
        ],
        "date": "2024/02/12",
        "pdf": "http://arxiv.org/pdf/2402.08078",
        "abstract": "By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08078"
    },
    "fb31c87283351112060f2a7cbc8d0fe7": {
        "title": "Large Language Models as Minecraft Agents",
        "authors": [
            "Chris Madge",
            "Massimo Poesio"
        ],
        "date": "2024/02/13",
        "pdf": "http://arxiv.org/pdf/2402.08392",
        "abstract": "In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08392"
    },
    "da5cb2cc846f66b14b41853d5ffe1299": {
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "authors": [
            "Xiangming Gu",
            "Xiaosen Zheng",
            "Tianyu Pang",
            "Chao Du",
            "Qian Liu",
            "Ye Wang",
            "Jing Jiang",
            "Min Lin"
        ],
        "date": "2024/02/13",
        "pdf": "http://arxiv.org/pdf/2402.08567",
        "abstract": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.",
        "code": "https://github.com/sail-sg/agent-smith",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08567"
    },
    "cc4a576822918876acbf397bee723c62": {
        "title": "Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications",
        "authors": [
            "Negar Arabzadeh",
            "Julia Kiseleva",
            "Qingyun Wu",
            "Chi Wang",
            "Ahmed Awadallah",
            "Victor Dibia",
            "Adam Fourney",
            "Charles Clarke"
        ],
        "date": "2024/02/14",
        "pdf": "http://arxiv.org/pdf/2402.09015",
        "abstract": "The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application&#39;s functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of quantifier&#39;s work.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.09015"
    },
    "ac40351f97fee2d30d652151eea8c078": {
        "title": "InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory",
        "authors": [
            "Chaojun Xiao",
            "Pengle Zhang",
            "Xu Han",
            "Guangxuan Xiao",
            "Yankai Lin",
            "Zhengyan Zhang",
            "Zhiyuan Liu",
            "Song Han",
            "Maosong Sun"
        ],
        "date": "2024/02/07",
        "pdf": "http://arxiv.org/pdf/2402.04617",
        "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining the ability to capture long-distance dependencies. Without any training, InfLLM enables LLMs pre-trained on sequences of a few thousand tokens to achieve superior performance than competitive baselines continually training these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.04617"
    },
    "062ae31f680adf5ec6f03a536d620444": {
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "authors": [
            "Farhad Moghimifar",
            "Yuan-Fang Li",
            "Robert Thomson",
            "Gholamreza Haffari"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11712",
        "abstract": "Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between political parties and predict the outcomes. We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.11712"
    },
    "1243ff5fb9ecf0de62aacd39f8af85c0": {
        "title": "Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations",
        "authors": [
            "Nuo Chen",
            "Hongguang Li",
            "Juhua Huang",
            "Baoyuan Wang",
            "Jia Li"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.11975",
        "abstract": "Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a &#39;&#39;One-for-All&#39;&#39; approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY&#39;s superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences. Our codes are available at https://github.com/nuochenpku/COMEDY.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.11975"
    }
}