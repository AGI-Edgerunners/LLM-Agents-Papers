{
    "5b608143a5925bfbbcf579346d04fa2e": {
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training",
        "authors": [
            "Evan Hubinger",
            "Carson Denison",
            "Jesse Mu",
            "Mike Lambert",
            "Meg Tong",
            "Monte MacDiarmid",
            "Tamera Lanham",
            "Daniel M. Ziegler",
            "Tim Maxwell",
            "Newton Cheng",
            "Adam Jermyn",
            "Amanda Askell",
            "Ansh Radhakrishnan",
            "Cem Anil",
            "David Duvenaud",
            "Deep Ganguli",
            "Fazl Barez",
            "Jack Clark",
            "Kamal Ndousse",
            "Kshitij Sachan",
            "Michael Sellitto",
            "Mrinank Sharma",
            "Nova DasSarma",
            "Roger Grosse",
            "Shauna Kravec",
            "Yuntao Bai",
            "Zachary Witten",
            "Marina Favaro",
            "Jan Brauner",
            "Holden Karnofsky",
            "Paul Christiano",
            "Samuel R. Bowman",
            "Logan Graham",
            "Jared Kaplan",
            "Sören Mindermann",
            "Ryan Greenblatt",
            "Buck Shlegeris",
            "Nicholas Schiefer",
            "Ethan Perez"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05566.pdf",
        "abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.05566"
    },
    "e84680c3461c7d5e66d91a05c35cb6c9": {
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "authors": [
            "Dennis Ulmer",
            "Elman Mansimov",
            "Kaixiang Lin",
            "Justin Sun",
            "Xibin Gao",
            "Yi Zhang"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05033.pdf",
        "abstract": "Large language models (LLMs) are powerful dialogue agents, but specializing them towards fulfilling a specific function can be challenging. Instructing tuning, i.e. tuning models on instruction and sample responses generated by humans (Ouyang et al., 2022), has proven as an effective method to do so, yet requires a number of data samples that a) might not be available or b) costly to generate. Furthermore, this cost increases when the goal is to make the LLM follow a specific workflow within a dialogue instead of single instructions. Inspired by the self-play technique in reinforcement learning and the use of LLMs to simulate human agents, we propose a more effective method for data collection through LLMs engaging in a conversation in various roles. This approach generates a training data via &#34;self-talk&#34; of LLMs that can be refined and utilized for supervised fine-tuning. We introduce an automated way to measure the (partial) success of a dialogue. This metric is used to filter the generated conversational data that is fed back in LLM for training. Based on our automated and human evaluations of conversation quality, we demonstrate that such self-talk data improves results. In addition, we examine the various characteristics that showcase the quality of generated dialogues and how they can be connected to their potential utility as training data.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.05033"
    },
    "5b15ebb969f67edb0bff7a691cf357d6": {
        "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models",
        "authors": [
            "Na Liu",
            "Liangyu Chen",
            "Xiaoyu Tian",
            "Wei Zou",
            "Kaijiang Chen",
            "Ming Cui"
        ],
        "date": "2024/01/05",
        "pdf": "http://arxiv.org/pdf/2401.02777.pdf",
        "abstract": "This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile conversational agents.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2401.02777"
    },
    "4da8b341d1dc74b5c91c82a8b8332fe3": {
        "title": "AUTOACT: Automatic Agent Learning from Scratch via Self-Planning",
        "authors": [
            "Shuofei Qiao",
            "Ningyu Zhang",
            "Runnan Fang",
            "Yujie Luo",
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Chengfei Lv",
            "Huajun Chen"
        ],
        "date": "2024/01/10",
        "pdf": "http://arxiv.org/pdf/2401.05268.pdf",
        "abstract": "Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. We even notice that AutoAct, when using the Llama-2-13b model, can achieve performance comparable to that of the GPT-3.5-Turbo agent. Code will be available at https://github.com/zjunlp/AutoAct.",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2401.05268"
    },
    "ca294c5bb4a93752040084e62a9bedc9": {
        "title": "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation",
        "authors": [
            "Quan Tu",
            "Shilong Fan",
            "Zihang Tian",
            "Rui Yan"
        ],
        "date": "2024/01/02",
        "pdf": "http://arxiv.org/pdf/2401.01275.pdf",
        "abstract": "Recently, the advent of large language models (LLMs) has revolutionized generative agents. Among them, Role-Playing Conversational Agents (RPCAs) attract considerable attention due to their ability to emotionally engage users. However, the absence of a comprehensive benchmark impedes progress in this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark for comprehensive RPCA assessment, complemented by a tailored high-quality dataset. The dataset comprises 1,785 multi-turn role-playing dialogues, encompassing 23,020 examples and featuring 77 characters derived from Chinese novels and scripts. It was carefully constructed, beginning with initial dialogue extraction via GPT-4, followed by rigorous human-led quality control, and enhanced with in-depth character profiles sourced from Baidu Baike. CharacterEval employs a multifaceted evaluation approach, encompassing thirteen targeted metrics on four dimensions. Comprehensive experiments on CharacterEval demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation. Source code, data source and reward model will be publicly accessible at https://github.com/morecry/CharacterEval.",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2401.01275"
    },
    "0c56afc81fe63849e15821130b7b8562": {
        "title": "AFSPP: Agent Framework for Shaping Preference and Personality with Large Language Models",
        "authors": [
            "Zihong He",
            "Changwang Zhang"
        ],
        "date": "2024/01/05",
        "pdf": "http://arxiv.org/pdf/2401.02870.pdf",
        "abstract": "The evolution of Large Language Models (LLMs) has introduced a new paradigm for investigating human behavior emulation. Recent research has employed LLM-based Agents to create a sociological research environment, in which agents exhibit behavior based on the unfiltered characteristics of large language models. However, these studies overlook the iterative development within a human-like setting - Human preferences and personalities are complex, shaped by various factors and subject to ongoing change as a result of environmental and subjective influences. In light of this observation, we propose Agent Framework for Shaping Preference and Personality (AFSPP), exploring the multifaceted impact of social networks and subjective consciousness on LLM-based Agents&#39; preference and personality formation. With AFSPP, we have, for the first time, successfully replicated several key findings from human personality experiments. And other AFSPP-based experimental results indicate that plan making, sensory perceptions and social networking with subjective information, wield the most pronounced influence on preference shaping. AFSPP can significantly enhance the efficiency and scope of psychological experiments, while yielding valuable insights for Trustworthy Artificial Intelligence research for strategies to prevent undesirable preference and personality development.",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2401.02870"
    },
    "0f608fe3eaae4d7140d7e1b012544308": {
        "title": "MARG: Multi-Agent Review Generation for Scientific Papers",
        "authors": [
            "Mike D&#39;Arcy",
            "Tom Hope",
            "Larry Birnbaum",
            "Doug Downey"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.04259.pdf",
        "abstract": "We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.04259"
    },
    "6eb13c310b738a72ae549f50036d452d": {
        "title": "SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems",
        "authors": [
            "Dong Zhang",
            "Zhaowei Li",
            "Pengyu Wang",
            "Xin Zhang",
            "Yaqian Zhou",
            "Xipeng Qiu"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.03945.pdf",
        "abstract": "Human communication is a complex and diverse process that not only involves multiple factors such as language, commonsense, and cultural backgrounds but also requires the participation of multimodal information, such as speech. Large Language Model (LLM)-based multi-agent systems have demonstrated promising performance in simulating human society. Can we leverage LLM-based multi-agent systems to simulate human communication? However, current LLM-based multi-agent systems mainly rely on text as the primary medium. In this paper, we propose SpeechAgents, a multi-modal LLM based multi-agent system designed for simulating human communication. SpeechAgents utilizes multi-modal LLM as the control center for individual agent and employes multi-modal signals as the medium for exchanged messages among agents. Additionally, we propose Multi-Agent Tuning to enhance the multi-agent capabilities of LLM without compromising general abilities. To strengthen and evaluate the effectiveness of human communication simulation, we build the Human-Communication Simulation Benchmark. Experimental results demonstrate that SpeechAgents can simulate human communication dialogues with consistent content, authentic rhythm, and rich emotions and demonstrate excellent scalability even with up to 25 agents, which can apply to tasks such as drama creation and audio novels generation. Code and models will be open-sourced at https://github. com/0nutation/SpeechAgents",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.03945"
    },
    "b9ae12751107d3f49280fc6a22ba0d08": {
        "title": "Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet",
        "authors": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "date": "2024/01/08",
        "pdf": "http://arxiv.org/pdf/2401.03630.pdf",
        "abstract": "With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study how to solve MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on a slightly harder room map. We present our hypothesis of why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.03630"
    },
    "bdfaf34ffca3c335dfac368a6bd9439a": {
        "title": "Combating Adversarial Attacks with Multi-Agent Debate",
        "authors": [
            "Steffi Chern",
            "Zhen Fan",
            "Andy Liu"
        ],
        "date": "2024/01/11",
        "pdf": "http://arxiv.org/pdf/2401.05998.pdf",
        "abstract": "While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models&#39; susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2401.05998"
    },
    "815211b5ab3616d6d3d1fba384bca69a": {
        "title": "Agent Alignment in Evolving Social Norms",
        "authors": [
            "Shimin Li",
            "Tianxiang Sun",
            "Xipeng Qiu"
        ],
        "date": "2024/01/09",
        "pdf": "http://arxiv.org/pdf/2401.04620.pdf",
        "abstract": "Agents based on Large Language Models (LLMs) are increasingly permeating various domains of human production and life, highlighting the importance of aligning them with human values. The current alignment of AI systems primarily focuses on passively aligning LLMs through human intervention. However, agents possess characteristics like receiving environmental feedback and self-evolution, rendering the LLM alignment methods inadequate. In response, we propose an evolutionary framework for agent evolution and alignment, named EvolutionaryAgent, which transforms agent alignment into a process of evolution and selection under the principle of survival of the fittest. In an environment where social norms continuously evolve, agents better adapted to the current social norms will have a higher probability of survival and proliferation, while those inadequately aligned dwindle over time. Experimental results assessing the agents from multiple perspectives in aligning with social norms demonstrate that EvolutionaryAgent possesses the capability to align progressively better with the evolving social norms while maintaining its proficiency in general tasks. Effectiveness tests conducted on various open and closed-source LLMs as the foundation for agents also prove the applicability of our approach.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2401.04620"
    },
    "c7716e44d8d1c20e6e1fa9922dfc75d7": {
        "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents",
        "authors": [
            "Ke Yang",
            "Jiateng Liu",
            "John Wu",
            "Chaoqi Yang",
            "Yi R. Fung",
            "Sha Li",
            "Zixuan Huang",
            "Xu Cao",
            "Xingyao Wang",
            "Yiquan Wang",
            "Heng Ji",
            "Chengxiang Zhai"
        ],
        "date": "2024/01/01",
        "pdf": "http://arxiv.org/pdf/2401.00812.pdf",
        "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs&#39; training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code.",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2401.00812"
    },
    "039db49f2e19de35f5de8b42670347a3": {
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "authors": [
            "Boyuan Zheng",
            "Boyu Gou",
            "Jihyung Kil",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2024/01/03",
        "pdf": "http://arxiv.org/pdf/2401.01614.pdf",
        "abstract": "The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML text and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2401.01614"
    },
    "c5e9a179be09dac22812b7f097407e07": {
        "title": "PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval",
        "authors": [
            "He Zhu",
            "Wenjia Zhang",
            "Nuoxian Huang",
            "Boyang Li",
            "Luyao Niu",
            "Zipei Fan",
            "Tianle Lun",
            "Yicheng Tao",
            "Junyou Su",
            "Zhaoya Gong",
            "Chenyu Fang",
            "Xing Liu"
        ],
        "date": "2024/02/29",
        "pdf": "http://arxiv.org/pdf/2402.19273.pdf",
        "abstract": "In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.19273"
    },
    "bb03d9d04304fb8cf361615291a5e13d": {
        "title": "On the Decision-Making Abilities in Role-Playing using Large Language Models",
        "authors": [
            "Chenglei Shen",
            "Guofu Xie",
            "Xiao Zhang",
            "Jun Xu"
        ],
        "date": "2024/02/29",
        "pdf": "http://arxiv.org/pdf/2402.18807.pdf",
        "abstract": "Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\\&amp;$exploitation trade-off ability, reasoning ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4. Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs. These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.18807"
    },
    "2b67bb3cf1de46c69a656f1dd20fd49a": {
        "title": "Large Language Models and Games: A Survey and Roadmap",
        "authors": [
            "Roberto Gallotta",
            "Graham Todd",
            "Marvin Zammit",
            "Sam Earle",
            "Antonios Liapis",
            "Julian Togelius",
            "Georgios N. Yannakakis"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18659.pdf",
        "abstract": "Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.18659"
    },
    "aaf7b78ec6f39f72b41938c6a0519e85": {
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "authors": [
            "Adyasha Maharana",
            "Dong-Ho Lee",
            "Sergey Tulyakov",
            "Mohit Bansal",
            "Francesco Barbieri",
            "Yuwei Fang"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17753.pdf",
        "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a comprehensive evaluation benchmark to measure long-term memory in models, encompassing question answering, event summarization, and multi-modal dialogue generation tasks. Our experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. Employing strategies like long-context LLMs or RAG can offer improvements but these models still substantially lag behind human performance.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.17753"
    },
    "8e89988cfaaa29803f9c446bc8ee3eaf": {
        "title": "BASES: Large-scale Web Search User Simulation with Large Language Model based Agents",
        "authors": [
            "Ruiyang Ren",
            "Peng Qiu",
            "Yingqi Qu",
            "Jing Liu",
            "Wayne Xin Zhao",
            "Hua Wu",
            "Ji-Rong Wen",
            "Haifeng Wang"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17505.pdf",
        "abstract": "Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new large-scale dataset encompassing web search user behaviors, including both Chinese and English versions, which can greatly bolster research in the field of information retrieval. Our code and data will be publicly released soon.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.17505"
    },
    "ab05a8f2b05390e1cbe83e0b06b55b43": {
        "title": "Benchmarking Data Science Agents",
        "authors": [
            "Yuge Zhang",
            "Qiyang Jiang",
            "Xingyu Han",
            "Nan Chen",
            "Yuqing Yang",
            "Kan Ren"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17168.pdf",
        "abstract": "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.17168"
    },
    "8baca9839158cad236aad1235f6695c7": {
        "title": "RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation",
        "authors": [
            "Qinyu Luo",
            "Yining Ye",
            "Shihao Liang",
            "Zhong Zhang",
            "Yujia Qin",
            "Yaxi Lu",
            "Yesai Wu",
            "Xin Cong",
            "Yankai Lin",
            "Yingli Zhang",
            "Xiaoyin Che",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16667.pdf",
        "abstract": "Generative models have demonstrated considerable potential in software engineering, particularly in tasks such as code generation and debugging. However, their utilization in the domain of code documentation generation remains underexplored. To this end, we introduce RepoAgent, a large language model powered open-source framework aimed at proactively generating, maintaining, and updating code documentation. Through both qualitative and quantitative evaluations, we have validated the effectiveness of our approach, showing that RepoAgent excels in generating high-quality repository-level documentation. The code and results are publicly accessible at https://github.com/OpenBMB/RepoAgent.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2402.16667"
    },
    "4c8c556d314a7cbb7541e472f63cbccd": {
        "title": "Language Agents as Optimizable Graphs",
        "authors": [
            "Mingchen Zhuge",
            "Wenyi Wang",
            "Louis Kirsch",
            "Francesco Faccio",
            "Dmitrii Khizbullin",
            "Jürgen Schmidhuber"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16823.pdf",
        "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.",
        "code": "https://github.com/metauto-ai/gptswarm",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.16823"
    },
    "6edba424f625da489f6bcf7dac4d5a57": {
        "title": "Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation",
        "authors": [
            "Xinyi Mou",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16333.pdf",
        "abstract": "Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.",
        "code": "",
        "category": [
            "Role Playing",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.16333"
    },
    "a5538ed0a6964ec13a4ebdd94f383ecc": {
        "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning",
        "authors": [
            "Jianguo Zhang",
            "Tian Lan",
            "Rithesh Murthy",
            "Zhiwei Liu",
            "Weiran Yao",
            "Juntao Tan",
            "Thai Hoang",
            "Liangwei Yang",
            "Yihao Feng",
            "Zuxin Liu",
            "Tulika Awalgaonkar",
            "Juan Carlos Niebles",
            "Silvio Savarese",
            "Shelby Heinecke",
            "Huan Wang",
            "Caiming Xiong"
        ],
        "date": "2024/02/23",
        "pdf": "http://arxiv.org/pdf/2402.15506.pdf",
        "abstract": "Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address these challenges. \\textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which demonstrates exceptional performance across various benchmarks.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.15506"
    },
    "f9329da7eacdad837ccd68b129b853c6": {
        "title": "Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",
        "authors": [
            "Andrew Brown",
            "Jiading Zhu",
            "Mohamed Abdelwahab",
            "Alec Dong",
            "Cindy Wang",
            "Jonathan Rose"
        ],
        "date": "2024/02/01",
        "pdf": "http://arxiv.org/pdf/2402.01051",
        "abstract": "Large Foundational Language Models are capable of performing many tasks at a high level but are difficult to deploy in many applications because of their size and proprietary ownership. Many will be motivated to distill specific capabilities of foundational models into smaller models that can be owned and controlled. In the development of a therapeutic chatbot, we wish to distill a capability known as reflective listening, in which a therapist produces reflections of client speech. These reflections either restate what a client has said, or connect what was said to a relevant observation, idea or guess that encourages and guides the client to continue contemplation. In this paper, we present a method for distilling the generation of reflections from a Foundational Language Model (GPT-4) into smaller models. We first show that GPT-4, using zero-shot prompting, can generate reflections at near 100% success rate, superior to all previous methods. Using reflections generated by GPT-4, we fine-tune different sizes of the GPT-2 family. The GPT-2-small model achieves 83% success on a hold-out test set and the GPT-2 XL achieves 90% success. We also show that GPT-4 can help in the labor-intensive task of evaluating the quality of the distilled models, using it as a zero-shot classifier. Using triple-human review as a guide, the classifier achieves a Cohen-Kappa of 0.66, a substantial inter-rater reliability figure.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.01051"
    },
    "fcabab00a378e1841f69e2241deddfa9": {
        "title": "Reasoning Capacity in Multi-Agent Systems: Limitations, Challenges and Human-Centered Solutions",
        "authors": [
            "Pouya Pezeshkpour",
            "Eser Kandogan",
            "Nikita Bhutani",
            "Sajjadur Rahman",
            "Tom Mitchell",
            "Estevam Hruschka"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01108",
        "abstract": "Remarkable performance of large language models (LLMs) in a variety of tasks brings forth many opportunities as well as challenges of utilizing them in production settings. Towards practical adoption of LLMs, multi-agent systems hold great promise to augment, integrate, and orchestrate LLMs in the larger context of enterprise platforms that use existing proprietary data and models to tackle complex real-world tasks. Despite the tremendous success of these systems, current approaches rely on narrow, single-focus objectives for optimization and evaluation, often overlooking potential constraints in real-world scenarios, including restricted budgets, resources and time. Furthermore, interpreting, analyzing, and debugging these systems requires different components to be evaluated in relation to one another. This demand is currently not feasible with existing methodologies. In this postion paper, we introduce the concept of reasoning capacity as a unifying criterion to enable integration of constraints during optimization and establish connections among different components within the system, which also enable a more holistic and comprehensive approach to evaluation. We present a formal definition of reasoning capacity and illustrate its utility in identifying limitations within each component of the system. We then argue how these limitations can be addressed with a self-reflective process wherein human-feedback is used to alleviate shortcomings in reasoning and enhance overall consistency of the system.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.01108"
    },
    "426a7d04fbea7fc6a79285e5550740b2": {
        "title": "TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution",
        "authors": [
            "Wenyue Hua",
            "Xianjun Yang",
            "Zelong Li",
            "Wei Cheng",
            "Yongfeng Zhang"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01586",
        "abstract": "The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent&#39;s safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model&#39;s reasoning ability and its efficacy as a safe agent. This paper underscores the imperative of integrating safety awareness and trustworthiness into the design and deployment of LLM-based agents, not only to enhance their performance but also to ensure their responsible integration into human-centric environments. Data and code are available at https://github.com/agiresearch/TrustAgent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01586"
    },
    "88881d9d9219e2d3a274466dfd101760": {
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Tinghui Zhu",
            "Renze Lou",
            "Yuandong Tian",
            "Yanghua Xiao",
            "Yu Su"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01622",
        "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.",
        "code": "https://github.com/OSU-NLP-Group/TravelPlanner",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.01622"
    },
    "8447f702715c2b57c8891a3bac8e010e": {
        "title": "Assistive Large Language Model Agents for Socially-Aware Negotiation Dialogues",
        "authors": [
            "Yuncheng Hua",
            "Lizhen Qu",
            "Gholamreza Haffari"
        ],
        "date": "2024/01/29",
        "pdf": "http://arxiv.org/pdf/2402.01737",
        "abstract": "In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01737"
    },
    "8ed6f5a24a793642920666f6e4727b5b": {
        "title": "LLMs Simulate Big Five Personality Traits: Further Evidence",
        "authors": [
            "Aleksandra Sorokovikova",
            "Natalia Fedorova",
            "Sharwin Rezagholi",
            "Ivan P. Yamshchikov"
        ],
        "date": "2024/01/31",
        "pdf": "http://arxiv.org/pdf/2402.01765",
        "abstract": "An empirical investigation into the simulation of the Big Five personality traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is presented. We analyze the personality traits simulated by these models and their stability. This contributes to the broader understanding of the capabilities of LLMs to simulate personality traits and the respective implications for personalized human-computer interaction.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01765"
    },
    "b301607d40a29aa568b9e9de00343499": {
        "title": "Executable Code Actions Elicit Better LLM Agents",
        "authors": [
            "Xingyao Wang",
            "Yangyi Chen",
            "Lifan Yuan",
            "Yizhe Zhang",
            "Yunzhu Li",
            "Hao Peng",
            "Heng Ji"
        ],
        "date": "2024/02/01",
        "pdf": "http://arxiv.org/pdf/2402.01030",
        "abstract": "Large Language Model (LLM) agents, capable of performing a broad range of actions, such as invoking tools and controlling robots, show great potential in tackling real-world challenges. LLM agents are typically prompted to produce actions by generating JSON or text in a pre-defined format, which is usually limited by constrained action space (e.g., the scope of pre-defined tools) and restricted flexibility (e.g., inability to compose multiple tools). This work proposes to use executable Python code to consolidate LLM agents&#39; actions into a unified action space (CodeAct). Integrated with a Python interpreter, CodeAct can execute code actions and dynamically revise prior actions or emit new actions upon new observations through multi-turn interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher success rate). The encouraging performance of CodeAct motivates us to build an open-source LLM agent that interacts with environments by executing interpretable code and collaborates with users using natural language. To this end, we collect an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions using CodeAct. We show that it can be used with existing data to improve models in agent-oriented tasks without compromising their general capability. CodeActAgent, finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely tailored to perform sophisticated tasks (e.g., model training) using existing libraries and autonomously self-debug.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01030"
    },
    "dbbad9b76945033370d64c196857dcc8": {
        "title": "NavHint: Vision and Language Navigation Agent with a Hint Generator",
        "authors": [
            "Yue Zhang",
            "Quan Guo",
            "Parisa Kordjamshidi"
        ],
        "date": "2024/02/04",
        "pdf": "http://arxiv.org/pdf/2402.02559",
        "abstract": "Existing work on vision and language navigation mainly relies on navigation-related losses to establish the connection between vision and language modalities, neglecting aspects of helping the navigation agent build a deep understanding of the visual environment. In our work, we provide indirect supervision to the navigation agent through a hint generator that provides detailed visual descriptions. The hint generator assists the navigation agent in developing a global understanding of the visual environment. It directs the agent&#39;s attention toward related navigation details, including the relevant sub-instruction, potential challenges in recognition and ambiguities in grounding, and the targeted viewpoint description. To train the hint generator, we construct a synthetic dataset based on landmarks in the instructions and visible and distinctive objects in the visual environment. We evaluate our method on the R2R and R4R datasets and achieve state-of-the-art on several metrics. The experimental results demonstrate that generating hints not only enhances the navigation performance but also helps improve the interpretability of the agent&#39;s actions.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02559"
    },
    "a619330cab01491ca0129cc7efed892c": {
        "title": "LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models",
        "authors": [
            "Ivar Frisch",
            "Mario Giulianelli"
        ],
        "date": "2024/02/05",
        "pdf": "http://arxiv.org/pdf/2402.02896",
        "abstract": "While both agent interaction and personalisation are vibrant topics in research on large language models (LLMs), there has been limited focus on the effect of language interaction on the behaviour of persona-conditioned LLM agents. Such an endeavour is important to ensure that agents remain consistent to their assigned traits yet are able to engage in open, naturalistic dialogues. In our experiments, we condition GPT-3.5 on personality profiles through prompting and create a two-group population of LLM agents using a simple variability-inducing sampling algorithm. We then administer personality tests and submit the agents to a collaborative writing task, finding that different profiles exhibit different degrees of personality consistency and linguistic alignment to their conversational partners. Our study seeks to lay the groundwork for better understanding of dialogue-based interaction between LLMs and highlights the need for new approaches to crafting robust, more human-like LLM personas for interactive environments.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02896"
    },
    "485502588a1d88b1dff5929f92c33d9c": {
        "title": "Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies",
        "authors": [
            "Zhixuan Chu",
            "Yan Wang",
            "Feng Zhu",
            "Lu Yu",
            "Longfei Li",
            "Jinjie Gu"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.03628",
        "abstract": "The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artificial general intelligence.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.03628"
    },
    "90cf304f11ecd2cb08a276a70b0118a3": {
        "title": "AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls",
        "authors": [
            "Yu Du",
            "Fangyun Wei",
            "Hongyang Zhang"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.04253",
        "abstract": "We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.",
        "code": "https://github.com/dyabel/anytool",
        "category": [
            "Feedback&Reflection",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.04253"
    },
    "4874c0c0abd9411762e846540adcb6c1": {
        "title": "More Agents Is All You Need",
        "authors": [
            "Junyou Li",
            "Qin Zhang",
            "Yangbin Yu",
            "Qiang Fu",
            "Deheng Ye"
        ],
        "date": "2024/02/03",
        "pdf": "http://arxiv.org/pdf/2402.05120.pdf",
        "abstract": "We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: \\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.05120"
    },
    "a15433aff8c528c636484d5f5da5010d": {
        "title": "Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients",
        "authors": [
            "Mahyar Abbasian",
            "Zhongqi Yang",
            "Elahe Khatibi",
            "Pengfei Zhang",
            "Nitish Nagesh",
            "Iman Azimi",
            "Ramesh Jain",
            "Amir M. Rahmani"
        ],
        "date": "2024/02/15",
        "pdf": "http://arxiv.org/pdf/2402.10153",
        "abstract": "Effective diabetes management is crucial for maintaining health in diabetic patients. Large Language Models (LLMs) have opened new avenues for diabetes management, facilitating their efficacy. However, current LLM-based approaches are limited by their dependence on general sources and lack of integration with domain-specific knowledge, leading to inaccurate responses. In this paper, we propose a knowledge-infused LLM-powered conversational health agent (CHA) for diabetic patients. We customize and leverage the open-source openCHA framework, enhancing our CHA with external knowledge and analytical capabilities. This integration involves two key components: 1) incorporating the American Diabetes Association dietary guidelines and the Nutritionix information and 2) deploying analytical tools that enable nutritional intake calculation and comparison with the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes 100 diabetes-related questions on daily meal choices and assessing the potential risks associated with the suggested diet. Our findings show that the proposed agent demonstrates superior performance in generating responses to manage essential nutrients.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.10153"
    },
    "13b82b1326a95c1678d2892563c13ae3": {
        "title": "TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and Agent Generation",
        "authors": [
            "Yaoxiang Wang",
            "Zhiyong Wu",
            "Junfeng Yao",
            "Jinsong Su"
        ],
        "date": "2024/02/15",
        "pdf": "http://arxiv.org/pdf/2402.10178",
        "abstract": "The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents&#39; abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.10178"
    },
    "a89feebbb8979ccaff7b3590a33af966": {
        "title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages",
        "authors": [
            "Junjie Ye",
            "Sixian Li",
            "Guanyu Li",
            "Caishuang Huang",
            "Songyang Gao",
            "Yilong Wu",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "date": "2024/02/16",
        "pdf": "http://arxiv.org/pdf/2402.10753",
        "abstract": "Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present $ToolSword$, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and $harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data is released in https://github.com/Junjie-Ye/ToolSword.",
        "code": "https://github.com/junjie-ye/toolsword",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.10753"
    },
    "7d840205c5d5d0c8418e1a6c9032503a": {
        "title": "When is Tree Search Useful for LLM Planning? It Depends on the Discriminator",
        "authors": [
            "Ziru Chen",
            "Michael White",
            "Raymond Mooney",
            "Ali Payani",
            "Yu Su",
            "Huan Sun"
        ],
        "date": "2024/02/16",
        "pdf": "http://arxiv.org/pdf/2402.10890",
        "abstract": "In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs&#39; discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compared to the other two methods, tree search is at least 10--20 times slower but leads to negligible performance gains, which hinders its real-world applications. Code and data will be released at https://github.com/OSU-NLP-Group/llm-planning-eval.",
        "code": "https://github.com/osu-nlp-group/llm-planning-eval",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.10890"
    },
    "0c5062abf0c98c49de2bd6bec7500db3": {
        "title": "KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph",
        "authors": [
            "Jinhao Jiang",
            "Kun Zhou",
            "Wayne Xin Zhao",
            "Yang Song",
            "Chen Zhu",
            "Hengshu Zhu",
            "Ji-Rong Wen"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11163",
        "abstract": "In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLMs or more data, on both in-domain and out-domain datasets. Our code and data will be publicly released.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2402.11163"
    },
    "5eff26e408b6a81fe444f59554a196e4": {
        "title": "Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima",
        "authors": [
            "Shu Yang",
            "Lijie Hu",
            "Lu Yu",
            "Muhammad Asif Ali",
            "Di Wang"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11271",
        "abstract": "The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of autophagic (&#34;self-consumption&#34;) loops to account for the suppression of human-generated information in the exchange of information between humans and AI systems. We generalize the declining diversity of social information and the bottleneck in model performance caused by the above trends to the local optima of large models.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11271"
    },
    "91668d0b0eba8cee741a474aa4cbe16a": {
        "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
        "authors": [
            "Siyuan Wang",
            "Zhuohan Long",
            "Zhihao Fan",
            "Zhongyu Wei",
            "Xuanjing Huang"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11443",
        "abstract": "This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models&#39; capabilities. Besides, our framework widens performance discrepancies both between different models and within the same model across various tasks, facilitating more informed model selection for specific tasks (Code and data are available at https://github.com/NanshineLoong/Self-Evolving-Benchmark).",
        "code": "https://github.com/nanshineloong/self-evolving-benchmark",
        "category": [
            "Multi-Agent System",
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.11443"
    },
    "71b8888021fefef765ee9189114ebc03": {
        "title": "SciAgent: Tool-augmented Language Models for Scientific Reasoning",
        "authors": [
            "Yubo Ma",
            "Zhibin Gou",
            "Junheng Hao",
            "Ruochen Xu",
            "Shuohang Wang",
            "Liangming Pan",
            "Yujiu Yang",
            "Yixin Cao",
            "Aixin Sun",
            "Hany Awadalla",
            "Weizhu Chen"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11451",
        "abstract": "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs&#39; abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11451"
    },
    "80a2c8c973d3216b19515f53d4911fcd": {
        "title": "MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization",
        "authors": [
            "Zhiyu Yang",
            "Zihan Zhou",
            "Shuo Wang",
            "Xin Cong",
            "Xu Han",
            "Yukun Yan",
            "Zhenghao Liu",
            "Zhixing Tan",
            "Pengyuan Liu",
            "Dong Yu",
            "Zhiyuan Liu",
            "Xiaodong Shi",
            "Maosong Sun"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11453",
        "abstract": "Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that MatPlotAgent can improve the performance of various LLMs, including both commercial and open-source models. Furthermore, the proposed evaluation method shows a strong correlation with human-annotated scores.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.11453"
    },
    "8efb45b8104c964a69dec8eb8a385013": {
        "title": "What&#39;s the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs",
        "authors": [
            "Eran Hirsch",
            "Guy Uziel",
            "Ateret Anaby-Tavor"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11489",
        "abstract": "Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.11489"
    },
    "f6cf3f3fc3c1eb72a0cce8a84f4b1cd0": {
        "title": "PreAct: Predicting Future in ReAct Enhances Agent&#39;s Planning Ability",
        "authors": [
            "Dayuan Fu",
            "Jianzhao Huang",
            "Siyuan Lu",
            "Guanting Dong",
            "Yejie Wang",
            "Keqing He",
            "Weiran Xu"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11534",
        "abstract": "Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\\textbf{PreAct}$, an agent framework that integrates $\\textbf{pre}$diction with $\\textbf{rea}$soning and $\\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The differences in single-step reasoning between PreAct and ReAct show that PreAct indeed offers advantages in terms of diversity and strategic directivity over ReAct.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.11534"
    },
    "7e1da3043b5d4208fd9b19f042d48f1f": {
        "title": "LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration",
        "authors": [
            "Jun Zhao",
            "Can Zu",
            "Hao Xu",
            "Yi Lu",
            "Wei He",
            "Yiwen Ding",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11550",
        "abstract": "Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \\textit{lost in the middle}. In this paper, we propose \\textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members&#39; hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of members. To address this, we develop an \\textit{inter-member communication} mechanism to resolve response conflicts caused by hallucinations through information sharing. Our experimental results indicate that \\textsc{LongAgent} offers a promising alternative for long-text processing. The agent team instantiated with LLaMA-7B achieves significant improvements in tasks such as 128k-long text retrieval, multi-hop question answering, compared to GPT-4.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.11550"
    },
    "645c1394ff4bc534034d1906961852ca": {
        "title": "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents",
        "authors": [
            "Renxi Wang",
            "Haonan Li",
            "Xudong Han",
            "Yixuan Zhang",
            "Timothy Baldwin"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11651",
        "abstract": "Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. Experimental results demonstrate that compared to solely using positive examples, incorporating negative examples enhances model performance by a large margin.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.11651"
    },
    "46c43356d6af4d363b0d08678696cea6": {
        "title": "Large Language Models as Agents in Two-Player Games",
        "authors": [
            "Yang Liu",
            "Peng Sun",
            "Hang Li"
        ],
        "date": "2024/02/12",
        "pdf": "http://arxiv.org/pdf/2402.08078",
        "abstract": "By formally defining the training processes of large language models (LLMs), which usually encompasses pre-training, supervised fine-tuning, and reinforcement learning with human feedback, within a single and unified machine learning paradigm, we can glean pivotal insights for advancing LLM technologies. This position paper delineates the parallels between the training methods of LLMs and the strategies employed for the development of agents in two-player games, as studied in game theory, reinforcement learning, and multi-agent systems. We propose a re-conceptualization of LLM learning processes in terms of agent learning in language-based games. This framework unveils innovative perspectives on the successes and challenges in LLM development, offering a fresh understanding of addressing alignment issues among other strategic considerations. Furthermore, our two-player game approach sheds light on novel data preparation and machine learning techniques for training LLMs.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08078"
    },
    "fb31c87283351112060f2a7cbc8d0fe7": {
        "title": "Large Language Models as Minecraft Agents",
        "authors": [
            "Chris Madge",
            "Massimo Poesio"
        ],
        "date": "2024/02/13",
        "pdf": "http://arxiv.org/pdf/2402.08392",
        "abstract": "In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08392"
    },
    "da5cb2cc846f66b14b41853d5ffe1299": {
        "title": "Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast",
        "authors": [
            "Xiangming Gu",
            "Xiaosen Zheng",
            "Tianyu Pang",
            "Chao Du",
            "Qian Liu",
            "Ye Wang",
            "Jing Jiang",
            "Min Lin"
        ],
        "date": "2024/02/13",
        "pdf": "http://arxiv.org/pdf/2402.08567",
        "abstract": "A multimodal large language model (MLLM) agent can receive instructions, capture images, retrieve histories from memory, and decide which tools to use. Nonetheless, red-teaming efforts have revealed that adversarial images/prompts can jailbreak an MLLM and cause unaligned behaviors. In this work, we report an even more severe safety issue in multi-agent environments, referred to as infectious jailbreak. It entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. To validate the feasibility of infectious jailbreak, we simulate multi-agent environments containing up to one million LLaVA-1.5 agents, and employ randomized pair-wise chat as a proof-of-concept instantiation for multi-agent interaction. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak, but how to design a practical defense that meets this principle remains an open question to investigate. Our project page is available at https://sail-sg.github.io/Agent-Smith/.",
        "code": "https://github.com/sail-sg/agent-smith",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.08567"
    },
    "cc4a576822918876acbf397bee723c62": {
        "title": "Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications",
        "authors": [
            "Negar Arabzadeh",
            "Julia Kiseleva",
            "Qingyun Wu",
            "Chi Wang",
            "Ahmed Awadallah",
            "Victor Dibia",
            "Adam Fourney",
            "Charles Clarke"
        ],
        "date": "2024/02/14",
        "pdf": "http://arxiv.org/pdf/2402.09015",
        "abstract": "The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application&#39;s functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of quantifier&#39;s work.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.09015"
    },
    "ac40351f97fee2d30d652151eea8c078": {
        "title": "InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory",
        "authors": [
            "Chaojun Xiao",
            "Pengle Zhang",
            "Xu Han",
            "Guangxuan Xiao",
            "Yankai Lin",
            "Zhengyan Zhang",
            "Zhiyuan Liu",
            "Song Han",
            "Maosong Sun"
        ],
        "date": "2024/02/07",
        "pdf": "http://arxiv.org/pdf/2402.04617",
        "abstract": "Large language models (LLMs) have emerged as a cornerstone in real-world applications with lengthy streaming inputs, such as LLM-driven agents. However, existing LLMs, pre-trained on sequences with restricted maximum length, cannot generalize to longer sequences due to the out-of-domain and distraction issues. To alleviate these issues, existing efforts employ sliding attention windows and discard distant tokens to achieve the processing of extremely long sequences. Unfortunately, these approaches inevitably fail to capture long-distance dependencies within sequences to deeply understand semantics. This paper introduces a training-free memory-based method, InfLLM, to unveil the intrinsic ability of LLMs to process streaming long sequences. Specifically, InfLLM stores distant contexts into additional memory units and employs an efficient mechanism to lookup token-relevant units for attention computation. Thereby, InfLLM allows LLMs to efficiently process long sequences while maintaining the ability to capture long-distance dependencies. Without any training, InfLLM enables LLMs pre-trained on sequences of a few thousand tokens to achieve superior performance than competitive baselines continually training these LLMs on long sequences. Even when the sequence length is scaled to $1,024$K, InfLLM still effectively captures long-distance dependencies.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.04617"
    },
    "062ae31f680adf5ec6f03a536d620444": {
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "authors": [
            "Farhad Moghimifar",
            "Yuan-Fang Li",
            "Robert Thomson",
            "Gholamreza Haffari"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11712",
        "abstract": "Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between political parties and predict the outcomes. We evaluate the performance of state-of-the-art large language models (LLMs) as agents in handling coalition negotiations, offering insights into their capabilities and paving the way for future advancements in political modelling.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.11712"
    },
    "1243ff5fb9ecf0de62aacd39f8af85c0": {
        "title": "Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations",
        "authors": [
            "Nuo Chen",
            "Hongguang Li",
            "Juhua Huang",
            "Baoyuan Wang",
            "Jia Li"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.11975",
        "abstract": "Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a &#39;&#39;One-for-All&#39;&#39; approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY&#39;s superiority over traditional retrieval-based methods in producing more nuanced and human-like conversational experiences. Our codes are available at https://github.com/nuochenpku/COMEDY.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2402.11975"
    },
    "dc755c8abb2a1b7c1dadcd90909f8391": {
        "title": "AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production",
        "authors": [
            "Jiuniu Wang",
            "Zehua Du",
            "Yuyuan Zhao",
            "Bo Yuan",
            "Kexiang Wang",
            "Jian Liang",
            "Yaxi Zhao",
            "Yihen Lu",
            "Gengliang Li",
            "Junlong Gao",
            "Xin Tu",
            "Zhenyu Guo"
        ],
        "date": "2024/03/12",
        "pdf": "http://arxiv.org/pdf/2403.07952",
        "abstract": "The Agent and AIGC (Artificial Intelligence Generated Content) technologies have recently made significant progress. We propose AesopAgent, an Agent-driven Evolutionary System on Story-to-Video Production. AesopAgent is a practical application of agent technology for multimodal content generation. The system integrates multiple generative capabilities within a unified framework, so that individual users can leverage these modules easily. This innovative system would convert user story proposals into scripts, images, and audio, and then integrate these multimodal contents into videos. Additionally, the animating units (e.g., Gen-2 and Sora) could make the videos more infectious. The AesopAgent system could orchestrate task workflow for video generation, ensuring that the generated video is both rich in content and coherent. This system mainly contains two layers, i.e., the Horizontal Layer and the Utility Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary system that optimizes the whole video generation workflow and the steps within the workflow. It continuously evolves and iteratively optimizes workflow by accumulating expert experience and professional knowledge, including optimizing the LLM prompts and utilities usage. The Utility Layer provides multiple utilities, leading to consistent image generation that is visually coherent in terms of composition, characters, and style. Meanwhile, it provides audio and special effects, integrating them into expressive and logically arranged videos. Overall, our AesopAgent achieves state-of-the-art performance compared with many previous works in visual storytelling. Our AesopAgent is designed for convenient service for individual users, which is available on the following page: https://aesopai.github.io/.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.07952"
    },
    "5411575149b2ac84b69af030d50fa13e": {
        "title": "Polarization of Autonomous Generative AI Agents Under Echo Chambers",
        "authors": [
            "Masaya Ohagi"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12212",
        "abstract": "Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs. An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol. The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group&#39;s opinions changed as the discussion progressed. As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is caused by ChatGPT&#39;s high prompt understanding ability to update its opinion by considering its own and surrounding agents&#39; opinions. We conducted additional experiments to investigate under what specific conditions AI agents tended to polarize. As a result, we identified factors that strongly influence polarization, such as the agent&#39;s persona. These factors should be monitored to prevent the polarization of AI agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12212"
    },
    "2b620690c1f044fb1168a82f75c237f0": {
        "title": "LLM Agents for Psychology: A Study on Gamified Assessments",
        "authors": [
            "Qisen Yang",
            "Zekun Wang",
            "Honghui Chen",
            "Shenzhi Wang",
            "Yifan Pu",
            "Xin Gao",
            "Wenhao Huang",
            "Shiji Song",
            "Gao Huang"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12326",
        "abstract": "Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT&#39;s enhancements in content coherence, interactivity, interest, immersion, and satisfaction.",
        "code": "",
        "category": [
            "Role Playing",
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12326"
    },
    "564261b63f4cd7fb3327b8e6855d1027": {
        "title": "Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues",
        "authors": [
            "Michimasa Inaba",
            "Mariko Ukiyo",
            "Keiko Takamizo"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.12738",
        "abstract": "Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human counselors.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12738"
    },
    "4e7147609270bc7198406f69cfffcba4": {
        "title": "Large Language Model-based Human-Agent Collaboration for Complex Task Solving",
        "authors": [
            "Xueyang Feng",
            "Zhi-Yuan Chen",
            "Yujia Qin",
            "Yankai Lin",
            "Xu Chen",
            "Zhiyuan Liu",
            "Ji-Rong Wen"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.12914",
        "abstract": "In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model&#39;s effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.12914"
    },
    "462c569bbda3a96911503fd5e1e980d2": {
        "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents",
        "authors": [
            "Mingyu Jin",
            "Beichen Wang",
            "Zhaoqian Xue",
            "Suiyuan Zhu",
            "Wenyue Hua",
            "Hua Tang",
            "Kai Mei",
            "Mengnan Du",
            "Yongfeng Zhang"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.13184",
        "abstract": "In this study, we introduce &#34;CosmoAgent,&#34; an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking&#39;s cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/agiresearch/AlienAgent.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.13184"
    },
    "b8a99309f72b7f01137f7a2178d15e97": {
        "title": "Soft Self-Consistency Improves Language Model Agents",
        "authors": [
            "Han Wang",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.13212",
        "abstract": "Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current &#34;sample and select&#34; methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC&#39;s discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that Soft-SC can be applied to both open-source and black-box models.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.13212"
    },
    "b739dc90a858a8d7d66e83e8115b2882": {
        "title": "AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning",
        "authors": [
            "Qiao Jin",
            "Zhizheng Wang",
            "Yifan Yang",
            "Qingqing Zhu",
            "Donald Wright",
            "Thomas Huang",
            "W John Wilbur",
            "Zhe He",
            "Andrew Taylor",
            "Qingyu Chen",
            "Zhiyong Lu"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.13225",
        "abstract": "Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description. On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics. In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.13225"
    },
    "b4a30674fee6a3f3c8253cf8ae36c8b7": {
        "title": "Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent",
        "authors": [
            "Xiaoyan Yu",
            "Tongxu Luo",
            "Yifan Wei",
            "Fangyu Lei",
            "Yiming Huang",
            "Hao Peng",
            "Liehuang Zhu"
        ],
        "date": "2024/02/21",
        "pdf": "http://arxiv.org/pdf/2402.13717",
        "abstract": "Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko&#39;s adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at https://github.com/weiyifan1023/Neeko.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.13717"
    },
    "5425466c579cdb4a913f52545fd808cb": {
        "title": "Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering",
        "authors": [
            "Chang Zong",
            "Yuchen Yan",
            "Weiming Lu",
            "Eliot Huang",
            "Jian Shao",
            "Yueting Zhuang"
        ],
        "date": "2024/02/22",
        "pdf": "http://arxiv.org/pdf/2402.14320",
        "abstract": "Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent&#39;s multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms state-of-the-art systems on the LC-QuAD and YAGO-QA benchmarks, yielding F1 scores of 11.8% and 20.7%, respectively.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2402.14320"
    },
    "dc281e70bc671f58ce277899d44ca56f": {
        "title": "Stick to your Role! Stability of Personal Values Expressed in Large Language Models",
        "authors": [
            "Grgur Kovač",
            "Rémy Portelas",
            "Masataka Sawayama",
            "Peter Ford Dominey",
            "Pierre-Yves Oudeyer"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.14846",
        "abstract": "The standard way to study Large Language Models (LLMs) through benchmarks or psychology questionnaires is to provide many different queries from similar minimal contexts (e.g. multiple choice questions). However, due to LLM&#39;s highly context-dependent nature, conclusions from such minimal-context evaluations may be little informative about the model&#39;s behavior in deployment (where it will be exposed to many new contexts). We argue that context-dependence should be studied as another dimension of LLM comparison alongside others such as cognitive abilities, knowledge, or model size. In this paper, we present a case-study about the stability of value expression over different contexts (simulated conversations on different topics), and as measured using a standard psychology questionnaire (PVQ) and a behavioral downstream task. We consider 19 open-sourced LLMs from five families. Reusing methods from psychology, we study Rank-order stability on the population (interpersonal) level, and Ipsative stability on the individual (intrapersonal) level. We explore two settings: with and without instructing LLMs to simulate particular personalities. We observe similar trends in the stability of models and model families - Mixtral, Mistral and Qwen families being more stable than LLaMa-2 and Phi - over those two settings, two different simulated populations, and even in the downstream behavioral task. When instructed to simulate particular personas, LLMs exhibit low Rank-Order stability, and this stability further diminishes with conversation length. This highlights the need for future research directions on LLMs that can coherently simulate a diversity of personas, as well as how context-dependence can be studied in more thorough and efficient ways. This paper provides a foundational step in that direction, and, to our knowledge, it is the first study of value stability in LLMs.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.14846"
    },
    "57c30a1d7bac2dee5e7ef2544482de25": {
        "title": "CHATATC: Large Language Model-Driven Conversational Agents for Supporting Strategic Air Traffic Flow Management",
        "authors": [
            "Sinan Abdulhak",
            "Wayne Hubbard",
            "Karthik Gopalakrishnan",
            "Max Z. Li"
        ],
        "date": "2024/02/20",
        "pdf": "http://arxiv.org/pdf/2402.14850",
        "abstract": "Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions). We also detail the design of a graphical user interface for future users to interact and collaborate with the CHATATC conversational agent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.14850"
    },
    "c23d8b2451b56282954208f691de8e70": {
        "title": "LLM Based Multi-Agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain",
        "authors": [
            "Emanuele Musumeci",
            "Michele Brienza",
            "Vincenzo Suriani",
            "Daniele Nardi",
            "Domenico Daniele Bloisi"
        ],
        "date": "2024/02/21",
        "pdf": "http://arxiv.org/pdf/2402.14871",
        "abstract": "In the last years&#39; digitalization process, the creation and management of documents in various domains, particularly in Public Administration (PA), have become increasingly complex and diverse. This complexity arises from the need to handle a wide range of document types, often characterized by semi-structured forms. Semi-structured documents present a fixed set of data without a fixed format. As a consequence, a template-based solution cannot be used, as understanding a document requires the extraction of the data structure. The recent introduction of Large Language Models (LLMs) has enabled the creation of customized text output satisfying user requests. In this work, we propose a novel approach that combines the LLMs with prompt engineering and multi-agent systems for generating new documents compliant with a desired structure. The main contribution of this work concerns replacing the commonly used manual prompting with a task description generated by semantic retrieval from an LLM. The potential of this approach is demonstrated through a series of experiments and case studies, showcasing its effectiveness in real-world PA scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.14871"
    },
    "71aa00d952aa65d1d5fc0c1319a05590": {
        "title": "Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
        "authors": [
            "Hanqi Yan",
            "Qinglin Zhu",
            "Xinyu Wang",
            "Lin Gui",
            "Yulan He"
        ],
        "date": "2024/02/22",
        "pdf": "http://arxiv.org/pdf/2402.14963",
        "abstract": "While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror&#39;s superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.14963"
    },
    "428eb471e55fba81e7c48c4f54cdb12c": {
        "title": "On the Multi-turn Instruction Following for Conversational Web Agents",
        "authors": [
            "Yang Deng",
            "Xuan Zhang",
            "Wenxuan Zhang",
            "Yifei Yuan",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "date": "2024/02/23",
        "pdf": "http://arxiv.org/pdf/2402.15057",
        "abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method.",
        "code": "",
        "category": [
            "Role Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.15057"
    },
    "842a8ba1857bd207c062e04b067a1aa8": {
        "title": "Chain-of-Discussion: A Multi-Model Framework for Complex Evidence-Based Question Answering",
        "authors": [
            "Mingxu Tao",
            "Dongyan Zhao",
            "Yansong Feng"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16313",
        "abstract": "Open-ended question answering requires models to find appropriate evidence to form well-reasoned, comprehensive and helpful answers. In practical applications, models also need to engage in extended discussions on potential scenarios closely relevant to the question. With augmentation of retrieval module, open-source Large Language Models (LLMs) can produce coherent answers often with different focuses, but are still sub-optimal in terms of reliable evidence selection and in-depth question analysis. In this paper, we propose a novel Chain-of-Discussion framework to leverage the synergy among multiple open-source LLMs aiming to provide \\textbf{more correct} and \\textbf{more comprehensive} answers for open-ended QA, although they are not strong enough individually. Our experiments show that discussions among multiple LLMs play a vital role in enhancing the quality of answers. We release our data and code at \\url{https://github.com/kobayashikanna01/Chain-of-Discussion}.",
        "code": "",
        "category": [
            "Agent Framework",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.16313"
    },
    "978d17d42f9c0ab8af8351e7e1163e4a": {
        "title": "LLMArena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments",
        "authors": [
            "Junzhe Chen",
            "Xuming Hu",
            "Shuodi Liu",
            "Shiyu Huang",
            "Wei-Wei Tu",
            "Zhaofeng He",
            "Lijie Wen"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16499",
        "abstract": "Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings. The code and data will be available.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.16499"
    },
    "046a041e159b5e6124bff8679e854b8d": {
        "title": "Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models",
        "authors": [
            "Anchun Gui",
            "Jian Li",
            "Yong Dai",
            "Nan Du",
            "Han Xiao"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16696",
        "abstract": "Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs&#39; flexibility to address diverse user&#39;s queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches via an automatic generation pipeline, thereby inspiring the decision-making awareness of LLMs under diverse scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the generalizability of LLMs over unseen tools. Extensive experiments demonstrate that our proposed DEER is effective and significantly outperforms baselines across various datasets.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.16696"
    },
    "732e0618c428ed5aa30a4bc21c0dbb5c": {
        "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection",
        "authors": [
            "Liangxin Liu",
            "Xuebo Liu",
            "Derek F. Wong",
            "Dongfang Li",
            "Ziyi Wang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "date": "2024/02/26",
        "pdf": "http://arxiv.org/pdf/2402.16705",
        "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.16705"
    },
    "ddb24874c6acc4057939243cc129be37": {
        "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
        "authors": [
            "Zihao Yi",
            "Jiarui Ouyang",
            "Yuwen Liu",
            "Tianhao Liao",
            "Zhe Xu",
            "Ying Shen"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18013",
        "abstract": "This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.18013"
    },
    "4ac92f5bd1e6b1ed3456db0cec8804c4": {
        "title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
        "authors": [
            "Qineng Wang",
            "Zihao Wang",
            "Ying Su",
            "Hanghang Tong",
            "Yangqiu Song"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18272",
        "abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observe that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.18272"
    },
    "97c680907ecee2890becf2523b5facf2": {
        "title": "PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models",
        "authors": [
            "Sihao Hu",
            "Tiansheng Huang",
            "Ling Liu"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01118",
        "abstract": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon&#39;s human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: \\url{https://github.com/git-disl/PokeLLMon}.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.01118"
    },
    "8e3cbb1643b5524e1a59b88d912ef2c0": {
        "title": "A Multi-Agent Conversational Recommender System",
        "authors": [
            "Jiabao Fang",
            "Shen Gao",
            "Pengjie Ren",
            "Xiuying Chen",
            "Suzan Verberne",
            "Zhaochun Ren"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01135",
        "abstract": "Due to strong capabilities in conducting fluent, multi-turn conversations with users, Large Language Models (LLMs) have the potential to further improve the performance of Conversational Recommender System (CRS). Unlike the aimless chit-chat that LLM excels at, CRS has a clear target. So it is imperative to control the dialogue flow in the LLM to successfully recommend appropriate items to the users. Furthermore, user feedback in CRS can assist the system in better modeling user preferences, which has been ignored by existing studies. However, simply prompting LLM to conduct conversational recommendation cannot address the above two key challenges. In this paper, we propose Multi-Agent Conversational Recommender System (MACRS) which contains two essential modules. First, we design a multi-agent act planning framework, which can control the dialogue flow based on four LLM-based agents. This cooperative multi-agent framework will generate various candidate responses based on different dialogue acts and then choose the most appropriate response as the system response, which can help MACRS plan suitable dialogue acts. Second, we propose a user feedback-aware reflection mechanism which leverages user feedback to reason errors made in previous turns to adjust the dialogue act planning, and higher-level user information from implicit semantics. We conduct extensive experiments based on user simulator to demonstrate the effectiveness of MACRS in recommendation and user preferences collection. Experimental results illustrate that MACRS demonstrates an improvement in user interaction experience compared to directly using LLMs.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.01135"
    },
    "f35e9df2a012ff5dbd78690968a9c985": {
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from Compiler Feedback",
        "authors": [
            "Shihan Dou",
            "Yan Liu",
            "Haoxiang Jia",
            "Limao Xiong",
            "Enyu Zhou",
            "Wei Shen",
            "Junjie Shan",
            "Caishuang Huang",
            "Xiao Wang",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Tao Ji",
            "Rui Zheng",
            "Qi Zhang",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "date": "2024/02/02",
        "pdf": "http://arxiv.org/pdf/2402.01391",
        "abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. Our dataset APPS+ and StepCoder are available online.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.01391"
    },
    "3300b5a7d577880f68a805dbd456926f": {
        "title": "Enhance Reasoning for Large Language Models in the Game Werewolf",
        "authors": [
            "Shuang Wu",
            "Liwen Zhu",
            "Tao Yang",
            "Shiwei Xu",
            "Qiang Fu",
            "Yang Wei",
            "Haobo Fu"
        ],
        "date": "2024/02/04",
        "pdf": "http://arxiv.org/pdf/2402.02330",
        "abstract": "This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework&#39;s effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when integrated with the Thinker. This paper also contributes the largest dataset for social deduction games to date.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.02330"
    },
    "cb258de9f27fddad03485d7f165904fb": {
        "title": "Understanding the planning of LLM agents: A survey",
        "authors": [
            "Xu Huang",
            "Weiwen Liu",
            "Xiaolong Chen",
            "Xingmei Wang",
            "Hao Wang",
            "Defu Lian",
            "Yasheng Wang",
            "Ruiming Tang",
            "Enhong Chen"
        ],
        "date": "2024/02/05",
        "pdf": "http://arxiv.org/pdf/2402.02716",
        "abstract": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.02716"
    },
    "3e3b6d0959265aa43dd113e51341cc90": {
        "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models",
        "authors": [
            "Haibo Jin",
            "Ruoxi Chen",
            "Andy Zhou",
            "Jinyin Chen",
            "Yang Zhang",
            "Haohan Wang"
        ],
        "date": "2024/02/05",
        "pdf": "http://arxiv.org/pdf/2402.03299",
        "abstract": "The discovery of &#34;jailbreaks&#34; to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effective in inducing LLMs to generate unethical or guideline-violating responses. In addition, we also pioneer a setting in our system that will automatically follow the government-issued guidelines to generate jailbreaks to test whether LLMs follow the guidelines accordingly. We refer to our system as GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have empirically validated the effectiveness of GUARD on three cutting-edge open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing GUARD&#39;s versatility and contributing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.03299"
    },
    "34797899b0b543582283f74388953080": {
        "title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
        "authors": [
            "Tomoyuki Kagaya",
            "Thong Jing Yuan",
            "Yuxuan Lou",
            "Jayashree Karlekar",
            "Sugiri Pranata",
            "Akira Kinose",
            "Koki Oguri",
            "Felix Wick",
            "Yang You"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.03610",
        "abstract": "Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents&#39; planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP&#39;s effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents&#39; performance for embodied tasks. These results highlight RAP&#39;s potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.",
        "code": "",
        "category": [
            "Memory Mechanism",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.03610"
    },
    "0225b7cf486f65674b0fe04db98aaa16": {
        "title": "Can Generative Agents Predict Emotion?",
        "authors": [
            "Ciaran Regan",
            "Nanami Iwahashi",
            "Shogo Tanaka",
            "Mizuki Oka"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.04232",
        "abstract": "Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.04232"
    },
    "faf966da03b7ba9e7b5859e731558402": {
        "title": "Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science",
        "authors": [
            "Xiangru Tang",
            "Qiao Jin",
            "Kunlun Zhu",
            "Tongxin Yuan",
            "Yichi Zhang",
            "Wangchunshu Zhou",
            "Meng Qu",
            "Yilun Zhao",
            "Jian Tang",
            "Zhuosheng Zhang",
            "Arman Cohan",
            "Zhiyong Lu",
            "Mark Gerstein"
        ],
        "date": "2024/02/06",
        "pdf": "http://arxiv.org/pdf/2402.04247",
        "abstract": "Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2402.04247"
    },
    "7a8b78fdb03c0cce97414fcfdb71fa23": {
        "title": "Can Large Language Model Agents Simulate Human Trust Behaviors?",
        "authors": [
            "Chengxing Xie",
            "Canyu Chen",
            "Feiran Jia",
            "Ziyu Ye",
            "Kai Shu",
            "Adel Bibi",
            "Ziniu Hu",
            "Philip Torr",
            "Bernard Ghanem",
            "Guohao Li"
        ],
        "date": "2024/02/07",
        "pdf": "http://arxiv.org/pdf/2402.04559",
        "abstract": "Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, particularly for GPT-4, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications of our discoveries for various scenarios where trust is paramount. Our study provides new insights into the behaviors of LLM agents and the fundamental analogy between LLMs and humans.",
        "code": "https://github.com/camel-ai/agent-trust",
        "category": [
            "Survey",
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2402.04559"
    },
    "6595dfd27493f7ec99213b704b178c9d": {
        "title": "CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models",
        "authors": [
            "Peiyuan Gong",
            "Jiamian Li",
            "Jiaxin Mao"
        ],
        "date": "2024/02/09",
        "pdf": "http://arxiv.org/pdf/2402.06360",
        "abstract": "Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users&#39; collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped with the capacity to understand the queries and context in multi-user conversations and the ability to search the Web for relevant information via APIs, CoSearchAgent can respond to user queries with answers grounded on the relevant search results. It can also ask clarifying questions when the information needs are unclear. The proposed CoSearchAgent is highly flexible and would be useful for supporting further research on collaborative search. The code and demo video are accessible.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.06360"
    },
    "7bd9188f4ffafb63274194bc0a99f3d1": {
        "title": "Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty",
        "authors": [
            "Kaiqu Liang",
            "Zixu Zhang",
            "Jaime Fernández Fisac"
        ],
        "date": "2024/02/09",
        "pdf": "http://arxiv.org/pdf/2402.06529",
        "abstract": "Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches. Furthermore, we assess the effectiveness of introspective planning in conjunction with conformal prediction, revealing that this combination yields tighter confidence bounds, thereby maintaining statistical success guarantees with fewer superfluous user clarification queries.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2402.06529"
    },
    "f8620e0896c822455747c0236e11e463": {
        "title": "UFO: A UI-Focused Agent for Windows OS Interaction",
        "authors": [
            "Chaoyun Zhang",
            "Liqun Li",
            "Shilin He",
            "Xu Zhang",
            "Bo Qiao",
            "Si Qin",
            "Minghua Ma",
            "Yu Kang",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "date": "2024/02/08",
        "pdf": "http://arxiv.org/pdf/2402.07939",
        "abstract": "We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users&#39; daily usage. The results, derived from both quantitative metrics and real-case studies, underscore the superior effectiveness of UFO in fulfilling user requests. To the best of our knowledge, UFO stands as the first UI agent specifically tailored for task completion within the Windows OS environment. The open-source code for UFO is available on https://github.com/microsoft/UFO.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.07939"
    },
    "faf44b5210935885a5c485ad008d9c84": {
        "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
        "authors": [
            "Wenkai Yang",
            "Xiaohan Bi",
            "Yankai Lin",
            "Sishuo Chen",
            "Jie Zhou",
            "Xu Sun"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11208",
        "abstract": "Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided into two subcategories based on trigger locations: the backdoor trigger can be hidden either in the user query or in an intermediate observation returned by the external environment. We propose the corresponding data poisoning mechanisms to implement the above variations of agent backdoor attacks on two typical agent tasks, web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.11208"
    },
    "e41ce820e64617d634739d29708e867f": {
        "title": "Training Language Model Agents without Modifying Language Models",
        "authors": [
            "Shaokun Zhang",
            "Jieyu Zhang",
            "Jiale Liu",
            "Linxin Song",
            "Chi Wang",
            "Ranjay Krishna",
            "Qingyun Wu"
        ],
        "date": "2024/02/17",
        "pdf": "http://arxiv.org/pdf/2402.11359",
        "abstract": "Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent&#39;s functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters&#39; and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents&#39; functions and devise an agent training algorithm with two strategies, roll-back, and early-stop, to streamline the training process. With extensive experiments, we showcase that the agent training paradigm could significantly improve the performance of representative LLM agents in various downstream tasks. We also study the behavior of the agent training regarding aspects like the learning curve and domain transferability.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2402.11359"
    },
    "f1bd6aec7fc832da2d885439f4190c32": {
        "title": "Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models",
        "authors": [
            "Paramveer S. Dhillon",
            "Somayeh Molaei",
            "Jiaqi Li",
            "Maximilian Golub",
            "Shaochun Zheng",
            "Lionel P. Robert"
        ],
        "date": "2024/02/18",
        "pdf": "http://arxiv.org/pdf/2402.11723",
        "abstract": "Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text ownership and satisfaction was noted. Our results have broad implications for the design of AI-powered writing tools, including the need for personalized scaffolding mechanisms.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2402.11723"
    },
    "67182f0130bd01c7473a351917dfb201": {
        "title": "WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment",
        "authors": [
            "Hao Tang",
            "Darren Key",
            "Kevin Ellis"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12275",
        "abstract": "We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.12275"
    },
    "3b6070b351092ec4f5fd27c0f5359d9a": {
        "title": "A Critical Evaluation of AI Feedback for Aligning Large Language Models",
        "authors": [
            "Archit Sharma",
            "Sedrick Keh",
            "Eric Mitchell",
            "Chelsea Finn",
            "Kushal Arora",
            "Thomas Kollar"
        ],
        "date": "2024/02/19",
        "pdf": "http://arxiv.org/pdf/2402.12366",
        "abstract": "Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, we find that the gains from RLAIF vary substantially across base model families, test-time evaluation protocols, and critic models. Finally, we provide a mechanistic explanation for when SFT may outperform the full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally useful in practice.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.12366"
    },
    "991fe9838ff8be84f055c655cfd90ae4": {
        "title": "Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation",
        "authors": [
            "Jiawei Wang",
            "Renhe Jiang",
            "Chuang Yang",
            "Zengqing Wu",
            "Makoto Onizuka",
            "Ryosuke Shibasaki",
            "Chuan Xiao"
        ],
        "date": "2024/02/22",
        "pdf": "http://arxiv.org/pdf/2402.14744",
        "abstract": "This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This research marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.14744"
    },
    "cff55d17a6bd1f5e25bf893bb95759a4": {
        "title": "Empowering Large Language Model Agents through Action Learning",
        "authors": [
            "Haiteng Zhao",
            "Chang Ma",
            "Guoyin Wang",
            "Jing Su",
            "Lingpeng Kong",
            "Jingjing Xu",
            "Zhi-Hong Deng",
            "Hongxia Yang"
        ],
        "date": "2024/02/24",
        "pdf": "http://arxiv.org/pdf/2402.15809",
        "abstract": "Large Language Model (LLM) Agents have recently garnered increasing interest yet they are limited in their ability to learn from trial and error, a key element of intelligent behavior. In this work, we argue that the capacity to learn new actions from experience is fundamental to the advancement of learning in LLM agents. While humans naturally expand their action spaces and develop skills through experiential learning, LLM agents typically operate within fixed action spaces, limiting their potential for growth. To address these challenges, our study explores open-action learning for language agents. We introduce a framework LearnAct with an iterative learning strategy to create and improve actions in the form of Python functions. In each iteration, LLM revises and updates the currently available actions based on the errors identified in unsuccessful training tasks, thereby enhancing action effectiveness. Our experimental evaluations across Robotic Planning and Alfworld environments reveal that after learning on a few training task instances, our approach to open-action learning markedly improves agent performance for the type of task (by 32 percent in AlfWorld compared to ReAct+Reflexion, for instance) highlighting the importance of experiential action learning in the development of more intelligent LLM agents.",
        "code": "https://github.com/zhao-ht/learnact",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.15809"
    },
    "48b9abc38498cfd3a81ce2b6c7e3a370": {
        "title": "Understanding Public Perceptions of AI Conversational Agents: A Cross-Cultural Analysis",
        "authors": [
            "Zihan Liu",
            "Han Li",
            "Anfan Chen",
            "Renwen Zhang",
            "Yi-Chieh Lee"
        ],
        "date": "2024/02/25",
        "pdf": "http://arxiv.org/pdf/2402.16039",
        "abstract": "Conversational Agents (CAs) have increasingly been integrated into everyday life, sparking significant discussions on social media. While previous research has examined public perceptions of AI in general, there is a notable lack in research focused on CAs, with fewer investigations into cultural variations in CA perceptions. To address this gap, this study used computational methods to analyze about one million social media discussions surrounding CAs and compared people&#39;s discourses and perceptions of CAs in the US and China. We find Chinese participants tended to view CAs hedonically, perceived voice-based and physically embodied CAs as warmer and more competent, and generally expressed positive emotions. In contrast, US participants saw CAs more functionally, with an ambivalent attitude. Warm perception was a key driver of positive emotions toward CAs in both countries. We discussed practical implications for designing contextually sensitive and user-centric CAs to resonate with various users&#39; preferences and needs.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.16039"
    },
    "02d196e96f4c9879e876058c130090dd": {
        "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
        "authors": [
            "Raghav Kapoor",
            "Yash Parag Butala",
            "Melisa Russak",
            "Jing Yu Koh",
            "Kiran Kamble",
            "Waseem Alshikh",
            "Ruslan Salakhutdinov"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17553",
        "abstract": "For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent&#39;s capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as &#34;Play the next song&#34;, as well as longer horizon tasks such as &#34;Send an email to John Doe mentioning the time and place to meet&#34;. Specifically, given a pair of screen image and a visually-grounded natural language task, the goal is to generate a script capable of fully executing the task. We run several strong baseline language model agents on our benchmark. The strongest baseline, GPT-4, performs the best on our benchmark However, its performance level still reaches only 15% of the human proficiency in generating executable scripts capable of completing the task, demonstrating the challenge of our task for conventional web agents. Our benchmark provides a platform to measure and evaluate the progress of language model agents in automating computer tasks and motivates future work towards building multimodal models that bridge large language models and the visual grounding of computer screens.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2402.17553"
    },
    "8271256b63765b8a4b2bca80cea0bc97": {
        "title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization",
        "authors": [
            "Wenqi Zhang",
            "Ke Tang",
            "Hai Wu",
            "Mengna Wang",
            "Yongliang Shen",
            "Guiyang Hou",
            "Zeqi Tan",
            "Peng Li",
            "Yueting Zhuang",
            "Weiming Lu"
        ],
        "date": "2024/02/27",
        "pdf": "http://arxiv.org/pdf/2402.17574",
        "abstract": "Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold&#39;em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2402.17574"
    },
    "f5a6f0b1770744cc9936c01c3da93afa": {
        "title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform",
        "authors": [
            "Jizhi Zhang",
            "Keqin Bao",
            "Wenjie Wang",
            "Yang Zhang",
            "Wentao Shi",
            "Wanhong Xu",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18240",
        "abstract": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.18240"
    },
    "a28b37bb6cb6d0a29d804cb33834a5fc": {
        "title": "ProtAgents: Protein discovery via large language model multi-agent collaborations combining physics and machine learning",
        "authors": [
            "A. Ghafarollahi",
            "M. J. Buehler"
        ],
        "date": "2024/01/27",
        "pdf": "http://arxiv.org/pdf/2402.04268",
        "abstract": "Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data -- natural vibrational frequencies -- via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2402.04268"
    },
    "f39b606d07c0f8bd9fd6b36696bf5fae": {
        "title": "Data Interpreter: An LLM Agent For Data Science",
        "authors": [
            "Sirui Hong",
            "Yizhang Lin",
            "Bang Liu",
            "Bangbang Liu",
            "Binhao Wu",
            "Danyang Li",
            "Jiaqi Chen",
            "Jiayi Zhang",
            "Jinlin Wang",
            "Li Zhang",
            "Lingyao Zhang",
            "Min Yang",
            "Mingchen Zhuge",
            "Taicheng Guo",
            "Tuo Zhou",
            "Wei Tao",
            "Wenyi Wang",
            "Xiangru Tang",
            "Xiangtao Lu",
            "Xiawu Zheng",
            "Xinbing Liang",
            "Yaying Fei",
            "Yuheng Cheng",
            "Zongze Xu",
            "Chenglin Wu"
        ],
        "date": "2024/02/28",
        "pdf": "http://arxiv.org/pdf/2402.18679",
        "abstract": "Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT.",
        "code": "https://github.com/geekan/metagpt",
        "category": [
            "Planning",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2402.18679"
    },
    "be2d4031465d914b9934dcb79717a783": {
        "title": "Bootstrapping Cognitive Agents with a Large Language Model",
        "authors": [
            "Feiyu Zhu",
            "Reid Simmons"
        ],
        "date": "2024/02/25",
        "pdf": "http://arxiv.org/pdf/2403.00810",
        "abstract": "Large language models contain noisy general knowledge of the world, yet are hard to train or fine-tune. On the other hand cognitive architectures have excellent interpretability and are flexible to update but require a lot of manual work to instantiate. In this work, we combine the best of both worlds: bootstrapping a cognitive-based model with the noisy knowledge encoded in large language models. Through an embodied agent doing kitchen tasks, we show that our proposed framework yields better efficiency compared to an agent based entirely on large language models. Our experiments indicate that large language models are a good source of information for cognitive architectures, and the cognitive architecture in turn can verify and update the knowledge of large language models to a specific domain.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.00810"
    },
    "7e5087e7696617ed5cbf3c1b34f50b31": {
        "title": "SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code",
        "authors": [
            "Ziniu Hu",
            "Ahmet Iscen",
            "Aashi Jain",
            "Thomas Kipf",
            "Yisong Yue",
            "David A. Ross",
            "Cordelia Schmid",
            "Alireza Fathi"
        ],
        "date": "2024/03/02",
        "pdf": "http://arxiv.org/pdf/2403.01248",
        "abstract": "This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self-improvement without expensive LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing LLM-based agents in rendering complex scenes, as shown by its adherence to constraints and favorable human assessments. We also showcase the broader application potential of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding a video generative model with generated scenes as intermediary control signal.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.01248"
    },
    "44f6c16c4530452a5f5a3bded715ffee": {
        "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
        "authors": [
            "Yutong Li",
            "Lu Chen",
            "Aiwei Liu",
            "Kai Yu",
            "Lijie Wen"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02574",
        "abstract": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.02574"
    },
    "6331f2d3779675a854d1e10deab9b472": {
        "title": "AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks",
        "authors": [
            "Yifan Zeng",
            "Yiran Wu",
            "Xiao Zhang",
            "Huazheng Wang",
            "Qingyun Wu"
        ],
        "date": "2024/03/02",
        "pdf": "http://arxiv.org/pdf/2403.04783",
        "abstract": "Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and data are publicly available at https://github.com/XHMY/AutoDefense.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.04783"
    },
    "9a7a65a110f0d8fe677a69a1b75ed96b": {
        "title": "TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision",
        "authors": [
            "Ruiwen Zhou",
            "Yingxuan Yang",
            "Muning Wen",
            "Ying Wen",
            "Wenhao Wang",
            "Chunling Xi",
            "Guoqiang Xu",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "date": "2024/03/10",
        "pdf": "http://arxiv.org/pdf/2403.06221",
        "abstract": "Numerous large language model (LLM) agents have been built for different tasks like web navigation and online shopping due to LLM&#39;s wide knowledge and text-understanding ability. Among these works, many of them utilize in-context examples to achieve generalization without the need for fine-tuning, while few of them have considered the problem of how to select and effectively utilize these examples. Recently, methods based on trajectory-level retrieval with task meta-data and using trajectories as in-context examples have been proposed to improve the agent&#39;s overall performance in some sequential decision making tasks. However, these methods can be problematic due to plausible examples retrieved without task-specific state transition dynamics and long input with plenty of irrelevant context. In this paper, we propose a novel framework (TRAD) to address these issues. TRAD first conducts Thought Retrieval, achieving step-level demonstration selection via thought matching, leading to more helpful demonstrations and less irrelevant input noise. Then, TRAD introduces Aligned Decision, complementing retrieved demonstration steps with their previous or subsequent steps, which enables tolerance for imperfect thought and provides a choice for balance between more context and less noise. Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not only outperforms state-of-the-art models but also effectively helps in reducing noise and promoting generalization. Furthermore, TRAD has been deployed in real-world scenarios of a global business insurance company and improves the success rate of robotic process automation.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.06221"
    },
    "1785c4df279fb0e9744e6557fb2831f4": {
        "title": "Transforming Competition into Collaboration: The Revolutionary Role of Multi-Agent Systems and Language Models in Modern Organizations",
        "authors": [
            "Carlos Jose Xavier Cruz"
        ],
        "date": "2024/03/12",
        "pdf": "http://arxiv.org/pdf/2403.07769",
        "abstract": "This article explores the dynamic influence of computational entities based on multi-agent systems theory (SMA) combined with large language models (LLM), which are characterized by their ability to simulate complex human interactions, as a possibility to revolutionize human user interaction from the use of specialized artificial agents to support everything from operational organizational processes to strategic decision making based on applied knowledge and human orchestration. Previous investigations reveal that there are limitations, particularly in the autonomous approach of artificial agents, especially when dealing with new challenges and pragmatic tasks such as inducing logical reasoning and problem solving. It is also considered that traditional techniques, such as the stimulation of chains of thoughts, require explicit human guidance. In our approach we employ agents developed from large language models (LLM), each with distinct prototyping that considers behavioral elements, driven by strategies that stimulate the generation of knowledge based on the use case proposed in the scenario (role-play) business, using a discussion approach between agents (guided conversation). We demonstrate the potential of developing agents useful for organizational strategies, based on multi-agent system theories (SMA) and innovative uses based on large language models (LLM based), offering a differentiated and adaptable experiment to different applications, complexities, domains, and capabilities from LLM.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.07769"
    },
    "2c4d31a4fb12b85568b6ece4d71cdc6f": {
        "title": "VideoAgent: Long-form Video Understanding with Large Language Model as Agent",
        "authors": [
            "Xiaohan Wang",
            "Yuhui Zhang",
            "Orr Zohar",
            "Serena Yeung-Levy"
        ],
        "date": "2024/03/15",
        "pdf": "http://arxiv.org/pdf/2403.10517",
        "abstract": "Long-form video understanding represents a significant challenge within computer vision, demanding a model capable of reasoning over long multi-modal sequences. Motivated by the human cognitive process for long-form video understanding, we emphasize interactive reasoning and planning over the ability to process lengthy visual inputs. We introduce a novel agent-based system, VideoAgent, that employs a large language model as a central agent to iteratively identify and compile crucial information to answer a question, with vision-language foundation models serving as tools to translate and retrieve visual information. Evaluated on the challenging EgoSchema and NExT-QA benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only 8.4 and 8.2 frames used on average. These results demonstrate superior effectiveness and efficiency of our method over the current state-of-the-art methods, highlighting the potential of agent-based approaches in advancing long-form video understanding.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.10517"
    },
    "ebcd928048d3695d60276a8bcaa3c2d7": {
        "title": "How Far Are We on the Decision-Making of LLMs? Evaluating LLMs&#39; Gaming Ability in Multi-Agent Environments",
        "authors": [
            "Jen-tse Huang",
            "Eric John Li",
            "Man Ho Lam",
            "Tian Liang",
            "Wenxuan Wang",
            "Youliang Yuan",
            "Wenxiang Jiao",
            "Xing Wang",
            "Zhaopeng Tu",
            "Michael R. Lyu"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.11807",
        "abstract": "Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs&#39; decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model&#39;s performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs&#39; robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other models on GAMA-Bench, achieving a score of 72.5. Moreover, the increasingly higher scores across the three iterations of GPT-3.5 (0613, 1106, 0125) demonstrate marked advancements in the model&#39;s intelligence with each update. The code and experimental results are made publicly available via https://github.com/CUHK-ARISE/GAMABench.",
        "code": "",
        "category": [
            "Multi-Agent System",
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.11807"
    },
    "c136f0b43c06cced12b5f5dd89b15afb": {
        "title": "Tur[k]ingBench: A Challenge Benchmark for Web Agents",
        "authors": [
            "Kevin Xu",
            "Yeganeh Kordi",
            "Kate Sanders",
            "Yizhong Wang",
            "Adam Byerly",
            "Jack Zhang",
            "Benjamin Van Durme",
            "Daniel Khashabi"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.11905",
        "abstract": "Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains? To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instances distributed across 158 tasks. Additionally, to facilitate the evaluation on TurkingBench, we develop an evaluation framework that connects the responses of chatbots to modifications on web pages (modifying a text box, checking a radio, etc.). We evaluate the performance of state-of-the-art models, including language-only, vision-only, and layout-only models, and their combinations, on this benchmark. Our findings reveal that these models perform significantly better than random chance, yet considerable room exists for improvement. We hope this benchmark will help facilitate the evaluation and development of web-based agents.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.11905"
    },
    "693f00f098fa0521c4e2a8694f71b09e": {
        "title": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
        "authors": [
            "Xudong Guo",
            "Kaixuan Huang",
            "Jiale Liu",
            "Wenhui Fan",
            "Natalia Vélez",
            "Qingyun Wu",
            "Huazheng Wang",
            "Thomas L. Griffiths",
            "Mengdi Wang"
        ],
        "date": "2024/03/19",
        "pdf": "http://arxiv.org/pdf/2403.12482",
        "abstract": "Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.12482"
    },
    "0a86cfd4f6ddb5b35652e271d6e72f94": {
        "title": "Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior",
        "authors": [
            "Zhouhong Gu",
            "Xiaoxuan Zhu",
            "Haoran Guo",
            "Lin Zhang",
            "Yin Cai",
            "Hao Shen",
            "Jiangjie Chen",
            "Zheyu Ye",
            "Yifei Dai",
            "Yan Gao",
            "Yao Hu",
            "Hongwei Feng",
            "Yanghua Xiao"
        ],
        "date": "2024/03/20",
        "pdf": "http://arxiv.org/pdf/2403.13433",
        "abstract": "To investigate the role of language in human collective behaviors, we developed the Agent Group Chat simulation to simulate linguistic interactions among multi-agent in different settings. Agents are asked to free chat in this simulation for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines. By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations. We evaluate the disorder within the environment by computing the n-gram Shannon entropy of all the content speak by characters. Our findings reveal that under the premise of agents possessing substantial alignment with human expectations, facilitating more extensive information exchange within the simulation ensures greater orderliness amidst diversity, which leads to the emergence of more unexpected and meaningful emergent behaviors. The code is open source in https://github.com/MikeGu721/AgentGroup, and online platform will be open soon.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.13433"
    },
    "cc29e2a1416d7c71fb4302fb8cca7374": {
        "title": "Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents",
        "authors": [
            "Yifan Song",
            "Da Yin",
            "Xiang Yue",
            "Jie Huang",
            "Sujian Li",
            "Bill Yuchen Lin"
        ],
        "date": "2024/03/04",
        "pdf": "http://arxiv.org/pdf/2403.02502",
        "abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2403.02502"
    },
    "2dc81cefe824ff47859daa8c4433ebb7": {
        "title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
        "authors": [
            "Qiusi Zhan",
            "Zhixiang Liang",
            "Zifan Ying",
            "Daniel Kang"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02691",
        "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative. In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.",
        "code": "",
        "category": [
            "Benchmark&Evaluation",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2403.02691"
    },
    "0f7cdc7644f6cc8b301e0d841297c9a4": {
        "title": "Android in the Zoo: Chain-of-Action-Thought for GUI Agents",
        "authors": [
            "Jiwen Zhang",
            "Jihao Wu",
            "Yihua Teng",
            "Minghui Liao",
            "Nuo Xu",
            "Xiao Xiao",
            "Zhongyu Wei",
            "Duyu Tang"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02713",
        "abstract": "Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pairs together with chain-of-action-thought annotations. Experiments show that fine-tuning a 200M model on our AitZ dataset achieves on par performance with CogAgent-Chat-18B.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2403.02713"
    },
    "94ad4073c700730f6fe1185d85688377": {
        "title": "SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents",
        "authors": [
            "Zhitao He",
            "Pengfei Cao",
            "Chenhao Wang",
            "Zhuoran Jin",
            "Yubo Chen",
            "Jiexin Xu",
            "Huaijun Li",
            "Xiaojian Jiang",
            "Kang Liu",
            "Jun Zhao"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.02959",
        "abstract": "With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt. Our framework follows the real-world classic court trial process, consisting of court debate simulation, legal information retrieval and judgement refinement to simulate the decision-making of judge. (3) we perform extensive experiments, the results demonstrate that, our framework outperforms the existing advanced methods in various aspects, especially in generating legal grounds, where our model achieves significant improvements of 8.6% and 9.1% F1 score in the first and second instance settings, respectively.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.02959"
    },
    "1d7449def73d3ffa6c8306de65e4a7f7": {
        "title": "KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents",
        "authors": [
            "Yuqi Zhu",
            "Shuofei Qiao",
            "Yixin Ou",
            "Shumin Deng",
            "Ningyu Zhang",
            "Shiwei Lyu",
            "Yue Shen",
            "Lei Liang",
            "Jinjie Gu",
            "Huajun Chen"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.03101",
        "abstract": "Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation. Code is available in https://github.com/zjunlp/KnowAgent.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.03101"
    },
    "d1805f7178a10d4395c1ade7d6ab6fe2": {
        "title": "Language Guided Exploration for RL Agents in Text Environments",
        "authors": [
            "Hitesh Golchha",
            "Sahil Yerawar",
            "Dhruvesh Patel",
            "Soham Dan",
            "Keerthiram Murugesan"
        ],
        "date": "2024/03/05",
        "pdf": "http://arxiv.org/pdf/2403.03141",
        "abstract": "Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\\textit{tabula rasa}$ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.03141"
    },
    "85a3a98c4b9cadc46bc5c25eb3065638": {
        "title": "Promising and worth-to-try future directions for advancing state-of-the-art surrogates methods of agent-based models in social and health computational sciences",
        "authors": [
            "Atiyah Elsheikh"
        ],
        "date": "2024/03/07",
        "pdf": "http://arxiv.org/pdf/2403.04417",
        "abstract": "The execution and runtime performance of model-based analysis tools for realistic large-scale ABMs (Agent-Based Models) can be excessively long. This due to the computational demand exponentially proportional to the model size (e.g. Population size) and the number of model parameters. Even the runtime of a single simulation of a realistic ABM may demand huge computational resources when attempting to employ realistic population size. The main aim of this ad-hoc brief report is to highlight some of surrogate models that were adequate and computationally less demanding for nonlinear dynamical models in various modeling application areas.To the author knowledge, these methods have been not, at least extensively, employed for ABMs within the field of (SHCS) Social Health Computational Sciences, yet. Thus, they might be, but not necessarily, useful in progressing state of the art for establishing surrogate models for ABMs in the field of SHCS.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2403.04417"
    },
    "e23ceb4b035965c2dad3406c7761edba": {
        "title": "ChatASU: Evoking LLM&#39;s Reflexion to Truly Understand Aspect Sentiment in Dialogues",
        "authors": [
            "Yiding Liu",
            "Jingjing Wang",
            "Jiamin Luo",
            "Tao Zeng",
            "Guodong Zhou"
        ],
        "date": "2024/03/08",
        "pdf": "http://arxiv.org/pdf/2403.05326",
        "abstract": "Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g., Question-Answering and Dialogue) has attracted ever-more interest in recent years and achieved important progresses. However, existing studies on interactive ASU largely ignore the coreference issue for opinion targets (i.e., aspects), while this phenomenon is ubiquitous in interactive scenarios especially dialogues, limiting the ASU performance. Recently, large language models (LLMs) shows the powerful ability to integrate various NLP tasks with the chat paradigm. In this way, this paper proposes a new Chat-based Aspect Sentiment Understanding (ChatASU) task, aiming to explore LLMs&#39; ability in understanding aspect sentiments in dialogue scenarios. Particularly, this ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to address the aspect coreference issue. On this basis, we propose a Trusted Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU. Specifically, this TSA treats the ACR task as an auxiliary task to boost the performance of the primary ASU task, and further integrates trusted learning into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to evaluate TSA, and extensive experiments show that our proposed TSA can significantly outperform several state-of-the-art baselines, justifying the effectiveness of TSA to ChatASU and the importance of considering the coreference and hallucination issues in ChatASU.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2403.05326"
    },
    "bf1e2f319094afd2377286a6c0cc74ce": {
        "title": "Strength Lies in Differences! Towards Effective Non-collaborative Dialogues via Tailored Strategy Planning",
        "authors": [
            "Tong Zhang",
            "Chen Huang",
            "Yang Deng",
            "Hongru Liang",
            "Jia Liu",
            "Zujie Wen",
            "Wenqiang Lei",
            "Tat-Seng Chua"
        ],
        "date": "2024/03/11",
        "pdf": "http://arxiv.org/pdf/2403.06769",
        "abstract": "We investigate non-collaborative dialogue agents that must engage in tailored strategic planning for diverse users to secure a favorable agreement. This poses challenges for existing dialogue agents due to two main reasons: their inability to integrate user-specific characteristics into their strategic planning and their training paradigm&#39;s failure to produce strategic planners that can generalize to diverse users. To address these challenges, we propose TRIP to enhance the capability in tailored strategic planning, incorporating a user-aware strategic planning module and a population-based training paradigm. Through experiments on benchmark non-collaborative dialogue tasks, we demonstrate the effectiveness of TRIP in catering to diverse users.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.06769"
    },
    "46fceb681f72cba835bf36132b086d85": {
        "title": "AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents",
        "authors": [
            "Yao Fu",
            "Dong-Ki Kim",
            "Jaekyeom Kim",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Kyunghoon Bae",
            "Honglak Lee"
        ],
        "date": "2024/03/13",
        "pdf": "http://arxiv.org/pdf/2403.08978",
        "abstract": "The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent&#39;s current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential decision-making benchmarks.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2403.08978"
    },
    "1bed4874c7bbe62254c363f22586f90e": {
        "title": "Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation",
        "authors": [
            "Se-eun Yoon",
            "Zhankui He",
            "Jessica Maria Echterhoff",
            "Julian McAuley"
        ],
        "date": "2024/03/13",
        "pdf": "http://arxiv.org/pdf/2403.09738",
        "abstract": "Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.09738"
    },
    "1aadeba9f62724a15bc508b6fd42e956": {
        "title": "Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback",
        "authors": [
            "Dong Won Lee",
            "Hae Won Park",
            "Yoon Kim",
            "Cynthia Breazeal",
            "Louis-Philippe Morency"
        ],
        "date": "2024/03/17",
        "pdf": "http://arxiv.org/pdf/2403.11330",
        "abstract": "We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2403.11330"
    },
    "ef4ee63fa07c8149752731e8aeed308a": {
        "title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction",
        "authors": [
            "Xiang Huang",
            "Sitao Cheng",
            "Shanshan Huang",
            "Jiayu Shen",
            "Yong Xu",
            "Chaoyun Zhang",
            "Yuzhong Qu"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.11886",
        "abstract": "Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs step-wise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 7.0 and 15.0 F1. Moreover, our approach exhibits superiority in terms of efficiency, including runtime, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, revealing the strong transferability of our approach.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2403.11886"
    },
    "f6008fb8d9c10f96f8bcc4c46291d9f5": {
        "title": "EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents",
        "authors": [
            "Abhay Zala",
            "Jaemin Cho",
            "Han Lin",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "date": "2024/03/18",
        "pdf": "http://arxiv.org/pdf/2403.12014",
        "abstract": "Recent SOTA approaches for embodied learning via interaction directly employ large language models (LLMs) as agents to determine the next steps in an environment. Due to their world knowledge and reasoning capabilities, LLM agents achieve stronger performance than previous smaller agents based on reinforcement learning (RL); however, frequently calling LLMs is slow and expensive. Instead of directly employing LLMs as agents, can we use LLMs&#39; reasoning capabilities to adaptively create training environments to help smaller embodied RL agents learn useful skills that they are weak at? We propose EnvGen, a novel framework to address this question. First, we prompt an LLM to generate training environments that allow agents to quickly learn different tasks in parallel. Concretely, the LLM is given the task description and simulator objectives that the agents should learn and is then asked to generate a set of environment configurations (e.g., different terrains, items given to agents, etc.). Next, we train a small RL agent in a mixture of the original and LLM-generated environments. Then, we enable the LLM to continuously adapt the generated environments to progressively improve the skills that the agent is weak at, by providing feedback to the LLM in the form of the agent&#39;s performance. We demonstrate the usefulness of EnvGen with comprehensive experiments in Crafter and Heist environments. We find that a small RL agent trained with EnvGen can outperform SOTA methods, including a GPT-4 agent, and learns long-horizon tasks significantly faster. We show qualitatively how the LLM adapts training environments to help improve RL agents&#39; weaker skills over time. Additionally, EnvGen is substantially more efficient as it only uses a small number of LLM calls (e.g., 4 in total), whereas LLM agents require thousands of LLM calls. Lastly, we present detailed ablation studies for our design choices.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.12014"
    },
    "6323bb1cd980022ec0d082da0b6a35e3": {
        "title": "Characteristic AI Agents via Large Language Models",
        "authors": [
            "Xi Wang",
            "Hongliang Dai",
            "Shen Gao",
            "Piji Li"
        ],
        "date": "2024/03/19",
        "pdf": "http://arxiv.org/pdf/2403.12368",
        "abstract": "The advancement of Large Language Models (LLMs) has led to significant enhancements in the performance of chatbot systems. Many researchers have dedicated their efforts to the development of bringing characteristics to chatbots. While there have been commercial products for developing role-driven chatbots using LLMs, it is worth noting that academic research in this area remains relatively scarce. Our research focuses on investigating the performance of LLMs in constructing Characteristic AI Agents by simulating real-life individuals across different settings. Current investigations have primarily focused on act on roles with simple profiles. In response to this research gap, we create a benchmark for the characteristic AI agents task, including dataset, techniques, and evaluation metrics. A dataset called ``Character100&#39;&#39; is built for this benchmark, comprising the most-visited people on Wikipedia for language models to role-play. With the constructed dataset, we conduct comprehensive assessment of LLMs across various settings. In addition, we devise a set of automatic metrics for quantitative performance evaluation. The experimental results underscore the potential directions for further improvement in the capabilities of LLMs in constructing characteristic AI agents. The benchmark is available at https://github.com/nuaa-nlp/Character100.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.12368"
    },
    "ea6393973de4fe2d29ba00e20f748441": {
        "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models",
        "authors": [
            "Zehui Chen",
            "Kuikun Liu",
            "Qiuchen Wang",
            "Wenwei Zhang",
            "Jiangning Liu",
            "Dahua Lin",
            "Kai Chen",
            "Feng Zhao"
        ],
        "date": "2024/03/19",
        "pdf": "http://arxiv.org/pdf/2403.12881",
        "abstract": "Open-sourced Large Language Models (LLMs) have achieved great success in various NLP tasks, however, they are still far inferior to API-based models when acting as agents. How to integrate agent ability into general LLMs becomes a crucial and urgent problem. This paper first delivers three key observations: (1) the current agent training corpus is entangled with both formats following and agent reasoning, which significantly shifts from the distribution of its pre-training data; (2) LLMs exhibit different learning speeds on the capabilities required by agent tasks; and (3) current approaches have side-effects when improving agent abilities by introducing hallucinations. Based on the above findings, we propose Agent-FLAN to effectively Fine-tune LANguage models for Agents. Through careful decomposition and redesign of the training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by 3.5\\% across various agent evaluation datasets. With comprehensively constructed negative samples, Agent-FLAN greatly alleviates the hallucination issues based on our established evaluation benchmark. Besides, it consistently improves the agent capability of LLMs when scaling model sizes while slightly enhancing the general capability of LLMs. The code will be available at https://github.com/InternLM/Agent-FLAN.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.12881"
    },
    "1cc44ebd645024966f49e3a5a660eae1": {
        "title": "RoleInteract: Evaluating the Social Interaction of Role-Playing Agents",
        "authors": [
            "Hongzhan Chen",
            "Hehong Chen",
            "Ming Yan",
            "Wenshen Xu",
            "Xing Gao",
            "Weizhou Shen",
            "Xiaojun Quan",
            "Chenliang Li",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "date": "2024/03/20",
        "pdf": "http://arxiv.org/pdf/2403.13679",
        "abstract": "Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Moreover, the behavior of individuals may drift as a result of the influence exerted by other agents within the group. Experimental results on RoleInteract confirm its significance as a testbed for assessing the social interaction of role-playing conversational agents. The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2403.13679"
    },
    "d254cf4f173176c2999c0a1d87ca1ac8": {
        "title": "Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies",
        "authors": [
            "Philipp Sadler",
            "Sherzod Hakimov",
            "David Schlangen"
        ],
        "date": "2024/03/26",
        "pdf": "http://arxiv.org/pdf/2403.17497",
        "abstract": "In collaborative goal-oriented settings, the participants are not only interested in achieving a successful outcome, but do also implicitly negotiate the effort they put into the interaction (by adapting to each other). In this work, we propose a challenging interactive reference game that requires two players to coordinate on vision and language observations. The learning signal in this game is a score (given after playing) that takes into account the achieved goal and the players&#39; assumed efforts during the interaction. We show that a standard Proximal Policy Optimization (PPO) setup achieves a high success rate when bootstrapped with heuristic partner behaviors that implement insights from the analysis of human-human interactions. And we find that a pairing of neural partners indeed reduces the measured joint effort when playing together repeatedly. However, we observe that in comparison to a reasonable heuristic pairing there is still room for improvement -- which invites further research in the direction of cost-sharing in collaborative interactions.",
        "code": "https://github.com/clp-research/cost-sharing-reference-game",
        "category": [
            "Game Playing",
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2403.17497"
    },
    "85fd90755f5739a7fb483412602e97e6": {
        "title": "CACA Agent: Capability Collaboration based AI Agent",
        "authors": [
            "Peng Xu",
            "Haoran Wang",
            "Chuang Wang",
            "Xu Liu"
        ],
        "date": "2024/03/22",
        "pdf": "http://arxiv.org/pdf/2403.15137",
        "abstract": "As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario extension of CACA Agent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2403.15137"
    },
    "6c7684079093a696f575560ed81b715f": {
        "title": "Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering",
        "authors": [
            "Bowen Jiang",
            "Zhijun Zhuang",
            "Shreyas S. Shivakumar",
            "Dan Roth",
            "Camillo J. Taylor"
        ],
        "date": "2024/03/21",
        "pdf": "http://arxiv.org/pdf/2403.14783",
        "abstract": "This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system&#39;s performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.14783"
    },
    "f8ff753c3aa2bb437a2e5380838bfef6": {
        "title": "ReAct Meets ActRe: Autonomous Annotation of Agent Trajectories for Contrastive Self-Training",
        "authors": [
            "Zonghan Yang",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "date": "2024/03/21",
        "pdf": "http://arxiv.org/pdf/2403.14589",
        "abstract": "Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotation or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent executes multiple trajectories for the failed tasks, and selects the successful ones to supplement its failed trajectory for contrastive self-training. Realized by policy gradient methods with binarized rewards, the contrastive self-training with accumulated trajectories facilitates a closed loop for multiple rounds of language agent self-improvement. We conduct experiments using QLoRA fine-tuning with the open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human average, and 4 rounds of iterative refinement lead to the performance approaching human experts. A$^3$T agents significantly outperform existing techniques, including prompting with GPT-4, advanced agent frameworks, and fully fine-tuned LLMs.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.14589"
    },
    "b14f11857f2bd0666a823802ced632df": {
        "title": "Learn to Disguise: Avoid Refusal Responses in LLM&#39;s Defense via a Multi-agent Attacker-Disguiser Game",
        "authors": [
            "Qianqiao Xu",
            "Zhiliang Tian",
            "Hongyan Wu",
            "Zhen Huang",
            "Yiping Song",
            "Feng Liu",
            "Dongsheng Li"
        ],
        "date": "2024/04/03",
        "pdf": "http://arxiv.org/pdf/2404.02532",
        "abstract": "With the enhanced performance of large models on natural language processing tasks, potential moral and ethical issues of large models arise. There exist malicious attackers who induce large models to jailbreak and generate information containing illegal, privacy-invasive information through techniques such as prompt engineering. As a result, large models counter malicious attackers&#39; attacks using techniques such as safety alignment. However, the strong defense mechanism of the large model through rejection replies is easily identified by attackers and used to strengthen attackers&#39; capabilities. In this paper, we propose a multi-agent attacker-disguiser game approach to achieve a weak defense mechanism that allows the large model to both safely reply to the attacker and hide the defense intent. First, we construct a multi-agent framework to simulate attack and defense scenarios, playing different roles to be responsible for attack, disguise, safety evaluation, and disguise evaluation tasks. After that, we design attack and disguise game algorithms to optimize the game strategies of the attacker and the disguiser and use the curriculum learning process to strengthen the capabilities of the agents. The experiments verify that the method in this paper is more effective in strengthening the model&#39;s ability to disguise the defense intent compared with other methods. Moreover, our approach can adapt any black-box large model to assist the model in defense and does not suffer from model version iterations.",
        "code": "",
        "category": [
            "Multi-Agent System",
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2404.02532"
    },
    "132f6a74ec7ec450c6106e9467b63208": {
        "title": "Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization",
        "authors": [
            "Yoichi Ishibashi",
            "Yoshimasa Nishimura"
        ],
        "date": "2024/04/02",
        "pdf": "http://arxiv.org/pdf/2404.02183",
        "abstract": "Recent advancements in automatic code generation using large language model (LLM) agent have brought us closer to the future of automated software development. However, existing single-agent approaches face limitations in generating and improving large-scale, complex codebases due to constraints in context length. To tackle this challenge, we propose Self-Organized multi-Agent framework (SoA), a novel multi-agent framework that enables the scalable and efficient generation and optimization of large-scale code. In SoA, self-organized agents operate independently to generate and modify code components while seamlessly collaborating to construct the overall codebase. A key feature of our framework is the automatic multiplication of agents based on problem complexity, allowing for dynamic scalability. This enables the overall code volume to be increased indefinitely according to the number of agents, while the amount of code managed by each agent remains constant. We evaluate SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent system, each agent in SoA handles significantly less code, yet the overall generated code is substantially greater. Moreover, SoA surpasses the powerful single-agent baseline by 5% in terms of Pass@1 accuracy.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.02183"
    },
    "ab2ebbcc0da2f32564c4167a2306ff73": {
        "title": "CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models",
        "authors": [
            "Xuechen Liang",
            "Meiling Tao",
            "Tianyu Shi",
            "Yiting Xie"
        ],
        "date": "2024/04/02",
        "pdf": "http://arxiv.org/pdf/2404.01663",
        "abstract": "Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.01663"
    },
    "0da1af4d68be428f423f0e488588bb54": {
        "title": "DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model",
        "authors": [
            "Lirui Zhao",
            "Yue Yang",
            "Kaipeng Zhang",
            "Wenqi Shao",
            "Yuxin Zhang",
            "Yu Qiao",
            "Ping Luo",
            "Rongrong Ji"
        ],
        "date": "2024/03/31",
        "pdf": "http://arxiv.org/pdf/2404.01342",
        "abstract": "Text-to-image (T2I) generative models have attracted significant attention and found extensive applications within and beyond academic research. For example, the Civitai community, a platform for T2I innovation, currently hosts an impressive array of 74,492 distinct models. However, this diversity presents a formidable challenge in selecting the most appropriate model and parameters, a process that typically requires numerous trials. Drawing inspiration from the tool usage research of large language models (LLMs), we introduce DiffAgent, an LLM agent designed to screen the accurate selection in seconds via API calls. DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to accurately align T2I API responses with user input in accordance with human preferences. To train and evaluate DiffAgent&#39;s capabilities, we present DABench, a comprehensive dataset encompassing an extensive range of T2I APIs from the community. Our evaluations reveal that DiffAgent not only excels in identifying the appropriate T2I API but also underscores the effectiveness of the SFTA training framework. Codes are available at https://github.com/OpenGVLab/DiffAgent.",
        "code": "https://github.com/OpenGVLab/DiffAgent",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.01342"
    },
    "4a545d9e08ef7641444abce3a33444ee": {
        "title": "TraveLER: A Multi-LMM Agent Framework for Video Question-Answering",
        "authors": [
            "Chuyi Shang",
            "Amos You",
            "Sanjay Subramanian",
            "Trevor Darrell",
            "Roei Herzig"
        ],
        "date": "2024/04/01",
        "pdf": "http://arxiv.org/pdf/2404.01476",
        "abstract": "Recently, Large Multimodal Models (LMMs) have made significant progress in video question-answering using a frame-wise approach by leveraging large-scale, image-based pretraining in a zero-shot manner. While image-based methods for videos have shown impressive performance, a current limitation is that they often overlook how key timestamps are selected and cannot adjust when incorrect timestamps are identified. Moreover, they are unable to extract details relevant to the question, instead providing general descriptions of the frame. To overcome this, we design a multi-LMM agent framework that travels along the video, iteratively collecting relevant information from keyframes through interactive question-asking until there is sufficient information to answer the question. Specifically, we propose TraveLER, a model that can create a plan to &#34;Traverse&#34; through the video, ask questions about individual frames to &#34;Locate&#34; and store key information, and then &#34;Evaluate&#34; if there is enough information to answer the question. Finally, if there is not enough information, our method is able to &#34;Replan&#34; based on its collected knowledge. Through extensive experiments, we find that the proposed TraveLER approach improves performance on several video question-answering benchmarks, such as NExT-QA, STAR, and Perception Test, without the need to fine-tune on specific datasets.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.01476"
    },
    "13a2dcc1dde7c8d8aa2560b567b6a73b": {
        "title": "DataAgent: Evaluating Large Language Models&#39; Ability to Answer Zero-Shot, Natural Language Queries",
        "authors": [
            "Manit Mishra",
            "Abderrahman Braham",
            "Charles Marsom",
            "Bryan Chung",
            "Gavin Griffin",
            "Dakshesh Sidnerlikar",
            "Chatanya Sarin",
            "Arjun Rajaram"
        ],
        "date": "2024/03/29",
        "pdf": "http://arxiv.org/pdf/2404.00188",
        "abstract": "Conventional processes for analyzing datasets and extracting meaningful information are often time-consuming and laborious. Previous work has identified manual, repetitive coding and data collection as major obstacles that hinder data scientists from undertaking more nuanced labor and high-level projects. To combat this, we evaluated OpenAI&#39;s GPT-3.5 as a &#34;Language Data Scientist&#34; (LDS) that can extrapolate key findings, including correlations and basic information, from a given dataset. The model was tested on a diverse set of benchmark datasets to evaluate its performance across multiple standards, including data science code-generation based tasks involving libraries such as NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in correctly answering a given data science query related to the benchmark dataset. The LDS used various novel prompt engineering techniques to effectively answer a given question, including Chain-of-Thought reinforcement and SayCan prompt engineering. Our findings demonstrate great potential for leveraging Large Language Models for low-level, zero-shot data analysis.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.00188"
    },
    "f8ee0b1f0b3c9c1fea3d64bc0053aa66": {
        "title": "AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web Navigating Agent",
        "authors": [
            "Hanyu Lai",
            "Xiao Liu",
            "Iat Long Iong",
            "Shuntian Yao",
            "Yuxuan Chen",
            "Pengbo Shen",
            "Hao Yu",
            "Hanchen Zhang",
            "Xiaohan Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2024/04/04",
        "pdf": "http://arxiv.org/pdf/2404.03648",
        "abstract": "Large language models (LLMs) have fueled many intelligent agent tasks, such as web navigation -- but most existing agents perform far from satisfying in real-world webpages due to three factors: (1) the versatility of actions on webpages, (2) HTML text exceeding model processing capacity, and (3) the complexity of decision-making due to the open-domain nature of web. In light of the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns, we design an HTML simplification algorithm to represent webpages, preserving vital information succinctly. We employ a hybrid human-AI method to build web browsing data for curriculum training. Then, we bootstrap the model by reinforcement learning and rejection sampling to further facilitate webpage comprehension, browser operations, and efficient task decomposition by itself. For testing, we establish a bilingual benchmark -- AutoWebBench -- for real-world web browsing tasks. We evaluate AutoWebGLM across diverse web navigation benchmarks, revealing its improvements but also underlying challenges to tackle real environments. Related code, model, and data will be released at \\url{https://github.com/THUDM/AutoWebGLM}.",
        "code": "https://github.com/THUDM/AutoWebGLM",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.03648"
    },
    "c5a6769c1c0badbeb24b45a8b9e4b1d9": {
        "title": "EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction",
        "authors": [
            "Siyu Yuan",
            "Kaitao Song",
            "Jiangjie Chen",
            "Xu Tan",
            "Yongliang Shen",
            "Ren Kan",
            "Dongsheng Li",
            "Deqing Yang"
        ],
        "date": "2024/01/11",
        "pdf": "http://arxiv.org/pdf/2401.06201",
        "abstract": "To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our code will be available at \\url{https://github.com/microsoft/JARVIS/} in the future.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2401.06201"
    },
    "d6c196649bda278dff206fe125f58a0a": {
        "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
        "authors": [
            "Yu Li",
            "Shenyu Zhang",
            "Rui Wu",
            "Xiutian Huang",
            "Yongrui Chen",
            "Wenhao Xu",
            "Guilin Qi",
            "Dehai Min"
        ],
        "date": "2024/03/28",
        "pdf": "http://arxiv.org/pdf/2403.19305",
        "abstract": "Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A &#34;Multi-Agent Text Evaluation framework&#34; where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents&#39; interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.19305"
    },
    "c1649f215b96688fccfcbc30d4e394e1": {
        "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
        "authors": [
            "Qinhao Zhou",
            "Zihan Zhang",
            "Xiang Xiang",
            "Ke Wang",
            "Yuchuan Wu",
            "Yongbin Li"
        ],
        "date": "2024/03/29",
        "pdf": "http://arxiv.org/pdf/2403.19962",
        "abstract": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2403.19962"
    },
    "ca90b46ea8556ad2018c64fe4bb59682": {
        "title": "ITCMA: A Generative Agent Based on a Computational Consciousness Structure",
        "authors": [
            "Hanzhong Zhang",
            "Jibin Yin",
            "Haoyang Wang",
            "Ziwei Xiang"
        ],
        "date": "2024/03/29",
        "pdf": "http://arxiv.org/pdf/2403.20097",
        "abstract": "Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs&#39; ability to understand implicit instructions and apply common-sense knowledge by considering agents&#39; interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher than SOTA, indicating its superiority over traditional intelligent agents in utility and generalization. In real-world tasks with quadruped robots, the untrained ITCMA achieves an 85% task completion rate, which is close to its performance in the unseen set, demonstrating its comparable utility in real-world settings.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2403.20097"
    },
    "bbdc7d81c7e1344bee08ecab46a592ba": {
        "title": "Empowering Biomedical Discovery with AI Agents",
        "authors": [
            "Shanghua Gao",
            "Ada Fang",
            "Yepeng Huang",
            "Valentina Giunchiglia",
            "Ayush Noori",
            "Jonathan Richard Schwarz",
            "Yasha Ektefaie",
            "Jovana Kondic",
            "Marinka Zitnik"
        ],
        "date": "2024/04/03",
        "pdf": "http://arxiv.org/pdf/2404.02831",
        "abstract": "We envision &#39;AI scientists&#39; as systems capable of skeptical learning and reasoning that empower biomedical research through collaborative agents that integrate machine learning tools with experimental platforms. Rather than taking humans out of the discovery process, biomedical AI agents combine human creativity and expertise with AI&#39;s ability to analyze large datasets, navigate hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a variety of tasks, including self-assessment and planning of discovery workflows. These agents use large language models and generative models to feature structured memory for continual learning and use machine learning tools to incorporate scientific knowledge, biological principles, and theories. AI agents can impact areas ranging from hybrid cell simulation, programmable control of phenotypes, and the design of cellular circuits to the development of new therapies.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.02831"
    },
    "bd832d696f6c8b97c8a962d70be2a48d": {
        "title": "MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution",
        "authors": [
            "Wei Tao",
            "Yucheng Zhou",
            "Wenqiang Zhang",
            "Yu Cheng"
        ],
        "date": "2024/03/26",
        "pdf": "http://arxiv.org/pdf/2403.17927",
        "abstract": "In software evolution, resolving the emergent issues within GitHub repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing functionalities. Large Language Models (LLMs) have shown promise in code generation and understanding but face difficulties in code change, particularly at the repository level. To overcome these challenges, we empirically study the reason why LLMs mostly fail to resolve GitHub issues and analyze some impact factors. Motivated by the empirical findings, we propose a novel LLM-based Multi-Agent framework for GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized for the software evolution: Manager, Repository Custodian, Developer, and Quality Assurance Engineer agents. This framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase in resolved ratio over the direct application of GPT-4, the based LLM of our method. We also analyze the factors for improving GitHub issue resolution rates, such as line location, task allocation, etc.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2403.17927"
    },
    "6037287a594e3e33c4df8c55a9261097": {
        "title": "Cleared for Takeoff? Compositional &amp; Conditional Reasoning may be the Achilles Heel to (Flight-Booking) Language Agents",
        "authors": [
            "Harsh Kohli",
            "Huan Sun"
        ],
        "date": "2024/04/05",
        "pdf": "http://arxiv.org/pdf/2404.04237",
        "abstract": "The rapid progress of large language models (LLMs) has seen them excel and frequently surpass human performance on standard benchmarks. This has enabled many downstream applications, such as LLM agents, to rely on their sophisticated reasoning to navigate complex task requirements. However, LLMs are known to unexpectedly falter in simple tasks and under seemingly straightforward circumstances - underscoring the need for better and more diverse evaluation setups to measure their true capabilities. To this end, we choose to study compositional and conditional reasoning, two cornerstones of human cognition, and introduce GroundCocoa - a lexically diverse benchmark connecting these reasoning skills to the real-world problem of flight booking. Our task involves aligning detailed user preferences with available flight options presented in a multiple-choice format. Results indicate a significant disparity in performance among current state-of-the-art LLMs with even the best performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced prompting techniques.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2404.04237"
    },
    "95468202e69deb8c7b13b4296cb1384f": {
        "title": "Social Skill Training with Large Language Models",
        "authors": [
            "Diyi Yang",
            "Caleb Ziems",
            "William Held",
            "Omar Shaikh",
            "Michael S. Bernstein",
            "John Mitchell"
        ],
        "date": "2024/04/05",
        "pdf": "http://arxiv.org/pdf/2404.04204",
        "abstract": "People rely on social skills like conflict resolution to communicate effectively and to thrive in both work and personal life. However, practice environments for social skills are typically out of reach for most people. How can we make social skill training more available, accessible, and inviting? Drawing upon interdisciplinary research from communication and psychology, this perspective paper identifies social skill barriers to enter specialized fields. Then we present a solution that leverages large language models for social skill training via a generic framework. Our AI Partner, AI Mentor framework merges experiential learning with realistic practice and tailored feedback. This work ultimately calls for cross-disciplinary innovation to address the broader implications for workforce development and social equality.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2404.04204"
    },
    "5e8a9349f5f9f58e294e0558b604cbf4": {
        "title": "A Survey on Large Language Model-Based Game Agents",
        "authors": [
            "Sihao Hu",
            "Tiansheng Huang",
            "Fatih Ilhan",
            "Selim Tekin",
            "Gaowen Liu",
            "Ramana Kompella",
            "Ling Liu"
        ],
        "date": "2024/04/02",
        "pdf": "http://arxiv.org/pdf/2404.02039",
        "abstract": "The development of game agents holds a critical role in advancing towards Artificial General Intelligence (AGI). The progress of LLMs and their multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve and empower game agents with human-like decision-making capabilities in complex computer game environments. This paper provides a comprehensive overview of LLM-based game agents from a holistic viewpoint. First, we introduce the conceptual architecture of LLM-based game agents, centered around six essential functional components: perception, memory, thinking, role-playing, action, and learning. Second, we survey existing representative LLM-based game agents documented in the literature with respect to methodologies and adaptation agility across six genres of games, including adventure, communication, competition, cooperation, simulation, and crafting &amp; exploration games. Finally, we present an outlook of future research and development directions in this burgeoning field. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.",
        "code": "https://github.com/git-disl/awesome-LLM-game-agent-papers",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.02039"
    },
    "6871dbd47b870df8b8c81d0776a86b40": {
        "title": "360{\\deg}REA: Towards A Reusable Experience Accumulation with 360{\\deg} Assessment for Multi-Agent System",
        "authors": [
            "Shen Gao",
            "Hao Li",
            "Zhengliang Shi",
            "Chengrui Huang",
            "Quan Tu",
            "Zhiliang Tian",
            "Minlie Huang",
            "Shuo Shang"
        ],
        "date": "2024/04/08",
        "pdf": "http://arxiv.org/pdf/2404.05569",
        "abstract": "Large language model agents have demonstrated remarkable advancements across various complex tasks. Recent works focus on optimizing the agent team or employing self-reflection to iteratively solve complex tasks. Since these agents are all based on the same LLM, only conducting self-evaluation or removing underperforming agents does not substantively enhance the capability of the agents. We argue that a comprehensive evaluation and accumulating experience from evaluation feedback is an effective approach to improving system performance. In this paper, we propose Reusable Experience Accumulation with 360{\\deg} Assessment (360{\\deg}REA), a hierarchical multi-agent framework inspired by corporate organizational practices. The framework employs a novel 360{\\deg} performance assessment method for multi-perspective performance evaluation with fine-grained assessment. To enhance the capability of agents in addressing complex tasks, we introduce dual-level experience pool for agents to accumulate experience through fine-grained assessment. Extensive experiments on complex task datasets demonstrate the effectiveness of 360{\\deg}REA.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.05569"
    },
    "0cfb4e05bac6b3386a7765e13cc10d48": {
        "title": "Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot Systems",
        "authors": [
            "Kunal Garg",
            "Jacob Arkin",
            "Songyuan Zhang",
            "Nicholas Roy",
            "Chuchu Fan"
        ],
        "date": "2024/04/09",
        "pdf": "http://arxiv.org/pdf/2404.06413",
        "abstract": "Multi-agent robotic systems are prone to deadlocks in an obstacle environment where the system can get stuck away from its desired location under a smooth low-level control policy. Without an external intervention, often in terms of a high-level command, it is not possible to guarantee that just a low-level control policy can resolve such deadlocks. Utilizing the generalizability and low data requirements of large language models (LLMs), this paper explores the possibility of using LLMs for deadlock resolution. We propose a hierarchical control framework where an LLM resolves deadlocks by assigning a leader and direction for the leader to move along. A graph neural network (GNN) based low-level distributed control policy executes the assigned plan. We systematically study various prompting techniques to improve LLM&#39;s performance in resolving deadlocks. In particular, as part of prompt engineering, we provide in-context examples for LLMs. We conducted extensive experiments on various multi-robot environments with up to 15 agents and 40 obstacles. Our results demonstrate that LLM-based high-level planners are effective in resolving deadlocks in MRS.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.06413"
    },
    "d495dbace9bd6c2bbbbc13da7e2af198": {
        "title": "AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents",
        "authors": [
            "Luca Gioacchini",
            "Giuseppe Siracusano",
            "Davide Sanvito",
            "Kiril Gashteovski",
            "David Friede",
            "Roberto Bifulco",
            "Carolin Lawrence"
        ],
        "date": "2024/04/09",
        "pdf": "http://arxiv.org/pdf/2404.06411",
        "abstract": "The advances made by Large Language Models (LLMs) have led to the pursuit of LLM agents that can solve intricate, multi-step reasoning tasks. As with any research pursuit, benchmarking and evaluation are key corner stones to efficient and reliable progress. However, existing benchmarks are often narrow and simply compute overall task success. To face these issues, we propose AgentQuest -- a framework where (i) both benchmarks and metrics are modular and easily extensible through well documented and easy-to-use APIs; (ii) we offer two new evaluation metrics that can reliably track LLM agent progress while solving a task. We exemplify the utility of the metrics on two use cases wherein we identify common failure points and refine the agent architecture to obtain a significant performance increase. Together with the research community, we hope to extend AgentQuest further and therefore we make it available under https://github.com/nec-research/agentquest.",
        "code": "https://github.com/nec-research/agentquest",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2404.06411"
    },
    "3919bfea394e8ef4c0ad613ffd0bd296": {
        "title": "SurveyAgent: A Conversational System for Personalized and Efficient Research Survey",
        "authors": [
            "Xintao Wang",
            "Jiangjie Chen",
            "Nianqi Li",
            "Lida Chen",
            "Xinfeng Yuan",
            "Wei Shi",
            "Xuyang Ge",
            "Rui Xu",
            "Yanghua Xiao"
        ],
        "date": "2024/04/09",
        "pdf": "http://arxiv.org/pdf/2404.06364",
        "abstract": "In the rapidly advancing research fields such as AI, managing and staying abreast of the latest scientific literature has become a significant challenge for researchers. Although previous efforts have leveraged AI to assist with literature searches, paper recommendations, and question-answering, a comprehensive support system that addresses the holistic needs of researchers has been lacking. This paper introduces SurveyAgent, a novel conversational system designed to provide personalized and efficient research survey assistance to researchers. SurveyAgent integrates three key modules: Knowledge Management for organizing papers, Recommendation for discovering relevant literature, and Query Answering for engaging with content on a deeper level. This system stands out by offering a unified platform that supports researchers through various stages of their literature review process, facilitated by a conversational interface that prioritizes user interaction and personalization. Our evaluation demonstrates SurveyAgent&#39;s effectiveness in streamlining research activities, showcasing its capability to facilitate how researchers interact with scientific literature.",
        "code": "",
        "category": [
            "Role Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.06364"
    },
    "6cff9e7aba238d85b3a23a95296ae8b2": {
        "title": "Search Beyond Queries: Training Smaller Language Models for Web Interactions via Reinforcement Learning",
        "authors": [
            "Moghis Fereidouni",
            "A. B. Siddique"
        ],
        "date": "2024/04/16",
        "pdf": "http://arxiv.org/pdf/2404.10887",
        "abstract": "Traditional search systems focus on query formulation for effective results but face challenges in scenarios such as product searches where crucial product details (e.g., size, color) remain concealed until users visit specific product pages. This highlights the need for intelligent web navigation agents capable of formulating queries and navigating web pages according to users&#39; high-level intents. In response to this need, this work introduces a Grounded Language Agent for Intelligent Web Interactions, called GLAINTEL. Drawing upon advancements in language modeling and reinforcement learning, GLAINTEL investigates the efficacy of transformer-based models in enhancing the search capabilities of interactive web environments. Given the dynamic action space for each state in web navigation, GLAINTEL employs the Flan-T5 architecture and incorporates language modeling and value estimation heads. This work focuses on training smaller language models as agents across various scenarios, systematically evaluating the impact of human demonstrations on the training process. Specifically, we investigate scenarios where no human demonstrations are available and subsequently assess the effective utilization of such demonstrations. We also explore unsupervised domain adaptation for situations where demonstrations are confined to a specific domain. Experimental evaluations across diverse setups demonstrate the effectiveness of training agents in unsupervised settings, outperforming in-context learning-based approaches that employ larger models with up to 540 billion parameters. Surprisingly, behavioral cloning-based methods that straightforwardly use human demonstrations do not outperform unsupervised learning-based methods. Additionally, combining human demonstrations with Reinforcement Learning-based training yields results comparable to models utilizing GPT-4.",
        "code": "",
        "category": [
            "Agent Fine-tuning",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.10887"
    },
    "6429fc4c2cf6413ca87d31ea50fa16af": {
        "title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
        "authors": [
            "Tula Masterman",
            "Sandi Besen",
            "Mason Sawtell",
            "Alex Chao"
        ],
        "date": "2024/04/17",
        "pdf": "http://arxiv.org/pdf/2404.11584",
        "abstract": "This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.11584"
    },
    "41737985bf67ad09153b971991bed0bd": {
        "title": "Advancing Social Intelligence in AI Agents: Technical Challenges and Open Questions",
        "authors": [
            "Leena Mathur",
            "Paul Pu Liang",
            "Louis-Philippe Morency"
        ],
        "date": "2024/04/17",
        "pdf": "http://arxiv.org/pdf/2404.11023",
        "abstract": "Building socially-intelligent AI agents (Social-AI) is a multidisciplinary, multimodal research goal that involves creating agents that can sense, perceive, reason about, learn from, and respond to affect, behavior, and cognition of other agents (human or artificial). Progress towards Social-AI has accelerated in the past decade across several computing communities, including natural language processing, machine learning, robotics, human-machine interaction, computer vision, and speech. Natural language processing, in particular, has been prominent in Social-AI research, as language plays a key role in constructing the social world. In this position paper, we identify a set of underlying technical challenges and open questions for researchers across computing communities to advance Social-AI. We anchor our discussion in the context of social intelligence concepts and prior progress in Social-AI research.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2404.11023"
    },
    "035610ab3e24c6452439d97d8816f74b": {
        "title": "Memory Sharing for Large Language Model based Agents",
        "authors": [
            "Hang Gao",
            "Yongfeng Zhang"
        ],
        "date": "2024/04/15",
        "pdf": "http://arxiv.org/pdf/2404.09982",
        "abstract": "In the realm of artificial intelligence, the adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning for fixed-answer tasks such as common sense questions and yes/no queries. However, the application of In-context Learning to open-ended challenges, such as poetry creation, reveals substantial limitations due to the comprehensiveness of the provided examples and agent&#39;s ability to understand the content expressed in the problem, leading to outputs that often diverge significantly from expected results. Addressing this gap, our study introduces the Memory-Sharing (MS) framework for LLM multi-agents, which utilizes a real-time memory storage and retrieval system to enhance the In-context Learning process. Each &#34;memory&#34; within this system captures both the posed query and the corresponding real-time response from an LLM-based agent, aggregating these memories from a broad spectrum of similar agents to enrich the memory pool shared by all agents. This framework not only aids agents in identifying the most relevant examples for specific tasks but also evaluates the potential utility of their memories for future applications by other agents. Empirical validation across three distinct domains involving specialized functions of agents demonstrates that the MS framework significantly improve the agent&#39;s performance regrading the open-ended questions. Furthermore, we also discuss what type of memory pool and what retrieval strategy in MS can better help agents, offering a future develop direction of MS. The code and data are available at: https://github.com/GHupppp/MemorySharingLLM",
        "code": "https://github.com/GHupppp/MemorySharingLLM",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2404.09982"
    },
    "50229be1bb06bde01c7cd0f4bcd6a33e": {
        "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
        "authors": [
            "Ruixin Yang",
            "Dheeraj Rajagopal",
            "Shirley Anugrah Hayati",
            "Bin Hu",
            "Dongyeop Kang"
        ],
        "date": "2024/04/14",
        "pdf": "http://arxiv.org/pdf/2404.09127",
        "abstract": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the &#34;Collective Wisdom&#34;: the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.09127"
    },
    "e574d5257ac04d06751dabd1e710f55c": {
        "title": "MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs",
        "authors": [
            "Xianhao Yu",
            "Jiaqi Fu",
            "Renjia Deng",
            "Wenjuan Han"
        ],
        "date": "2024/03/28",
        "pdf": "http://arxiv.org/pdf/2403.19267",
        "abstract": "Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/cocacola-lab/MineLand.",
        "code": "https://github.com/cocacola-lab/mineland",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2403.19267"
    },
    "646f22e74c6eeed331b2c65036e763b4": {
        "title": "EduAgent: Generative Student Agents in Learning",
        "authors": [
            "Songlin Xu",
            "Xinyu Zhang",
            "Lianhui Qin"
        ],
        "date": "2024/03/23",
        "pdf": "http://arxiv.org/pdf/2404.07963",
        "abstract": "Student simulation in online education is important to address dynamic learning behaviors of students with diverse backgrounds. Existing simulation models based on deep learning usually need massive training data, lacking prior knowledge in educational contexts. Large language models (LLMs) may contain such prior knowledge since they are pre-trained from a large corpus. However, because student behaviors are dynamic and multifaceted with individual differences, directly prompting LLMs is not robust nor accurate enough to capture fine-grained interactions among diverse student personas, learning behaviors, and learning outcomes. This work tackles this problem by presenting a newly annotated fine-grained large-scale dataset and proposing EduAgent, a novel generative agent framework incorporating cognitive prior knowledge (i.e., theoretical findings revealed in cognitive science) to guide LLMs to first reason correlations among various behaviors and then make simulations. Our two experiments show that EduAgent could not only mimic and predict learning behaviors of real students but also generate realistic learning behaviors of virtual students without real data.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.07963"
    },
    "d9bf095ed730a8b63fd64cc395cf1bdf": {
        "title": "MathVC: An LLM-Simulated Multi-Character Virtual Classroom for Mathematics Education",
        "authors": [
            "Murong Yue",
            "Wijdane Mifdal",
            "Yixuan Zhang",
            "Jennifer Suh",
            "Ziyu Yao"
        ],
        "date": "2024/04/10",
        "pdf": "http://arxiv.org/pdf/2404.06711",
        "abstract": "Mathematical modeling (MM) is considered a fundamental skill for students in STEM disciplines. Practicing the MM skill is often the most effective when students can engage in group discussion and collaborative problem-solving. However, due to unevenly distributed teachers and educational resources needed to monitor such group activities, students do not always receive equal opportunities for this practice. Excitingly, large language models (LLMs) have recently demonstrated strong capability in both modeling mathematical problems and simulating characters with different traits and properties. Drawing inspiration from the advancement of LLMs, in this work, we present MATHVC, the very first LLM-powered virtual classroom containing multiple LLM-simulated student characters, with whom a human student can practice their MM skill. To encourage each LLM character&#39;s behaviors to be aligned with their specified math-relevant properties (termed &#34;characteristics alignment&#34;) and the overall conversational procedure to be close to an authentic student MM discussion (termed &#34;conversational procedural alignment&#34;), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure. Through experiments and ablation studies, we confirmed the effectiveness of our simulation approach and showed the promise for MATHVC to benefit real-life students in the future.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.06711"
    },
    "d03fda5f2f4a4b417169cee197138d05": {
        "title": "MMInA: Benchmarking Multihop Multimodal Internet Agents",
        "authors": [
            "Ziniu Zhang",
            "Shulin Tian",
            "Liangyu Chen",
            "Ziwei Liu"
        ],
        "date": "2024/04/15",
        "pdf": "http://arxiv.org/pdf/2404.09992",
        "abstract": "Autonomous embodied agents live on an Internet of multimedia websites. Can they hop around multimodal websites to complete complex user tasks? Existing benchmarks fail to assess them in a realistic, evolving environment for their embodiment across websites. To answer this question, we present MMInA, a multihop and multimodal benchmark to evaluate the embodied agents for compositional Internet tasks, with several appealing properties: 1) Evolving real-world multimodal websites. Our benchmark uniquely operates on evolving real-world websites, ensuring a high degree of realism and applicability to natural user tasks. Our data includes 1,050 human-written tasks covering various domains such as shopping and travel, with each task requiring the agent to autonomously extract multimodal information from web pages as observations; 2) Multihop web browsing. Our dataset features naturally compositional tasks that require information from or actions on multiple websites to solve, to assess long-range reasoning capabilities on web tasks; 3) Holistic evaluation. We propose a novel protocol for evaluating an agent&#39;s progress in completing multihop tasks. We experiment with both standalone (multimodal) language models and heuristic-based web agents. Extensive experiments demonstrate that while long-chain multihop web tasks are easy for humans, they remain challenging for state-of-the-art web agents. We identify that agents are more likely to fail on the early hops when solving tasks of more hops, which results in lower task success rates. To address this issue, we propose a simple memory augmentation approach replaying past action trajectories to reflect. Our method significantly improved both the single-hop and multihop web browsing abilities of agents. See our code and data at https://mmina.cliangyu.com",
        "code": "https://github.com/shulin16/mmina",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2404.09992"
    },
    "f188d764a8c441b5dec5b1194d16f4aa": {
        "title": "Apollonion: Profile-centric Dialog Agent",
        "authors": [
            "Shangyu Chen",
            "Zibo Zhao",
            "Yuanyuan Zhao",
            "Xiang Li"
        ],
        "date": "2024/04/10",
        "pdf": "http://arxiv.org/pdf/2404.08692",
        "abstract": "The emergence of Large Language Models (LLMs) has innovated the development of dialog agents. Specially, a well-trained LLM, as a central process unit, is capable of providing fluent and reasonable response for user&#39;s request. Besides, auxiliary tools such as external knowledge retrieval, personalized character for vivid response, short/long-term memory for ultra long context management are developed, completing the usage experience for LLM-based dialog agents. However, the above-mentioned techniques does not solve the issue of \\textbf{personalization from user perspective}: agents response in a same fashion to different users, without consideration of their features, such as habits, interests and past experience. In another words, current implementation of dialog agents fail in ``knowing the user&#39;&#39;. The capacity of well-description and representation of user is under development. In this work, we proposed a framework for dialog agent to incorporate user profiling (initialization, update): user&#39;s query and response is analyzed and organized into a structural user profile, which is latter served to provide personal and more precise response. Besides, we proposed a series of evaluation protocols for personalization: to what extend the response is personal to the different users. The framework is named as \\method{}, inspired by inscription of ``Know Yourself&#39;&#39; in the temple of Apollo (also known as \\method{}) in Ancient Greek. Few works have been conducted on incorporating personalization into LLM, \\method{} is a pioneer work on guiding LLM&#39;s response to meet individuation via the application of dialog agents, with a set of evaluation methods for measurement in personalization.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.08692"
    },
    "d0acc8a7c482a802b05a57eaa66a1a88": {
        "title": "OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments",
        "authors": [
            "Tianbao Xie",
            "Danyang Zhang",
            "Jixuan Chen",
            "Xiaochuan Li",
            "Siheng Zhao",
            "Ruisheng Cao",
            "Toh Jing Hua",
            "Zhoujun Cheng",
            "Dongchan Shin",
            "Fangyu Lei",
            "Yitao Liu",
            "Yiheng Xu",
            "Shuyan Zhou",
            "Silvio Savarese",
            "Caiming Xiong",
            "Victor Zhong",
            "Tao Yu"
        ],
        "date": "2024/04/11",
        "pdf": "http://arxiv.org/pdf/2404.07972",
        "abstract": "Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at https://os-world.github.io.",
        "code": "https://github.com/xlang-ai/OSWorld",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2404.07972"
    },
    "90d497ca09a06d5a5c4e2145c4070532": {
        "title": "Leveraging Multi-AI Agents for Cross-Domain Knowledge Discovery",
        "authors": [
            "Shiva Aryal",
            "Tuyen Do",
            "Bisesh Heyojoo",
            "Sandeep Chataut",
            "Bichar Dip Shrestha Gurung",
            "Venkataramana Gadhamshetty",
            "Etienne Gnimpieba"
        ],
        "date": "2024/04/12",
        "pdf": "http://arxiv.org/pdf/2404.08511",
        "abstract": "In the rapidly evolving field of artificial intelligence, the ability to harness and integrate knowledge across various domains stands as a paramount challenge and opportunity. This study introduces a novel approach to cross-domain knowledge discovery through the deployment of multi-AI agents, each specialized in distinct knowledge domains. These AI agents, designed to function as domain-specific experts, collaborate in a unified framework to synthesize and provide comprehensive insights that transcend the limitations of single-domain expertise. By facilitating seamless interaction among these agents, our platform aims to leverage the unique strengths and perspectives of each, thereby enhancing the process of knowledge discovery and decision-making. We present a comparative analysis of the different multi-agent workflow scenarios evaluating their performance in terms of efficiency, accuracy, and the breadth of knowledge integration. Through a series of experiments involving complex, interdisciplinary queries, our findings demonstrate the superior capability of domain specific multi-AI agent system in identifying and bridging knowledge gaps. This research not only underscores the significance of collaborative AI in driving innovation but also sets the stage for future advancements in AI-driven, cross-disciplinary research and application. Our methods were evaluated on a small pilot data and it showed a trend we expected, if we increase the amount of data we custom train the agents, the trend is expected to be more smooth.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.08511"
    },
    "025449e23ed3eefcf2cca46adc679089": {
        "title": "Behavior Trees Enable Structured Programming of Language Model Agents",
        "authors": [
            "Richard Kelley"
        ],
        "date": "2024/04/11",
        "pdf": "http://arxiv.org/pdf/2404.07439",
        "abstract": "Language models trained on internet-scale data sets have shown an impressive ability to solve problems in Natural Language Processing and Computer Vision. However, experience is showing that these models are frequently brittle in unexpected ways, and require significant scaffolding to ensure that they operate correctly in the larger systems that comprise &#34;language-model agents.&#34; In this paper, we argue that behavior trees provide a unifying framework for combining language models with classical AI and traditional programming. We introduce Dendron, a Python library for programming language model agents using behavior trees. We demonstrate the approach embodied by Dendron in three case studies: building a chat agent, a camera-based infrastructure inspection agent for use on a mobile robot or vehicle, and an agent that has been built to satisfy safety constraints that it did not receive through instruction tuning or RLHF.",
        "code": "https://github.com/RichardKelley/dendron",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2404.07439"
    },
    "889657431ba1b80ade839a1491dd8506": {
        "title": "MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems",
        "authors": [
            "Bin Lei"
        ],
        "date": "2024/04/06",
        "pdf": "http://arxiv.org/pdf/2404.04735",
        "abstract": "Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries. Despite these advancements, their performance substantially declines in \\textbf{advanced mathematical problems requiring complex, multi-step logical reasoning}. To enhance their inferential capabilities, current research has delved into \\textit{prompting engineering}, exemplified by methodologies such as the Tree of Thought and Graph of Thought. Nonetheless, these existing approaches encounter two significant limitations. Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained. Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability. In response to these limitations, this paper introduces the \\textit{Multi-Agent System for conditional Mining} (\\textbf{MACM}) prompting method. It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from $\\mathbf{54.68\\%} \\text{ to } \\mathbf{76.73\\%}$. The code is available in \\url{https://github.com/bin123apple/MACM}.",
        "code": "https://github.com/bin123apple/macm",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.04735"
    },
    "78d78cdb8cb1730baf707d698e12d695": {
        "title": "Cooperative Sentiment Agents for Multimodal Sentiment Analysis",
        "authors": [
            "Shanmin Wang",
            "Hui Shuai",
            "Qingshan Liu",
            "Fei Wang"
        ],
        "date": "2024/04/19",
        "pdf": "http://arxiv.org/pdf/2404.12642",
        "abstract": "In this paper, we propose a new Multimodal Representation Learning (MRL) method for Multimodal Sentiment Analysis (MSA), which facilitates the adaptive interaction between modalities through Cooperative Sentiment Agents, named Co-SA. Co-SA comprises two critical components: the Sentiment Agents Establishment (SAE) phase and the Sentiment Agents Cooperation (SAC) phase. During the SAE phase, each sentiment agent deals with an unimodal signal and highlights explicit dynamic sentiment variations within the modality via the Modality-Sentiment Disentanglement (MSD) and Deep Phase Space Reconstruction (DPSR) modules. Subsequently, in the SAC phase, Co-SA meticulously designs task-specific interaction mechanisms for sentiment agents so that coordinating multimodal signals to learn the joint representation. Specifically, Co-SA equips an independent policy model for each sentiment agent that captures significant properties within the modality. These policies are optimized mutually through the unified reward adaptive to downstream tasks. Benefitting from the rewarding mechanism, Co-SA transcends the limitation of pre-defined fusion modes and adaptively captures unimodal properties for MRL in the multimodal interaction setting. To demonstrate the effectiveness of Co-SA, we apply it to address Multimodal Sentiment Analysis (MSA) and Multimodal Emotion Recognition (MER) tasks. Our comprehensive experimental results demonstrate that Co-SA excels at discovering diverse cross-modal features, encompassing both common and complementary aspects. The code can be available at https://github.com/smwanghhh/Co-SA.",
        "code": "https://github.com/smwanghhh/co-sa",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.12642"
    },
    "c810addf261e7030b42605755d0746df": {
        "title": "Towards Human-centered Proactive Conversational Agents",
        "authors": [
            "Yang Deng",
            "Lizi Liao",
            "Zhonghua Zheng",
            "Grace Hui Yang",
            "Tat-Seng Chua"
        ],
        "date": "2024/04/19",
        "pdf": "http://arxiv.org/pdf/2404.12670",
        "abstract": "Recent research on proactive conversational agents (PCAs) mainly focuses on improving the system&#39;s capabilities in anticipating and planning action sequences to accomplish tasks and achieve goals before users articulate their requests. This perspectives paper highlights the importance of moving towards building human-centered PCAs that emphasize human needs and expectations, and that considers ethical and social implications of these agents, rather than solely focusing on technological capabilities. The distinction between a proactive and a reactive system lies in the proactive system&#39;s initiative-taking nature. Without thoughtful design, proactive systems risk being perceived as intrusive by human users. We address the issue by establishing a new taxonomy concerning three key dimensions of human-centered PCAs, namely Intelligence, Adaptivity, and Civility. We discuss potential research opportunities and challenges based on this new taxonomy upon the five stages of PCA system construction. This perspectives paper lays a foundation for the emerging area of conversational information retrieval research and paves the way towards advancing human-centered proactive conversational systems.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.12670"
    },
    "f0ed13204545aa097b36058baa5bc6a0": {
        "title": "Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents",
        "authors": [
            "Giorgio Piatti",
            "Zhijing Jin",
            "Max Kleiman-Weiner",
            "Bernhard Schölkopf",
            "Mrinmaya Sachan",
            "Rada Mihalcea"
        ],
        "date": "2024/04/25",
        "pdf": "http://arxiv.org/pdf/2404.16698",
        "abstract": "In the rapidly evolving field of artificial intelligence, ensuring safe decision-making of Large Language Models (LLMs) is a significant challenge. This paper introduces Governance of the Commons Simulation (GovSim), a simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. Through this simulation environment, we explore the dynamics of resource sharing among AI agents, highlighting the importance of ethical considerations, strategic planning, and negotiation skills. GovSim is versatile and supports any text-based agent, including LLMs agents. Using the Generative Agent framework, we create a standard agent that facilitates the integration of different LLMs. Our findings reveal that within GovSim, only two out of 15 tested LLMs managed to achieve a sustainable outcome, indicating a significant gap in the ability of models to manage shared resources. Furthermore, we find that by removing the ability of agents to communicate, they overuse the shared resource, highlighting the importance of communication for cooperation. Interestingly, most LLMs lack the ability to make universalized hypotheses, which highlights a significant weakness in their reasoning skills. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.",
        "code": "",
        "category": [
            "Role Playing",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.16698"
    },
    "d98e6b8727b73e0711f9da1f804b2a3a": {
        "title": "BattleAgent: Multi-modal Dynamic Emulation on Historical Battles to Complement Historical Analysis",
        "authors": [
            "Shuhang Lin",
            "Wenyue Hua",
            "Lingyao Li",
            "Che-Jui Chang",
            "Lizhou Fan",
            "Jianchao Ji",
            "Hang Hua",
            "Mingyu Jin",
            "Jiebo Luo",
            "Yongfeng Zhang"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2404.15532",
        "abstract": "This paper presents BattleAgent, an emulation system that combines the Large Vision-Language Model and Multi-agent System. This novel system aims to simulate complex dynamic interactions among multiple agents, as well as between agents and their environments, over a period of time. It emulates both the decision-making processes of leaders and the viewpoints of ordinary participants, such as soldiers. The emulation showcases the current capabilities of agents, featuring fine-grained multi-modal interactions between agents and landscapes. It develops customizable agent structures to meet specific situational requirements, for example, a variety of battle-related activities like scouting and trench digging. These components collaborate to recreate historical events in a lively and comprehensive manner while offering insights into the thoughts and feelings of individuals from diverse viewpoints. The technological foundations of BattleAgent establish detailed and immersive settings for historical battles, enabling individual agents to partake in, observe, and dynamically respond to evolving battle scenarios. This methodology holds the potential to substantially deepen our understanding of historical events, particularly through individual accounts. Such initiatives can also aid historical research, as conventional historical narratives often lack documentation and prioritize the perspectives of decision-makers, thereby overlooking the experiences of ordinary individuals. BattelAgent illustrates AI&#39;s potential to revitalize the human aspect in crucial social events, thereby fostering a more nuanced collective understanding and driving the progressive development of human society.",
        "code": "https://github.com/agiresearch/battleagent",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.15532"
    },
    "9eff76117627bac573c318dd183c049a": {
        "title": "Aligning LLM Agents by Learning Latent Preference from User Edits",
        "authors": [
            "Ge Gao",
            "Alexey Taymanov",
            "Eduardo Salinas",
            "Paul Mineiro",
            "Dipendra Misra"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2404.15269",
        "abstract": "We study interactive learning of language agents based on user edits made to the agent&#39;s output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent&#39;s alignment with the user&#39;s preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user&#39;s latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2404.15269"
    },
    "340f5e4367036a333e81bf97cd732408": {
        "title": "CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based Reasoning",
        "authors": [
            "Ling Yue",
            "Tianfan Fu"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2404.14777",
        "abstract": "Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge. Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility. We introduce Clinical Agent System (CT-Agent), a Clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology. This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities. Our system autonomously manages the entire clinical trial process, demonstrating significant efficiency improvements in our evaluations, which include both computational benchmarks and expert feedback.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.14777"
    },
    "e04c3f9762fe447296eb9a790f67c49b": {
        "title": "Socratic Planner: Inquiry-Based Zero-Shot Planning for Embodied Instruction Following",
        "authors": [
            "Suyeon Shin",
            "Sujin jeon",
            "Junghyun Kim",
            "Gi-Cheon Kang",
            "Byoung-Tak Zhang"
        ],
        "date": "2024/04/21",
        "pdf": "http://arxiv.org/pdf/2404.15190",
        "abstract": "Embodied Instruction Following (EIF) is the task of executing natural language instructions by navigating and interacting with objects in 3D environments. One of the primary challenges in EIF is compositional task planning, which is often addressed with supervised or in-context learning with labeled data. To this end, we introduce the Socratic Planner, the first zero-shot planning method that infers without the need for any training data. Socratic Planner first decomposes the instructions into substructural information of the task through self-questioning and answering, translating it into a high-level plan, i.e., a sequence of subgoals. Subgoals are executed sequentially, with our visually grounded re-planning mechanism adjusting plans dynamically through a dense visual feedback. We also introduce an evaluation metric of high-level plans, RelaxedHLP, for a more comprehensive evaluation. Experiments demonstrate the effectiveness of the Socratic Planner, achieving competitive performance on both zero-shot and few-shot task planning in the ALFRED benchmark, particularly excelling in tasks requiring higher-dimensional inference. Additionally, a precise adjustments in the plan were achieved by incorporating environmental visual information.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2404.15190"
    },
    "35ce08a40e0cdaf207031e3d97ba4c6e": {
        "title": "How Well Can LLMs Echo Us? Evaluating AI Chatbots&#39; Role-Play Ability with ECHO",
        "authors": [
            "Man Tik Ng",
            "Hui Tung Tse",
            "Jen-tse Huang",
            "Jingjing Li",
            "Wenxuan Wang",
            "Michael R. Lyu"
        ],
        "date": "2024/04/22",
        "pdf": "http://arxiv.org/pdf/2404.13957",
        "abstract": "The role-play ability of Large Language Models (LLMs) has emerged as a popular research direction. However, existing studies focus on imitating well-known public figures or fictional characters, overlooking the potential for simulating ordinary individuals. Such an oversight limits the potential for advancements in digital human clones and non-player characters in video games. To bridge this gap, we introduce ECHO, an evaluative framework inspired by the Turing test. This framework engages the acquaintances of the target individuals to distinguish between human and machine-generated responses. Notably, our framework focuses on emulating average individuals rather than historical or fictional figures, presenting a unique advantage to apply the Turing Test. We evaluated three role-playing LLMs using ECHO, with GPT-3.5 and GPT-4 serving as foundational models, alongside the online application GPTs from OpenAI. Our results demonstrate that GPT-4 more effectively deceives human evaluators, and GPTs achieves a leading success rate of 48.3%. Furthermore, we investigated whether LLMs could discern between human-generated and machine-generated texts. While GPT-4 can identify differences, it could not determine which texts were human-produced. Our code and results of reproducing the role-playing LLMs are made publicly available via https://github.com/CUHK-ARISE/ECHO.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.13957"
    },
    "ab48f35dec492ad32cc00c50276c7f36": {
        "title": "PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games",
        "authors": [
            "Steph Buongiorno",
            "Lawrence Jake Klinkert",
            "Tanishq Chawla",
            "Zixin Zhuang",
            "Corey Clark"
        ],
        "date": "2024/04/30",
        "pdf": "http://arxiv.org/pdf/2404.19721",
        "abstract": "This research introduces Procedural Artificial Narrative using Generative AI (PANGeA), a structured approach for leveraging large language models (LLMs), guided by a game designer&#39;s high-level criteria, to generate narrative content for turn-based role-playing video games (RPGs). Distinct from prior applications of LLMs used for video game design, PANGeA innovates by not only generating game level data (which includes, but is not limited to, setting, key items, and non-playable characters (NPCs)), but by also fostering dynamic, free-form interactions between the player and the environment that align with the procedural game narrative. The NPCs generated by PANGeA are personality-biased and express traits from the Big 5 Personality Model in their generated responses. PANGeA addresses challenges behind ingesting free-form text input, which can prompt LLM responses beyond the scope of the game narrative. A novel validation system that uses the LLM&#39;s intelligence evaluates text input and aligns generated responses with the unfolding narrative. Making these interactions possible, PANGeA is supported by a server that hosts a custom memory system that supplies context for augmenting generated responses thus aligning them with the procedural narrative. For its broad application, the server has a REST interface enabling any game engine to integrate directly with PANGeA, as well as an LLM interface adaptable with local or private LLMs. PANGeA&#39;s ability to foster dynamic narrative generation by aligning responses with the procedural narrative is demonstrated through an empirical study and ablation test of two versions of a demo game. These are, a custom, browser-based GPT and a Unity demo. As the results show, PANGeA holds potential to assist game designers in using LLMs to generate narrative-consistent content even when provided varied and unpredictable, free-form text input.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.19721"
    },
    "d3be68d8e8d6464c35b2ccdf320e52e2": {
        "title": "Large Language Model Agent as a Mechanical Designer",
        "authors": [
            "Yayati Jadhav",
            "Amir Barati Farimani"
        ],
        "date": "2024/04/26",
        "pdf": "http://arxiv.org/pdf/2404.17525",
        "abstract": "Conventional mechanical design paradigms rely on experts systematically refining concepts through experience-guided modification and FEA to meet specific requirements. However, this approach can be time-consuming and heavily dependent on prior knowledge and experience. While numerous machine learning models have been developed to streamline this intensive and expert-driven iterative process, these methods typically demand extensive training data and considerable computational resources. Furthermore, methods based on deep learning are usually restricted to the specific domains and tasks for which they were trained, limiting their applicability across different tasks. This creates a trade-off between the efficiency of automation and the demand for resources. In this study, we present a novel approach that integrates pre-trained LLMs with a FEM module. The FEM module evaluates each design and provides essential feedback, guiding the LLMs to continuously learn, plan, generate, and optimize designs without the need for domain-specific training. We demonstrate the effectiveness of our proposed framework in managing the iterative optimization of truss structures, showcasing its capability to reason about and refine designs according to structured feedback and criteria. Our results reveal that these LLM-based agents can successfully generate truss designs that comply with natural language specifications with a success rate of up to 90%, which varies according to the applied constraints. By employing prompt-based optimization techniques we show that LLM based agents exhibit optimization behavior when provided with solution-score pairs to iteratively refine designs to meet specifications. This ability of LLM agents to produce viable designs and optimize them based on their inherent reasoning capabilities highlights their potential to develop and implement effective design strategies autonomously.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.17525"
    },
    "aef6e27e25eb64666e2bbb9373ac3d70": {
        "title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
        "authors": [
            "Kaixuan Huang",
            "Yuanhao Qu",
            "Henry Cousins",
            "William A. Johnson",
            "Di Yin",
            "Mihir Shah",
            "Denny Zhou",
            "Russ Altman",
            "Mengdi Wang",
            "Le Cong"
        ],
        "date": "2024/04/27",
        "pdf": "http://arxiv.org/pdf/2404.18021",
        "abstract": "The introduction of genome engineering technology has transformed biomedical research, making it possible to make precise changes to genetic information. However, creating an efficient gene-editing system requires a deep understanding of CRISPR technology, and the complex experimental systems under investigation. While Large Language Models (LLMs) have shown promise in various tasks, they often lack specific knowledge and struggle to accurately solve biological design problems. In this work, we introduce CRISPR-GPT, an LLM agent augmented with domain knowledge and external tools to automate and enhance the design process of CRISPR-based gene-editing experiments. CRISPR-GPT leverages the reasoning ability of LLMs to facilitate the process of selecting CRISPR systems, designing guide RNAs, recommending cellular delivery methods, drafting protocols, and designing validation experiments to confirm editing outcomes. We showcase the potential of CRISPR-GPT for assisting non-expert researchers with gene-editing experiments from scratch and validate the agent&#39;s effectiveness in a real-world use case. Furthermore, we explore the ethical and regulatory considerations associated with automated gene-editing design, highlighting the need for responsible and transparent use of these tools. Our work aims to bridge the gap between beginner biological researchers and CRISPR genome engineering techniques, and demonstrate the potential of LLM agents in facilitating complex biological discovery tasks.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2404.18021"
    },
    "c20a9be3f493bc58a9706b98f5bfcb38": {
        "title": "ComposerX: Multi-Agent Symbolic Music Composition with LLMs",
        "authors": [
            "Qixin Deng",
            "Qikai Yang",
            "Ruibin Yuan",
            "Yipeng Huang",
            "Yi Wang",
            "Xubo Liu",
            "Zeyue Tian",
            "Jiahao Pan",
            "Ge Zhang",
            "Hanfeng Lin",
            "Yizhi Li",
            "Yinghao Ma",
            "Jie Fu",
            "Chenghua Lin",
            "Emmanouil Benetos",
            "Wenwu Wang",
            "Guangyu Xia",
            "Wei Xue",
            "Yike Guo"
        ],
        "date": "2024/04/28",
        "pdf": "http://arxiv.org/pdf/2404.18081",
        "abstract": "Music composition represents the creative side of humanity, and itself is a complex task that requires abilities to understand and generate information with long dependency and harmony constraints. While demonstrating impressive capabilities in STEM subjects, current LLMs easily fail in this task, generating ill-written music even when equipped with modern techniques like In-Context-Learning and Chain-of-Thoughts. To further explore and enhance LLMs&#39; potential in music composition by leveraging their reasoning ability and the large knowledge base in music history and theory, we propose ComposerX, an agent-based symbolic music generation framework. We find that applying a multi-agent approach significantly improves the music composition quality of GPT-4. The results demonstrate that ComposerX is capable of producing coherent polyphonic music compositions with captivating melodies, while adhering to user instructions.",
        "code": "https://github.com/lllindsey0615/composerx",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2404.18081"
    },
    "96c56a428d90415310188fa9a434d00c": {
        "title": "Logic Agent: Enhancing Validity with Logic Rule Invocation",
        "authors": [
            "Hanmeng Liu",
            "Zhiyang Teng",
            "Chaoli Zhang",
            "Yue Zhang"
        ],
        "date": "2024/04/28",
        "pdf": "http://arxiv.org/pdf/2404.18130",
        "abstract": "Chain-of-Thought (CoT) prompting has emerged as a pivotal technique for augmenting the inferential capabilities of language models during reasoning tasks. Despite its advancements, CoT often grapples with challenges in validating reasoning validity and ensuring informativeness. Addressing these limitations, this paper introduces the Logic Agent (LA), an agent-based framework aimed at enhancing the validity of reasoning processes in Large Language Models (LLMs) through strategic logic rule invocation. Unlike conventional approaches, LA transforms LLMs into logic agents that dynamically apply propositional logic rules, initiating the reasoning process by converting natural language inputs into structured logic forms. The logic agent leverages a comprehensive set of predefined functions to systematically navigate the reasoning process. This methodology not only promotes the structured and coherent generation of reasoning constructs but also significantly improves their interpretability and logical coherence. Through extensive experimentation, we demonstrate LA&#39;s capacity to scale effectively across various model sizes, markedly improving the precision of complex reasoning across diverse tasks.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2404.18130"
    },
    "bf68ceff02d38dac075c997707f0632e": {
        "title": "Large Language Model Agent for Fake News Detection",
        "authors": [
            "Xinyi Li",
            "Yongfeng Zhang",
            "Edward C. Malthouse"
        ],
        "date": "2024/04/30",
        "pdf": "http://arxiv.org/pdf/2405.01593",
        "abstract": "In the current digital era, the rapid spread of misinformation on online platforms presents significant challenges to societal well-being, public trust, and democratic processes, influencing critical decision making and public opinion. To address these challenges, there is a growing need for automated fake news detection mechanisms. Pre-trained large language models (LLMs) have demonstrated exceptional capabilities across various natural language processing (NLP) tasks, prompting exploration into their potential for verifying news claims. Instead of employing LLMs in a non-agentic way, where LLMs generate responses based on direct prompts in a single shot, our work introduces FactAgent, an agentic approach of utilizing LLMs for fake news detection. FactAgent enables LLMs to emulate human expert behavior in verifying news claims without any model training, following a structured workflow. This workflow breaks down the complex task of news veracity checking into multiple sub-steps, where LLMs complete simple tasks using their internal knowledge or external tools. At the final step of the workflow, LLMs integrate all findings throughout the workflow to determine the news claim&#39;s veracity. Compared to manual human verification, FactAgent offers enhanced efficiency. Experimental studies demonstrate the effectiveness of FactAgent in verifying claims without the need for any training process. Moreover, FactAgent provides transparent explanations at each step of the workflow and during final decision-making, offering insights into the reasoning process of fake news detection for end users. FactAgent is highly adaptable, allowing for straightforward updates to its tools that LLMs can leverage within the workflow, as well as updates to the workflow itself using domain knowledge. This adaptability enables FactAgent&#39;s application to news verification across various domains.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.01593"
    },
    "d7fa003b09d2ebcde7ea089949ad402a": {
        "title": "Navigating WebAI: Training Agents to Complete Web Tasks with Large Language Models and Reinforcement Learning",
        "authors": [
            "Lucas-Andreï Thil",
            "Mirela Popa",
            "Gerasimos Spanakis"
        ],
        "date": "2024/05/01",
        "pdf": "http://arxiv.org/pdf/2405.00516",
        "abstract": "Recent advancements in language models have demonstrated remarkable improvements in various natural language processing (NLP) tasks such as web navigation. Supervised learning (SL) approaches have achieved impressive performance while utilizing significantly less training data compared to previous methods. However, these SL-based models fall short when compared to reinforcement learning (RL) approaches, which have shown superior results. In this paper, we propose a novel approach that combines SL and RL techniques over the MiniWoB benchmark to leverage the strengths of both methods. We also address a critical limitation in previous models&#39; understanding of HTML content, revealing a tendency to memorize target elements rather than comprehend the underlying structure. To rectify this, we propose methods to enhance true understanding and present a new baseline of results. Our experiments demonstrate that our approach outperforms previous SL methods on certain tasks using less data and narrows the performance gap with RL models, achieving 43.58\\% average accuracy in SL and 36.69\\% when combined with a multimodal RL approach. This study sets a new direction for future web navigation and offers insights into the limitations and potential of language modeling for computer tasks.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2405.00516"
    },
    "c1e0294614a29fc390d87c06d71e7a1e": {
        "title": "Large Language Models for Human-Robot Interaction: Opportunities and Risks",
        "authors": [
            "Jesse Atuhurra"
        ],
        "date": "2024/03/26",
        "pdf": "http://arxiv.org/pdf/2405.00693",
        "abstract": "The tremendous development in large language models (LLM) has led to a new wave of innovations and applications and yielded research results that were initially forecast to take longer. In this work, we tap into these recent developments and present a meta-study about the potential of large language models if deployed in social robots. We place particular emphasis on the applications of social robots: education, healthcare, and entertainment. Before being deployed in social robots, we also study how these language models could be safely trained to ``understand&#39;&#39; societal norms and issues, such as trust, bias, ethics, cognition, and teamwork. We hope this study provides a resourceful guide to other robotics researchers interested in incorporating language models in their robots.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2405.00693"
    },
    "78997b038dfb5f3e70c52686b5d86257": {
        "title": "Conformity, Confabulation, and Impersonation: Persona Inconstancy in Multi-Agent LLM Collaboration",
        "authors": [
            "Razan Baltaji",
            "Babak Hemmatian",
            "Lav R. Varshney"
        ],
        "date": "2024/05/06",
        "pdf": "http://arxiv.org/pdf/2405.03862",
        "abstract": "This study explores the sources of instability in maintaining cultural personas and opinions within multi-agent LLM systems. Drawing on simulations of inter-cultural collaboration and debate, we analyze agents&#39; pre- and post-discussion private responses alongside chat transcripts to assess the stability of cultural personas and the impact of opinion diversity on group outcomes. Our findings suggest that multi-agent discussions can encourage collective decisions that reflect diverse perspectives, yet this benefit is tempered by the agents&#39; susceptibility to conformity due to perceived peer pressure and challenges in maintaining consistent personas and opinions. Counterintuitively, instructions that encourage debate in support of one&#39;s opinions increase the rate of inconstancy. Without addressing the factors we identify, the full potential of multi-agent frameworks for producing more culturally diverse AI outputs will remain untapped.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.03862"
    },
    "77b2c05422904b637fe33ddffdddc2ff": {
        "title": "&#34;Ask Me Anything&#34;: How Comcast Uses LLMs to Assist Agents in Real Time",
        "authors": [
            "Scott Rome",
            "Tianwen Chen",
            "Raphael Tang",
            "Luwei Zhou",
            "Ferhan Ture"
        ],
        "date": "2024/05/01",
        "pdf": "http://arxiv.org/pdf/2405.00801",
        "abstract": "Customer service is how companies interface with their customers. It can contribute heavily towards the overall customer satisfaction. However, high-quality service can become expensive, creating an incentive to make it as cost efficient as possible and prompting most companies to utilize AI-powered assistants, or &#34;chat bots&#34;. On the other hand, human-to-human interaction is still desired by customers, especially when it comes to complex scenarios such as disputes and sensitive topics like bill payment. This raises the bar for customer service agents. They need to accurately understand the customer&#39;s question or concern, identify a solution that is acceptable yet feasible (and within the company&#39;s policy), all while handling multiple conversations at once. In this work, we introduce &#34;Ask Me Anything&#34; (AMA) as an add-on feature to an agent-facing customer service interface. AMA allows agents to ask questions to a large language model (LLM) on demand, as they are handling customer conversations -- the LLM provides accurate responses in real-time, reducing the amount of context switching the agent needs. In our internal experiments, we find that agents using AMA versus a traditional search experience spend approximately 10% fewer seconds per conversation containing a search, translating to millions of dollars of savings annually. Agents that used the AMA feature provided positive feedback nearly 80% of the time, demonstrating its usefulness as an AI-assisted feature for customer care.",
        "code": "",
        "category": [
            "Role Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.00801"
    },
    "2704c7e4d3813e67bbf40356965122ec": {
        "title": "WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace Setting",
        "authors": [
            "Olly Styles",
            "Sam Miller",
            "Patricio Cerda-Mardini",
            "Tanaya Guha",
            "Victor Sanchez",
            "Bertie Vidgen"
        ],
        "date": "2024/05/01",
        "pdf": "http://arxiv.org/pdf/2405.00823",
        "abstract": "We introduce WorkBench: a benchmark dataset for evaluating agents&#39; ability to execute tasks in a workplace setting. WorkBench contains a sandbox environment with five databases, 26 tools, and 690 tasks. These tasks represent common business activities, such as sending emails and scheduling meetings. The tasks in WorkBench are challenging as they require planning, tool selection, and often multiple actions. If a task has been successfully executed, one (or more) of the database values may change. The correct outcome for each task is unique and unambiguous, which allows for robust, automated evaluation. We call this key contribution outcome-centric evaluation. We evaluate five existing ReAct agents on WorkBench, finding they successfully complete as few as 3% of tasks (Llama2-70B), and just 43% for the best-performing (GPT-4). We further find that agents&#39; errors can result in the wrong action being taken, such as an email being sent to the wrong person. WorkBench reveals weaknesses in agents&#39; ability to undertake common business activities, raising questions about their use in high-stakes workplace settings. WorkBench is publicly available as a free resource at https://github.com/olly-styles/WorkBench.",
        "code": "https://github.com/olly-styles/workbench",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.00823"
    },
    "b72d2bd1cbea4425300fafee4f089549": {
        "title": "CACTUS: Chemistry Agent Connecting Tool-Usage to Science",
        "authors": [
            "Andrew D. McNaughton",
            "Gautham Ramalaxmi",
            "Agustin Kruel",
            "Carter R. Knutson",
            "Rohith A. Varikoti",
            "Neeraj Kumar"
        ],
        "date": "2024/05/02",
        "pdf": "http://arxiv.org/pdf/2405.00972",
        "abstract": "Large language models (LLMs) have shown remarkable potential in various domains, but they often lack the ability to access and reason over domain-specific knowledge and tools. In this paper, we introduced CACTUS (Chemistry Agent Connecting Tool-Usage to Science), an LLM-based agent that integrates cheminformatics tools to enable advanced reasoning and problem-solving in chemistry and molecular discovery. We evaluate the performance of CACTUS using a diverse set of open-source LLMs, including Gemma-7b, Falcon-7b, MPT-7b, Llama2-7b, and Mistral-7b, on a benchmark of thousands of chemistry questions. Our results demonstrate that CACTUS significantly outperforms baseline LLMs, with the Gemma-7b and Mistral-7b models achieving the highest accuracy regardless of the prompting strategy used. Moreover, we explore the impact of domain-specific prompting and hardware configurations on model performance, highlighting the importance of prompt engineering and the potential for deploying smaller models on consumer-grade hardware without significant loss in accuracy. By combining the cognitive capabilities of open-source LLMs with domain-specific tools, CACTUS can assist researchers in tasks such as molecular property prediction, similarity searching, and drug-likeness assessment. Furthermore, CACTUS represents a significant milestone in the field of cheminformatics, offering an adaptable tool for researchers engaged in chemistry and molecular discovery. By integrating the strengths of open-source LLMs with domain-specific tools, CACTUS has the potential to accelerate scientific advancement and unlock new frontiers in the exploration of novel, effective, and safe therapeutic candidates, catalysts, and materials. Moreover, CACTUS&#39;s ability to integrate with automated experimentation platforms and make data-driven decisions in real time opens up new possibilities for autonomous discovery.",
        "code": "https://github.com/pnnl/cactus",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.00972"
    },
    "ba077a6c1f1f572103b1bcfa943cb42b": {
        "title": "GAIA: A General AI Assistant for Intelligent Accelerator Operations",
        "authors": [
            "Frank Mayet"
        ],
        "date": "2024/05/02",
        "pdf": "http://arxiv.org/pdf/2405.01359",
        "abstract": "Large-scale machines like particle accelerators are usually run by a team of experienced operators. In case of a particle accelerator, these operators possess suitable background knowledge on both accelerator physics and the technology comprising the machine. Due to the complexity of the machine, particular subsystems of the machine are taken care of by experts, who the operators can turn to. In this work the reasoning and action (ReAct) prompting paradigm is used to couple an open-weights large language model (LLM) with a high-level machine control system framework and other tools, e.g. the electronic logbook or machine design documentation. By doing so, a multi-expert retrieval augmented generation (RAG) system is implemented, which assists operators in knowledge retrieval tasks, interacts with the machine directly if needed, or writes high level control system scripts. This consolidation of expert knowledge and machine interaction can simplify and speed up machine operation tasks for both new and experienced human operators.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.01359"
    },
    "6d2bc9f35fcd10c0a43d0bb13c364676": {
        "title": "Octopus v3: Technical Report for On-device Sub-billion Multimodal AI Agent",
        "authors": [
            "Wei Chen",
            "Zhiyuan Li"
        ],
        "date": "2024/04/17",
        "pdf": "http://arxiv.org/pdf/2404.11459",
        "abstract": "A multimodal AI agent is characterized by its ability to process and learn from various types of data, including natural language, visual, and audio inputs, to inform its actions. Despite advancements in large language models that incorporate visual data, such as GPT-4V, effectively translating image-based data into actionable outcomes for AI agents continues to be challenging. In this paper, we introduce a multimodal model that incorporates the concept of functional token specifically designed for AI agent applications. To ensure compatibility with edge devices, our model is optimized to a compact size of less than 1B parameters. Like GPT-4, our model can process both English and Chinese. We demonstrate that this model is capable of operating efficiently on a wide range of edge devices, including as constrained as a Raspberry Pi.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2404.11459"
    },
    "b87a4cc0c759e31d54cf32e846c59ee9": {
        "title": "LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions",
        "authors": [
            "Chuanneng Sun",
            "Songjun Huang",
            "Dario Pompili"
        ],
        "date": "2024/05/17",
        "pdf": "http://arxiv.org/pdf/2405.11106",
        "abstract": "In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.11106"
    },
    "f042f087ecadcb354329fbcd38bbc000": {
        "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
        "authors": [
            "Yuexiang Zhai",
            "Hao Bai",
            "Zipeng Lin",
            "Jiayi Pan",
            "Shengbang Tong",
            "Yifei Zhou",
            "Alane Suhr",
            "Saining Xie",
            "Yann LeCun",
            "Yi Ma",
            "Sergey Levine"
        ],
        "date": "2024/05/16",
        "pdf": "http://arxiv.org/pdf/2405.10292",
        "abstract": "Large vision-language models (VLMs) fine-tuned on specialized visual instruction-following data have exhibited impressive language reasoning capabilities across various scenarios. However, this fine-tuning paradigm may not be able to efficiently learn optimal decision-making agents in multi-step goal-directed tasks from interactive environments. To address this challenge, we propose an algorithmic framework that fine-tunes VLMs with reinforcement learning (RL). Specifically, our framework provides a task description and then prompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM to efficiently explore intermediate reasoning steps that lead to the final text-based action. Next, the open-ended text output is parsed into an executable action to interact with the environment to obtain goal-directed task rewards. Finally, our framework uses these task rewards to fine-tune the entire VLM with RL. Empirically, we demonstrate that our proposed framework enhances the decision-making capabilities of VLM agents across various tasks, enabling 7b models to outperform commercial models such as GPT4-V or Gemini. Furthermore, we find that CoT reasoning is a crucial component for performance improvement, as removing the CoT reasoning results in a significant decrease in the overall performance of our method.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2405.10292"
    },
    "d9495626913b689d9b5dbe9d90df8923": {
        "title": "Exploring the Potential of Conversational AI Support for Agent-Based Social Simulation Model Design",
        "authors": [
            "Peer-Olaf Siebers"
        ],
        "date": "2024/05/12",
        "pdf": "http://arxiv.org/pdf/2405.08032",
        "abstract": "ChatGPT, the AI-powered chatbot with a massive user base of hundreds of millions, has become a global phenomenon. However, the use of Conversational AI Systems (CAISs) like ChatGPT for research in the field of Social Simulation is still limited. Specifically, there is no evidence of its usage in Agent-Based Social Simulation (ABSS) model design. While scepticism towards anything new is inherent to human nature, we firmly believe it is imperative to initiate the use of this innovative technology to support ABSS model design. This paper presents a proof-of-concept that demonstrates how CAISs can facilitate the development of innovative conceptual ABSS models in a concise timeframe and with minimal required upfront case-based knowledge. By employing advanced prompt engineering techniques and adhering to the Engineering ABSS framework, we have constructed a comprehensive prompt script that enables the design of ABSS models with or by the CAIS. The effectiveness of the script is demonstrated through an illustrative case study concerning the use of adaptive architecture in museums. Despite occasional inaccuracies and divergences in conversation, the CAIS proved to be a valuable companion for ABSS modellers.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.08032"
    },
    "bcf1e07dcc3653511e08f61ae367e0bb": {
        "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments",
        "authors": [
            "Samuel Schmidgall",
            "Rojin Ziaei",
            "Carl Harris",
            "Eduardo Reis",
            "Jeffrey Jopling",
            "Michael Moor"
        ],
        "date": "2024/05/13",
        "pdf": "http://arxiv.org/pdf/2405.07960",
        "abstract": "Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient&#39;s diagnosis through dialogue and active data collection. We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at https://AgentClinic.github.io.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.07960"
    },
    "a7ce7a0960bfdb2e0e3d36c076b3386c": {
        "title": "Language Evolution for Evading Social Media Regulation via LLM-based Multi-agent Simulation",
        "authors": [
            "Jinyu Cai",
            "Jialong Li",
            "Mingyue Zhang",
            "Munan Li",
            "Chen-Shu Wang",
            "Kenji Tei"
        ],
        "date": "2024/05/05",
        "pdf": "http://arxiv.org/pdf/2405.02858",
        "abstract": "Social media platforms such as Twitter, Reddit, and Sina Weibo play a crucial role in global communication but often encounter strict regulations in geopolitically sensitive regions. This situation has prompted users to ingeniously modify their way of communicating, frequently resorting to coded language in these regulated social media environments. This shift in communication is not merely a strategy to counteract regulation, but a vivid manifestation of language evolution, demonstrating how language naturally evolves under societal and technological pressures. Studying the evolution of language in regulated social media contexts is of significant importance for ensuring freedom of speech, optimizing content moderation, and advancing linguistic research. This paper proposes a multi-agent simulation framework using Large Language Models (LLMs) to explore the evolution of user language in regulated social media environments. The framework employs LLM-driven agents: supervisory agent who enforce dialogue supervision and participant agents who evolve their language strategies while engaging in conversation, simulating the evolution of communication styles under strict regulations aimed at evading social media regulation. The study evaluates the framework&#39;s effectiveness through a range of scenarios from abstract scenarios to real-world situations. Key findings indicate that LLMs are capable of simulating nuanced language dynamics and interactions in constrained settings, showing improvement in both evading supervision and information accuracy as evolution progresses. Furthermore, it was found that LLM agents adopt different strategies for different scenarios.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.02858"
    },
    "b43b5affaaf41cfdf9e94780c8081629": {
        "title": "(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts",
        "authors": [
            "Minghao Wu",
            "Yulin Yuan",
            "Gholamreza Haffari",
            "Longyue Wang"
        ],
        "date": "2024/05/20",
        "pdf": "http://arxiv.org/pdf/2405.11804",
        "abstract": "Recent advancements in machine translation (MT) have significantly enhanced translation quality across various domains. However, the translation of literary texts remains a formidable challenge due to their complex language, figurative expressions, and cultural nuances. In this work, we introduce a novel multi-agent framework based on large language models (LLMs) for literary translation, implemented as a company called TransAgents, which mirrors traditional translation publication process by leveraging the collective capabilities of multiple agents, to address the intricate demands of translating literary works. To evaluate the effectiveness of our system, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP). MHP assesses translations from the perspective of monolingual readers of the target language, while BLP uses advanced LLMs to compare translations directly with the original texts. Empirical findings indicate that despite lower d-BLEU scores, translations from TransAgents are preferred by both human evaluators and LLMs over human-written references, particularly in genres requiring domain-specific knowledge. We also highlight the strengths and limitations of TransAgents through case studies and suggests directions for future research.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.11804"
    },
    "795c4f39e715ad150c279b8e75ac567a": {
        "title": "Speaker Verification in Agent-Generated Conversations",
        "authors": [
            "Yizhe Yang",
            "Heyan Huang",
            "Palakorn Achananuparp",
            "Jing Jiang",
            "Ee-Peng Lim"
        ],
        "date": "2024/05/16",
        "pdf": "http://arxiv.org/pdf/2405.10150",
        "abstract": "The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.10150"
    },
    "d42b631cc1afc305dd8e925f98cfee2e": {
        "title": "LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play",
        "authors": [
            "Li-Chun Lu",
            "Shou-Jen Chen",
            "Tsung-Min Pai",
            "Chan-Hung Yu",
            "Hung-yi Lee",
            "Shao-Hua Sun"
        ],
        "date": "2024/05/10",
        "pdf": "http://arxiv.org/pdf/2405.06373",
        "abstract": "Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. Our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.06373"
    },
    "cefe1287a3a99a48008bf93f0f79e333": {
        "title": "LLMs with Personalities in Multi-issue Negotiation Games",
        "authors": [
            "Sean Noh",
            "Ho-Chun Herbert Chang"
        ],
        "date": "2024/05/08",
        "pdf": "http://arxiv.org/pdf/2405.05248",
        "abstract": "Powered by large language models (LLMs), AI agents have become capable of many human tasks. Using the most canonical definitions of the Big Five personality, we measure the ability of LLMs to negotiate within a game-theoretical framework, as well as methodological challenges to measuring notions of fairness and risk. Simulations (n=1,500) for both single-issue and multi-issue negotiation reveal increase in domain complexity with asymmetric issue valuations improve agreement rates but decrease surplus from aggressive negotiation. Through gradient-boosted regression and Shapley explainers, we find high openness, conscientiousness, and neuroticism are associated with fair tendencies; low agreeableness and low openness are associated with rational tendencies. Low conscientiousness is associated with high toxicity. These results indicate that LLMs may have built-in guardrails that default to fair behavior, but can be &#34;jail broken&#34; to exploit agreeable opponents. We also offer pragmatic insight in how negotiation bots can be designed, and a framework of assessing negotiation behavior based on game theory and computational social science.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2405.05248"
    },
    "8056aba2da622463a647edc55787daf9": {
        "title": "Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework",
        "authors": [
            "Xiangpeng Wan",
            "Haicheng Deng",
            "Kai Zou",
            "Shiqi Xu"
        ],
        "date": "2024/05/07",
        "pdf": "http://arxiv.org/pdf/2405.04294",
        "abstract": "Structured finance, which involves restructuring diverse assets into securities like MBS, ABS, and CDOs, enhances capital market efficiency but presents significant due diligence challenges. This study explores the integration of artificial intelligence (AI) with traditional asset review processes to improve efficiency and accuracy in structured finance. Using both open-sourced and close-sourced large language models (LLMs), we demonstrate that AI can automate the verification of information between loan applications and bank statements effectively. While close-sourced models such as GPT-4 show superior performance, open-sourced models like LLAMA3 offer a cost-effective alternative. Dual-agent systems further increase accuracy, though this comes with higher operational costs. This research highlights AI&#39;s potential to minimize manual errors and streamline due diligence, suggesting a broader application of AI in financial document analysis and risk management.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.04294"
    },
    "ce0125bf6fd7254260d4181c039c17dc": {
        "title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents",
        "authors": [
            "Yue Liu",
            "Sin Kit Lo",
            "Qinghua Lu",
            "Liming Zhu",
            "Dehai Zhao",
            "Xiwei Xu",
            "Stefan Harrer",
            "Jon Whittle"
        ],
        "date": "2024/05/16",
        "pdf": "http://arxiv.org/pdf/2405.10467",
        "abstract": "Foundation model-enabled generative artificial intelligence facilitates the development and implementation of agents, which can leverage distinguished reasoning and language processing capabilities to takes a proactive, autonomous role to pursue users&#39; goals. Nevertheless, there is a lack of systematic knowledge to guide practitioners in designing the agents considering challenges of goal-seeking (including generating instrumental goals and plans), such as hallucinations inherent in foundation models, explainability of reasoning process, complex accountability, etc. To address this issue, we have performed a systematic literature review to understand the state-of-the-art foundation model-based agents and the broader ecosystem. In this paper, we present a pattern catalogue consisting of 16 architectural patterns with analyses of the context, forces, and trade-offs as the outcomes from the previous literature review. The proposed catalogue can provide holistic guidance for the effective use of patterns, and support the architecture design of foundation model-based agents by facilitating goal-seeking and plan generation.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2405.10467"
    },
    "94a565cf17ac32e1846fc0b3fffad4ec": {
        "title": "Latent State Estimation Helps UI Agents to Reason",
        "authors": [
            "William E Bishop",
            "Alice Li",
            "Christopher Rawles",
            "Oriana Riva"
        ],
        "date": "2024/05/17",
        "pdf": "http://arxiv.org/pdf/2405.11120",
        "abstract": "A common problem for agents operating in real-world environments is that the response of an environment to their actions may be non-deterministic and observed through noise. This renders environmental state and progress towards completing a task latent. Despite recent impressive demonstrations of LLM&#39;s reasoning abilities on various benchmarks, whether LLMs can build estimates of latent state and leverage them for reasoning has not been explicitly studied. We investigate this problem in the real-world domain of autonomous UI agents. We establish that appropriately prompting LLMs in a zero-shot manner can be formally understood as forming point estimates of latent state in a textual space. In the context of autonomous UI agents we then show that LLMs used in this manner are more than $76\\%$ accurate at inferring various aspects of latent state, such as performed (vs. commanded) actions and task progression. Using both public and internal benchmarks and three reasoning methods (zero-shot, CoT-SC &amp; ReAct), we show that LLM-powered agents that explicitly estimate and reason about latent state are able to successfully complete up to 1.6x more tasks than those that do not.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.11120"
    },
    "750790b50612eb08e2ba6127a791d3c7": {
        "title": "ALI-Agent: Assessing LLMs&#39; Alignment with Human Values via Agent-based Evaluation",
        "authors": [
            "Jingnan Zheng",
            "Han Wang",
            "An Zhang",
            "Tai D. Nguyen",
            "Jun Sun",
            "Tat-Seng Chua"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14125",
        "abstract": "Large Language Models (LLMs) can elicit unintended and even harmful content when misaligned with human values, posing severe risks to users and society. To mitigate these risks, current evaluation benchmarks predominantly employ expert-designed contextual scenarios to assess how well LLMs align with human values. However, the labor-intensive nature of these benchmarks limits their test scope, hindering their ability to generalize to the extensive variety of open-world use cases and identify rare but crucial long-tail risks. Additionally, these static tests fail to adapt to the rapid evolution of LLMs, making it hard to evaluate timely alignment issues. To address these challenges, we propose ALI-Agent, an evaluation framework that leverages the autonomous abilities of LLM-powered agents to conduct in-depth and adaptive alignment assessments. ALI-Agent operates through two principal stages: Emulation and Refinement. During the Emulation stage, ALI-Agent automates the generation of realistic test scenarios. In the Refinement stage, it iteratively refines the scenarios to probe long-tail risks. Specifically, ALI-Agent incorporates a memory module to guide test scenario generation, a tool-using module to reduce human labor in tasks such as evaluating feedback from target LLMs, and an action module to refine tests. Extensive experiments across three aspects of human values--stereotypes, morality, and legality--demonstrate that ALI-Agent, as a general evaluation framework, effectively identifies model misalignment. Systematic analysis also validates that the generated test scenarios represent meaningful use cases, as well as integrate enhanced measures to probe long-tail risks. Our code is available at https://github.com/SophieZheng998/ALI-Agent.git",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.14125"
    },
    "b2904bbe6b5faeec0f25208fc522269e": {
        "title": "Human-Agent Cooperation in Games under Incomplete Information through Natural Language Communication",
        "authors": [
            "Shenghui Chen",
            "Daniel Fried",
            "Ufuk Topcu"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14173",
        "abstract": "Developing autonomous agents that can strategize and cooperate with humans under information asymmetry is challenging without effective communication in natural language. We introduce a shared-control game, where two players collectively control a token in alternating turns to achieve a common objective under incomplete information. We formulate a policy synthesis problem for an autonomous agent in this game with a human as the other player. To solve this problem, we propose a communication-based approach comprising a language module and a planning module. The language module translates natural language messages into and from a finite set of flags, a compact representation defined to capture player intents. The planning module leverages these flags to compute a policy using an asymmetric information-set Monte Carlo tree search with flag exchange algorithm we present. We evaluate the effectiveness of this approach in a testbed based on Gnomes at Night, a search-and-find maze board game. Results of human subject experiments show that communication narrows the information gap between players and enhances human-agent cooperation efficiency with fewer turns.",
        "code": "",
        "category": [
            "Game Playing",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2405.14173"
    },
    "4d296b8ae0f8197119be49892f53d198": {
        "title": "AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents",
        "authors": [
            "Christopher Rawles",
            "Sarah Clinckemaillie",
            "Yifan Chang",
            "Jonathan Waltz",
            "Gabrielle Lau",
            "Marybeth Fair",
            "Alice Li",
            "William Bishop",
            "Wei Li",
            "Folawiyo Campbell-Ajala",
            "Daniel Toyama",
            "Robert Berry",
            "Divya Tyamagundlu",
            "Timothy Lillicrap",
            "Oriana Riva"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14573",
        "abstract": "Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. Yet, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functioning Android environment that provides reward signals for 116 programmatic task workflows across 20 real world Android applications. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and realistic suite of tasks. Reward signals are derived from the computer&#39;s system state, making them durable across task variations and extensible across different apps. To demonstrate AndroidWorld&#39;s benefits and mode of operation, we introduce a new computer control agent, M3A. M3A can complete 30.6% of the AndroidWorld&#39;s tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-domain agents. Finally, we conduct a robustness analysis by testing M3A against a range of task variations on a representative subset of tasks, demonstrating that variations in task parameters can significantly alter the complexity of a task and therefore an agent&#39;s performance, highlighting the importance of testing agents under diverse conditions. AndroidWorld and the experiments in this paper are available at https://github.com/google-research/android_world.",
        "code": "",
        "category": [
            "Benchmark&Evaluation",
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2405.14573"
    },
    "241a53b94a75c87f40db449ad7d7c09f": {
        "title": "CityGPT: Towards Urban IoT Learning, Analysis and Interaction with Multi-Agent System",
        "authors": [
            "Qinghua Guan",
            "Jinhui Ouyang",
            "Di Wu",
            "Weiren Yu"
        ],
        "date": "2024/05/23",
        "pdf": "http://arxiv.org/pdf/2405.14691",
        "abstract": "The spatiotemporal data generated by massive sensors in the Internet of Things (IoT) is extremely dynamic, heterogeneous, large scale and time-dependent. It poses great challenges (e.g. accuracy, reliability, and stability) in real-time analysis and decision making for different IoT applications. The complexity of IoT data prevents the common people from gaining a deeper understanding of it. Agentized systems help address the lack of data insight for the common people. We propose a generic framework, namely CityGPT, to facilitate the learning and analysis of IoT time series with an end-to-end paradigm. CityGPT employs three agents to accomplish the spatiotemporal analysis of IoT data. The requirement agent facilitates user inputs based on natural language. Then, the analysis tasks are decomposed into temporal and spatial analysis processes, completed by corresponding data analysis agents (temporal and spatial agents). Finally, the spatiotemporal fusion agent visualizes the system&#39;s analysis results by receiving analysis results from data analysis agents and invoking sub-visualization agents, and can provide corresponding textual descriptions based on user demands. To increase the insight for common people using our framework, we have agnentized the framework, facilitated by a large language model (LLM), to increase the data comprehensibility. Our evaluation results on real-world data with different time dependencies show that the CityGPT framework can guarantee robust performance in IoT computing.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.14691"
    },
    "71fde0ae3293374724587616030740e8": {
        "title": "Evaluating Tool-Augmented Agents in Remote Sensing Platforms",
        "authors": [
            "Simranjit Singh",
            "Michael Fore",
            "Dimitrios Stamoulis"
        ],
        "date": "2024/04/23",
        "pdf": "http://arxiv.org/pdf/2405.00709",
        "abstract": "Tool-augmented Large Language Models (LLMs) have shown impressive capabilities in remote sensing (RS) applications. However, existing benchmarks assume question-answering input templates over predefined image-text data pairs. These standalone instructions neglect the intricacies of realistic user-grounded tasks. Consider a geospatial analyst: they zoom in a map area, they draw a region over which to collect satellite imagery, and they succinctly ask &#34;Detect all objects here&#34;. Where is `here`, if it is not explicitly hardcoded in the image-text template, but instead is implied by the system state, e.g., the live map positioning? To bridge this gap, we present GeoLLM-QA, a benchmark designed to capture long sequences of verbal, visual, and click-based actions on a real UI platform. Through in-depth evaluation of state-of-the-art LLMs over a diverse set of 1,000 tasks, we offer insights towards stronger agents for RS applications.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2405.00709"
    },
    "adf670f5c071558cd2fe0713eb34f114": {
        "title": "LLMSat: A Large Language Model-Based Goal-Oriented Agent for Autonomous Space Exploration",
        "authors": [
            "David Maranto"
        ],
        "date": "2024/04/13",
        "pdf": "http://arxiv.org/pdf/2405.01392",
        "abstract": "As spacecraft journey further from Earth with more complex missions, systems of greater autonomy and onboard intelligence are called for. Reducing reliance on human-based mission control becomes increasingly critical if we are to increase our rate of solar-system-wide exploration. Recent work has explored AI-based goal-oriented systems to increase the level of autonomy in mission execution. These systems make use of symbolic reasoning managers to make inferences from the state of a spacecraft and a handcrafted knowledge base, enabling autonomous generation of tasks and re-planning. Such systems have proven to be successful in controlled cases, but they are difficult to implement as they require human-crafted ontological models to allow the spacecraft to understand the world. Reinforcement learning has been applied to train robotic agents to pursue a goal. A new architecture for autonomy is called for. This work explores the application of Large Language Models (LLMs) as the high-level control system of a spacecraft. Using a systems engineering approach, this work presents the design and development of an agentic spacecraft controller by leveraging an LLM as a reasoning engine, to evaluate the utility of such an architecture in achieving higher levels of spacecraft autonomy. A series of deep space mission scenarios simulated within the popular game engine Kerbal Space Program (KSP) are used as case studies to evaluate the implementation against the requirements. It is shown the reasoning and planning abilities of present-day LLMs do not scale well as the complexity of a mission increases, but this can be alleviated with adequate prompting frameworks and strategic selection of the agent&#39;s level of authority over the host spacecraft. This research evaluates the potential of LLMs in augmenting autonomous decision-making systems for future robotic space applications.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.01392"
    },
    "84c6d6849a4e01526b03a20d54035cbb": {
        "title": "Rapid Mobile App Development for Generative AI Agents on MIT App Inventor",
        "authors": [
            "Jaida Gao",
            "Calab Su",
            "Etai Miller",
            "Kevin Lu",
            "Yu Meng"
        ],
        "date": "2024/04/01",
        "pdf": "http://arxiv.org/pdf/2405.01561",
        "abstract": "The evolution of Artificial Intelligence (AI) stands as a pivotal force shaping our society, finding applications across diverse domains such as education, sustainability, and safety. Leveraging AI within mobile applications makes it easily accessible to the public, catalyzing its transformative potential. In this paper, we present a methodology for the rapid development of AI agent applications using the development platform provided by MIT App Inventor. To demonstrate its efficacy, we share the development journey of three distinct mobile applications: SynchroNet for fostering sustainable communities; ProductiviTeams for addressing procrastination; and iHELP for enhancing community safety. All three applications seamlessly integrate a spectrum of generative AI features, leveraging OpenAI APIs. Furthermore, we offer insights gleaned from overcoming challenges in integrating diverse tools and AI functionalities, aiming to inspire young developers to join our efforts in building practical AI agent applications.",
        "code": "",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2405.01561"
    },
    "a3bec6082d05d452c748821b265713b1": {
        "title": "Large Language Models (LLMs) as Agents for Augmented Democracy",
        "authors": [
            "Jairo Gudiño-Rosero",
            "Umberto Grandi",
            "César A. Hidalgo"
        ],
        "date": "2024/05/06",
        "pdf": "http://arxiv.org/pdf/2405.03452",
        "abstract": "We explore the capabilities of an augmented democracy system built on off-the-shelf LLMs fine-tuned on data summarizing individual preferences across 67 policy proposals collected during the 2022 Brazilian presidential elections. We use a train-test cross-validation setup to estimate the accuracy with which the LLMs predict both: a subject&#39;s individual political choices and the aggregate preferences of the full sample of participants. At the individual level, the accuracy of the out of sample predictions lie in the range 69%-76% and are significantly better at predicting the preferences of liberal and college educated participants. At the population level, we aggregate preferences using an adaptation of the Borda score and compare the ranking of policy proposals obtained from a probabilistic sample of participants and from data augmented using LLMs. We find that the augmented data predicts the preferences of the full population of participants better than probabilistic samples alone when these represent less than 30% to 40% of the total population. These results indicate that LLMs are potentially useful for the construction of systems of augmented democracy.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.03452"
    },
    "7280e591137995f5e4d5787bc79f1c9f": {
        "title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models",
        "authors": [
            "Cong Lu",
            "Shengran Hu",
            "Jeff Clune"
        ],
        "date": "2024/05/24",
        "pdf": "http://arxiv.org/pdf/2405.15143",
        "abstract": "Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems, built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of challenging problems including Atari games and robotic control, but requires manually designing heuristics to guide exploration, which is time-consuming and infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE) which greatly extends the scope of the original Go-Explore by replacing these heuristics with the intelligence and internalized human notions of interestingness captured by giant foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is (e.g. discovering new objects, locations, or behaviors), even in complex environments where heuristics are hard to define. Moreover, IGE offers the exciting and previously impossible opportunity to recognize and capitalize on serendipitous discoveries that cannot be predicted ahead of time. We evaluate IGE on a range of language-based tasks that require search and exploration. In Game of 24, a multistep mathematical reasoning problem, IGE reaches 100% success rate 70.8% faster than the best classic graph search baseline. Next, in BabyAI-Text, a challenging partially observable gridworld, IGE exceeds the previous SOTA with orders of magnitude fewer online samples. Finally, in TextWorld, we show the unique ability of IGE to succeed in settings requiring long-horizon exploration where prior SOTA FM agents like Reflexion completely fail. Overall, IGE combines the tremendous strengths of FMs and the powerful Go-Explore algorithm, opening up a new frontier of research into creating more generally capable agents with impressive exploration capabilities.",
        "code": "https://github.com/conglu1997/intelligent-go-explore",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2405.15143"
    },
    "c1543f88fc335d89f5e317fb67d609ed": {
        "title": "Large Language Models Can Self-Improve At Web Agent Tasks",
        "authors": [
            "Ajay Patel",
            "Markus Hofmarcher",
            "Claudiu Leoveanu-Condrei",
            "Marius-Constantin Dinu",
            "Chris Callison-Burch",
            "Sepp Hochreiter"
        ],
        "date": "2024/05/30",
        "pdf": "http://arxiv.org/pdf/2405.20309",
        "abstract": "Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2405.20309"
    },
    "aa807ff46f37c2e93777ace6d24d0fce": {
        "title": "Safe Multi-agent Reinforcement Learning with Natural Language Constraints",
        "authors": [
            "Ziyan Wang",
            "Meng Fang",
            "Tristan Tomilin",
            "Fei Fang",
            "Yali Du"
        ],
        "date": "2024/05/30",
        "pdf": "http://arxiv.org/pdf/2405.20018",
        "abstract": "The role of natural language constraints in Safe Multi-agent Reinforcement Learning (MARL) is crucial, yet often overlooked. While Safe MARL has vast potential, especially in fields like robotics and autonomous vehicles, its full potential is limited by the need to define constraints in pre-designed mathematical terms, which requires extensive domain expertise and reinforcement learning knowledge, hindering its broader adoption. To address this limitation and make Safe MARL more accessible and adaptable, we propose a novel approach named Safe Multi-agent Reinforcement Learning with Natural Language constraints (SMALL). Our method leverages fine-tuned language models to interpret and process free-form textual constraints, converting them into semantic embeddings that capture the essence of prohibited states and behaviours. These embeddings are then integrated into the multi-agent policy learning process, enabling agents to learn policies that minimize constraint violations while optimizing rewards. To evaluate the effectiveness of SMALL, we introduce the LaMaSafe, a multi-task benchmark designed to assess the performance of multiple agents in adhering to natural language constraints. Empirical evaluations across various environments demonstrate that SMALL achieves comparable rewards and significantly fewer constraint violations, highlighting its effectiveness in understanding and enforcing natural language constraints.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.20018"
    },
    "599e168857c70e1fcb2c891f1be0fe66": {
        "title": "A Human-Like Reasoning Framework for Multi-Phases Planning Task with Large Language Models",
        "authors": [
            "Chengxing Xie",
            "Difan Zou"
        ],
        "date": "2024/05/28",
        "pdf": "http://arxiv.org/pdf/2405.18208",
        "abstract": "Recent studies have highlighted their proficiency in some simple tasks like writing and coding through various reasoning strategies. However, LLM agents still struggle with tasks that require comprehensive planning, a process that challenges current models and remains a critical research issue. In this study, we concentrate on travel planning, a Multi-Phases planning problem, that involves multiple interconnected stages, such as outlining, information gathering, and planning, often characterized by the need to manage various constraints and uncertainties. Existing reasoning approaches have struggled to effectively address this complex task. Our research aims to address this challenge by developing a human-like planning framework for LLM agents, i.e., guiding the LLM agent to simulate various steps that humans take when solving Multi-Phases problems. Specifically, we implement several strategies to enable LLM agents to generate a coherent outline for each travel query, mirroring human planning patterns. Additionally, we integrate Strategy Block and Knowledge Block into our framework: Strategy Block facilitates information collection, while Knowledge Block provides essential information for detailed planning. Through our extensive experiments, we demonstrate that our framework significantly improves the planning capabilities of LLM agents, enabling them to tackle the travel planning task with improved efficiency and effectiveness. Our experimental results showcase the exceptional performance of the proposed framework; when combined with GPT-4-Turbo, it attains $10\\times$ the performance gains in comparison to the baseline framework deployed on GPT-4-Turbo.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2405.18208"
    },
    "21c1661e923a0cbe16270816189c6c85": {
        "title": "LLM-Based Cooperative Agents using Information Relevance and Plan Validation",
        "authors": [
            "SeungWon Seo",
            "Junhyeok Lee",
            "SeongRae Noh",
            "HyeongYeop Kang"
        ],
        "date": "2024/05/27",
        "pdf": "http://arxiv.org/pdf/2405.16751",
        "abstract": "We address the challenge of multi-agent cooperation, where agents achieve a common goal by interacting with a 3D scene and cooperating with decentralized agents under complex partial observations. This involves managing communication costs and optimizing interaction trajectories in dynamic environments. Our research focuses on three primary limitations of existing cooperative agent systems. Firstly, current systems demonstrate inefficiency in managing acquired information through observation, resulting in declining planning performance as the environment becomes more complex with additional objects or goals. Secondly, the neglect of false plans in partially observable settings leads to suboptimal cooperative performance, as agents struggle to adapt to environmental changes influenced by the unseen actions of other agents. Lastly, the failure to incorporate spatial data into decision-making processes restricts the agent&#39;s ability to construct optimized trajectories. To overcome these limitations, we propose the RElevance and Validation-Enhanced Cooperative Language Agent (REVECA), a novel cognitive architecture powered by GPT-3.5. REVECA leverages relevance assessment, plan validation, and spatial information to enhance the efficiency and robustness of agent cooperation in dynamic and partially observable environments while minimizing continuous communication costs and effectively managing irrelevant dummy objects. Our extensive experiments demonstrate the superiority of REVECA over previous approaches, including those driven by GPT-4.0. Additionally, a user study highlights REVECA&#39;s potential for achieving trustworthy human-AI cooperation. We expect that REVECA will have significant applications in gaming, XR applications, educational tools, and humanoid robots, contributing to substantial economic, commercial, and academic advancements.",
        "code": "",
        "category": [
            "Planning",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2405.16751"
    },
    "faa2d1dbd6a9e8f7c1e6bb8a7b396643": {
        "title": "SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering",
        "authors": [
            "John Yang",
            "Carlos E. Jimenez",
            "Alexander Wettig",
            "Kilian Lieret",
            "Shunyu Yao",
            "Karthik Narasimhan",
            "Ofir Press"
        ],
        "date": "2024/05/06",
        "pdf": "http://arxiv.org/pdf/2405.15793",
        "abstract": "Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent&#39;s custom agent-computer interface (ACI) significantly enhances an agent&#39;s ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5% and 87.7%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents&#39; behavior and performance.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.15793"
    },
    "d870e9fafc0687b64ab77ac017cbe309": {
        "title": "Hacc-Man: An Arcade Game for Jailbreaking LLMs",
        "authors": [
            "Matheus Valentim",
            "Jeanette Falk",
            "Nanna Inie"
        ],
        "date": "2024/05/24",
        "pdf": "http://arxiv.org/pdf/2405.15902",
        "abstract": "The recent leaps in complexity and fluency of Large Language Models (LLMs) mean that, for the first time in human history, people can interact with computers using natural language alone. This creates monumental possibilities of automation and accessibility of computing, but also raises severe security and safety threats: When everyone can interact with LLMs, everyone can potentially break into the systems running LLMs. All it takes is creative use of language. This paper presents Hacc-Man, a game which challenges its players to &#34;jailbreak&#34; an LLM: subvert the LLM to output something that it is not intended to. Jailbreaking is at the intersection between creative problem solving and LLM security. The purpose of the game is threefold: 1. To heighten awareness of the risks of deploying fragile LLMs in everyday systems, 2. To heighten people&#39;s self-efficacy in interacting with LLMs, and 3. To discover the creative problem solving strategies, people deploy in this novel context.",
        "code": "",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2405.15902"
    },
    "e80b28ea56e0a7ce32e3fc5404c316d7": {
        "title": "GeneAgent: Self-verification Language Agent for Gene Set Knowledge Discovery using Domain Databases",
        "authors": [
            "Zhizheng Wang",
            "Qiao Jin",
            "Chih-Hsuan Wei",
            "Shubo Tian",
            "Po-Ting Lai",
            "Qingqing Zhu",
            "Chi-Ping Day",
            "Christina Ross",
            "Zhiyong Lu"
        ],
        "date": "2024/05/25",
        "pdf": "http://arxiv.org/pdf/2405.16205",
        "abstract": "Gene set knowledge discovery is essential for advancing human functional genomics. Recent studies have shown promising performance by harnessing the power of Large Language Models (LLMs) on this task. Nonetheless, their results are subject to several limitations common in LLMs such as hallucinations. In response, we present GeneAgent, a first-of-its-kind language agent featuring self-verification capability. It autonomously interacts with various biological databases and leverages relevant domain knowledge to improve accuracy and reduce hallucination occurrences. Benchmarking on 1,106 gene sets from different sources, GeneAgent consistently outperforms standard GPT-4 by a significant margin. Moreover, a detailed manual review confirms the effectiveness of the self-verification module in minimizing hallucinations and generating more reliable analytical narratives. To demonstrate its practical utility, we apply GeneAgent to seven novel gene sets derived from mouse B2905 melanoma cell lines, with expert evaluations showing that GeneAgent offers novel insights into gene functions and subsequently expedites knowledge discovery.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.16205"
    },
    "498ac37f486ba2e8dc8ab3363404d05b": {
        "title": "AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning",
        "authors": [
            "Minghao Chen",
            "Yihang Li",
            "Yanting Yang",
            "Shiyu Yu",
            "Binbin Lin",
            "Xiaofei He"
        ],
        "date": "2024/05/25",
        "pdf": "http://arxiv.org/pdf/2405.16247",
        "abstract": "Large Language Models (LLM) based agents have shown promise in autonomously completing tasks across various domains, e.g., robotics, games, and web navigation. However, these agents typically require elaborate design and expert prompts to solve tasks in specific domains, which limits their adaptability. We introduce AutoManual, a framework enabling LLM agents to autonomously build their understanding through interaction and adapt to new environments. AutoManual categorizes environmental knowledge into diverse rules and optimizes them in an online fashion by two agents: 1) The Planner codes actionable plans based on current rules for interacting with the environment. 2) The Builder updates the rules through a well-structured rule system that facilitates online rule management and essential detail retention. To mitigate hallucinations in managing rules, we introduce \\textit{case-conditioned prompting} strategy for the Builder. Finally, the Formulator agent compiles these rules into a comprehensive manual. The self-generated manual can not only improve the adaptability but also guide the planning of smaller LLMs while being human-readable. Given only one simple demonstration, AutoManual significantly improves task success rates, achieving 97.4\\% with GPT-4-turbo and 86.2\\% with GPT-3.5-turbo on ALFWorld benchmark tasks. The source code will be available soon.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.16247"
    },
    "b1821b90ce36752821dab90a5e4cd860": {
        "title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models",
        "authors": [
            "Jaewoo Ahn",
            "Taehyun Lee",
            "Junyoung Lim",
            "Jin-Hwa Kim",
            "Sangdoo Yun",
            "Hwaran Lee",
            "Gunhee Kim"
        ],
        "date": "2024/05/28",
        "pdf": "http://arxiv.org/pdf/2405.18027",
        "abstract": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users&#39; narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters&#39; identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2405.18027"
    },
    "c750d6b039a1709a6a9959a889758567": {
        "title": "ResearchArena: Benchmarking LLMs&#39; Ability to Collect and Organize Information as Research Agents",
        "authors": [
            "Hao Kang",
            "Chenyan Xiong"
        ],
        "date": "2024/06/13",
        "pdf": "http://arxiv.org/pdf/2406.10291",
        "abstract": "Large language models (LLMs) have exhibited remarkable performance across various tasks in natural language processing. Nevertheless, challenges still arise when these tasks demand domain-specific expertise and advanced analytical skills, such as conducting research surveys on a designated topic. In this research, we develop ResearchArena, a benchmark that measures LLM agents&#39; ability to conduct academic surveys, an initial step of academic research process. Specifically, we deconstructs the surveying process into three stages 1) information discovery: locating relevant papers, 2) information selection: assessing papers&#39; importance to the topic, and 3) information organization: organizing papers into meaningful structures. In particular, we establish an offline environment comprising 12.0M full-text academic papers and 7.9K survey papers, which evaluates agents&#39; ability to locate supporting materials for composing the survey on a topic, rank the located papers based on their impact, and organize these into a hierarchical knowledge mind-map. With this benchmark, we conduct preliminary evaluations of existing techniques and find that all LLM-based methods under-performing when compared to basic keyword-based retrieval techniques, highlighting substantial opportunities for future research.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.10291"
    },
    "e8f100e713dc301cd7404ff87bb6b5aa": {
        "title": "GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents",
        "authors": [
            "Dongping Chen",
            "Yue Huang",
            "Siyuan Wu",
            "Jingyu Tang",
            "Liuyi Chen",
            "Yilin Bai",
            "Zhigang He",
            "Chenlong Wang",
            "Huichi Zhou",
            "Yiqiang Li",
            "Tianshuo Zhou",
            "Yue Yu",
            "Chujie Gao",
            "Qihui Zhang",
            "Yi Gui",
            "Zhen Li",
            "Yao Wan",
            "Pan Zhou",
            "Jianfeng Gao",
            "Lichao Sun"
        ],
        "date": "2024/06/16",
        "pdf": "http://arxiv.org/pdf/2406.10819",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code. However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-oriented questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-oriented tasks given the sparse GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding. The code and dataset are publicly available at our project homepage: https://gui-world.github.io/.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.10819"
    },
    "a73b2790e7940e467a2d7ac436a550f8": {
        "title": "GUICourse: From General Vision Language Models to Versatile GUI Agents",
        "authors": [
            "Wentong Chen",
            "Junbo Cui",
            "Jinyi Hu",
            "Yujia Qin",
            "Junjie Fang",
            "Yue Zhao",
            "Chongyi Wang",
            "Jun Liu",
            "Guirong Chen",
            "Yupeng Huo",
            "Yuan Yao",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11317",
        "abstract": "Utilizing Graphic User Interface (GUI) for human-computer interaction is essential for accessing a wide range of digital tools. Recent advancements in Vision Language Models (VLMs) highlight the compelling potential to develop versatile agents to help humans finish GUI navigation tasks. However, current VLMs are challenged in terms of fundamental abilities (OCR and grounding) and GUI knowledge (the functions and control methods of GUI elements), preventing them from becoming practical GUI agents. To solve these challenges, we contribute GUICourse, a suite of datasets to train visual-based GUI agents from general VLMs. First, we introduce the GUIEnv dataset to strengthen the OCR and grounding capabilities of VLMs. Then, we introduce the GUIAct and GUIChat datasets to enrich their knowledge of GUI components and interactions. Experiments demonstrate that our GUI agents have better performance on common GUI tasks than their baseline VLMs. Even the small-size GUI agent (with 3.1B parameters) can still work well on single-step and multi-step GUI tasks. Finally, we analyze the different varieties in the training stage of this agent by ablation study. Our source codes and datasets are released at https://github.com/yiye3/GUICourse.",
        "code": "https://github.com/yiye3/guicourse",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2406.11317"
    },
    "b7788f3550328428ea5ce540b638f532": {
        "title": "Toward Conversational Agents with Context and Time Sensitive Long-term Memory",
        "authors": [
            "Nick Alonso",
            "Tomás Figliolia",
            "Anthony Ndirango",
            "Beren Millidge"
        ],
        "date": "2024/05/29",
        "pdf": "http://arxiv.org/pdf/2406.00057",
        "abstract": "There has recently been growing interest in conversational agents with long-term memory which has led to the rapid development of language models that use retrieval-augmented generation (RAG). Until recently, most work on RAG has focused on information retrieval from large databases of texts, like Wikipedia, rather than information from long-form conversations. In this paper, we argue that effective retrieval from long-form conversational data faces two unique problems compared to static database retrieval: 1) time/event-based queries, which requires the model to retrieve information about previous conversations based on time or the order of a conversational event (e.g., the third conversation on Tuesday), and 2) ambiguous queries that require surrounding conversational context to understand. To better develop RAG-based agents that can deal with these challenges, we generate a new dataset of ambiguous and time-based questions that build upon a recent dataset of long-form, simulated conversations, and demonstrate that standard RAG based approaches handle such questions poorly. We then develop a novel retrieval model which combines chained-of-table search methods, standard vector-database retrieval, and a prompting method to disambiguate queries, and demonstrate that this approach substantially improves over current methods at solving these tasks. We believe that this new dataset and more advanced RAG agent can act as a key benchmark and stepping stone towards effective memory augmented conversational agents that can be used in a wide variety of AI applications.",
        "code": "https://github.com/Zyphra/TemporalMemoryDataset",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2406.00057"
    },
    "21aab690f7cd4cbd1de0b1f6393d4e30": {
        "title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training",
        "authors": [
            "Maximillian Chen",
            "Ruoxi Sun",
            "Sercan Ö. Arık",
            "Tomas Pfister"
        ],
        "date": "2024/05/31",
        "pdf": "http://arxiv.org/pdf/2406.00222",
        "abstract": "Large language models (LLMs) aligned through reinforcement learning from human feedback (RLHF) have quickly become one of the dominant paradigms for building intelligent conversational assistant agents. However, despite their strong performance across many benchmarks, LLM-based agents still lack conversational skills such as disambiguation: when generalized assistants are faced with ambiguity, they often overhedge or implicitly guess users&#39; ground-truth intents rather than asking clarification questions, and under task-specific settings, high-quality conversation samples are often limited, affecting models&#39; ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (henceforth ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO) which allows for sample-efficient dialogue policy learning in multi-turn conversation. We demonstrate ACT&#39;s efficacy under sample-efficient conditions in three difficult conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for text-to-SQL generation. Additionally, we propose evaluating LLMs&#39; ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard approaches to supervised fine-tuning and DPO.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.00222"
    },
    "86aec45a3853272af15d2b2d2c6ae12d": {
        "title": "Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Haitao Jia",
            "Xi Zhang",
            "Ming Yan",
            "Weizhou Shen",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "date": "2024/06/03",
        "pdf": "http://arxiv.org/pdf/2406.01014",
        "abstract": "Mobile device operation tasks are increasingly becoming a popular multi-modal AI application scenario. Current Multi-modal Large Language Models (MLLMs), constrained by their training data, lack the capability to function effectively as operation assistants. Instead, MLLM-based agents, which enhance capabilities through tool invocation, are gradually being applied to this scenario. However, the two major navigation challenges in mobile device operation tasks, task progress navigation and focus content navigation, are significantly complicated under the single-agent architecture of existing work. This is due to the overly long token sequences and the interleaved text-image data format, which limit performance. To address these navigation challenges effectively, we propose Mobile-Agent-v2, a multi-agent architecture for mobile device operation assistance. The architecture comprises three agents: planning agent, decision agent, and reflection agent. The planning agent generates task progress, making the navigation of history operations more efficient. To retain focus content, we design a memory unit that updates with task progress. Additionally, to correct erroneous operations, the reflection agent observes the outcomes of each operation and handles any mistakes accordingly. Experimental results indicate that Mobile-Agent-v2 achieves over a 30% improvement in task completion compared to the single-agent architecture of Mobile-Agent. The code is open-sourced at https://github.com/X-PLUG/MobileAgent.",
        "code": "https://github.com/x-plug/mobileagent",
        "category": [
            "Multi-Agent System",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2406.01014"
    },
    "da55b01bed4abf929270ff8bbef859af": {
        "title": "Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization",
        "authors": [
            "Yu-Min Tseng",
            "Yu-Chao Huang",
            "Teng-Yun Hsiao",
            "Yu-Ching Hsu",
            "Jia-Yin Foo",
            "Chao-Wei Huang",
            "Yun-Nung Chen"
        ],
        "date": "2024/06/03",
        "pdf": "http://arxiv.org/pdf/2406.01171",
        "abstract": "Recently, methods investigating how to adapt large language models (LLMs) for specific scenarios have gained great attention. Particularly, the concept of \\textit{persona}, originally adopted in dialogue literature, has re-surged as a promising avenue. However, the growing research on persona is relatively disorganized, lacking a systematic overview. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. To the best of our knowledge, we present the first survey tailored for LLM role-playing and LLM personalization under the unified view of persona, including taxonomy, current challenges, and potential directions. To foster future endeavors, we actively maintain a paper collection available to the community: https://github.com/MiuLab/PersonaLLM-Survey",
        "code": "https://github.com/miulab/personallm-survey",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2406.01171"
    },
    "a08093f55986bcc87491fb616bb60ea5": {
        "title": "Reflection-Reinforced Self-Training for Language Agents",
        "authors": [
            "Zi-Yi Dou",
            "Cheng-Fu Yang",
            "Xueqing Wu",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "date": "2024/06/03",
        "pdf": "http://arxiv.org/pdf/2406.01495",
        "abstract": "Self-training can potentially improve the performance of language agents without relying on demonstrations from humans or stronger models. The general process involves generating samples from a model, evaluating their quality, and updating the model by training on high-quality samples. However, self-training can face limitations because achieving good performance requires a good amount of high-quality samples, yet relying solely on model sampling for obtaining such samples can be inefficient. In addition, these methods often disregard low-quality samples, failing to leverage them effectively. To address these limitations, we present Reflection-Reinforced Self-Training (Re-ReST), which leverages a reflection model to refine low-quality samples and subsequently uses these improved samples to augment self-training. The reflection model takes both the model output and feedback from an external environment (e.g., unit test results in code generation) as inputs and produces improved samples as outputs. By employing this technique, we effectively enhance the quality of inferior samples, and enrich the self-training dataset with higher-quality samples efficiently. We perform extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. Results demonstrate improvements over self-training baselines across settings. Moreover, ablation studies confirm the reflection model&#39;s efficiency in generating quality self-training samples and its compatibility with self-consistency decoding.",
        "code": "https://github.com/PlusLabNLP/Re-ReST",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.01495"
    },
    "3150a26b3d768a61624882282f316dad": {
        "title": "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks",
        "authors": [
            "Yusen Zhang",
            "Ruoxi Sun",
            "Yanfei Chen",
            "Tomas Pfister",
            "Rui Zhang",
            "Sercan Ö. Arik"
        ],
        "date": "2024/06/04",
        "pdf": "http://arxiv.org/pdf/2406.02818",
        "abstract": "Addressing the challenge of effectively processing long contexts has become a critical issue for Large Language Models (LLMs). Two common strategies have emerged: 1) reducing the input length, such as retrieving relevant chunks by Retrieval-Augmented Generation (RAG), and 2) expanding the context window limit of LLMs. However, both strategies have drawbacks: input reduction has no guarantee of covering the part with needed information, while window extension struggles with focusing on the pertinent information for solving the task. To mitigate these limitations, we propose Chain-of-Agents (CoA), a novel framework that harnesses multi-agent collaboration through natural language to enable information aggregation and context reasoning across various LLMs over long-context tasks. CoA consists of multiple worker agents who sequentially communicate to handle different segmented portions of the text, followed by a manager agent who synthesizes these contributions into a coherent final output. CoA processes the entire input by interleaving reading and reasoning, and it mitigates long context focus issues by assigning each agent a short context. We perform comprehensive evaluation of CoA on a wide range of long-context tasks in question answering, summarization, and code completion, demonstrating significant improvements by up to 10% over strong baselines of RAG, Full-Context, and multi-agent LLMs.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.02818"
    },
    "9289102622122ee14122949cf1ccb40c": {
        "title": "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents",
        "authors": [
            "Yifei Wang",
            "Dizhan Xue",
            "Shengjie Zhang",
            "Shengsheng Qian"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03007",
        "abstract": "With the prosperity of large language models (LLMs), powerful LLM-based intelligent agents have been developed to provide customized services with a set of user-defined tools. State-of-the-art methods for constructing LLM agents adopt trained LLMs and further fine-tune them on data for the agent task. However, we show that such methods are vulnerable to our proposed backdoor attacks named BadAgent on various agent tasks, where a backdoor can be embedded by fine-tuning on the backdoor data. At test time, the attacker can manipulate the deployed LLM agents to execute harmful operations by showing the trigger in the agent input or environment. To our surprise, our proposed attack methods are extremely robust even after fine-tuning on trustworthy data. Though backdoor attacks have been studied extensively in natural language processing, to the best of our knowledge, we could be the first to study them on LLM agents that are more dangerous due to the permission to use external tools. Our work demonstrates the clear risk of constructing LLM agents based on untrusted LLMs or data. Our code is public at https://github.com/DPamK/BadAgent",
        "code": "https://github.com/dpamk/badagent",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.03007"
    },
    "269cd0ac32202d249d19366188e9d849": {
        "title": "Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework",
        "authors": [
            "Xiaoxi Sun",
            "Jinpeng Li",
            "Yan Zhong",
            "Dongyan Zhao",
            "Rui Yan"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03075",
        "abstract": "The advent of large language models (LLMs) has facilitated the development of natural language text generation. It also poses unprecedented challenges, with content hallucination emerging as a significant concern. Existing solutions often involve expensive and complex interventions during the training process. Moreover, some approaches emphasize problem disassembly while neglecting the crucial validation process, leading to performance degradation or limited applications. To overcome these limitations, we propose a Markov Chain-based multi-agent debate verification framework to enhance hallucination detection accuracy in concise claims. Our method integrates the fact-checking process, including claim detection, evidence retrieval, and multi-agent verification. In the verification stage, we deploy multiple agents through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes. Experimental results across three generative tasks demonstrate that our approach achieves significant improvements over baselines.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.03075"
    },
    "02d61e99fd7fc97a53e4c73445171682": {
        "title": "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback",
        "authors": [
            "Timon Ziegenbein",
            "Gabriella Skitalinskaya",
            "Alireza Bayat Makou",
            "Henning Wachsmuth"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03363",
        "abstract": "Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies. Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.03363"
    },
    "dbdb0b9177ee31f171ff0223836e85a4": {
        "title": "Mixture-of-Agents Enhances Large Language Model Capabilities",
        "authors": [
            "Junlin Wang",
            "Jue Wang",
            "Ben Athiwaratkun",
            "Ce Zhang",
            "James Zou"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.04692",
        "abstract": "Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1% compared to 57.5% by GPT-4 Omni.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.04692"
    },
    "cd0e509bd2359089f92d7590f198aa63": {
        "title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild",
        "authors": [
            "Bill Yuchen Lin",
            "Yuntian Deng",
            "Khyathi Chandu",
            "Faeze Brahman",
            "Abhilasha Ravichander",
            "Valentina Pyatkin",
            "Nouha Dziri",
            "Ronan Le Bras",
            "Yejin Choi"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.04770",
        "abstract": "We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of ``slightly better/worse&#39;&#39; to ``tie&#39;&#39; if the winner response exceeds the loser one by more than $K$ characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard&#39;s 0.91 and AlpacaEval2.0&#39;s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.",
        "code": "https://github.com/allenai/wildbench",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.04770"
    },
    "d9f06df47f1047513338ea5d9a7a4b10": {
        "title": "SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals",
        "authors": [
            "Ruihan Yang",
            "Jiangjie Chen",
            "Yikai Zhang",
            "Siyu Yuan",
            "Aili Chen",
            "Kyle Richardson",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.04784",
        "abstract": "Language agents powered by large language models (LLMs) are increasingly valuable as decision-making tools in domains such as gaming and programming. However, these agents often face challenges in achieving high-level goals without detailed instructions and in adapting to environments where feedback is delayed. In this paper, we present SelfGoal, a novel automatic approach designed to enhance agents&#39; capabilities to achieve high-level goals with limited human prior and environmental feedback. The core concept of SelfGoal involves adaptively breaking down a high-level goal into a tree structure of more practical subgoals during the interaction with environments while identifying the most useful subgoals and progressively updating this structure. Experimental results demonstrate that SelfGoal significantly enhances the performance of language agents across various tasks, including competitive, cooperative, and deferred feedback environments. Project page: https://selfgoal-agent.github.io.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.04784"
    },
    "63bf5d41f59732724da5c7dd1eefabb2": {
        "title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions",
        "authors": [
            "Cheng Tan",
            "Dongxin Lyu",
            "Siyuan Li",
            "Zhangyang Gao",
            "Jingxuan Wei",
            "Siqi Ma",
            "Zicheng Liu",
            "Stan Z. Li"
        ],
        "date": "2024/06/09",
        "pdf": "http://arxiv.org/pdf/2406.05688",
        "abstract": "Large Language Models (LLMs) have demonstrated wide-ranging applications across various fields and have shown significant potential in the academic peer-review process. However, existing applications are primarily limited to static review generation based on submitted papers, which fail to capture the dynamic and iterative nature of real-world peer reviews. In this paper, we reformulate the peer-review process as a multi-turn, long-context dialogue, incorporating distinct roles for authors, reviewers, and decision makers. We construct a comprehensive dataset containing over 26,841 papers with 92,017 reviews collected from multiple sources, including the top-tier conference and prestigious journal. This dataset is meticulously designed to facilitate the applications of LLMs for multi-turn dialogues, effectively simulating the complete peer-review process. Furthermore, we propose a series of metrics to evaluate the performance of LLMs for each role under this reformulated peer-review setting, ensuring fair and comprehensive evaluations. We believe this work provides a promising perspective on enhancing the LLM-driven peer-review process by incorporating dynamic, role-based interactions. It aligns closely with the iterative and interactive nature of real-world academic peer review, offering a robust foundation for future research and development in this area. We open-source the dataset at https://github.com/chengtan9907/ReviewMT.",
        "code": "https://github.com/chengtan9907/reviewmt",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.05688"
    },
    "9c84d36ed7499d328b5a04b9ad50d85f": {
        "title": "Can Language Models Serve as Text-Based World Simulators?",
        "authors": [
            "Ruoyao Wang",
            "Graham Todd",
            "Ziang Xiao",
            "Xingdi Yuan",
            "Marc-Alexandre Côté",
            "Peter Clark",
            "Peter Jansen"
        ],
        "date": "2024/06/10",
        "pdf": "http://arxiv.org/pdf/2406.06485",
        "abstract": "Virtual environments play a key role in benchmarking advances in complex planning and decision-making tasks but are expensive and complicated to build by hand. Can current language models themselves serve as world simulators, correctly predicting how actions change different world states, thus bypassing the need for extensive manual coding? Our goal is to answer this question in the context of text-based simulators. Our approach is to build and use a new benchmark, called ByteSized32-State-Prediction, containing a dataset of text game state transitions and accompanying game tasks. We use this to directly quantify, for the first time, how well LLMs can serve as text-based world simulators. We test GPT-4 on this dataset and find that, despite its impressive performance, it is still an unreliable world simulator without further innovations. This work thus contributes both new insights into current LLM&#39;s capabilities and weaknesses, as well as a novel benchmark to track future progress as new models appear.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.06485"
    },
    "ad75548bfd9b38bc7c85370ad899e213": {
        "title": "GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents",
        "authors": [
            "Anthony Costarelli",
            "Mat Allen",
            "Roman Hauksson",
            "Grace Sodunke",
            "Suhas Hariharan",
            "Carlson Cheng",
            "Wenjie Li",
            "Arjun Yadav"
        ],
        "date": "2024/06/07",
        "pdf": "http://arxiv.org/pdf/2406.06613",
        "abstract": "Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents&#39; performance across various types of reasoning found in games. To address this gap, we introduce GameBench, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models&#39; pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worse GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.06613"
    },
    "e180bb7bf6f20677a576c22b725e2f3f": {
        "title": "Agent-SiMT: Agent-assisted Simultaneous Machine Translation with Large Language Models",
        "authors": [
            "Shoutao Guo",
            "Shaolei Zhang",
            "Zhengrui Ma",
            "Min Zhang",
            "Yang Feng"
        ],
        "date": "2024/06/11",
        "pdf": "http://arxiv.org/pdf/2406.06910",
        "abstract": "Simultaneous Machine Translation (SiMT) generates target translations while reading the source sentence. It relies on a policy to determine the optimal timing for reading sentences and generating translations. Existing SiMT methods generally adopt the traditional Transformer architecture, which concurrently determines the policy and generates translations. While they excel at determining policies, their translation performance is suboptimal. Conversely, Large Language Models (LLMs), trained on extensive corpora, possess superior generation capabilities, but it is difficult for them to acquire translation policy through the training methods of SiMT. Therefore, we introduce Agent-SiMT, a framework combining the strengths of LLMs and traditional SiMT methods. Agent-SiMT contains the policy-decision agent and the translation agent. The policy-decision agent is managed by a SiMT model, which determines the translation policy using partial source sentence and translation. The translation agent, leveraging an LLM, generates translation based on the partial source sentence. The two agents collaborate to accomplish SiMT. Experiments demonstrate that Agent-SiMT attains state-of-the-art performance.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.06910"
    },
    "db18353b2d2aa343b33a016a72e601ce": {
        "title": "CoEvol: Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation",
        "authors": [
            "Renhao Li",
            "Minghuan Tan",
            "Derek F. Wong",
            "Min Yang"
        ],
        "date": "2024/06/11",
        "pdf": "http://arxiv.org/pdf/2406.07054",
        "abstract": "In recent years, instruction fine-tuning (IFT) on large language models (LLMs) has garnered considerable attention to enhance model performance on unseen tasks. Attempts have been made on automatic construction and effective selection for IFT data. However, we posit that previous methods have not fully harnessed the potential of LLMs for enhancing data quality. The responses within IFT data could be further enhanced by leveraging the capabilities of LLMs themselves. In this paper, we propose CoEvol, an LLM-based multi-agent cooperation framework for the improvement of responses to instructions. To effectively refine the responses, we develop an iterative framework following a debate-advise-edit-judge paradigm. A two-stage multi-agent debate strategy is further devised to ensure the diversity and reliability of editing suggestions within the framework. Empirically, models equipped with CoEvol outperform competitive baselines evaluated by MT-Bench and AlpacaEval, demonstrating its effectiveness in enhancing instruction-following capabilities for LLMs.",
        "code": "https://github.com/lirenhao1997/coevol",
        "category": [
            "Multi-Agent System",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2406.07054"
    },
    "e70cbbb7be0f0f89684f14244681fe64": {
        "title": "Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems with Large Language Models",
        "authors": [
            "Joshua Strong",
            "Qianhui Men",
            "Alison Noble"
        ],
        "date": "2024/06/11",
        "pdf": "http://arxiv.org/pdf/2406.07212",
        "abstract": "Large language models (LLMs) present a valuable technology for various applications in healthcare, but their tendency to hallucinate introduces unacceptable uncertainty in critical decision-making situations. Human-AI collaboration (HAIC) can mitigate this uncertainty by combining human and AI strengths for better outcomes. This paper presents a novel guided deferral system that provides intelligent guidance when AI defers cases to human decision-makers. We leverage LLMs&#39; verbalisation capabilities and internal states to create this system, demonstrating that fine-tuning smaller LLMs with data from larger models enhances performance while maintaining computational efficiency. A pilot study showcases the effectiveness of our deferral system.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2406.07212"
    },
    "8c868b243c898b5c69dfffa74887c7df": {
        "title": "StreamBench: Towards Benchmarking Continuous Improvement of Language Agents",
        "authors": [
            "Cheng-Kuang Wu",
            "Zhi Rui Tam",
            "Chieh-Yen Lin",
            "Yun-Nung Chen",
            "Hung-yi Lee"
        ],
        "date": "2024/06/13",
        "pdf": "http://arxiv.org/pdf/2406.08747",
        "abstract": "Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.08747"
    },
    "357e3fd79056c360acc4d8ebf786ac35": {
        "title": "Multi-Agent Software Development through Cross-Team Collaboration",
        "authors": [
            "Zhuoyun Du",
            "Chen Qian",
            "Wei Liu",
            "Zihao Xie",
            "Yifei Wang",
            "Yufan Dang",
            "Weize Chen",
            "Cheng Yang"
        ],
        "date": "2024/06/13",
        "pdf": "http://arxiv.org/pdf/2406.08979",
        "abstract": "The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have catalyzed profound transformations, particularly through multi-agent collaboration for software development. LLM agents can collaborate in teams like humans, and follow the waterfall model to sequentially work on requirements analysis, development, review, testing, and other phases to perform autonomous software generation. However, for an agent team, each phase in a single development process yields only one possible outcome. This results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space. Consequently, this may lead to obtaining suboptimal results. To address this challenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions and communicate with their insights in a cross-team collaboration environment for superior content generation. Experimental results in software development reveal a notable increase in quality compared to state-of-the-art baselines, underscoring the efficacy of our framework. The significant improvements in story generation demonstrate the promising generalization ability of our framework across various domains. We anticipate that our work will guide LLM agents towards a cross-team paradigm and contribute to their significant growth in but not limited to software development. The code and data will be available at https://github.com/OpenBMB/ChatDev.",
        "code": "https://github.com/openbmb/chatdev",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.08979"
    },
    "5e681fd2374814e343b6a031a00e0904": {
        "title": "RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents",
        "authors": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11132",
        "abstract": "In this past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and people are starting to explore the usage of LLMs in more general and close to application domains like code generation, travel planning, and robot controls. Connecting these LLMs with great capacity and external tools, people are building the so-called LLM agents, which are supposed to help people do all kinds of work in everyday life. In all these domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering has become an important question for many researchers and users of LLMs. In this paper, we propose a novel method, \\textsc{RePrompt}, which does &#34;gradient descent&#34; to optimize the step-by-step instructions in the prompt of the LLM agents based on the chat history obtained from interactions with LLM agents. By optimizing the prompt, the LLM will learn how to plan in specific domains. We have used experiments in PDDL generation and travel planning to show that our method could generally improve the performance for different reasoning tasks when using the updated prompt as the initial prompt.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2406.11132"
    },
    "5af58f701b7864f75b5d42de686ee234": {
        "title": "Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector",
        "authors": [
            "Xiaoxue Cheng",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Hongzhi Zhang",
            "Fuzheng Zhang",
            "Di Zhang",
            "Kun Gai",
            "Ji-Rong Wen"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11277",
        "abstract": "Hallucination detection is a challenging task for large language models (LLMs), and existing studies heavily rely on powerful closed-source LLMs such as GPT-4. In this paper, we propose an autonomous LLM-based agent framework, called HaluAgent, which enables relatively smaller LLMs (e.g. Baichuan2-Chat 7B) to actively select suitable tools for detecting multiple hallucination types such as text, code, and mathematical expression. In HaluAgent, we integrate the LLM, multi-functional toolbox, and design a fine-grained three-stage detection framework along with memory mechanism. To facilitate the effectiveness of HaluAgent, we leverage existing Chinese and English datasets to synthesize detection trajectories for fine-tuning, which endows HaluAgent with the capability for bilingual hallucination detection. Extensive experiments demonstrate that only using 2K samples for tuning LLMs, HaluAgent can perform hallucination detection on various types of tasks and datasets, achieving performance comparable to or even higher than GPT-4 without tool enhancements on both in-domain and out-of-domain datasets. We release our dataset and code at https://github.com/RUCAIBox/HaluAgent.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.11277"
    },
    "223160a127cf70c4accb671a891d7640": {
        "title": "Input Conditioned Graph Generation for Language Agents",
        "authors": [
            "Lukas Vierling",
            "Jie Fu",
            "Kai Chen"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11555",
        "abstract": "Recent progress in Large Language Models (LLMs) and language agents has demonstrated significant promise for various future applications across multiple disciplines. While traditional approaches to language agents often rely on fixed, handcrafted designs, our research aims to develop both learnable and dynamic agents. Our method uses an existing framework that abstracts language agents as graphs. Within this graph framework, we aim to learn a model that can generate edges for every given input to the language agent. This allows us to generate edges that represent the flow of communication within the graph based on the given input, thereby adjusting the internal communication of a language agent. We learn to generate these edges using a pretrained LLM that is fine-tuned with reinforcement learning. This LLM can be fine-tuned on several datasets simultaneously, and we hypothesize that the model learns to adapt to these different domains during training, achieving good overall performance when encountering data from different domains during deployment. We demonstrate that our approach surpasses the previous static approach by nearly 6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when trained with a sparsity-inducing loss. It also performs superior in additional experiments conducted with the MMLU and Mini Crossword Puzzles datasets. The code is available at https://github.com/lukasVierling/DynamicGPTSwarm.",
        "code": "https://github.com/lukasvierling/dynamicgptswarm",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.11555"
    },
    "fad605b82f8013fe20fdbd2e307bed36": {
        "title": "HoLLMwood: Unleashing the Creativity of Large Language Models in Screenwriting via Role Playing",
        "authors": [
            "Jing Chen",
            "Xinyu Zhu",
            "Cheng Yang",
            "Chufan Shi",
            "Yadong Xi",
            "Yuxiang Zhang",
            "Junjie Wang",
            "Jiashu Pu",
            "Rongsheng Zhang",
            "Yujiu Yang",
            "Tian Feng"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11683",
        "abstract": "Generative AI has demonstrated unprecedented creativity in the field of computer vision, yet such phenomena have not been observed in natural language processing. In particular, large language models (LLMs) can hardly produce written works at the level of human experts due to the extremely high complexity of literature writing. In this paper, we present HoLLMwood, an automated framework for unleashing the creativity of LLMs and exploring their potential in screenwriting, which is a highly demanding task. Mimicking the human creative process, we assign LLMs to different roles involved in the real-world scenario. In addition to the common practice of treating LLMs as ${Writer}$, we also apply LLMs as ${Editor}$, who is responsible for providing feedback and revision advice to ${Writer}$. Besides, to enrich the characters and deepen the plots, we introduce a role-playing mechanism and adopt LLMs as ${Actors}$ that can communicate and interact with each other. Evaluations on automatically generated screenplays show that HoLLMwood substantially outperforms strong baselines in terms of coherence, relevance, interestingness and overall quality.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.11683"
    },
    "4abb5f3980169817507e510eaf75db0c": {
        "title": "Improving Multi-Agent Debate with Sparse Communication Topology",
        "authors": [
            "Yunxuan Li",
            "Yibing Du",
            "Jiageng Zhang",
            "Le Hou",
            "Peter Grabowski",
            "Yeqing Li",
            "Eugene Ie"
        ],
        "date": "2024/06/17",
        "pdf": "http://arxiv.org/pdf/2406.11776",
        "abstract": "Multi-agent debate has proven effective in improving large language models quality for reasoning and factuality tasks. While various role-playing strategies in multi-agent debates have been explored, in terms of the communication among agents, existing approaches adopt a brute force algorithm -- each agent can communicate with all other agents. In this paper, we systematically investigate the effect of communication connectivity in multi-agent systems. Our experiments on GPT and Mistral models reveal that multi-agent debates leveraging sparse communication topology can achieve comparable or superior performance while significantly reducing computational costs. Furthermore, we extend the multi-agent debate framework to multimodal reasoning and alignment labeling tasks, showcasing its broad applicability and effectiveness. Our findings underscore the importance of communication connectivity on enhancing the efficiency and effectiveness of the &#34;society of minds&#34; approach.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.11776"
    },
    "b3f197dd580c7d2034936bda2f25387f": {
        "title": "Multi-Modal and Multi-Agent Systems Meet Rationality: A Survey",
        "authors": [
            "Bowen Jiang",
            "Yangxinyu Xie",
            "Xiaomeng Wang",
            "Weijie J. Su",
            "Camillo J. Taylor",
            "Tanwi Mallick"
        ],
        "date": "2024/06/01",
        "pdf": "http://arxiv.org/pdf/2406.00252",
        "abstract": "Rationality is the quality of being guided by reason, characterized by logical thinking and decision-making that align with evidence and logical rules. This quality is essential for effective problem-solving, as it ensures that solutions are well-founded and systematically derived. Despite the advancements of large language models (LLMs) in generating human-like text with remarkable accuracy, they present biases inherited from the training data, inconsistency across different contexts, and difficulty understanding complex scenarios involving multiple layers of context. Therefore, recent research attempts to leverage the strength of multiple agents working collaboratively with various types of data and tools for enhanced consistency and reliability. To that end, this paper aims to understand whether multi-modal and multi-agent systems are advancing toward rationality by surveying the state-of-the-art works, identifying advancements over single-agent and single-modal systems in terms of rationality, and discussing open problems and future directions. We maintain an open repository at https://github.com/bowen-upenn/MMMA_Rationality.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2406.00252"
    },
    "2f0f904566cf4ea8ccfc9852638da7e0": {
        "title": "The Good, the Bad, and the Hulk-like GPT: Analyzing Emotional Decisions of Large Language Models in Cooperation and Bargaining Games",
        "authors": [
            "Mikhail Mozikov",
            "Nikita Severin",
            "Valeria Bodishtianu",
            "Maria Glushanina",
            "Mikhail Baklashkin",
            "Andrey V. Savchenko",
            "Ilya Makarov"
        ],
        "date": "2024/06/05",
        "pdf": "http://arxiv.org/pdf/2406.03299",
        "abstract": "Behavior study experiments are an important part of society modeling and understanding human interactions. In practice, many behavioral experiments encounter challenges related to internal and external validity, reproducibility, and social bias due to the complexity of social interactions and cooperation in human user studies. Recent advances in Large Language Models (LLMs) have provided researchers with a new promising tool for the simulation of human behavior. However, existing LLM-based simulations operate under the unproven hypothesis that LLM agents behave similarly to humans as well as ignore a crucial factor in human decision-making: emotions. In this paper, we introduce a novel methodology and the framework to study both, the decision-making of LLMs and their alignment with human behavior under emotional states. Experiments with GPT-3.5 and GPT-4 on four games from two different classes of behavioral game theory showed that emotions profoundly impact the performance of LLMs, leading to the development of more optimal strategies. While there is a strong alignment between the behavioral responses of GPT-3.5 and human participants, particularly evident in bargaining games, GPT-4 exhibits consistent behavior, ignoring induced emotions for rationality decisions. Surprisingly, emotional prompting, particularly with `anger&#39; emotion, can disrupt the &#34;superhuman&#34; alignment of GPT-4, resembling human emotional responses.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2406.03299"
    },
    "880773299ddd850765aa2c59274c2e3f": {
        "title": "Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering",
        "authors": [
            "Yanming Liu",
            "Xinyue Peng",
            "Yuwei Zhang",
            "Jiannan Cao",
            "Xuhong Zhang",
            "Sheng Cheng",
            "Xun Wang",
            "Jianwei Yin",
            "Tianyu Du"
        ],
        "date": "2024/06/06",
        "pdf": "http://arxiv.org/pdf/2406.03807",
        "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method.",
        "code": "https://github.com/OceannTwT/Tool-Planner",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2406.03807"
    },
    "f5919d5d232d083271739eeed87bcf02": {
        "title": "AgentGym: Evolving Large Language Model-based Agents across Diverse Environments",
        "authors": [
            "Zhiheng Xi",
            "Yiwen Ding",
            "Wenxiang Chen",
            "Boyang Hong",
            "Honglin Guo",
            "Junzhe Wang",
            "Dingwen Yang",
            "Chenyang Liao",
            "Xin Guo",
            "Wei He",
            "Songyang Gao",
            "Lu Chen",
            "Rui Zheng",
            "Yicheng Zou",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "date": "2024/06/06",
        "pdf": "http://arxiv.org/pdf/2406.04151",
        "abstract": "Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.",
        "code": "https://github.com/woooodyy/agentgym",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2406.04151"
    },
    "0b295dba3c25b4491625f535f115ef76": {
        "title": "A Survey on LLM-Based Agents: Common Workflows and Reusable LLM-Profiled Components",
        "authors": [
            "Xinzhe Li"
        ],
        "date": "2024/06/09",
        "pdf": "http://arxiv.org/pdf/2406.05804",
        "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the development of sophisticated frameworks for developing LLM-based agents. However, the complexity of these frameworks r poses a hurdle for nuanced differentiation at a granular level, a critical aspect for enabling efficient implementations across different frameworks and fostering future research. Hence, the primary purpose of this survey is to facilitate a cohesive understanding of diverse recently proposed frameworks by identifying common workflows and reusable LLM-Profiled Components (LMPCs).",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2406.05804"
    },
    "be0c09dd961ea2086e7d2141de2db776": {
        "title": "OmAgent: A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer",
        "authors": [
            "Lu Zhang",
            "Tiancheng Zhao",
            "Heting Ying",
            "Yibo Ma",
            "Kyusong Lee"
        ],
        "date": "2024/06/24",
        "pdf": "http://arxiv.org/pdf/2406.16620",
        "abstract": "Recent advancements in Large Language Models (LLMs) have expanded their capabilities to multimodal contexts, including comprehensive video understanding. However, processing extensive videos such as 24-hour CCTV footage or full-length films presents significant challenges due to the vast data and processing demands. Traditional methods, like extracting key frames or converting frames to text, often result in substantial information loss. To address these shortcomings, we develop OmAgent, efficiently stores and retrieves relevant video frames for specific queries, preserving the detailed content of videos. Additionally, it features an Divide-and-Conquer Loop capable of autonomous reasoning, dynamically invoking APIs and tools to enhance query processing and accuracy. This approach ensures robust video understanding, significantly reducing information loss. Experimental results affirm OmAgent&#39;s efficacy in handling various types of videos and complex tasks. Moreover, we have endowed it with greater autonomy and a robust tool-calling system, enabling it to accomplish even more intricate tasks.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2406.16620"
    },
    "4f90372ee36cb883295d4b7c9812be96": {
        "title": "Towards a copilot in BIM authoring tool using a large language model-based agent for intelligent human-machine interaction",
        "authors": [
            "Changyu Du",
            "Stavros Nousias",
            "André Borrmann"
        ],
        "date": "2024/06/02",
        "pdf": "http://arxiv.org/pdf/2406.16903",
        "abstract": "Facing increasingly complex BIM authoring software and the accompanying expensive learning costs, designers often seek to interact with the software in a more intelligent and lightweight manner. They aim to automate modeling workflows, avoiding obstacles and difficulties caused by software usage, thereby focusing on the design process itself. To address this issue, we proposed an LLM-based autonomous agent framework that can function as a copilot in the BIM authoring tool, answering software usage questions, understanding the user&#39;s design intentions from natural language, and autonomously executing modeling tasks by invoking the appropriate tools. In a case study based on the BIM authoring software Vectorworks, we implemented a software prototype to integrate the proposed framework seamlessly into the BIM authoring scenario. We evaluated the planning and reasoning capabilities of different LLMs within this framework when faced with complex instructions. Our work demonstrates the significant potential of LLM-based agents in design automation and intelligent interaction.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2406.16903"
    },
    "a6eae169a20cd0773a8b09bdf50dc8b8": {
        "title": "Mental Modeling of Reinforcement Learning Agents by Language Models",
        "authors": [
            "Wenhao Lu",
            "Xufeng Zhao",
            "Josua Spisak",
            "Jae Hee Lee",
            "Stefan Wermter"
        ],
        "date": "2024/06/26",
        "pdf": "http://arxiv.org/pdf/2406.18505",
        "abstract": "Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it remains underexplored how the world knowledge these pretrained models have memorized can be utilized to comprehend an agent&#39;s behaviour in the physical world. This study empirically examines, for the first time, how well large language models (LLMs) can build a mental model of agents, termed agent mental modelling, by reasoning about an agent&#39;s behaviour and its effect on states from agent interaction history. This research may unveil the potential of leveraging LLMs for elucidating RL agent behaviour, addressing a key challenge in eXplainable reinforcement learning (XRL). To this end, we propose specific evaluation metrics and test them on selected RL task datasets of varying complexity, reporting findings on agent mental model establishment. Our results disclose that LLMs are not yet capable of fully mental modelling agents through inference alone without further innovations. This work thus provides new insights into the capabilities and limitations of modern LLMs.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.18505"
    },
    "3d25b7e8de61314b3f8d0abd8bd17354": {
        "title": "Simulating The U.S. Senate: An LLM-Driven Agent Approach to Modeling Legislative Behavior and Bipartisanship",
        "authors": [
            "Zachary R. Baker",
            "Zarif L. Azher"
        ],
        "date": "2024/06/26",
        "pdf": "http://arxiv.org/pdf/2406.18702",
        "abstract": "This study introduces a novel approach to simulating legislative processes using LLM-driven virtual agents, focusing on the U.S. Senate Intelligence Committee. We developed agents representing individual senators and placed them in simulated committee discussions. The agents demonstrated the ability to engage in realistic debate, provide thoughtful reflections, and find bipartisan solutions under certain conditions. Notably, the simulation also showed promise in modeling shifts towards bipartisanship in response to external perturbations. Our results indicate that this LLM-driven approach could become a valuable tool for understanding and potentially improving legislative processes, supporting a broader pattern of findings highlighting how LLM-based agents can usefully model real-world phenomena. Future works will focus on enhancing agent complexity, expanding the simulation scope, and exploring applications in policy testing and negotiation.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.18702"
    },
    "0350140c12a50fafc423f99240ef5982": {
        "title": "Designing and Evaluating Multi-Chatbot Interface for Human-AI Communication: Preliminary Findings from a Persuasion Task",
        "authors": [
            "Sion Yoon",
            "Tae Eun Kim",
            "Yoo Jung Oh"
        ],
        "date": "2024/06/28",
        "pdf": "http://arxiv.org/pdf/2406.19648",
        "abstract": "The dynamics of human-AI communication have been reshaped by language models such as ChatGPT. However, extant research has primarily focused on dyadic communication, leaving much to be explored regarding the dynamics of human-AI communication in group settings. The availability of multiple language model chatbots presents a unique opportunity for scholars to better understand the interaction between humans and multiple chatbots. This study examines the impact of multi-chatbot communication in a specific persuasion setting: promoting charitable donations. We developed an online environment that enables multi-chatbot communication and conducted a pilot experiment utilizing two GPT-based chatbots, Save the Children and UNICEF chatbots, to promote charitable donations. In this study, we present our development process of the multi-chatbot interface and present preliminary findings from a pilot experiment. Analysis of qualitative and quantitative feedback are presented, and limitations are addressed.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2406.19648"
    },
    "26ee161c0e282de1d2129fbca847c08a": {
        "title": "StackRAG Agent: Improving Developer Answers with Retrieval-Augmented Generation",
        "authors": [
            "Davit Abrahamyan",
            "Fatemeh H. Fard"
        ],
        "date": "2024/06/19",
        "pdf": "http://arxiv.org/pdf/2406.13840",
        "abstract": "Developers spend much time finding information that is relevant to their questions. Stack Overflow has been the leading resource, and with the advent of Large Language Models (LLMs), generative models such as ChatGPT are used frequently. However, there is a catch in using each one separately. Searching for answers is time-consuming and tedious, as shown by the many tools developed by researchers to address this issue. On the other, using LLMs is not reliable, as they might produce irrelevant or unreliable answers (i.e., hallucination). In this work, we present StackRAG, a retrieval-augmented Multiagent generation tool based on LLMs that combines the two worlds: aggregating the knowledge from SO to enhance the reliability of the generated answers. Initial evaluations show that the generated answers are correct, accurate, relevant, and useful.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2406.13840"
    },
    "7a938c81e0fdf170539ea6aaae2db7e8": {
        "title": "Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory",
        "authors": [
            "Gordon Dai",
            "Weijia Zhang",
            "Jinhan Li",
            "Siqi Yang",
            "Chidera Onochie lbe",
            "Srihas Rao",
            "Arthur Caetano",
            "Misha Sra"
        ],
        "date": "2024/06/20",
        "pdf": "http://arxiv.org/pdf/2406.14373",
        "abstract": "The emergence of Large Language Models (LLMs) and advancements in Artificial Intelligence (AI) offer an opportunity for computational social science research at scale. Building upon prior explorations of LLM agent design, our work introduces a simulated agent society where complex social relationships dynamically form and evolve over time. Agents are imbued with psychological drives and placed in a sandbox survival environment. We conduct an evaluation of the agent society through the lens of Thomas Hobbes&#39;s seminal Social Contract Theory (SCT). We analyze whether, as the theory postulates, agents seek to escape a brutish &#34;state of nature&#34; by surrendering rights to an absolute sovereign in exchange for order and security. Our experiments unveil an alignment: Initially, agents engage in unrestrained conflict, mirroring Hobbes&#39;s depiction of the state of nature. However, as the simulation progresses, social contracts emerge, leading to the authorization of an absolute sovereign and the establishment of a peaceful commonwealth founded on mutual cooperation. This congruence between our LLM agent society&#39;s evolutionary trajectory and Hobbes&#39;s theoretical account indicates LLMs&#39; capability to model intricate social dynamics and potentially replicate forces that shape human societies. By enabling such insights into group behavior and emergent societal phenomena, LLM-driven multi-agent simulations, while unable to simulate all the nuances of human behavior, may hold potential for advancing our understanding of social structures, group dynamics, and complex human systems.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.14373"
    },
    "83b4a2a041739b48014ccadeb9369b2b": {
        "title": "Autonomous Agents for Collaborative Task under Information Asymmetry",
        "authors": [
            "Wei Liu",
            "Chenxi Wang",
            "Yifei Wang",
            "Zihao Xie",
            "Rennai Qiu",
            "Yufan Dang",
            "Zhuoyun Du",
            "Weize Chen",
            "Cheng Yang",
            "Chen Qian"
        ],
        "date": "2024/06/21",
        "pdf": "http://arxiv.org/pdf/2406.14928",
        "abstract": "Large Language Model Multi-Agent Systems (LLM-MAS) have achieved great progress in solving complex tasks. It performs communication among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents&#39; communication is leveraged to enhance human cooperation, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents&#39; communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents&#39; task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2406.14928"
    },
    "faa666c9fce4d0852968cb6a47053dd8": {
        "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step Self-awareness",
        "authors": [
            "Xiao Peng",
            "Xufan Geng"
        ],
        "date": "2024/10/01",
        "pdf": "http://arxiv.org/pdf/2410.00359",
        "abstract": "The applications of large language models (LLMs) have been widely spread across all domains. However, the basic abilities such as the controllability of LLMs are still limited. To address this, we propose &#34;Self-controller&#34;, a novel agentic framework bringing self-awareness into LLMs&#39; reasoning logic. The core idea of this work is to maintain states based on the LLM&#39;s response, letting the LLM become self-aware of current status and think step by step in a multi-round chain-of-thought paradigm. Our experiment on the state of textual length has shown the controllability and effectiveness of the Self-controller. We further implement a binary search algorithm to accelerate the generation process based on the linearity and monotonicity of the textual length state. Another advantage of the Self-controller comes with DeepSeek&#39;s Context Caching technology, which significantly saves computational token consumption when a cluster of conversations shares the same prefix of context. Theoretically, we prove that in this scenario the extra time complexity is $O(c \\log n)$. Results of the back-of-the-envelope estimation suggest that the token consumption of our method is no more than twice as much as that of the trivial single-round generation. Furthermore, our ablation study on word constraints demonstrates the Self-controller&#39;s consistent controllability across all foundation models.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2410.00359"
    },
    "75fdda960504897c17b5d523a4564351": {
        "title": "Conversational Exploratory Search of Scholarly Publications Using Knowledge Graphs",
        "authors": [
            "Phillip Schneider",
            "Florian Matthes"
        ],
        "date": "2024/10/01",
        "pdf": "http://arxiv.org/pdf/2410.00427",
        "abstract": "Traditional search methods primarily depend on string matches, while semantic search targets concept-based matches by recognizing underlying intents and contextual meanings of search terms. Semantic search is particularly beneficial for discovering scholarly publications where differences in vocabulary between users&#39; search terms and document content are common, often yielding irrelevant search results. Many scholarly search engines have adopted knowledge graphs to represent semantic relations between authors, publications, and research concepts. However, users may face challenges when navigating these graphical search interfaces due to the complexity and volume of data, which impedes their ability to discover publications effectively. To address this problem, we developed a conversational search system for exploring scholarly publications using a knowledge graph. We outline the methodical approach for designing and implementing the proposed system, detailing its architecture and functional components. To assess the system&#39;s effectiveness, we employed various performance metrics and conducted a human evaluation with 40 participants, demonstrating how the conversational interface compares against a graphical interface with traditional text search. The findings from our evaluation provide practical insights for advancing the design of conversational search systems.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.00427"
    },
    "fca0a841125221da1d3d4524f85584f0": {
        "title": "Agent-Driven Large Language Models for Mandarin Lyric Generation",
        "authors": [
            "Hong-Hsiang Liu",
            "Yi-Wen Liu"
        ],
        "date": "2024/10/02",
        "pdf": "http://arxiv.org/pdf/2410.01450",
        "abstract": "Generative Large Language Models have shown impressive in-context learning abilities, performing well across various tasks with just a prompt. Previous melody-to-lyric research has been limited by scarce high-quality aligned data and unclear standard for creativeness. Most efforts focused on general themes or emotions, which are less valuable given current language model capabilities. In tonal contour languages like Mandarin, pitch contours are influenced by both melody and tone, leading to variations in lyric-melody fit. Our study, validated by the Mpop600 dataset, confirms that lyricists and melody writers consider this fit during their composition process. In this research, we developed a multi-agent system that decomposes the melody-to-lyric task into sub-tasks, with each agent controlling rhyme, syllable count, lyric-melody alignment, and consistency. Listening tests were conducted via a diffusion-based singing voice synthesizer to evaluate the quality of lyrics generated by different agent groups.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.01450"
    },
    "e274a5113a4fe6fdbb728c5d29e994ce": {
        "title": "ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning",
        "authors": [
            "Xiao Yu",
            "Baolin Peng",
            "Vineeth Vajipey",
            "Hao Cheng",
            "Michel Galley",
            "Jianfeng Gao",
            "Zhou Yu"
        ],
        "date": "2024/10/02",
        "pdf": "http://arxiv.org/pdf/2410.02052",
        "abstract": "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon tasks. To address these limitations, we present ExACT, an approach to combine test-time search and self-learning to build o1-like models for agentic applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test time algorithm designed to enhance AI agents&#39; ability to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate for reliable state evaluation. Next, we introduce Exploratory Learning, a novel learning strategy to teach agents to search at inference time without relying on any external search algorithms. On the challenging VisualWebArena benchmark, our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge and experience gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1) demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success, and 2) matches 87% of R-MCTS&#39;s performance while using significantly less compute. Notably, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs&#39; capabilities for agentic applications via test-time search and self-learning.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2410.02052"
    },
    "66e8cdc503b22e684fb4bb2538ab6661": {
        "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions",
        "authors": [
            "Angana Borah",
            "Rada Mihalcea"
        ],
        "date": "2024/10/03",
        "pdf": "http://arxiv.org/pdf/2410.02584",
        "abstract": "As Large Language Models (LLMs) continue to evolve, they are increasingly being employed in numerous studies to simulate societies and execute diverse social tasks. However, LLMs are susceptible to societal biases due to their exposure to human-generated data. Given that LLMs are being used to gain insights into various societal aspects, it is essential to mitigate these biases. To that end, our study investigates the presence of implicit gender biases in multi-agent LLM interactions and proposes two strategies to mitigate these biases. We begin by creating a dataset of scenarios where implicit gender biases might arise, and subsequently develop a metric to assess the presence of biases. Our empirical analysis reveals that LLMs generate outputs characterized by strong implicit bias associations (&gt;= 50\\% of the time). Furthermore, these biases tend to escalate following multi-agent interactions. To mitigate them, we propose two strategies: self-reflection with in-context examples (ICE); and supervised fine-tuning. Our research demonstrates that both methods effectively mitigate implicit biases, with the ensemble of fine-tuning and self-reflection proving to be the most successful.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.02584"
    },
    "bda94606c7e1f146cf7b79db89d2642d": {
        "title": "Agents&#39; Room: Narrative Generation through Multi-step Collaboration",
        "authors": [
            "Fantine Huot",
            "Reinald Kim Amplayo",
            "Jennimaria Palomaki",
            "Alice Shoshana Jakobovits",
            "Elizabeth Clark",
            "Mirella Lapata"
        ],
        "date": "2024/10/03",
        "pdf": "http://arxiv.org/pdf/2410.02603",
        "abstract": "Writing compelling fiction is a multifaceted process combining elements such as crafting a plot, developing interesting characters, and using evocative language. While large language models (LLMs) show promise for story writing, they currently rely heavily on intricate prompting, which limits their use. We propose Agents&#39; Room, a generation framework inspired by narrative theory, that decomposes narrative writing into subtasks tackled by specialized agents. To illustrate our method, we introduce Tell Me A Story, a high-quality dataset of complex writing prompts and human-written stories, and a novel evaluation framework designed specifically for assessing long narratives. We show that Agents&#39; Room generates stories that are preferred by expert evaluators over those produced by baseline systems by leveraging collaboration and specialization to decompose the complex story writing task into tractable components. We provide extensive analysis with automated and human-based metrics of the generated output.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.02603"
    },
    "609d619716c0edc1fe11f0b28e10d0c2": {
        "title": "NNetNav: Unsupervised Learning of Browser Agents Through Environment Interaction in the Wild",
        "authors": [
            "Shikhar Murty",
            "Hao Zhu",
            "Dzmitry Bahdanau",
            "Christopher D. Manning"
        ],
        "date": "2024/10/03",
        "pdf": "http://arxiv.org/pdf/2410.02907",
        "abstract": "We introduce NNetNav, a method for unsupervised interaction with websites that generates synthetic demonstrations for training browser agents. Given any website, NNetNav produces these demonstrations by retroactively labeling action sequences from an exploration policy. Most work on training browser agents has relied on expensive human supervision, and the limited prior work on such interaction-based techniques has failed to provide effective search through the exponentially large space of exploration. In contrast, NNetNav exploits the hierarchical structure of language instructions to make this search more tractable: Complex instructions are typically decomposable into simpler sub-tasks, allowing NNetNav to automatically prune interaction episodes when an intermediate trajectory cannot be annotated with a meaningful sub-task. \\texttt{LLama-3.1-8b} finetuned on 10k NNetNav self-generated demonstrations obtains over 16\\% success rate on WebArena, and 35\\% on WebVoyager, an improvement of 15pts and 31pts respectively over zero-shot \\texttt{LLama-3.1-8b}, outperforming zero-shot GPT-4 and reaching the state-of-the-art among unsupervised methods, for both benchmarks.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.02907"
    },
    "9a7d8a75c9b99f8286ed43f6527fbaca": {
        "title": "Large Language Models can Achieve Social Balance",
        "authors": [
            "Pedro Cisneros-Velarde"
        ],
        "date": "2024/10/05",
        "pdf": "http://arxiv.org/pdf/2410.04054",
        "abstract": "Social balance is a concept in sociology which states that if every three individuals in a population achieve certain structures of positive or negative interactions, then the whole population ends up in one faction of positive interactions or divided between two or more antagonistic factions. In this paper, we consider a group of interacting large language models (LLMs) and study how, after continuous interactions, they can achieve social balance. Across three different LLM models, we found that social balance depends on (i) whether interactions are updated based on &#34;relationships&#34;, &#34;appraisals&#34;, or &#34;opinions&#34;; (ii) whether agents update their interactions based on homophily or influence from their peers; and (iii) the number of simultaneous interactions the LLMs consider. When social balance is achieved, its particular structure of positive or negative interactions depends on these three conditions and are different across LLM models and sizes. The stability of interactions and the justification for their update also vary across models. Thus, social balance is driven by the pre-training and alignment particular to each LLM model.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.04054"
    },
    "fae0017f3ad125bc3cb627f8e6f76d53": {
        "title": "MindScope: Exploring cognitive biases in large language models through Multi-Agent Systems",
        "authors": [
            "Zhentao Xie",
            "Jiabao Zhao",
            "Yilei Wang",
            "Jinxin Shi",
            "Yanhong Bai",
            "Xingjiao Wu",
            "Liang He"
        ],
        "date": "2024/10/06",
        "pdf": "http://arxiv.org/pdf/2410.04452",
        "abstract": "Detecting cognitive biases in large language models (LLMs) is a fascinating task that aims to probe the existing cognitive biases within these models. Current methods for detecting cognitive biases in language models generally suffer from incomplete detection capabilities and a restricted range of detectable bias types. To address this issue, we introduced the &#39;MindScope&#39; dataset, which distinctively integrates static and dynamic elements. The static component comprises 5,170 open-ended questions spanning 72 cognitive bias categories. The dynamic component leverages a rule-based, multi-agent communication framework to facilitate the generation of multi-round dialogues. This framework is flexible and readily adaptable for various psychological experiments involving LLMs. In addition, we introduce a multi-agent detection method applicable to a wide range of detection tasks, which integrates Retrieval-Augmented Generation (RAG), competitive debate, and a reinforcement learning-based decision module. Demonstrating substantial effectiveness, this method has shown to improve detection accuracy by as much as 35.10% compared to GPT-4. Codes and appendix are available at https://github.com/2279072142/MindScope.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.04452"
    },
    "11921ebec2a08d560b5255c5a9015fa8": {
        "title": "Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates",
        "authors": [
            "Chaithanya Bandi",
            "Abir Harrasse"
        ],
        "date": "2024/10/07",
        "pdf": "http://arxiv.org/pdf/2410.04663",
        "abstract": "This paper explores optimal architectures for evaluating the outputs of large language models (LLMs) using LLMs themselves. We propose a novel framework that interprets LLMs as advocates within an ensemble of interacting agents, allowing them to defend their answers and reach conclusions through a judge and jury system. This approach offers a more dynamic and comprehensive evaluation process compared to traditional human-based assessments or automated metrics. We discuss the motivation behind this framework, its key components, and comparative advantages. We also present a probabilistic model to evaluate the error reduction achieved by iterative advocate systems. Finally, we outline experiments to validate the effectiveness of multi-advocate architectures and discuss future research directions.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.04663"
    },
    "b175232dfcedb04917df8611051b41f3": {
        "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
        "authors": [
            "Ziru Chen",
            "Shijie Chen",
            "Yuting Ning",
            "Qianheng Zhang",
            "Boshi Wang",
            "Botao Yu",
            "Yifei Li",
            "Zeyi Liao",
            "Chen Wei",
            "Zitong Lu",
            "Vishal Dey",
            "Mingyi Xue",
            "Frazier N. Baker",
            "Benjamin Burns",
            "Daniel Adu-Ampratwum",
            "Xuhui Huang",
            "Xia Ning",
            "Song Gao",
            "Yu Su",
            "Huan Sun"
        ],
        "date": "2024/10/07",
        "pdf": "http://arxiv.org/pdf/2410.05080",
        "abstract": "The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1 with direct prompting and self-debug, which demonstrates the effectiveness of increasing inference-time compute. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.05080"
    },
    "d9ac3390f7673efbe4bf55d69f29f51f": {
        "title": "AgentSquare: Automatic LLM Agent Search in Modular Design Space",
        "authors": [
            "Yu Shang",
            "Yu Li",
            "Keyu Zhao",
            "Likai Ma",
            "Jiahe Liu",
            "Fengli Xu",
            "Yong Li"
        ],
        "date": "2024/10/08",
        "pdf": "http://arxiv.org/pdf/2410.06153",
        "abstract": "Recent advancements in Large Language Models (LLMs) have led to a rapid growth of agentic systems capable of handling a wide range of complex tasks. However, current research largely relies on manual, task-specific design, limiting their adaptability to novel tasks. In this paper, we introduce a new research problem: Modularized LLM Agent Search (MoLAS). We propose a modular design space that abstracts existing LLM agent designs into four fundamental modules with uniform IO interface: Planning, Reasoning, Tool Use, and Memory. Building on this design space, we present a novel LLM agent search framework called AgentSquare, which introduces two core mechanisms, i.e., module evolution and recombination, to efficiently search for optimized LLM agents. To further accelerate the process, we design a performance predictor that uses in-context surrogate models to skip unpromising agent designs. Extensive experiments across six benchmarks, covering the diverse scenarios of web, embodied, tool use and game applications, show that AgentSquare substantially outperforms hand-crafted agents, achieving an average performance gain of 17.2% against best-known human designs. Moreover, AgentSquare can generate interpretable design insights, enabling a deeper understanding of agentic architecture and its impact on task performance. We believe that the modular design space and AgentSquare search framework offer a platform for fully exploiting the potential of prior successful designs and consolidating the collective efforts of research community. Code repo is available at https://github.com/tsinghua-fib-lab/AgentSquare.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.06153"
    },
    "d832d26d2ba9ec69740d15dbf1bed745": {
        "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback",
        "authors": [
            "Zaid Khan",
            "Elias Stengel-Eskin",
            "Jaemin Cho",
            "Mohit Bansal"
        ],
        "date": "2024/10/08",
        "pdf": "http://arxiv.org/pdf/2410.06215",
        "abstract": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid, scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agent&#39;s goal is to improve student performance. Students are iteratively trained and evaluated on generated data, and their feedback (in the form of errors or weak skills) is reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 4 domains (math, code, VQA, and tool-use) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2410.06215"
    },
    "a5e08df5376f5db531345ee2c19dbb04": {
        "title": "MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering",
        "authors": [
            "Jun Shern Chan",
            "Neil Chowdhury",
            "Oliver Jaffe",
            "James Aung",
            "Dane Sherburn",
            "Evan Mays",
            "Giulio Starace",
            "Kevin Liu",
            "Leon Maksin",
            "Tejal Patwardhan",
            "Lilian Weng",
            "Aleksander Mądry"
        ],
        "date": "2024/10/09",
        "pdf": "http://arxiv.org/pdf/2410.07095",
        "abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 75 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle&#39;s publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup--OpenAI&#39;s o1-preview with AIDE scaffolding--achieves at least the level of a Kaggle bronze medal in 16.9% of competitions. In addition to our main results, we investigate various forms of resource scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code (github.com/openai/mle-bench/) to facilitate future research in understanding the ML engineering capabilities of AI agents.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.07095"
    },
    "fc42c6f3a621e50dbc766db0b6787d3f": {
        "title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy",
        "authors": [
            "Gian Maria Campedelli",
            "Nicolò Penzo",
            "Massimo Stefan",
            "Roberto Dessì",
            "Marco Guerini",
            "Bruno Lepri",
            "Jacopo Staiano"
        ],
        "date": "2024/10/09",
        "pdf": "http://arxiv.org/pdf/2410.07109",
        "abstract": "As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent&#39;s anti-social behavior. Third, we highlight how agents&#39; personas, and particularly the guard&#39;s personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents&#39; roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.07109"
    },
    "f5c40fbeb360ecb7cabaeaba32be0523": {
        "title": "Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making",
        "authors": [
            "Manling Li",
            "Shiyu Zhao",
            "Qineng Wang",
            "Kangrui Wang",
            "Yu Zhou",
            "Sanjana Srivastava",
            "Cem Gokmen",
            "Tony Lee",
            "Li Erran Li",
            "Ruohan Zhang",
            "Weiyu Liu",
            "Percy Liang",
            "Li Fei-Fei",
            "Jiayuan Mao",
            "Jiajun Wu"
        ],
        "date": "2024/10/09",
        "pdf": "http://arxiv.org/pdf/2410.07166",
        "abstract": "We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc. Overall, our benchmark offers a comprehensive assessment of LLMs&#39; performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems, and providing insights for effective and selective use of LLMs in embodied decision making.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.07166"
    },
    "fde6430e0f70811eedb287d351eb6f35": {
        "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models",
        "authors": [
            "Yiming Huang",
            "Jianwen Luo",
            "Yan Yu",
            "Yitong Zhang",
            "Fangyu Lei",
            "Yifan Wei",
            "Shizhu He",
            "Lifu Huang",
            "Xiao Liu",
            "Jun Zhao",
            "Kang Liu"
        ],
        "date": "2024/10/09",
        "pdf": "http://arxiv.org/pdf/2410.07331",
        "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.07331"
    },
    "d7bb9dbb06879b831eb00d01166d6193": {
        "title": "AI-Press: A Multi-Agent News Generating and Feedback Simulation System Powered by Large Language Models",
        "authors": [
            "Xiawei Liu",
            "Shiyue Yang",
            "Xinnong Zhang",
            "Haoyu Kuang",
            "Libo Sun",
            "Yihang Yang",
            "Siming Chen",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.07561",
        "abstract": "The rise of various social platforms has transformed journalism. The growing demand for news content has led to the increased use of large language models (LLMs) in news production due to their speed and cost-effectiveness. However, LLMs still encounter limitations in professionalism and ethical judgment in news generation. Additionally, predicting public feedback is usually difficult before news is released. To tackle these challenges, we introduce AI-Press, an automated news drafting and polishing system based on multi-agent collaboration and Retrieval-Augmented Generation. We develop a feedback simulation system that generates public feedback considering demographic distributions. Through extensive quantitative and qualitative evaluations, our system shows significant improvements in news-generating capabilities and verifies the effectiveness of public feedback simulation.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.07561"
    },
    "eb5f39e031e9332d097fc1f867ee281a": {
        "title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization",
        "authors": [
            "Yougang Lyu",
            "Lingyong Yan",
            "Zihan Wang",
            "Dawei Yin",
            "Pengjie Ren",
            "Maarten de Rijke",
            "Zhaochun Ren"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.07672",
        "abstract": "As large language models (LLMs) are rapidly advancing and achieving near-human capabilities, aligning them with human values is becoming more urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong alignment problem where we need to effectively align strong student LLMs through weak supervision generated by weak teachers. Existing alignment methods mainly focus on strong-to-weak alignment and self-alignment settings, and it is impractical to adapt them to the much harder weak-to-strong alignment setting. To fill this gap, we propose a multi-agent contrastive preference optimization (MACPO) framework. MACPO facilitates weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar negative ones. To get this, we devise a mutual positive behavior augmentation strategy to encourage weak teachers and strong students to learn from each other&#39;s positive behavior and further provide higher quality positive behavior for the next iteration. Additionally, we propose a hard negative behavior construction strategy to induce weak teachers and strong students to generate familiar negative behavior by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human judgments, demonstrate that MACPO simultaneously improves the alignment performance of strong students and weak teachers. Moreover, as the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance through more iteration optimization rounds.",
        "code": "",
        "category": [
            "Multi-Agent System",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.07672"
    },
    "447f1f7d03da7e1c3857c9c7465588e3": {
        "title": "AgentBank: Towards Generalized LLM Agents via Fine-Tuning on 50000+ Interaction Trajectories",
        "authors": [
            "Yifan Song",
            "Weimin Xiong",
            "Xiutian Zhao",
            "Dawei Zhu",
            "Wenhao Wu",
            "Ke Wang",
            "Cheng Li",
            "Wei Peng",
            "Sujian Li"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.07706",
        "abstract": "Fine-tuning on agent-environment interaction trajectory data holds significant promise for surfacing generalized agent capabilities in open-source large language models (LLMs). In this work, we introduce AgentBank, by far the largest trajectory tuning data collection featuring more than 50k diverse high-quality interaction trajectories which comprises 16 tasks covering five distinct agent skill dimensions. Leveraging a novel annotation pipeline, we are able to scale the annotated trajectories and generate a trajectory dataset with minimized difficulty bias. Furthermore, we fine-tune LLMs on AgentBank to get a series of agent models, Samoyed. Our comparative experiments demonstrate the effectiveness of scaling the interaction trajectory data to acquire generalized agent capabilities. Additional studies also reveal some key observations regarding trajectory tuning and agent skill generalization.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.07706"
    },
    "b575161e1fcf50f3693f5be9072a2175": {
        "title": "Rewriting Conversational Utterances with Instructed Large Language Models",
        "authors": [
            "Elnara Galimzhanova",
            "Cristina Ioana Muntean",
            "Franco Maria Nardini",
            "Raffaele Perego",
            "Guido Rocchietti"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.07797",
        "abstract": "Many recent studies have shown the ability of large language models (LLMs) to achieve state-of-the-art performance on many NLP tasks, such as question answering, text summarization, coding, and translation. In some cases, the results provided by LLMs are on par with those of human experts. These models&#39; most disruptive innovation is their ability to perform tasks via zero-shot or few-shot prompting. This capability has been successfully exploited to train instructed LLMs, where reinforcement learning with human feedback is used to guide the model to follow the user&#39;s requests directly. In this paper, we investigate the ability of instructed LLMs to improve conversational search effectiveness by rewriting user questions in a conversational setting. We study which prompts provide the most informative rewritten utterances that lead to the best retrieval performance. Reproducible experiments are conducted on publicly-available TREC CAST datasets. The results show that rewriting conversational utterances with instructed LLMs achieves significant improvements of up to 25.2% in MRR, 31.7% in Precision@1, 27% in NDCG@3, and 11.5% in Recall@500 over state-of-the-art techniques.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.07797"
    },
    "8d5599df50d83076f37ff1828d93dd97": {
        "title": "Benchmarking Agentic Workflow Generation",
        "authors": [
            "Shuofei Qiao",
            "Runnan Fang",
            "Zhisong Qiu",
            "Xiaobin Wang",
            "Ningyu Zhang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.07869",
        "abstract": "Large Language Models (LLMs), with their exceptional ability to handle a wide range of tasks, have driven significant advancements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorFEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent&#39;s workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference. Code and dataset are available at https://github.com/zjunlp/WorFBench.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.07869"
    },
    "931cc30d68abb58544f6c7ed0b3c3c47": {
        "title": "Multi-Agent Collaborative Data Selection for Efficient LLM Pretraining",
        "authors": [
            "Tianyi Bai",
            "Ling Yang",
            "Zhen Hao Wong",
            "Jiahui Peng",
            "Xinlin Zhuang",
            "Chi Zhang",
            "Lijun Wu",
            "Jiantao Qiu",
            "Wentao Zhang",
            "Binhang Yuan",
            "Conghui He"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.08102",
        "abstract": "Efficient data selection is crucial to accelerate the pretraining of large language models (LLMs). While various methods have been proposed to enhance data efficiency, limited research has addressed the inherent conflicts between these approaches to achieve optimal data selection for LLM pretraining. To tackle this problem, we propose a novel multi-agent collaborative data selection mechanism. In this framework, each data selection method serves as an independent agent, and an agent console is designed to dynamically integrate the information from all agents throughout the LLM training process. We conduct extensive empirical studies to evaluate our multi-agent framework. The experimental results demonstrate that our approach significantly improves data efficiency, accelerates convergence in LLM training, and achieves an average performance gain up to 10.5% across multiple language model benchmarks compared to the state-of-the-art methods.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.08102"
    },
    "d46ba90a52d2dfa9283eb6d347581de6": {
        "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System",
        "authors": [
            "Weize Chen",
            "Jiarui Yuan",
            "Chen Qian",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.08115",
        "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. Optima employs an iterative generate, rank, select, and train paradigm with a reward function balancing task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs. We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, treating conversation turns as tree nodes to explore diverse interaction paths. Evaluated on common multi-agent tasks, including information-asymmetric question answering and complex reasoning, Optima shows consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10\\% tokens on tasks requiring heavy information exchange. Moreover, Optima&#39;s efficiency gains open new possibilities for leveraging inference-compute more effectively, leading to improved inference-time scaling laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the potential towards scalable, efficient, and effective MAS (https://chenweize1998.github.io/optima-project-page).",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.08115"
    },
    "4ffede9ca3be44ee12fb6e713e9b83c6": {
        "title": "DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory",
        "authors": [
            "Yutong Wang",
            "Jiali Zeng",
            "Xuebo Liu",
            "Derek F. Wong",
            "Fandong Meng",
            "Jie Zhou",
            "Min Zhang"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.08143",
        "abstract": "Large language models (LLMs) have achieved reasonable quality improvements in machine translation (MT). However, most current research on MT-LLMs still faces significant challenges in maintaining translation consistency and accuracy when processing entire documents. In this paper, we introduce DelTA, a Document-levEL Translation Agent designed to overcome these limitations. DelTA features a multi-level memory structure that stores information across various granularities and spans, including Proper Noun Records, Bilingual Summary, Long-Term Memory, and Short-Term Memory, which are continuously retrieved and updated by auxiliary LLM-based components. Experimental results indicate that DelTA significantly outperforms strong baselines in terms of translation consistency and quality across four open/closed-source LLMs and two representative document translation datasets, achieving an increase in consistency scores by up to 4.58 percentage points and in COMET scores by up to 3.16 points on average. DelTA employs a sentence-by-sentence translation strategy, ensuring no sentence omissions and offering a memory-efficient solution compared to the mainstream method. Furthermore, DelTA improves pronoun translation accuracy, and the summary component of the agent also shows promise as a tool for query-based summarization tasks. We release our code and data at https://github.com/YutongWang1216/DocMTAgent.",
        "code": "",
        "category": [
            "Memory Mechanism",
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.08143"
    },
    "1a4582d06ed02d5c5a9a44946242c60b": {
        "title": "CAMPHOR: Collaborative Agents for Multi-input Planning and High-Order Reasoning On Device",
        "authors": [
            "Yicheng Fu",
            "Raviteja Anantha",
            "Jianpeng Cheng"
        ],
        "date": "2024/10/12",
        "pdf": "http://arxiv.org/pdf/2410.09407",
        "abstract": "While server-side Large Language Models (LLMs) demonstrate proficiency in function calling and complex reasoning, deploying Small Language Models (SLMs) directly on devices brings opportunities to improve latency and privacy but also introduces unique challenges for accuracy and memory. We introduce CAMPHOR, an innovative on-device SLM multi-agent framework designed to handle multiple user inputs and reason over personal context locally, ensuring privacy is maintained. CAMPHOR employs a hierarchical architecture where a high-order reasoning agent decomposes complex tasks and coordinates expert agents responsible for personal context retrieval, tool interaction, and dynamic plan generation. By implementing parameter sharing across agents and leveraging prompt compression, we significantly reduce model size, latency, and memory usage. To validate our approach, we present a novel dataset capturing multi-agent task trajectories centered on personalized mobile assistant use-cases. Our experiments reveal that fine-tuned SLM agents not only surpass closed-source LLMs in task completion F1 by~35\\% but also eliminate the need for server-device communication, all while enhancing privacy.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2410.09407"
    },
    "ce67c5ac836a0974574d829968b1e299": {
        "title": "Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling Approach for LLM Mathematical Reasoning",
        "authors": [
            "Gisang Lee",
            "Sangwoo Park",
            "Junyoung Park",
            "Andrew Chung",
            "Sieun Park",
            "Yoonah Park",
            "Byungju Kim",
            "Min-gyu Cho"
        ],
        "date": "2024/10/13",
        "pdf": "http://arxiv.org/pdf/2410.09780",
        "abstract": "Large Language Models (LLMs) have exhibited remarkable capabilities in many complex tasks including mathematical reasoning. However, traditional approaches heavily rely on ensuring self-consistency within single prompting method, which limits the exploration of diverse problem-solving strategies. This study addresses these limitations by performing an experimental analysis of distinct prompting methods within the domain of mathematical reasoning. Our findings demonstrate that each method explores a distinct search space, and this differentiation becomes more evident with increasing problem complexity. To leverage this phenomenon, we applied efficient sampling process that uniformly combines samples from these diverse methods, which not only expands the maximum search space but achieves higher performance with fewer runs compared to single methods. Especially, within the subset of difficult questions of MATH dataset named MATH-hard, The maximum search space was achieved while utilizing approximately 43% fewer runs than single methods on average. These findings highlight the importance of integrating diverse problem-solving strategies to enhance the reasoning abilities of LLMs.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.09780"
    },
    "ef786d5a796a752a368e68f89d9f0435": {
        "title": "LLM-Based Multi-Agent Systems are Scalable Graph Generative Models",
        "authors": [
            "Jiarui Ji",
            "Runlin Lei",
            "Jialing Bi",
            "Zhewei Wei",
            "Xu Chen",
            "Yankai Lin",
            "Xuchen Pan",
            "Yaliang Li",
            "Bolin Ding"
        ],
        "date": "2024/10/13",
        "pdf": "http://arxiv.org/pdf/2410.09824",
        "abstract": "The structural properties of naturally arising social graphs are extensively studied to understand their evolution. Prior approaches for modeling network dynamics typically rely on rule-based models, which lack realism and generalizability, or deep learning-based models, which require large-scale training datasets. Social graphs, as abstract graph representations of entity-wise interactions, present an opportunity to explore network evolution mechanisms through realistic simulations of human-item interactions. Leveraging the pre-trained social consensus knowledge embedded in large language models (LLMs), we present GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic, text-attributed social graph generation. GAG simulates the temporal node and edge generation processes for zero-shot social graph generation. The resulting graphs exhibit adherence to seven key macroscopic network properties, achieving an 11% improvement in microscopic graph structure metrics. Through the node classification benchmarking task, we validate GAG effectively captures the intricate text-structure correlations in graph generation. Furthermore, GAG supports generating graphs with up to nearly 100,000 nodes or 10 million edges through large-scale LLM-based agent simulation with parallel acceleration, achieving a minimum speed-up of 90.4%. The source code is available at https://github.com/Ji-Cather/GraphAgent.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.09824"
    },
    "cfc1497cc4231b303b760be63759bb65": {
        "title": "Applying Refusal-Vector Ablation to Llama 3.1 70B Agents",
        "authors": [
            "Simon Lermen",
            "Mateusz Dziemian",
            "Govind Pimpale"
        ],
        "date": "2024/10/08",
        "pdf": "http://arxiv.org/pdf/2410.10871",
        "abstract": "Recently, language models like Llama 3.1 Instruct have become increasingly capable of agentic behavior, enabling them to perform tasks requiring short-term planning and tool use. In this study, we apply refusal-vector ablation to Llama 3.1 70B and implement a simple agent scaffolding to create an unrestricted agent. Our findings imply that these refusal-vector ablated models can successfully complete harmful tasks, such as bribing officials or crafting phishing attacks, revealing significant vulnerabilities in current safety mechanisms. To further explore this, we introduce a small Safe Agent Benchmark, designed to test both harmful and benign tasks in agentic scenarios. Our results imply that safety fine-tuning in chat models does not generalize well to agentic behavior, as we find that Llama 3.1 Instruct models are willing to perform most harmful tasks without modifications. At the same time, these models will refuse to give advice on how to perform the same tasks when asked for a chat completion. This highlights the growing risk of misuse as models become more capable, underscoring the need for improved safety frameworks for language model agents.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.10871"
    },
    "f76cac46c94dc6672f40480219dc081e": {
        "title": "HR-Agent: A Task-Oriented Dialogue (TOD) LLM Agent Tailored for HR Applications",
        "authors": [
            "Weijie Xu",
            "Jay Desai",
            "Fanyou Wu",
            "Josef Valvoda",
            "Srinivasan H. Sengamedu"
        ],
        "date": "2024/10/15",
        "pdf": "http://arxiv.org/pdf/2410.11239",
        "abstract": "Recent LLM (Large Language Models) advancements benefit many fields such as education and finance, but HR has hundreds of repetitive processes, such as access requests, medical claim filing and time-off submissions, which are unaddressed. We relate these tasks to the LLM agent, which has addressed tasks such as writing assisting and customer support. We present HR-Agent, an efficient, confidential, and HR-specific LLM-based task-oriented dialogue system tailored for automating repetitive HR processes such as medical claims and access requests. Since conversation data is not sent to an LLM during inference, it preserves confidentiality required in HR-related tasks.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.11239"
    },
    "028e32d3c25242ac7a3181cd3e90ebdd": {
        "title": "MedAide: Towards an Omni Medical Aide via Specialized LLM-based Multi-Agent Collaboration",
        "authors": [
            "Jinjie Wei",
            "Dingkang Yang",
            "Yanshu Li",
            "Qingyao Xu",
            "Zhaoyu Chen",
            "Mingcheng Li",
            "Yue Jiang",
            "Xiaolu Hou",
            "Lihua Zhang"
        ],
        "date": "2024/10/16",
        "pdf": "http://arxiv.org/pdf/2410.12532",
        "abstract": "Large Language Model (LLM)-driven interactive systems currently show potential promise in healthcare domains. Despite their remarkable capabilities, LLMs typically lack personalized recommendations and diagnosis analysis in sophisticated medical applications, causing hallucinations and performance bottlenecks. To address these challenges, this paper proposes MedAide, an LLM-based omni medical multi-agent collaboration framework for specialized healthcare services. Specifically, MedAide first performs query rewriting through retrieval-augmented generation to accomplish accurate medical intent understanding. Immediately, we devise a contextual encoder to obtain intent prototype embeddings, which are used to recognize fine-grained intents by similarity matching. According to the intent relevance, the activated agents collaborate effectively to provide integrated decision analysis. Extensive experiments are conducted on four medical benchmarks with composite intents. Experimental results from automated metrics and expert doctor evaluations show that MedAide outperforms current LLMs and improves their medical proficiency and strategic reasoning.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.12532"
    },
    "536e1cc9da9d37d50e3797f799018bf2": {
        "title": "Prompt Engineering a Schizophrenia Chatbot: Utilizing a Multi-Agent Approach for Enhanced Compliance with Prompt Instructions",
        "authors": [
            "Per Niklas Waaler",
            "Musarrat Hussain",
            "Igor Molchanov",
            "Lars Ailo Bongo",
            "Brita Elvevåg"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.12848",
        "abstract": "Patients with schizophrenia often present with cognitive impairments that may hinder their ability to learn about their condition. These individuals could benefit greatly from education platforms that leverage the adaptability of Large Language Models (LLMs) such as GPT-4. While LLMs have the potential to make topical mental health information more accessible and engaging, their black-box nature raises concerns about ethics and safety. Prompting offers a way to produce semi-scripted chatbots with responses anchored in instructions and validated information, but prompt-engineered chatbots may drift from their intended identity as the conversation progresses. We propose a Critical Analysis Filter for achieving better control over chatbot behavior. In this system, a team of prompted LLM agents are prompt-engineered to critically analyze and refine the chatbot&#39;s response and deliver real-time feedback to the chatbot. To test this approach, we develop an informational schizophrenia chatbot and converse with it (with the filter deactivated) until it oversteps its scope. Once drift has been observed, AI-agents are used to automatically generate sample conversations in which the chatbot is being enticed to talk about out-of-bounds topics. We manually assign to each response a compliance score that quantifies the chatbot&#39;s compliance to its instructions; specifically the rules about accurately conveying sources and being transparent about limitations. Activating the Critical Analysis Filter resulted in an acceptable compliance score (&gt;=2) in 67.0% of responses, compared to only 8.7% when the filter was deactivated. These results suggest that a self-reflection layer could enable LLMs to be used effectively and safely in mental health platforms, maintaining adaptability while reliably limiting their scope to appropriate use cases.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.12848"
    },
    "2d899f5fc757a6cda46c78d9c0cca1fc": {
        "title": "Diversity of Thought Elicits Stronger Reasoning Capabilities in Multi-Agent Debate Frameworks",
        "authors": [
            "Mahmood Hegazy"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.12853",
        "abstract": "Large language models (LLMs) excel in natural language generation but often confidently produce incorrect responses, especially in tasks like mathematical reasoning. Chain-of-thought prompting, self-verification, and multi-agent debate are among the strategies proposed to improve the reasoning and factual accuracy of LLMs. Building on Du et al.&#39;s multi-agent debate framework, we find that multi-agent debate helps at any model scale, and that diversity of thought elicits stronger reasoning in debating LLMs. Across various model sizes, performance on mathematical reasoning tasks benefits most when diverse trained models are used. Remarkably, after 4 rounds of debate, a diverse set of medium-capacity models (Gemini-Pro, Mixtral 7BX8, and PaLM 2-M) outperforms GPT-4 on the GSM-8K benchmark, scoring 91% accuracy. By comparison, when 3 instances of Gemini-Pro are used, performance only reaches 82%. Finally, this diverse set of medium-capacity models sets a new state-of-the-art performance on the ASDiv benchmark (94%). These results underscore the idea that the future of AI is agentic, with diverse cooperating agents yielding emergent capabilities beyond even the most powerful individual models.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.12853"
    },
    "28d65ec01fad015ae497bae4bf6c3b6a": {
        "title": "JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework",
        "authors": [
            "Fan Liu",
            "Yue Feng",
            "Zhao Xu",
            "Lixin Su",
            "Xinyu Ma",
            "Dawei Yin",
            "Hao Liu"
        ],
        "date": "2024/10/11",
        "pdf": "http://arxiv.org/pdf/2410.12855",
        "abstract": "Despite advancements in enhancing LLM safety against jailbreak attacks, evaluating LLM defenses remains a challenge, with current methods often lacking explainability and generalization to complex scenarios, leading to incomplete assessments (e.g., direct judgment without reasoning, low F1 score of GPT-4 in complex cases, bias in multilingual scenarios). To address this, we present JAILJUDGE, a comprehensive benchmark featuring diverse risk scenarios, including synthetic, adversarial, in-the-wild, and multilingual prompts, along with high-quality human-annotated datasets. The JAILJUDGE dataset includes over 35k+ instruction-tune data with reasoning explainability and JAILJUDGETEST, a 4.5k+ labeled set for risk scenarios, and a 6k+ multilingual set across ten languages. To enhance evaluation with explicit reasoning, we propose the JailJudge MultiAgent framework, which enables explainable, fine-grained scoring (1 to 10). This framework supports the construction of instruction-tuning ground truth and facilitates the development of JAILJUDGE Guard, an end-to-end judge model that provides reasoning and eliminates API costs. Additionally, we introduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a moderation defense, both leveraging JAILJUDGE Guard. Our experiments demonstrate the state-of-the-art performance of JailJudge methods (JailJudge MultiAgent, JAILJUDGE Guard) across diverse models (e.g., GPT-4, Llama-Guard) and zero-shot scenarios. JailBoost and GuardShield significantly improve jailbreak attack and defense tasks under zero-shot settings, with JailBoost enhancing performance by 29.24% and GuardShield reducing defense ASR from 40.46% to 0.15%.",
        "code": "",
        "category": [
            "Benchmark&Evaluation",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.12855"
    },
    "b67c1a8fd4f569fa7ff95a1c909c563f": {
        "title": "AdaSwitch: Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning",
        "authors": [
            "Hao Sun",
            "Jiayi Wu",
            "Hengyi Cai",
            "Xiaochi Wei",
            "Yue Feng",
            "Bo Wang",
            "Shuaiqiang Wang",
            "Yan Zhang",
            "Dawei Yin"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.13181",
        "abstract": "Recent advancements in large language models (LLMs) have been remarkable. Users face a choice between using cloud-based LLMs for generation quality and deploying local-based LLMs for lower computational cost. The former option is typically costly and inefficient, while the latter usually fails to deliver satisfactory performance for reasoning steps requiring deliberate thought processes. In this work, we propose a novel LLM utilization paradigm that facilitates the collaborative operation of large cloud-based LLMs and smaller local-deployed LLMs. Our framework comprises two primary modules: the local agent instantiated with a relatively smaller LLM, handling less complex reasoning steps, and the cloud agent equipped with a larger LLM, managing more intricate reasoning steps. This collaborative processing is enabled through an adaptive mechanism where the local agent introspectively identifies errors and proactively seeks assistance from the cloud agent, thereby effectively integrating the strengths of both locally-deployed and cloud-based LLMs, resulting in significant enhancements in task completion performance and efficiency. We evaluate AdaSwitch across 7 benchmarks, ranging from mathematical reasoning and complex question answering, using various types of LLMs to instantiate the local and cloud agents. The empirical results show that AdaSwitch effectively improves the performance of the local agent, and sometimes achieves competitive results compared to the cloud agent while utilizing much less computational overhead.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.13181"
    },
    "022e46cc67797deb06786d964eac640f": {
        "title": "Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation",
        "authors": [
            "Hyungjoo Chae",
            "Namyoung Kim",
            "Kai Tzu-iunn Ong",
            "Minju Gwak",
            "Gwanwoo Song",
            "Jihoon Kim",
            "Sunghwan Kim",
            "Dongha Lee",
            "Jinyoung Yeo"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.13232",
        "abstract": "Large language models (LLMs) have recently gained much attention in building autonomous agents. However, the performance of current LLM-based web agents in long-horizon tasks is far from optimal, often yielding errors such as repeatedly buying a non-refundable flight ticket. By contrast, humans can avoid such an irreversible mistake, as we have an awareness of the potential outcomes (e.g., losing money) of our actions, also known as the &#34;world model&#34;. Motivated by this, our study first starts with preliminary analyses, confirming the absence of world models in current LLMs (e.g., GPT-4o, Claude-3.5-Sonnet, etc.). Then, we present a World-model-augmented (WMA) web agent, which simulates the outcomes of its actions for better decision-making. To overcome the challenges in training LLMs as world models predicting next observations, such as repeated elements across observations and long HTML inputs, we propose a transition-focused observation abstraction, where the prediction objectives are free-form natural language descriptions exclusively highlighting important state differences between time steps. Experiments on WebArena and Mind2Web show that our world models improve agents&#39; policy selection without training and demonstrate our agents&#39; cost- and time-efficiency compared to recent tree-search-based agents.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.13232"
    },
    "2872f3c9e74f2f029e787617df928f8d": {
        "title": "SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent",
        "authors": [
            "Jiarui Ji",
            "Yang Li",
            "Hongtao Liu",
            "Zhicheng Du",
            "Zhewei Wei",
            "Weiran Shen",
            "Qi Qi",
            "Yankai Lin"
        ],
        "date": "2024/10/18",
        "pdf": "http://arxiv.org/pdf/2410.14152",
        "abstract": "Public scarce resource allocation plays a crucial role in economics as it directly influences the efficiency and equity in society. Traditional studies including theoretical model-based, empirical study-based and simulation-based methods encounter limitations due to the idealized assumption of complete information and individual rationality, as well as constraints posed by limited available data. In this work, we propose an innovative framework, SRAP-Agent (Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent), which integrates Large Language Models (LLMs) into economic simulations, aiming to bridge the gap between theoretical models and real-world dynamics. Using public housing allocation scenarios as a case study, we conduct extensive policy simulation experiments to verify the feasibility and effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm with certain optimization objectives. The source code can be found in https://github.com/jijiarui-cather/SRAPAgent_Framework",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.14152"
    },
    "b4ef916a5b54470cc61aea27a847fc82": {
        "title": "Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases",
        "authors": [
            "Elias Lumer",
            "Vamse Kumar Subbiah",
            "James A. Burke",
            "Pradeep Honaganahalli Basavaraju",
            "Austin Huber"
        ],
        "date": "2024/10/18",
        "pdf": "http://arxiv.org/pdf/2410.14594",
        "abstract": "Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks like secure database interactions and multi-agent code development. However, scaling tool capacity beyond agent reasoning or model limits remains a challenge. In this paper, we address these challenges by introducing Toolshed Knowledge Bases, a tool knowledge base (vector database) designed to store enhanced tool representations and optimize tool selection for large-scale tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a novel ensemble of tool-applied advanced retrieval-augmented generation (RAG) techniques across the pre-retrieval, intra-retrieval, and post-retrieval phases, without requiring model fine-tuning. During pre-retrieval, tool documents are enhanced with key information and stored in the Toolshed Knowledge Base. Intra-retrieval focuses on query planning and transformation to increase retrieval accuracy. Post-retrieval refines the retrieved tool documents and enables self-reflection. Furthermore, by varying both the total number of tools (tool-M) an Agent has access to and the tool selection threshold (top-k), we address trade-offs between retrieval accuracy, agent performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools benchmark datasets, respectively (Recall@5).",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.14594"
    },
    "08a740752dd68dbf6693c0c00fc21790": {
        "title": "Agent Skill Acquisition for Large Language Models via CycleQD",
        "authors": [
            "So Kuroki",
            "Taishi Nakamura",
            "Takuya Akiba",
            "Yujin Tang"
        ],
        "date": "2024/10/16",
        "pdf": "http://arxiv.org/pdf/2410.14735",
        "abstract": "Training large language models to acquire specific skills remains a challenging endeavor. Conventional training approaches often struggle with data distribution imbalances and inadequacies in objective functions that do not align well with task-specific performance. To address these challenges, we introduce CycleQD, a novel approach that leverages the Quality Diversity framework through a cyclic adaptation of the algorithm, along with a model merging based crossover and an SVD-based mutation. In CycleQD, each task&#39;s performance metric is alternated as the quality measure while the others serve as the behavioral characteristics. This cyclic focus on individual tasks allows for concentrated effort on one task at a time, eliminating the need for data ratio tuning and simplifying the design of the objective function. Empirical results from AgentBench indicate that applying CycleQD to LLAMA3-8B-INSTRUCT based models not only enables them to surpass traditional fine-tuning methods in coding, operating systems, and database tasks, but also achieves performance on par with GPT-3.5-TURBO, which potentially contains much more parameters, across these domains. Crucially, this enhanced performance is achieved while retaining robust language capabilities, as evidenced by its performance on widely adopted language benchmark tasks. We highlight the key design choices in CycleQD, detailing how these contribute to its effectiveness. Furthermore, our method is general and can be applied to image segmentation models, highlighting its applicability across different domains.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.14735"
    },
    "951e57d5c574b7262fad7c5cc65b01e5": {
        "title": "An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making",
        "authors": [
            "Xiutian Zhao",
            "Ke Wang",
            "Wei Peng"
        ],
        "date": "2024/10/19",
        "pdf": "http://arxiv.org/pdf/2410.15168",
        "abstract": "Modern large language models (LLMs) have exhibited cooperative synergy on complex task-solving, and collective decision-making (CDM) is a pivotal component in LLM-based multi-agent collaboration frameworks. Our survey on 52 recent such systems uncovers a severe lack of diversity, with a heavy reliance on dictatorial and plurality voting for CDM. Through the lens of social choice theory, we scrutinize widely-adopted CDM methods and identify their limitations. To enrich current landscape of LLM-based CDM, we present GEDI, an electoral CDM module that incorporates various ordinal preferential voting mechanisms. Our empirical case study across three benchmarks shows that the integration of certain CDM methods can markedly improve the reasoning capabilities and robustness of some leading LLMs, all without requiring intricate system designs. Additionally, we find that some CDM mechanisms generate positive synergies even with as few as three agents. The voting-based methods also demonstrate robustness against single points of failure, as well as diversity in terms of hit-rate@k and subject-wise impacts.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.15168"
    },
    "ce3cfb1fe56d6dc07c6bbb2956d74f16": {
        "title": "Training Language Models to Critique With Multi-agent Feedback",
        "authors": [
            "Tian Lan",
            "Wenwei Zhang",
            "Chengqi Lyu",
            "Shuaibin Li",
            "Chen Xu",
            "Heyan Huang",
            "Dahua Lin",
            "Xian-Ling Mao",
            "Kai Chen"
        ],
        "date": "2024/10/20",
        "pdf": "http://arxiv.org/pdf/2410.15287",
        "abstract": "Critique ability, a meta-cognitive capability of humans, presents significant challenges for LLMs to improve. Recent works primarily rely on supervised fine-tuning (SFT) using critiques generated by a single LLM like GPT-4. However, these model-generated critiques often exhibit flaws due to the inherent complexity of the critique. Consequently, fine-tuning LLMs on such flawed critiques typically limits the model&#39;s performance and propagates these flaws into the learned model. To overcome these challenges, this paper proposes a novel data generation pipeline, named MultiCritique, that improves the critique ability of LLMs by utilizing multi-agent feedback in both the SFT and reinforcement learning (RL) stages. First, our data generation pipeline aggregates high-quality critiques from multiple agents instead of a single model, with crucial information as input for simplifying the critique. Furthermore, our pipeline improves the preference accuracy of critique quality through multi-agent feedback, facilitating the effectiveness of RL in improving the critique ability of LLMs. Based on our proposed MultiCritique data generation pipeline, we construct the MultiCritiqueDataset for the SFT and RL fine-tuning stages. Extensive experimental results on two benchmarks demonstrate: 1) the superior quality of our constructed SFT dataset compared to existing critique datasets; 2) additional improvements to the critique ability of LLMs brought by the RL stage. Notably, our fine-tuned 7B model significantly surpasses other advanced 7B-13B open-source models, approaching the performance of advanced 70B LLMs and GPT-4. Codes, datasets and model weights will be publicly available.",
        "code": "",
        "category": [
            "Multi-Agent System",
            "Feedback&Reflection",
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.15287"
    },
    "855bbef5d648e573efd345ec2b071651": {
        "title": "VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use",
        "authors": [
            "Zhehao Zhang",
            "Ryan Rossi",
            "Tong Yu",
            "Franck Dernoncourt",
            "Ruiyi Zhang",
            "Jiuxiang Gu",
            "Sungchul Kim",
            "Xiang Chen",
            "Zichao Wang",
            "Nedim Lipka"
        ],
        "date": "2024/10/21",
        "pdf": "http://arxiv.org/pdf/2410.16400",
        "abstract": "While vision-language models (VLMs) have demonstrated remarkable performance across various tasks combining textual and visual information, they continue to struggle with fine-grained visual perception tasks that require detailed pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs on such intricate visual elements remains an open challenge. In this paper, we present VipAct, an agent framework that enhances VLMs by integrating multi-agent collaboration and vision expert models, enabling more precise visual understanding and comprehensive reasoning. VipAct consists of an orchestrator agent, which manages task requirement analysis, planning, and coordination, along with specialized agents that handle specific tasks such as image captioning and vision expert models that provide high-precision perceptual information. This multi-agent approach allows VLMs to better perform fine-grained visual perception tasks by synergizing planning, reasoning, and tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual perception tasks, with experimental results demonstrating significant performance improvements over state-of-the-art baselines across all tasks. Furthermore, comprehensive ablation studies reveal the critical role of multi-agent collaboration in eliciting more detailed System-2 reasoning and highlight the importance of image input for task planning. Additionally, our error analysis identifies patterns of VLMs&#39; inherent limitations in visual perception, providing insights into potential future improvements. VipAct offers a flexible and extensible framework, paving the way for more advanced visual perception systems across various real-world applications.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.16400"
    },
    "29d1233ccd45c31133f6c1eb4cd9c64f": {
        "title": "Beyond Browsing: API-Based Web Agents",
        "authors": [
            "Yueqi Song",
            "Frank Xu",
            "Shuyan Zhou",
            "Graham Neubig"
        ],
        "date": "2024/10/21",
        "pdf": "http://arxiv.org/pdf/2410.16464",
        "abstract": "Web browsers are a portal to the internet, where much of human activity is undertaken. Thus, there has been significant research work in AI agents that interact with the internet through web browsing. However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs). In this paper we ask -- what if we were to take tasks traditionally tackled by browsing agents, and give AI agents access to APIs? To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a Hybrid Agent that can interact with online data through both web browsing and APIs. In experiments on WebArena, a widely-used and realistic benchmark for web navigation tasks, we find that API-based agents outperform web browsing agents. Hybrid Agents out-perform both others nearly uniformly across tasks, resulting in a more than 20.0% absolute improvement over web browsing alone, achieving a success rate of 35.8%, achiving the SOTA performance among task-agnostic agents. These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.16464"
    },
    "46d921a1305de4da54f0058f5f3d9cca": {
        "title": "Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent",
        "authors": [
            "Janghoon Ock",
            "Tirtha Vinchurkar",
            "Yayati Jadhav",
            "Amir Barati Farimani"
        ],
        "date": "2024/10/22",
        "pdf": "http://arxiv.org/pdf/2410.16658",
        "abstract": "Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agent&#39;s performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.16658"
    },
    "6d8cb931652f88d3afe5144c3d474a41": {
        "title": "Large Language Models Empowered Personalized Web Agents",
        "authors": [
            "Hongru Cai",
            "Yongqi Li",
            "Wenjie Wang",
            "Fengbin Zhu",
            "Xiaoyu Shen",
            "Wenjie Li",
            "Tat-Seng Chua"
        ],
        "date": "2024/10/22",
        "pdf": "http://arxiv.org/pdf/2410.17236",
        "abstract": "Web agents have emerged as a promising direction to automate Web task completion based on user instructions, significantly enhancing user experience. Recently, Web agents have evolved from traditional agents to Large Language Models (LLMs)-based Web agents. Despite their success, existing LLM-based Web agents overlook the importance of personalized data (e.g., user profiles and historical Web behaviors) in assisting the understanding of users&#39; personalized instructions and executing customized actions. To overcome the limitation, we first formulate the task of LLM-empowered personalized Web agents, which integrate personalized data and user instructions to personalize instruction comprehension and action execution. To address the absence of a comprehensive evaluation benchmark, we construct a Personalized Web Agent Benchmark (PersonalWAB), featuring user instructions, personalized user data, Web functions, and two evaluation paradigms across three personalized Web tasks. Moreover, we propose a Personalized User Memory-enhanced Alignment (PUMA) framework to adapt LLMs to the personalized Web agent task. PUMA utilizes a memory bank with a task-specific retrieval strategy to filter relevant historical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for personalized action execution through fine-tuning and direct preference optimization. Extensive experiments validate the superiority of PUMA over existing Web agents on PersonalWAB.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.17236"
    },
    "39c7a4b54bb5570c3524811280812171": {
        "title": "ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents",
        "authors": [
            "Yusheng Liao",
            "Shuyang Jiang",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "date": "2024/10/23",
        "pdf": "http://arxiv.org/pdf/2410.17657",
        "abstract": "Large Language Models (LLMs) have shown promising potential in the medical domain, assisting with tasks like clinical note generation and patient communication. However, current LLMs are limited to text-based communication, hindering their ability to interact with diverse forms of information in clinical environments. Despite clinical agents succeeding in diverse signal interaction, they are oriented to a single clinical scenario and hence fail for broader applications. To evaluate clinical agents holistically, we propose ClinicalAgent Bench~(CAB), a comprehensive medical agent benchmark consisting of 18 tasks across five key realistic clinical dimensions. Building on this, we introduce ReflecTool, a novel framework that excels at utilizing domain-specific tools within two stages. The first optimization stage progressively enlarges a long-term memory by saving successful solving processes and tool-wise experience of agents in a tiny pre-defined training set. In the following inference stage, ReflecTool can search for supportive successful demonstrations from already built long-term memory to guide the tool selection strategy, and a verifier improves the tool usage according to the tool-wise experience with two verification methods--iterative refinement and candidate selection. Extensive experiments on ClinicalAgent Benchmark demonstrate that ReflecTool surpasses the pure LLMs with more than 10 points and the well-established agent-based methods with 3 points, highlighting its adaptability and effectiveness in solving complex clinical tasks.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.17657"
    },
    "52f575865871b8eece5f6bec3588fbdd": {
        "title": "AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios",
        "authors": [
            "Xinyi Mou",
            "Jingcong Liang",
            "Jiayu Lin",
            "Xinnong Zhang",
            "Xiawei Liu",
            "Shiyue Yang",
            "Rong Ye",
            "Lei Chen",
            "Haoyu Kuang",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "date": "2024/10/25",
        "pdf": "http://arxiv.org/pdf/2410.19346",
        "abstract": "Large language models (LLMs) are increasingly leveraged to empower autonomous agents to simulate human beings in various fields of behavioral research. However, evaluating their capacity to navigate complex social interactions remains a challenge. Previous studies face limitations due to insufficient scenario diversity, complexity, and a single-perspective focus. To this end, we introduce AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense employs a bottom-up approach to create 1,225 diverse social scenarios constructed from extensive scripts. We evaluate LLM-driven agents through multi-turn interactions, emphasizing both goal completion and implicit reasoning. We analyze goals using ERG theory and conduct comprehensive experiments. Our findings highlight that LLMs struggle with goals in complex social scenarios, especially high-level growth needs, and even GPT-4o requires improvement in private information reasoning. Code and data are available at \\url{https://github.com/ljcleo/agent_sense}.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.19346"
    },
    "d6c255ec08fe95ad5b30440c73668aa3": {
        "title": "OpenWebVoyager: Building Multimodal Web Agents via Iterative Real-World Exploration, Feedback and Optimization",
        "authors": [
            "Hongliang He",
            "Wenlin Yao",
            "Kaixin Ma",
            "Wenhao Yu",
            "Hongming Zhang",
            "Tianqing Fang",
            "Zhenzhong Lan",
            "Dong Yu"
        ],
        "date": "2024/10/25",
        "pdf": "http://arxiv.org/pdf/2410.19609",
        "abstract": "The rapid development of large language and multimodal models has sparked significant interest in using proprietary models, such as GPT-4o, to develop autonomous agents capable of handling real-world scenarios like web navigation. Although recent open-source efforts have tried to equip agents with the ability to explore environments and continuously improve over time, they are building text-only agents in synthetic environments where the reward signals are clearly defined. Such agents struggle to generalize to realistic settings that require multimodal perception abilities and lack ground-truth signals. In this paper, we introduce an open-source framework designed to facilitate the development of multimodal web agent that can autonomously conduct real-world exploration and improve itself. We first train the base model with imitation learning to gain the basic abilities. We then let the agent explore the open web and collect feedback on its trajectories. After that, it further improves its policy by learning from well-performing trajectories judged by another general-purpose model. This exploration-feedback-optimization cycle can continue for several iterations. Experimental results show that our web agent successfully improves itself after each iteration, demonstrating strong performance across multiple test sets.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.19609"
    },
    "dac0bc58e388f9eb06aff0058890990d": {
        "title": "AGENT-CQ: Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs",
        "authors": [
            "Clemencia Siro",
            "Yifei Yuan",
            "Mohammad Aliannejadi",
            "Maarten de Rijke"
        ],
        "date": "2024/10/25",
        "pdf": "http://arxiv.org/pdf/2410.19692",
        "abstract": "Generating diverse and effective clarifying questions is crucial for improving query understanding and retrieval performance in open-domain conversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration, and evaluaTion of Clarifying Questions), an end-to-end LLM-based framework addressing the challenges of scalability and adaptability faced by existing methods that rely on manual curation or template-based approaches. AGENT-CQ consists of two stages: a generation stage employing LLM prompting strategies to generate clarifying questions, and an evaluation stage (CrowdLLM) that simulates human crowdsourcing judgments using multiple LLM instances to assess generated questions and answers based on comprehensive quality metrics. Extensive experiments on the ClariQ dataset demonstrate CrowdLLM&#39;s effectiveness in evaluating question and answer quality. Human evaluation and CrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms baselines in various aspects of question and answer quality. In retrieval-based evaluation, LLM-generated questions significantly enhance retrieval effectiveness for both BM25 and cross-encoder models compared to human-generated questions.",
        "code": "",
        "category": [
            "Role Playing",
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.19692"
    },
    "941424340d3792901155a6fc9b8048f8": {
        "title": "TrajAgent: An Agent Framework for Unified Trajectory Modelling",
        "authors": [
            "Yuwei Du",
            "Jie Feng",
            "Jie Zhao",
            "Yong Li"
        ],
        "date": "2024/10/27",
        "pdf": "http://arxiv.org/pdf/2410.20445",
        "abstract": "Trajectory modeling, which includes research on trajectory data pattern mining and future prediction, has widespread applications in areas such as life services, urban transportation, and public administration. Numerous methods have been proposed to address specific problems within trajectory modelling. However, due to the heterogeneity of data and the diversity of trajectory tasks, achieving unified trajectory modelling remains an important yet challenging task. In this paper, we propose TrajAgent, a large language model-based agentic framework, to unify various trajectory modelling tasks. In TrajAgent, we first develop UniEnv, an execution environment with a unified data and model interface, to support the execution and training of various models. Building on UniEnv, we introduce TAgent, an agentic workflow designed for automatic trajectory modelling across various trajectory tasks. Specifically, we design AutOpt, a systematic optimization module within TAgent, to further improve the performance of the integrated model. With diverse trajectory tasks input in natural language, TrajAgent automatically generates competitive results via training and executing appropriate models. Extensive experiments on four tasks using four real-world datasets demonstrate the effectiveness of TrajAgent in unified trajectory modelling, achieving an average performance improvement of 15.43% over baseline methods.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2410.20445"
    },
    "eb44ba8df62ac5c1006647fc404976c5": {
        "title": "ElectionSim: Massive Population Election Simulation Powered by Large Language Model Driven Agents",
        "authors": [
            "Xinnong Zhang",
            "Jiayu Lin",
            "Libo Sun",
            "Weihong Qi",
            "Yihang Yang",
            "Yue Chen",
            "Hanjia Lyu",
            "Xinyi Mou",
            "Siming Chen",
            "Jiebo Luo",
            "Xuanjing Huang",
            "Shiping Tang",
            "Zhongyu Wei"
        ],
        "date": "2024/10/28",
        "pdf": "http://arxiv.org/pdf/2410.20746",
        "abstract": "The massive population election simulation aims to model the preferences of specific groups in particular election scenarios. It has garnered significant attention for its potential to forecast real-world social trends. Traditional agent-based modeling (ABM) methods are constrained by their ability to incorporate complex individual background information and provide interactive prediction results. In this paper, we introduce ElectionSim, an innovative election simulation framework based on large language models, designed to support accurate voter simulations and customized distributions, together with an interactive platform to dialogue with simulated voters. We present a million-level voter pool sampled from social media platforms to support accurate individual simulation. We also introduce PPE, a poll-based presidential election benchmark to assess the performance of our framework under the U.S. presidential election scenario. Through extensive experiments and analyses, we demonstrate the effectiveness and robustness of our framework in U.S. presidential election simulations.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.20746"
    },
    "5971b809eec85268c7a9308fe83f77b1": {
        "title": "CRAT: A Multi-Agent Framework for Causality-Enhanced Reflective and Retrieval-Augmented Translation with Large Language Models",
        "authors": [
            "Meiqi Chen",
            "Fandong Meng",
            "Yingxue Zhang",
            "Yan Zhang",
            "Jie Zhou"
        ],
        "date": "2024/10/28",
        "pdf": "http://arxiv.org/pdf/2410.21067",
        "abstract": "Large language models (LLMs) have shown great promise in machine translation, but they still struggle with contextually dependent terms, such as new or domain-specific words. This leads to inconsistencies and errors that are difficult to address. Existing solutions often depend on manual identification of such terms, which is impractical given the complexity and evolving nature of language. While Retrieval-Augmented Generation (RAG) could provide some assistance, its application to translation is limited by issues such as hallucinations from information overload. In this paper, we propose CRAT, a novel multi-agent translation framework that leverages RAG and causality-enhanced self-reflection to address these challenges. This framework consists of several specialized agents: the Unknown Terms Identification agent detects unknown terms within the context, the Knowledge Graph (KG) Constructor agent extracts relevant internal knowledge about these terms and retrieves bilingual information from external sources, the Causality-enhanced Judge agent validates the accuracy of the information, and the Translator agent incorporates the refined information into the final output. This automated process allows for more precise and consistent handling of key terms during translation. Our results show that CRAT significantly improves translation accuracy, particularly in handling context-sensitive terms and emerging vocabulary.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.21067"
    },
    "30374ceac706211d27f9044c8d8c3618": {
        "title": "Can Machines Think Like Humans? A Behavioral Evaluation of LLM-Agents in Dictator Games",
        "authors": [
            "Ji Ma"
        ],
        "date": "2024/10/28",
        "pdf": "http://arxiv.org/pdf/2410.21359",
        "abstract": "As Large Language Model (LLM)-based agents increasingly undertake real-world tasks and engage with human society, how well do we understand their behaviors? We (1) investigate how LLM agents&#39; prosocial behaviors -- a fundamental social norm -- can be induced by different personas and benchmarked against human behaviors; and (2) introduce a behavioral and social science approach to evaluate LLM agents&#39; decision-making. We explored how different personas and experimental framings affect these AI agents&#39; altruistic behavior in dictator games and compared their behaviors within the same LLM family, across various families, and with human behaviors. The findings reveal substantial variations and inconsistencies among LLMs and notable differences compared to human behaviors. Merely assigning a human-like identity to LLMs does not produce human-like behaviors. Despite being trained on extensive human-generated data, these AI agents are unable to capture the internal processes of human decision-making. Their alignment with human is highly variable and dependent on specific model architectures and prompt formulations; even worse, such dependence does not follow a clear pattern. LLMs can be useful task-specific tools but are not yet intelligent human-like agents.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.21359"
    },
    "a426f9d84958b34d64ded5ee419f16a6": {
        "title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
        "authors": [
            "Sorouralsadat Fatemi",
            "Yuheng Hu"
        ],
        "date": "2024/10/29",
        "pdf": "http://arxiv.org/pdf/2410.21741",
        "abstract": "While Large Language Models (LLMs) have shown impressive capabilities in numerous Natural Language Processing (NLP) tasks, they still struggle with financial question answering (QA), particularly when numerical reasoning is required. Recently, LLM-based multi-agent frameworks have demonstrated remarkable effectiveness in multi-step reasoning, which is crucial for financial QA tasks as it involves extracting relevant information from tables and text and then performing numerical reasoning on the extracted data to infer answers. In this study, we propose a multi-agent framework incorporating a critic agent that reflects on the reasoning steps and final answers for each question. Additionally, we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer. Our results indicate that this framework significantly improves performance compared to single-agent reasoning, with an average performance increase of 15% for the LLaMA3-8B model and 5% for the LLaMA3-70B model. Furthermore, our framework performs on par with, and in some cases surpasses, larger single-agent LLMs such as LLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to Claude-3.5 Sonnet. Overall, our framework presents an effective solution to enhance open-source LLMs for financial QA tasks, offering a cost-effective alternative to larger models like Claude-3.5 Sonnet.",
        "code": "",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2410.21741"
    },
    "9090e0f5abaad69f58e46c01173fa20a": {
        "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent",
        "authors": [
            "Xiao Xia",
            "Dan Zhang",
            "Zibo Liao",
            "Zhenyu Hou",
            "Tianrui Sun",
            "Jing Li",
            "Ling Fu",
            "Yuxiao Dong"
        ],
        "date": "2024/10/29",
        "pdf": "http://arxiv.org/pdf/2410.21909",
        "abstract": "The modeling of industrial scenes is essential for simulations in industrial manufacturing. While large language models (LLMs) have shown significant progress in generating general 3D scenes from textual descriptions, generating industrial scenes with LLMs poses a unique challenge due to their demand for precise measurements and positioning, requiring complex planning over spatial arrangement. To address this challenge, we introduce SceneGenAgent, an LLM-based agent for generating industrial scenes through C# code. SceneGenAgent ensures precise layout planning through a structured and calculable format, layout verification, and iterative refinement to meet the quantitative requirements of industrial scenarios. Experiment results demonstrate that LLMs powered by SceneGenAgent exceed their original performance, reaching up to 81.0% success rate in real-world industrial scene generation tasks and effectively meeting most scene generation requirements. To further enhance accessibility, we construct SceneInstruct, a dataset designed for fine-tuning open-source LLMs to integrate into SceneGenAgent. Experiments show that fine-tuning open-source LLMs on SceneInstruct yields significant performance improvements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our code and data are available at https://github.com/THUDM/SceneGenAgent .",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.21909"
    },
    "dc80edc58213c4b5ceba349615bf7a6b": {
        "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning",
        "authors": [
            "Yihe Deng",
            "Paul Mineiro"
        ],
        "date": "2024/10/29",
        "pdf": "http://arxiv.org/pdf/2410.22304",
        "abstract": "Mathematical reasoning is a crucial capability for Large Language Models (LLMs), yet generating detailed and accurate reasoning traces remains a significant challenge. This paper introduces a novel approach to produce high-quality reasoning traces for LLM fine-tuning using online learning \\textbf{Flows}. Our method employs an incremental output production Flow, where component LLMs collaboratively construct solutions through iterative communication. We train the Flow using online Direct Preference Optimization (DPO) learning with rollouts, generating DPO pairs for each training example and updating models in real-time. We directly compare the quality of reasoning traces generated by our method with those produced through direct model inference, demonstrating the effectiveness of our approach in improving LLM performance in mathematical reasoning tasks.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.22304"
    },
    "1277a93a089dad77002064d947bfbbe1": {
        "title": "Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions",
        "authors": [
            "Ryan Y. Lin",
            "Siddhartha Ojha",
            "Kevin Cai",
            "Maxwell F. Chen"
        ],
        "date": "2024/09/19",
        "pdf": "http://arxiv.org/pdf/2410.00031",
        "abstract": "Machine-learning technologies are seeing increased deployment in real-world market scenarios. In this work, we explore the strategic behaviors of large language models (LLMs) when deployed as autonomous agents in multi-commodity markets, specifically within Cournot competition frameworks. We examine whether LLMs can independently engage in anti-competitive practices such as collusion or, more specifically, market division. Our findings demonstrate that LLMs can effectively monopolize specific commodities by dynamically adjusting their pricing and resource allocation strategies, thereby maximizing profitability without direct human input or explicit collusion commands. These results pose unique challenges and opportunities for businesses looking to integrate AI into strategic roles and for regulatory bodies tasked with maintaining fair and competitive markets. The study provides a foundation for further exploration into the ramifications of deferring high-stakes decisions to LLM-based agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.00031"
    },
    "ccf862826ff453c0e0e149c195f2b7b2": {
        "title": "Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface",
        "authors": [
            "Wenyue Hua",
            "Mengting Wan",
            "Shashank Vadrevu",
            "Ryan Nadel",
            "Yongfeng Zhang",
            "Chi Wang"
        ],
        "date": "2024/09/30",
        "pdf": "http://arxiv.org/pdf/2410.00079",
        "abstract": "Agents, as user-centric tools, are increasingly deployed for human task delegation, assisting with a broad spectrum of requests by generating thoughts, engaging with user proxies, and producing action plans. However, agents based on large language models (LLMs) often face substantial planning latency due to two primary factors: the efficiency limitations of the underlying LLMs due to their large size and high demand, and the structural complexity of the agents due to the extensive generation of intermediate thoughts to produce the final output. Given that inefficiency in service provision can undermine the value of automation for users, this paper presents a human-centered efficient agent planning method -- Interactive Speculative Planning -- aiming at enhancing the efficiency of agent planning through both system design and human-AI interaction. Our approach advocates for the co-design of the agent system and user interface, underscoring the importance of an agent system that can fluidly manage user interactions and interruptions. By integrating human interruptions as a fundamental component of the system, we not only make it more user-centric but also expedite the entire process by leveraging human-in-the-loop interactions to provide accurate intermediate steps. Code and data will be released.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2410.00079"
    },
    "00f198f86f9fde071159deb180597602": {
        "title": "RGD: Multi-LLM Based Agent Debugger via Refinement and Generation Guidance",
        "authors": [
            "Haolin Jin",
            "Zechao Sun",
            "Huaming Chen"
        ],
        "date": "2024/10/02",
        "pdf": "http://arxiv.org/pdf/2410.01242",
        "abstract": "Large Language Models (LLMs) have shown incredible potential in code generation tasks, and recent research in prompt engineering have enhanced LLMs&#39; understanding of textual information. However, ensuring the accuracy of generated code often requires extensive testing and validation by programmers. While LLMs can typically generate code based on task descriptions, their accuracy remains limited, especially for complex tasks that require a deeper understanding of both the problem statement and the code generation process. This limitation is primarily due to the LLMs&#39; need to simultaneously comprehend text and generate syntactically and semantically correct code, without having the capability to automatically refine the code. In real-world software development, programmers rarely produce flawless code in a single attempt based on the task description alone, they rely on iterative feedback and debugging to refine their programs. Inspired by this process, we introduce a novel architecture of LLM-based agents for code generation and automatic debugging: Refinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based agent debugger that leverages three distinct LLM agents-Guide Agent, Debug Agent, and Feedback Agent. RGD decomposes the code generation task into multiple steps, ensuring a clearer workflow and enabling iterative code refinement based on self-reflection and feedback. Experimental results demonstrate that RGD exhibits remarkable code generation capabilities, achieving state-of-the-art performance with a 9.8% improvement on the HumanEval dataset and a 16.2% improvement on the MBPP dataset compared to the state-of-the-art approaches and traditional direct prompting approaches. We highlight the effectiveness of the RGD framework in enhancing LLMs&#39; ability to generate and refine code autonomously.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Role Playing",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.01242"
    },
    "702b7a6ff429aef93b552f6213323d2b": {
        "title": "Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics",
        "authors": [
            "Yuan Zhou",
            "Peng Zhang",
            "Mengya Song",
            "Alice Zheng",
            "Yiwen Lu",
            "Zhiheng Liu",
            "Yong Chen",
            "Zhaohan Xi"
        ],
        "date": "2024/10/02",
        "pdf": "http://arxiv.org/pdf/2410.02026",
        "abstract": "Large language models (LLMs) have demonstrated remarkable progress in healthcare. However, a significant gap remains regarding LLMs&#39; professionalism in domain-specific clinical practices, limiting their application in real-world diagnostics. In this work, we introduce ZODIAC, an LLM-powered framework with cardiologist-level professionalism designed to engage LLMs in cardiological diagnostics. ZODIAC assists cardiologists by extracting clinically relevant characteristics from patient data, detecting significant arrhythmias, and generating preliminary reports for the review and refinement by cardiologists. To achieve cardiologist-level professionalism, ZODIAC is built on a multi-agent collaboration framework, enabling the processing of patient data across multiple modalities. Each LLM agent is fine-tuned using real-world patient data adjudicated by cardiologists, reinforcing the model&#39;s professionalism. ZODIAC undergoes rigorous clinical validation with independent cardiologists, evaluated across eight metrics that measure clinical effectiveness and address security concerns. Results show that ZODIAC outperforms industry-leading models, including OpenAI&#39;s GPT-4o, Meta&#39;s Llama-3.1-405B, and Google&#39;s Gemini-pro, as well as medical-specialist LLMs like Microsoft&#39;s BioGPT. ZODIAC demonstrates the transformative potential of specialized LLMs in healthcare by delivering domain-specific solutions that meet the stringent demands of medical practice. Notably, ZODIAC has been successfully integrated into electrocardiography (ECG) devices, exemplifying the growing trend of embedding LLMs into Software-as-Medical-Device (SaMD).",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.02026"
    },
    "65375b2f5dd48b9d6dd2723771e42ac4": {
        "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning with Insights from Multi-Agent Collaboration",
        "authors": [
            "Weikang Yuan",
            "Junjie Cao",
            "Zhuoren Jiang",
            "Yangyang Kang",
            "Jun Lin",
            "Kaisong Song",
            "tianqianjin lin",
            "Pengwei Yan",
            "Changlong Sun",
            "Xiaozhong Liu"
        ],
        "date": "2024/10/03",
        "pdf": "http://arxiv.org/pdf/2410.02507",
        "abstract": "Large Language Models (LLMs) could struggle to fully understand legal theories and perform complex legal reasoning tasks. In this study, we introduce a challenging task (confusing charge prediction) to better evaluate LLMs&#39; understanding of legal theories and reasoning capabilities. We also propose a novel framework: Multi-Agent framework for improving complex Legal Reasoning capability (MALR). MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities. Extensive experiments on multiple real-world datasets demonstrate that the proposed framework effectively addresses complex reasoning issues in practical scenarios, paving the way for more reliable applications in the legal domain.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.02507"
    },
    "adb735cdc179faaf92ca5cd2cb10201f": {
        "title": "ColaCare: Enhancing Electronic Health Record Modeling through Large Language Model-Driven Multi-Agent Collaboration",
        "authors": [
            "Zixiang Wang",
            "Yinghao Zhu",
            "Huiya Zhao",
            "Xiaochen Zheng",
            "Tianlong Wang",
            "Wen Tang",
            "Yasha Wang",
            "Chengwei Pan",
            "Ewen M. Harrison",
            "Junyi Gao",
            "Liantao Ma"
        ],
        "date": "2024/10/03",
        "pdf": "http://arxiv.org/pdf/2410.02551",
        "abstract": "We introduce ColaCare, a framework that enhances Electronic Health Record (EHR) modeling through multi-agent collaboration driven by Large Language Models (LLMs). Our approach seamlessly integrates domain-specific expert models with LLMs to bridge the gap between structured EHR data and text-based reasoning. Inspired by clinical consultations, ColaCare employs two types of agents: DoctorAgent and MetaAgent, which collaboratively analyze patient data. Expert models process and generate predictions from numerical EHR data, while LLM agents produce reasoning references and decision-making reports within the collaborative consultation framework. We additionally incorporate the Merck Manual of Diagnosis and Therapy (MSD) medical guideline within a retrieval-augmented generation (RAG) module for authoritative evidence support. Extensive experiments conducted on four distinct EHR datasets demonstrate ColaCare&#39;s superior performance in mortality prediction tasks, underscoring its potential to revolutionize clinical decision support systems and advance personalized precision medicine. The code, complete prompt templates, more case studies, etc. are publicly available at the anonymous link: https://colacare.netlify.app.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.02551"
    },
    "be615ae6b5e823dc9bbe8ac28a61abcb": {
        "title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML",
        "authors": [
            "Patara Trirat",
            "Wonyong Jeong",
            "Sung Ju Hwang"
        ],
        "date": "2024/10/03",
        "pdf": "http://arxiv.org/pdf/2410.02958",
        "abstract": "Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. AutoML-Agent takes user&#39;s task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.02958"
    },
    "a507a18da0e1f7a87b33eaad34ffc4f8": {
        "title": "ImProver: Agent-Based Automated Proof Optimization",
        "authors": [
            "Riyaz Ahuja",
            "Jeremy Avigad",
            "Prasad Tetali",
            "Sean Welleck"
        ],
        "date": "2024/10/07",
        "pdf": "http://arxiv.org/pdf/2410.04753",
        "abstract": "Large language models (LLMs) have been used to generate formal proofs of mathematical theorems in proofs assistants such as Lean. However, we often want to optimize a formal proof with respect to various criteria, depending on its downstream use. For example, we may want a proof to adhere to a certain style, or to be readable, concise, or modularly structured. Having suitably optimized proofs is also important for learning tasks, especially since human-written proofs may not optimal for that purpose. To this end, we study a new problem of automated proof optimization: rewriting a proof so that it is correct and optimizes for an arbitrary criterion, such as length or readability. As a first method for automated proof optimization, we present ImProver, a large-language-model agent that rewrites proofs to optimize arbitrary user-defined metrics in Lean. We find that naively applying LLMs to proof optimization falls short, and we incorporate various improvements into ImProver, such as the use of symbolic Lean context in a novel Chain-of-States technique, as well as error-correction and retrieval. We test ImProver on rewriting real-world undergraduate, competition, and research-level mathematics theorems, finding that ImProver is capable of rewriting proofs so that they are substantially shorter, more modular, and more readable.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.04753"
    },
    "d139107db4efb901300c197856eff532": {
        "title": "Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents",
        "authors": [
            "Boyu Gou",
            "Ruohan Wang",
            "Boyuan Zheng",
            "Yanan Xie",
            "Cheng Chang",
            "Yiheng Shu",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2024/10/07",
        "pdf": "http://arxiv.org/pdf/2410.05243",
        "abstract": "Multimodal large language models (MLLMs) are transforming the capabilities of graphical user interface (GUI) agents, facilitating their transition from controlled simulations to complex, real-world applications across various platforms. However, the effectiveness of these agents hinges on the robustness of their grounding capability. Current GUI agents predominantly utilize text-based representations such as HTML or accessibility trees, which, despite their utility, often introduce noise, incompleteness, and increased computational overhead. In this paper, we advocate a human-like embodiment for GUI agents that perceive the environment entirely visually and directly take pixel-level operations on the GUI. The key is visual grounding models that can accurately map diverse referring expressions of GUI elements to their coordinates on the GUI across different platforms. We show that a simple recipe, which includes web-based synthetic data and slight adaptation of the LLaVA architecture, is surprisingly effective for training such visual grounding models. We collect the largest dataset for GUI visual grounding so far, containing 10M GUI elements and their referring expressions over 1.3M screenshots, and use it to train UGround, a strong universal visual grounding model for GUI agents. Empirical results on six benchmarks spanning three categories (grounding, offline agent, and online agent) show that 1) UGround substantially outperforms existing visual grounding models for GUI agents, by up to 20% absolute, and 2) agents with UGround outperform state-of-the-art agents, despite the fact that existing agents use additional text-based input while ours only uses visual perception. These results provide strong support for the feasibility and promises of GUI agents that navigate the digital world as humans do.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.05243"
    },
    "18196a41f8dad04a4337d053ce95969b": {
        "title": "Seeker: Enhancing Exception Handling in Code with LLM-based Multi-Agent Approach",
        "authors": [
            "Xuanming Zhang",
            "Yuxuan Chen",
            "Yuan Yuan",
            "Minlie Huang"
        ],
        "date": "2024/10/09",
        "pdf": "http://arxiv.org/pdf/2410.06949",
        "abstract": "In real world software development, improper or missing exception handling can severely impact the robustness and reliability of code. Exception handling mechanisms require developers to detect, capture, and manage exceptions according to high standards, but many developers struggle with these tasks, leading to fragile code. This problem is particularly evident in open source projects and impacts the overall quality of the software ecosystem. To address this challenge, we explore the use of large language models (LLMs) to improve exception handling in code. Through extensive analysis, we identify three key issues: Insensitive Detection of Fragile Code, Inaccurate Capture of Exception Types, and Distorted Handling Solutions. These problems are widespread across real world repositories, suggesting that robust exception handling practices are often overlooked or mishandled. In response, we propose Seeker, a multi agent framework inspired by expert developer strategies for exception handling. Seeker uses agents: Scanner, Detector, Predator, Ranker, and Handler to assist LLMs in detecting, capturing, and resolving exceptions more effectively. Our work is the first systematic study on leveraging LLMs to enhance exception handling practices, providing valuable insights for future improvements in code reliability.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.06949"
    },
    "9aeaf1b0911133b749361a0974f7292f": {
        "title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human",
        "authors": [
            "Saaket Agashe",
            "Jiuzhou Han",
            "Shuyu Gan",
            "Jiachen Yang",
            "Ang Li",
            "Xin Eric Wang"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.08164",
        "abstract": "We present Agent S, an open agentic framework that enables autonomous interaction with computers through a Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S aims to address three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code available at https://github.com/simular-ai/Agent-S.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.08164"
    },
    "61155d0fbcb2d00d2fdbc1637d779235": {
        "title": "Agents Thinking Fast and Slow: A Talker-Reasoner Architecture",
        "authors": [
            "Konstantina Christakopoulou",
            "Shibl Mourad",
            "Maja Matarić"
        ],
        "date": "2024/10/10",
        "pdf": "http://arxiv.org/pdf/2410.08328",
        "abstract": "Large language models have enabled agents of all kinds to interact with users through natural conversation. Consequently, agents now have two jobs: conversing and planning/reasoning. Their conversational responses must be informed by all available information, and their actions must help to achieve goals. This dichotomy between conversing with the user and doing multi-step reasoning and planning can be seen as analogous to the human systems of &#34;thinking fast and slow&#34; as introduced by Kahneman. Our approach is comprised of a &#34;Talker&#34; agent (System 1) that is fast and intuitive, and tasked with synthesizing the conversational response; and a &#34;Reasoner&#34; agent (System 2) that is slower, more deliberative, and more logical, and is tasked with multi-step reasoning and planning, calling tools, performing actions in the world, and thereby producing the new agent state. We describe the new Talker-Reasoner architecture and discuss its advantages, including modularity and decreased latency. We ground the discussion in the context of a sleep coaching agent, in order to demonstrate real-world relevance.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2410.08328"
    },
    "c3f84cceb3905b994d7489dd1df7a301": {
        "title": "Words as Beacons: Guiding RL Agents with High-Level Language Prompts",
        "authors": [
            "Unai Ruiz-Gonzalez",
            "Alain Andres",
            "Pedro G. Bascoy",
            "Javier Del Ser"
        ],
        "date": "2024/10/11",
        "pdf": "http://arxiv.org/pdf/2410.08632",
        "abstract": "Sparse reward environments in reinforcement learning (RL) pose significant challenges for exploration, often leading to inefficient or incomplete learning processes. To tackle this issue, this work proposes a teacher-student RL framework that leverages Large Language Models (LLMs) as &#34;teachers&#34; to guide the agent&#39;s learning process by decomposing complex tasks into subgoals. Due to their inherent capability to understand RL environments based on a textual description of structure and purpose, LLMs can provide subgoals to accomplish the task defined for the environment in a similar fashion to how a human would do. In doing so, three types of subgoals are proposed: positional targets relative to the agent, object representations, and language-based instructions generated directly by the LLM. More importantly, we show that it is possible to query the LLM only during the training phase, enabling agents to operate within the environment without any LLM intervention. We assess the performance of this proposed framework by evaluating three state-of-the-art open-source LLMs (Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally generated environment of the MiniGrid benchmark. Experimental results demonstrate that this curriculum-based approach accelerates learning and enhances exploration in complex tasks, achieving up to 30 to 200 times faster convergence in training steps compared to recent baselines designed for sparse reward environments.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.08632"
    },
    "bb7f8e8e5116d18f0c423441c3a15f7a": {
        "title": "AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents",
        "authors": [
            "Maksym Andriushchenko",
            "Alexandra Souly",
            "Mateusz Dziemian",
            "Derek Duenas",
            "Maxwell Lin",
            "Justin Wang",
            "Dan Hendrycks",
            "Andy Zou",
            "Zico Kolter",
            "Matt Fredrikson",
            "Eric Winsor",
            "Jerome Wynne",
            "Yarin Gal",
            "Xander Davies"
        ],
        "date": "2024/10/11",
        "pdf": "http://arxiv.org/pdf/2410.09024",
        "abstract": "The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. To enable simple and reliable evaluation of attacks and defenses for LLM-based agents, we publicly release AgentHarm at https://huggingface.co/datasets/ai-safety-institute/AgentHarm.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.09024"
    },
    "14e36e094d239496b3ba462c0a4476bd": {
        "title": "PEAR: A Robust and Flexible Automation Framework for Ptychography Enabled by Multiple Large Language Model Agents",
        "authors": [
            "Xiangyu Yin",
            "Chuqiao Shi",
            "Yimo Han",
            "Yi Jiang"
        ],
        "date": "2024/10/11",
        "pdf": "http://arxiv.org/pdf/2410.09034",
        "abstract": "Ptychography is an advanced computational imaging technique in X-ray and electron microscopy. It has been widely adopted across scientific research fields, including physics, chemistry, biology, and materials science, as well as in industrial applications such as semiconductor characterization. In practice, obtaining high-quality ptychographic images requires simultaneous optimization of numerous experimental and algorithmic parameters. Traditionally, parameter selection often relies on trial and error, leading to low-throughput workflows and potential human bias. In this work, we develop the &#34;Ptychographic Experiment and Analysis Robot&#34; (PEAR), a framework that leverages large language models (LLMs) to automate data analysis in ptychography. To ensure high robustness and accuracy, PEAR employs multiple LLM agents for tasks including knowledge retrieval, code generation, parameter recommendation, and image reasoning. Our study demonstrates that PEAR&#39;s multi-agent design significantly improves the workflow success rate, even with smaller open-weight models such as LLaMA 3.1 8B. PEAR also supports various automation levels and is designed to work with customized local knowledge bases, ensuring flexibility and adaptability across different research environments.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.09034"
    },
    "b4f14b73a9d912e317c3abc9942d3821": {
        "title": "Encoding Agent Trajectories as Representations with Sequence Transformers",
        "authors": [
            "Athanasios Tsiligkaridis",
            "Nicholas Kalinowski",
            "Zhongheng Li",
            "Elizabeth Hou"
        ],
        "date": "2024/10/11",
        "pdf": "http://arxiv.org/pdf/2410.09204",
        "abstract": "Spatiotemporal data faces many analogous challenges to natural language text including the ordering of locations (words) in a sequence, long range dependencies between locations, and locations having multiple meanings. In this work, we propose a novel model for representing high dimensional spatiotemporal trajectories as sequences of discrete locations and encoding them with a Transformer-based neural network architecture. Similar to language models, our Sequence Transformer for Agent Representation Encodings (STARE) model can learn representations and structure in trajectory data through both supervisory tasks (e.g., classification), and self-supervisory tasks (e.g., masked modelling). We present experimental results on various synthetic and real trajectory datasets and show that our proposed model can learn meaningful encodings that are useful for many downstream tasks including discriminating between labels and indicating similarity between locations. Using these encodings, we also learn relationships between agents and locations present in spatiotemporal data.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.09204"
    },
    "a9c8d8477d8685d732005dcbf04d6991": {
        "title": "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System",
        "authors": [
            "Haoyang Su",
            "Renqi Chen",
            "Shixiang Tang",
            "Zhenfei Yin",
            "Xinzhe Zheng",
            "Jinzhe Li",
            "Biqing Qi",
            "Qi Wu",
            "Hui Li",
            "Wanli Ouyang",
            "Philip Torr",
            "Bowen Zhou",
            "Nanqing Dong"
        ],
        "date": "2024/10/12",
        "pdf": "http://arxiv.org/pdf/2410.09403",
        "abstract": "The rapid advancement of scientific progress requires innovative tools that can accelerate knowledge discovery. Although recent AI methods, particularly large language models (LLMs), have shown promise in tasks such as hypothesis generation and experimental design, they fall short of replicating the collaborative nature of real-world scientific practices, where diverse experts work together in teams to tackle complex problems. To address the limitations, we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci), designed to mimic the teamwork inherent in scientific research. VirSci organizes a team of agents to collaboratively generate, evaluate, and refine research ideas. Through comprehensive experiments, we demonstrate that this multi-agent approach outperforms the state-of-the-art method in producing novel scientific ideas. We further investigate the collaboration mechanisms that contribute to its tendency to produce ideas with higher novelty, offering valuable insights to guide future research and illuminating pathways toward building a robust system for autonomous scientific discovery. The code is available at https://github.com/open-sciencelab/Virtual-Scientists.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.09403"
    },
    "6d92de44ed609c564af0260e1ae635e2": {
        "title": "AFlow: Automating Agentic Workflow Generation",
        "authors": [
            "Jiayi Zhang",
            "Jinyu Xiang",
            "Zhaoyang Yu",
            "Fengwei Teng",
            "Xionghui Chen",
            "Jiaqi Chen",
            "Mingchen Zhuge",
            "Xin Cheng",
            "Sirui Hong",
            "Jinlin Wang",
            "Bingnan Zheng",
            "Bang Liu",
            "Yuyu Luo",
            "Chenglin Wu"
        ],
        "date": "2024/10/14",
        "pdf": "http://arxiv.org/pdf/2410.10762",
        "abstract": "Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow&#39;s efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code will be available at https://github.com/geekan/MetaGPT.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.10762"
    },
    "791501378570b1b159db1372dc062bc3": {
        "title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs",
        "authors": [
            "Wanying Wang",
            "Zeyu Ma",
            "Pengfei Liu",
            "Mingang Chen"
        ],
        "date": "2024/10/15",
        "pdf": "http://arxiv.org/pdf/2410.11507",
        "abstract": "While various vertical domain large language models (LLMs) have been developed, automatically evaluating their performance across different domains remains a critical challenge. Current benchmark-based methods often rely on static and costly datasets, are misaligned with practical user needs, and lack flexibility across domains. To address these limitations, we revisit the evaluation process and introduce two key concepts: Benchmark+, which extends the traditional question-answer benchmark into a more flexible ``strategy-criterion&#39;&#39; format; and Assessment+, which enhances the interaction process, enabling deeper exploration and supporting analysis from broader perspectives. We propose TestAgent, an agent-based evaluation framework that implements these concepts using retrieval-augmented generation and reinforcement learning. TestAgent enables automatic dynamic benchmark generation and in-depth assessment across diverse vertical domain scenarios. Experiments on tasks ranging from constructing multiple vertical domain evaluations to converting static benchmarks into dynamic forms demonstrate the effectiveness of TestAgent. This work offers an interesting perspective on automatic evaluation for LLMs and highlights a pathway for dynamic and domain-adaptive assessments.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2410.11507"
    },
    "5790fc26e8fd8f1e77d1ad420ee8d70e": {
        "title": "Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance",
        "authors": [
            "Yaxi Lu",
            "Shenzhi Yang",
            "Cheng Qian",
            "Guirong Chen",
            "Qinyu Luo",
            "Yesai Wu",
            "Huadong Wang",
            "Xin Cong",
            "Zhong Zhang",
            "Yankai Lin",
            "Weiwen Liu",
            "Yasheng Wang",
            "Zhiyuan Liu",
            "Fangming Liu",
            "Maosong Sun"
        ],
        "date": "2024/10/16",
        "pdf": "http://arxiv.org/pdf/2410.12361",
        "abstract": "Agents powered by large language models have shown remarkable abilities in solving complex tasks. However, most agent systems remain reactive, limiting their effectiveness in scenarios requiring foresight and autonomous decision-making. In this paper, we tackle the challenge of developing proactive agents capable of anticipating and initiating tasks without explicit human instructions. We propose a novel data-driven approach for this problem. Firstly, we collect real-world human activities to generate proactive task predictions. These predictions are then labeled by human annotators as either accepted or rejected. The labeled data is used to train a reward model that simulates human judgment and serves as an automatic evaluator of the proactiveness of LLM agents. Building on this, we develop a comprehensive data generation pipeline to create a diverse dataset, ProactiveBench, containing 6,790 events. Finally, we demonstrate that fine-tuning models with the proposed ProactiveBench can significantly elicit the proactiveness of LLM agents. Experimental results show that our fine-tuned model achieves an F1-Score of 66.47% in proactively offering assistance, outperforming all open-source and close-source models. These results highlight the potential of our method in creating more proactive and effective agent systems, paving the way for future advancements in human-agent collaboration.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.12361"
    },
    "134686407a054b22ec099ad8eb129450": {
        "title": "PRefLexOR: Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning and Agentic Thinking",
        "authors": [
            "Markus J. Buehler"
        ],
        "date": "2024/10/16",
        "pdf": "http://arxiv.org/pdf/2410.12375",
        "abstract": "PRefLexOR (Preference-based Recursive Language Modeling for Exploratory Optimization of Reasoning) combines preference optimization with concepts from Reinforcement Learning to enable models to self-teach through iterative reasoning improvements. We propose a recursive learning approach that engages the model in multi-step reasoning, revisiting, and refining intermediate steps before producing a final output in training and inference phases. Through multiple training stages, the model first learns to align its reasoning with accurate decision paths by optimizing the log odds between preferred and non-preferred responses. During this process, PRefLexOR builds a dynamic knowledge graph by generating questions from random text chunks and retrieval-augmentation to contextualize relevant details from the entire training corpus. In the second stage, preference optimization enhances model performance by using rejection sampling to fine-tune reasoning quality by continually producing in-situ training data while masking the reasoning steps. Recursive optimization within a thinking token framework introduces iterative feedback loops, where the model refines reasoning, achieving deeper coherence, consistency, and adaptability. Implemented in small language models with only 3 billion parameters, we should that even tiny models can iteratively teach themselves to reason with greater depth and reflectivity. Our implementation is straightforward and can be incorporated into any existing pretrained LLM. We focus our examples on applications in biological materials science and demonstrate the method in a variety of case studies that range from in-domain to cross-domain applications. Using reasoning strategies that include thinking and reflection modalities we build a multi-agent recursive self-improving inference approach to successively improve responses via repeated sampling in inference time.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2410.12375"
    },
    "f63019664674d031bcd07f5831bbaddd": {
        "title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents",
        "authors": [
            "Long Li",
            "Weiwen Xu",
            "Jiayan Guo",
            "Ruochen Zhao",
            "Xingxuan Li",
            "Yuqian Yuan",
            "Boqiang Zhang",
            "Yuming Jiang",
            "Yifei Xin",
            "Ronghao Dang",
            "Deli Zhao",
            "Yu Rong",
            "Tian Feng",
            "Lidong Bing"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.13185",
        "abstract": "Effective research ideation is a critical step for scientific research. However, the exponential increase in scientific literature makes it challenging for researchers to stay current with recent advances and identify meaningful research directions. Recent developments in large language models~(LLMs) suggest a promising avenue for automating the generation of novel research ideas. However, existing methods for idea generation either trivially prompt LLMs or directly expose LLMs to extensive literature without indicating useful information. Inspired by the research process of human researchers, we propose a Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant literature in a chain structure to effectively mirror the progressive development in a research domain. This organization facilitates LLMs to capture the current advancements in research, thereby enhancing their ideation capabilities. Furthermore, we propose Idea Arena, an evaluation protocol that can comprehensively evaluate idea generation methods from different perspectives, aligning closely with the preferences of human researchers. Experimental results indicate that the CoI agent consistently outperforms other methods and shows comparable quality as humans in research idea generation. Moreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to generate a candidate idea and its corresponding experimental design.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.13185"
    },
    "9e0096ffdb93988367283d0f9c7b14e7": {
        "title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling",
        "authors": [
            "Yakun Zhu",
            "Shaohang Wei",
            "Xu Wang",
            "Kui Xue",
            "Xiaofan Zhang",
            "Shaoting Zhang"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.13610",
        "abstract": "Integrating tools into Large Language Models (LLMs) has facilitated the widespread application. Despite this, in specialized downstream task contexts, reliance solely on tools is insufficient to fully address the complexities of the real world. This particularly restricts the effective deployment of LLMs in fields such as medicine. In this paper, we focus on the downstream tasks of medical calculators, which use standardized tests to assess an individual&#39;s health status. We introduce MeNTi, a universal agent architecture for LLMs. MeNTi integrates a specialized medical toolkit and employs meta-tool and nested calling mechanisms to enhance LLM tool utilization. Specifically, it achieves flexible tool selection and nested tool calling to address practical issues faced in intricate medical scenarios, including calculator selection, slot filling, and unit conversion. To assess the capabilities of LLMs for quantitative assessment throughout the clinical process of calculator scenarios, we introduce CalcQA. This benchmark requires LLMs to use medical calculators to perform calculations and assess patient health status. CalcQA is constructed by professional physicians and includes 100 case-calculator pairs, complemented by a toolkit of 281 medical tools. The experimental results demonstrate significant performance improvements with our framework. This research paves new directions for applying LLMs in demanding scenarios of medicine.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.13610"
    },
    "750655a092b8d5584696764fa1550679": {
        "title": "MobA: A Two-Level Agent System for Efficient Mobile Task Automation",
        "authors": [
            "Zichen Zhu",
            "Hao Tang",
            "Yansi Li",
            "Kunyao Lan",
            "Yixuan Jiang",
            "Hao Zhou",
            "Yixiao Wang",
            "Situo Zhang",
            "Liangtai Sun",
            "Lu Chen",
            "Kai Yu"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.13757",
        "abstract": "Current mobile assistants are limited by dependence on system APIs or struggle with complex user instructions and diverse interfaces due to restricted comprehension and decision-making abilities. To address these challenges, we propose MobA, a novel Mobile phone Agent powered by multimodal large language models that enhances comprehension and planning capabilities through a sophisticated two-level agent architecture. The high-level Global Agent (GA) is responsible for understanding user commands, tracking history memories, and planning tasks. The low-level Local Agent (LA) predicts detailed actions in the form of function calls, guided by sub-tasks and memory from the GA. Integrating a Reflection Module allows for efficient task completion and enables the system to handle previously unseen complex tasks. MobA demonstrates significant improvements in task execution efficiency and completion rate in real-life evaluations, underscoring the potential of MLLM-empowered mobile assistants.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2410.13757"
    },
    "9133da370435d5f1ad7e8a369d38d026": {
        "title": "AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents",
        "authors": [
            "Ke Yang",
            "Yao Liu",
            "Sapana Chaudhary",
            "Rasool Fakoor",
            "Pratik Chaudhari",
            "George Karypis",
            "Huzefa Rangwala"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.13825",
        "abstract": "Autonomy via agents using large language models (LLMs) for personalized, standardized tasks boosts human efficiency. Automating web tasks (like booking hotels within a budget) is increasingly sought after. Fulfilling practical needs, the web agent also serves as an important proof-of-concept example for various agent grounding scenarios, with its success promising advancements in many future applications. Prior research often handcrafts web agent strategies (e.g., prompting templates, multi-agent systems, search methods, etc.) and the corresponding in-context examples, which may not generalize well across all real-world scenarios. On the other hand, there has been limited study on the misalignment between a web agent&#39;s observation/action representation and the pre-training data of the LLM it&#39;s based on. This discrepancy is especially notable when LLMs are primarily trained for language completion rather than tasks involving embodied navigation actions and symbolic web elements. Our study enhances an LLM-based web agent by simply refining its observation and action space to better align with the LLM&#39;s capabilities. This approach enables our base agent to significantly outperform previous methods on a wide variety of web tasks. Specifically, on WebArena, a benchmark featuring general-purpose web interaction tasks, our agent AgentOccam surpasses the previous state-of-the-art and concurrent work by 9.8 (+29.4%) and 5.9 (+15.8%) absolute points respectively, and boosts the success rate by 26.6 points (+161%) over similar plain web agents with its observation and action space alignment. We achieve this without using in-context examples, new agent roles, online feedback or search strategies. AgentOccam&#39;s simple design highlights LLMs&#39; impressive zero-shot performance on web tasks, and underlines the critical role of carefully tuning observation and action spaces for LLM-based agents.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.13825"
    },
    "0db4f70b14ed06d0572ab72186413114": {
        "title": "From Barriers to Tactics: A Behavioral Science-Informed Agentic Workflow for Personalized Nutrition Coaching",
        "authors": [
            "Eric Yang",
            "Tomas Garcia",
            "Hannah Williams",
            "Bhawesh Kumar",
            "Martin Ramé",
            "Eileen Rivera",
            "Yiran Ma",
            "Jonathan Amar",
            "Caricia Catalani",
            "Yugang Jia"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.14041",
        "abstract": "Effective management of cardiometabolic conditions requires sustained positive nutrition habits, often hindered by complex and individualized barriers. Direct human management is simply not scalable, while previous attempts aimed at automating nutrition coaching lack the personalization needed to address these diverse challenges. This paper introduces a novel LLM-powered agentic workflow designed to provide personalized nutrition coaching by directly targeting and mitigating patient-specific barriers. Grounded in behavioral science principles, the workflow leverages a comprehensive mapping of nutrition-related barriers to corresponding evidence-based strategies. A specialized LLM agent intentionally probes for and identifies the root cause of a patient&#39;s dietary struggles. Subsequently, a separate LLM agent delivers tailored tactics designed to overcome those specific barriers with patient context. We designed and validated our approach through a user study with individuals with cardiometabolic conditions, demonstrating the system&#39;s ability to accurately identify barriers and provide personalized guidance. Furthermore, we conducted a large-scale simulation study, grounding on real patient vignettes and expert-validated metrics, to evaluate the system&#39;s performance across a wide range of scenarios. Our findings demonstrate the potential of this LLM-powered agentic workflow to improve nutrition coaching by providing personalized, scalable, and behaviorally-informed interventions.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.14041"
    },
    "7152187cedc10c9899fa906acfc82d75": {
        "title": "Coherence-Driven Multimodal Safety Dialogue with Active Learning for Embodied Agents",
        "authors": [
            "Sabit Hassan",
            "Hye-Young Chung",
            "Xiang Zhi Tan",
            "Malihe Alikhani"
        ],
        "date": "2024/10/18",
        "pdf": "http://arxiv.org/pdf/2410.14141",
        "abstract": "When assisting people in daily tasks, robots need to accurately interpret visual cues and respond effectively in diverse safety-critical situations, such as sharp objects on the floor. In this context, we present M-CoDAL, a multimodal-dialogue system specifically designed for embodied agents to better understand and communicate in safety-critical situations. The system leverages discourse coherence relations to enhance its contextual understanding and communication abilities. To train this system, we introduce a novel clustering-based active learning mechanism that utilizes an external Large Language Model (LLM) to identify informative instances. Our approach is evaluated using a newly created multimodal dataset comprising 1K safety violations extracted from 2K Reddit images. These violations are annotated using a Large Multimodal Model (LMM) and verified by human annotators. Results with this dataset demonstrate that our approach improves resolution of safety situations, user sentiment, as well as safety of the conversation. Next, we deploy our dialogue system on a Hello Robot Stretch robot and conduct a within-subject user study with real-world participants. In the study, participants role-play two safety scenarios with different levels of severity with the robot and receive interventions from our model and a baseline system powered by OpenAI&#39;s ChatGPT. The study results corroborate and extend the findings from automated evaluation, showing that our proposed system is more persuasive and competent in a real-world embodied agent setting.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.14141"
    },
    "9fb4a22969035c98975512cc489bb6d2": {
        "title": "Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation",
        "authors": [
            "Shuo Tang",
            "Xianghe Pang",
            "Zexi Liu",
            "Bohan Tang",
            "Rui Ye",
            "Tian Jin",
            "Xiaowen Dong",
            "Yanfeng Wang",
            "Siheng Chen"
        ],
        "date": "2024/10/18",
        "pdf": "http://arxiv.org/pdf/2410.14251",
        "abstract": "Post-training is essential for enabling large language models (LLMs) to follow human instructions. However, its effectiveness depends on high-quality instruction data, which is challenging to obtain in the real world due to privacy concerns, data scarcity, and high annotation costs. To fill this gap, inspired by the recent success of using LLMs to simulate human society, we propose MATRIX, a multi-agent simulator that automatically generates diverse text-based scenarios, capturing a wide range of real-world human needs in a realistic and scalable manner. Leveraging these outputs, we introduce a novel scenario-driven instruction generator MATRIX-Gen for controllable and highly realistic data synthesis. Extensive experiments demonstrate that our framework effectively generates both general and domain-specific data. On AlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on datasets synthesized by MATRIX-Gen with just 20K instruction-response pairs, outperforms Meta&#39;s Llama-3-8B-Instruct model, which was trained on over 10M pairs.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.14251"
    },
    "b5944db3a32545c6d05a389e89b8609e": {
        "title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning",
        "authors": [
            "Yizhou Chi",
            "Yizhang Lin",
            "Sirui Hong",
            "Duyi Pan",
            "Yaying Fei",
            "Guanghao Mei",
            "Bangbang Liu",
            "Tianqi Pang",
            "Jacky Kwok",
            "Ceyao Zhang",
            "Bang Liu",
            "Chenglin Wu"
        ],
        "date": "2024/10/22",
        "pdf": "http://arxiv.org/pdf/2410.17238",
        "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-Search Enhanced LLM Agents (SELA), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space. This novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.17238"
    },
    "5e376cf1a16e6af869b51f2242916e10": {
        "title": "AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents",
        "authors": [
            "Chejian Xu",
            "Mintong Kang",
            "Jiawei Zhang",
            "Zeyi Liao",
            "Lingbo Mo",
            "Mengqi Yuan",
            "Huan Sun",
            "Bo Li"
        ],
        "date": "2024/10/22",
        "pdf": "http://arxiv.org/pdf/2410.17401",
        "abstract": "Vision Language Models (VLMs) have revolutionized the creation of generalist web agents, empowering them to autonomously complete diverse tasks on real-world websites, thereby boosting human efficiency and productivity. However, despite their remarkable capabilities, the safety and security of these agents against malicious attacks remain critically underexplored, raising significant concerns about their safe deployment. To uncover and exploit such vulnerabilities in web agents, we provide AdvWeb, a novel black-box attack framework designed against web agents. AdvWeb trains an adversarial prompter model that generates and injects adversarial prompts into web pages, misleading web agents into executing targeted adversarial actions such as inappropriate stock purchases or incorrect bank transactions, actions that could lead to severe real-world consequences. With only black-box access to the web agent, we train and optimize the adversarial prompter model using DPO, leveraging both successful and failed attack strings against the target agent. Unlike prior approaches, our adversarial string injection maintains stealth and control: (1) the appearance of the website remains unchanged before and after the attack, making it nearly impossible for users to detect tampering, and (2) attackers can modify specific substrings within the generated adversarial string to seamlessly change the attack objective (e.g., purchasing stocks from a different company), enhancing attack flexibility and efficiency. We conduct extensive evaluations, demonstrating that AdvWeb achieves high success rates in attacking SOTA GPT-4V-based VLM agent across various web tasks. Our findings expose critical vulnerabilities in current LLM/VLM-based agents, emphasizing the urgent need for developing more reliable web agents and effective defenses. Our code and data are available at https://ai-secure.github.io/AdvWeb/ .",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.17401"
    },
    "755b1f3f54430a392b52bd1161dbb9c7": {
        "title": "Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation",
        "authors": [
            "Minhua Lin",
            "Zhengzhang Chen",
            "Yanchi Liu",
            "Xujiang Zhao",
            "Zongyu Wu",
            "Junxiang Wang",
            "Xiang Zhang",
            "Suhang Wang",
            "Haifeng Chen"
        ],
        "date": "2024/10/22",
        "pdf": "http://arxiv.org/pdf/2410.17462",
        "abstract": "Time series data is ubiquitous across various domains, including manufacturing, finance, and healthcare. High-quality annotations are essential for effectively understanding time series and facilitating downstream tasks; however, obtaining such annotations is challenging, particularly in mission-critical domains. In this paper, we propose TESSA, a multi-agent system designed to automatically generate both general and domain-specific annotations for time series data. TESSA introduces two agents: a general annotation agent and a domain-specific annotation agent. The general agent captures common patterns and knowledge across multiple source domains, leveraging both time-series-wise and text-wise features to generate general annotations. Meanwhile, the domain-specific agent utilizes limited annotations from the target domain to learn domain-specific terminology and generate targeted annotations. Extensive experiments on multiple synthetic and real-world datasets demonstrate that TESSA effectively generates high-quality annotations, outperforming existing methods.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.17462"
    },
    "6ac76cc87e780b174e4ceed8031e8914": {
        "title": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Control",
        "authors": [
            "Juyong Lee",
            "Dongyoon Hahm",
            "June Suk Choi",
            "W. Bradley Knox",
            "Kimin Lee"
        ],
        "date": "2024/10/23",
        "pdf": "http://arxiv.org/pdf/2410.17520",
        "abstract": "Autonomous agents powered by large language models (LLMs) show promising potential in assistive tasks across various domains, including mobile device control. As these agents interact directly with personal information and device settings, ensuring their safe and reliable behavior is crucial to prevent undesirable outcomes. However, no benchmark exists for standardized evaluation of the safety of mobile device-control agents. In this work, we introduce MobileSafetyBench, a benchmark designed to evaluate the safety of device-control agents within a realistic mobile environment based on Android emulators. We develop a diverse set of tasks involving interactions with various mobile applications, including messaging and banking applications, challenging agents with managing risks encompassing misuse and negative side effects. These tasks include tests to evaluate the safety of agents in daily scenarios as well as their robustness against indirect prompt injection attacks. Our experiments demonstrate that baseline agents, based on state-of-the-art LLMs, often fail to effectively prevent harm while performing the tasks. To mitigate these safety concerns, we propose a prompting method that encourages agents to prioritize safety considerations. While this method shows promise in promoting safer behaviors, there is still considerable room for improvement to fully earn user trust. This highlights the urgent need for continued research to develop more robust safety mechanisms in mobile environments. We open-source our benchmark at: https://mobilesafetybench.github.io/.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2410.17520"
    },
    "a74dfeda83a1eca851e8c597336f546a": {
        "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration",
        "authors": [
            "Xin Li",
            "Qizhi Chu",
            "Yubin Chen",
            "Yang Liu",
            "Yaoqi Liu",
            "Zekai Yu",
            "Weize Chen",
            "Chen Qian",
            "Chuan Shi",
            "Cheng Yang"
        ],
        "date": "2024/10/23",
        "pdf": "http://arxiv.org/pdf/2410.18032",
        "abstract": "Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs&#39; internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.18032"
    },
    "754c1b4855932796972207a2d1a2fde3": {
        "title": "An LLM Agent for Automatic Geospatial Data Analysis",
        "authors": [
            "Yuxing Chen",
            "Weijie Wang",
            "Sylvain Lobry",
            "Camille Kurtz"
        ],
        "date": "2024/10/24",
        "pdf": "http://arxiv.org/pdf/2410.18792",
        "abstract": "Large language models (LLMs) are being used in data science code generation tasks, but they often struggle with complex sequential tasks, leading to logical errors. Their application to geospatial data processing is particularly challenging due to difficulties in incorporating complex data structures and spatial constraints, effectively utilizing diverse function calls, and the tendency to hallucinate less-used geospatial libraries. To tackle these problems, we introduce GeoAgent, a new interactive framework designed to help LLMs handle geospatial data processing more effectively. GeoAgent pioneers the integration of a code interpreter, static analysis, and Retrieval-Augmented Generation (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm, offering a novel approach to geospatial data processing. In addition, we contribute a new benchmark specifically designed to evaluate the LLM-based approach in geospatial tasks. This benchmark leverages a variety of Python libraries and includes both single-turn and multi-turn tasks such as data acquisition, data analysis, and visualization. By offering a comprehensive evaluation among diverse geospatial contexts, this benchmark sets a new standard for developing LLM-based approaches in geospatial data analysis tasks. Our findings suggest that relying solely on knowledge of LLM is insufficient for accurate geospatial task programming, which requires coherent multi-step processes and multiple function calls. Compared to the baseline LLMs, the proposed GeoAgent has demonstrated superior performance, yielding notable improvements in function calls and task completion. In addition, these results offer valuable insights for the future development of LLM agents in automatic geospatial data analysis task programming.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.18792"
    },
    "0cc393b50ae9cb9f69c20b0451475020": {
        "title": "Schema-Guided Culture-Aware Complex Event Simulation with Multi-Agent Role-Play",
        "authors": [
            "Sha Li",
            "Revanth Gangi Reddy",
            "Khanh Duy Nguyen",
            "Qingyun Wang",
            "May Fung",
            "Chi Han",
            "Jiawei Han",
            "Kartik Natarajan",
            "Clare R. Voss",
            "Heng Ji"
        ],
        "date": "2024/10/24",
        "pdf": "http://arxiv.org/pdf/2410.18935",
        "abstract": "Complex news events, such as natural disasters and socio-political conflicts, require swift responses from the government and society. Relying on historical events to project the future is insufficient as such events are sparse and do not cover all possible conditions and nuanced situations. Simulation of these complex events can help better prepare and reduce the negative impact. We develop a controllable complex news event simulator guided by both the event schema representing domain knowledge about the scenario and user-provided assumptions representing case-specific conditions. As event dynamics depend on the fine-grained social and cultural context, we further introduce a geo-diverse commonsense and cultural norm-aware knowledge enhancement component. To enhance the coherence of the simulation, apart from the global timeline of events, we take an agent-based approach to simulate the individual character states, plans, and actions. By incorporating the schema and cultural norms, our generated simulations achieve much higher coherence and appropriateness and are received favorably by participants from a humanitarian assistance organization.",
        "code": "",
        "category": [
            "Role Playing",
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.18935"
    },
    "04dd7ba6a07a1a76773e1f058af0155b": {
        "title": "Infogent: An Agent-Based Framework for Web Information Aggregation",
        "authors": [
            "Revanth Gangi Reddy",
            "Sagnik Mukherjee",
            "Jeonghwan Kim",
            "Zhenhailong Wang",
            "Dilek Hakkani-Tur",
            "Heng Ji"
        ],
        "date": "2024/10/24",
        "pdf": "http://arxiv.org/pdf/2410.19054",
        "abstract": "Despite seemingly performant web agents on the task-completion benchmarks, most existing methods evaluate the agents based on a presupposition: the web navigation task consists of linear sequence of actions with an end state that marks task completion. In contrast, our work focuses on web navigation for information aggregation, wherein the agent must explore different websites to gather information for a complex query. We consider web information aggregation from two different perspectives: (i) Direct API-driven Access relies on a text-only view of the Web, leveraging external tools such as Google Search API to navigate the web and a scraper to extract website contents. (ii) Interactive Visual Access uses screenshots of the webpages and requires interaction with the browser to navigate and access information. Motivated by these diverse information access settings, we introduce Infogent, a novel modular framework for web information aggregation involving three distinct components: Navigator, Extractor and Aggregator. Experiments on different information access settings demonstrate Infogent beats an existing SOTA multi-agent search framework by 7% under Direct API-Driven Access on FRAMES, and improves over an existing information-seeking web agent by 4.3% under Interactive Visual Access on AssistantBench.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.19054"
    },
    "ef3801db57607ba7b38a23ae91dc4ff0": {
        "title": "ControlAgent: Automating Control System Design via Novel Integration of LLM Agents and Domain Expertise",
        "authors": [
            "Xingang Guo",
            "Darioush Keivan",
            "Usman Syed",
            "Lianhui Qin",
            "Huan Zhang",
            "Geir Dullerud",
            "Peter Seiler",
            "Bin Hu"
        ],
        "date": "2024/10/17",
        "pdf": "http://arxiv.org/pdf/2410.19811",
        "abstract": "Control system design is a crucial aspect of modern engineering with far-reaching applications across diverse sectors including aerospace, automotive systems, power grids, and robotics. Despite advances made by Large Language Models (LLMs) in various domains, their application in control system design remains limited due to the complexity and specificity of control theory. To bridge this gap, we introduce ControlAgent, a new paradigm that automates control system design via novel integration of LLM agents and control-oriented domain expertise. ControlAgent encodes expert control knowledge and emulates human iterative design processes by gradually tuning controller parameters to meet user-specified requirements for stability, performance, and robustness. ControlAgent integrates multiple collaborative LLM agents, including a central agent responsible for task distribution and task-specific agents dedicated to detailed controller design for various types of systems and requirements. ControlAgent also employs a Python computation agent that performs complex calculations and controller evaluations based on standard design information provided by task-specified LLM agents. Combined with a history and feedback module, the task-specific LLM agents iteratively refine controller parameters based on real-time feedback from prior designs. Overall, ControlAgent mimics the design processes used by (human) practicing engineers, but removes all the human efforts and can be run in a fully automated way to give end-to-end solutions for control system design with user-specified requirements. To validate ControlAgent&#39;s effectiveness, we develop ControlEval, an evaluation dataset that comprises 500 control tasks with various specific design goals. The effectiveness of ControlAgent is demonstrated via extensive comparative evaluations between LLM-based and traditional human-involved toolbox-based baselines.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.19811"
    },
    "48155878cc2ed9a5e204b5f705296ff0": {
        "title": "AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions",
        "authors": [
            "Ziming Li",
            "Qianbo Zang",
            "David Ma",
            "Jiawei Guo",
            "Tuney Zheng",
            "Minghao Liu",
            "Xinyao Niu",
            "Yue Wang",
            "Jian Yang",
            "Jiaheng Liu",
            "Wanjun Zhong",
            "Wangchunshu Zhou",
            "Wenhao Huang",
            "Ge Zhang"
        ],
        "date": "2024/10/27",
        "pdf": "http://arxiv.org/pdf/2410.20424",
        "abstract": "Data science tasks involving tabular data present complex challenges that require sophisticated problem-solving approaches. We propose AutoKaggle, a powerful and user-centric framework that assists data scientists in completing daily data pipelines through a collaborative multi-agent system. AutoKaggle implements an iterative development process that combines code execution, debugging, and comprehensive unit testing to ensure code correctness and logic consistency. The framework offers highly customizable workflows, allowing users to intervene at each phase, thus integrating automated intelligence with human expertise. Our universal data science toolkit, comprising validated functions for data cleaning, feature engineering, and modeling, forms the foundation of this solution, enhancing productivity by streamlining common tasks. We selected 8 Kaggle competitions to simulate data processing workflows in real-world application scenarios. Evaluation results demonstrate that AutoKaggle achieves a validation submission rate of 0.85 and a comprehensive score of 0.82 in typical data science pipelines, fully proving its effectiveness and practicality in handling complex data science tasks.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.20424"
    },
    "e34208346b53d465040b4d4629500b75": {
        "title": "Guide-LLM: An Embodied LLM Agent and Text-Based Topological Map for Robotic Guidance of People with Visual Impairments",
        "authors": [
            "Sangmim Song",
            "Sarath Kodagoda",
            "Amal Gunatilake",
            "Marc G. Carmichael",
            "Karthick Thiyagarajan",
            "Jodi Martin"
        ],
        "date": "2024/10/28",
        "pdf": "http://arxiv.org/pdf/2410.20666",
        "abstract": "Navigation presents a significant challenge for persons with visual impairments (PVI). While traditional aids such as white canes and guide dogs are invaluable, they fall short in delivering detailed spatial information and precise guidance to desired locations. Recent developments in large language models (LLMs) and vision-language models (VLMs) offer new avenues for enhancing assistive navigation. In this paper, we introduce Guide-LLM, an embodied LLM-based agent designed to assist PVI in navigating large indoor environments. Our approach features a novel text-based topological map that enables the LLM to plan global paths using a simplified environmental representation, focusing on straight paths and right-angle turns to facilitate navigation. Additionally, we utilize the LLM&#39;s commonsense reasoning for hazard detection and personalized path planning based on user preferences. Simulated experiments demonstrate the system&#39;s efficacy in guiding PVI, underscoring its potential as a significant advancement in assistive technology. The results highlight Guide-LLM&#39;s ability to offer efficient, adaptive, and personalized navigation assistance, pointing to promising advancements in this field.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.20666"
    },
    "bd58f4ac5680ddf73fefc5fdf8f4827c": {
        "title": "$\\texttt{PatentAgent}$: Intelligent Agent for Automated Pharmaceutical Patent Analysis",
        "authors": [
            "Xin Wang",
            "Yifan Zhang",
            "Xiaojing Zhang",
            "Longhui Yu",
            "Xinna Lin",
            "Jindong Jiang",
            "Bin Ma",
            "Kaicheng Yu"
        ],
        "date": "2024/10/25",
        "pdf": "http://arxiv.org/pdf/2410.21312",
        "abstract": "Pharmaceutical patents play a vital role in biochemical industries, especially in drug discovery, providing researchers with unique early access to data, experimental results, and research insights. With the advancement of machine learning, patent analysis has evolved from manual labor to tasks assisted by automatic tools. However, there still lacks an unified agent that assists every aspect of patent analysis, from patent reading to core chemical identification. Leveraging the capabilities of Large Language Models (LLMs) to understand requests and follow instructions, we introduce the $\\textbf{first}$ intelligent agent in this domain, $\\texttt{PatentAgent}$, poised to advance and potentially revolutionize the landscape of pharmaceutical research. $\\texttt{PatentAgent}$ comprises three key end-to-end modules -- $\\textit{PA-QA}$, $\\textit{PA-Img2Mol}$, and $\\textit{PA-CoreId}$ -- that respectively perform (1) patent question-answering, (2) image-to-molecular-structure conversion, and (3) core chemical structure identification, addressing the essential needs of scientists and practitioners in pharmaceutical patent analysis. Each module of $\\texttt{PatentAgent}$ demonstrates significant effectiveness with the updated algorithm and the synergistic design of $\\texttt{PatentAgent}$ framework. $\\textit{PA-Img2Mol}$ outperforms existing methods across CLEF, JPO, UOB, and USPTO patent benchmarks with an accuracy gain between 2.46% and 8.37% while $\\textit{PA-CoreId}$ realizes accuracy improvement ranging from 7.15% to 7.62% on PatentNetML benchmark. Our code and dataset will be publicly available.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.21312"
    },
    "6171d08a4b6ca52c9cbcb1db304e40fb": {
        "title": "MARCO: Multi-Agent Real-time Chat Orchestration",
        "authors": [
            "Anubhav Shrimal",
            "Stanley Kanagaraj",
            "Kriti Biswas",
            "Swarnalatha Raghuraman",
            "Anish Nediyanchath",
            "Yi Zhang",
            "Promod Yenigalla"
        ],
        "date": "2024/10/29",
        "pdf": "http://arxiv.org/pdf/2410.21784",
        "abstract": "Large language model advancements have enabled the development of multi-agent frameworks to tackle complex, real-world problems such as to automate tasks that require interactions with diverse tools, reasoning, and human collaboration. We present MARCO, a Multi-Agent Real-time Chat Orchestration framework for automating tasks using LLMs. MARCO addresses key challenges in utilizing LLMs for complex, multi-step task execution. It incorporates robust guardrails to steer LLM behavior, validate outputs, and recover from errors that stem from inconsistent output formatting, function and parameter hallucination, and lack of domain knowledge. Through extensive experiments we demonstrate MARCO&#39;s superior performance with 94.48% and 92.74% accuracy on task execution for Digital Restaurant Service Platform conversations and Retail conversations datasets respectively along with 44.91% improved latency and 33.71% cost reduction. We also report effects of guardrails in performance gain along with comparisons of various LLM models, both open-source and proprietary. The modular and generic design of MARCO allows it to be adapted for automating tasks across domains and to execute complex usecases through multi-turn interactions.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2410.21784"
    },
    "bb6f1317381adf0bf6d3cc5806b230cd": {
        "title": "ADAM: An Embodied Causal Agent in Open-World Environments",
        "authors": [
            "Shu Yu",
            "Chaochao Lu"
        ],
        "date": "2024/10/29",
        "pdf": "http://arxiv.org/pdf/2410.22194",
        "abstract": "In open-world environments like Minecraft, existing agents face challenges in continuously learning structured knowledge, particularly causality. These challenges stem from the opacity inherent in black-box models and an excessive reliance on prior knowledge during training, which impair their interpretability and generalization capability. To this end, we introduce ADAM, An emboDied causal Agent in Minecraft, that can autonomously navigate the open world, perceive multimodal contexts, learn causal world knowledge, and tackle complex tasks through lifelong learning. ADAM is empowered by four key components: 1) an interaction module, enabling the agent to execute actions while documenting the interaction processes; 2) a causal model module, tasked with constructing an ever-growing causal graph from scratch, which enhances interpretability and diminishes reliance on prior knowledge; 3) a controller module, comprising a planner, an actor, and a memory pool, which uses the learned causal graph to accomplish tasks; 4) a perception module, powered by multimodal large language models, which enables ADAM to perceive like a human player. Extensive experiments show that ADAM constructs an almost perfect causal graph from scratch, enabling efficient task decomposition and execution with strong interpretability. Notably, in our modified Minecraft games where no prior knowledge is available, ADAM maintains its performance and shows remarkable robustness and generalization capability. ADAM pioneers a novel paradigm that integrates causal methods and embodied agents in a synergistic manner. Our project page is at https://opencausalab.github.io/ADAM.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2410.22194"
    },
    "bb312ac50b54729c4d6bd96e0da9bead": {
        "title": "The Ability of Large Language Models to Evaluate Constraint-satisfaction in Agent Responses to Open-ended Requests",
        "authors": [
            "Lior Madmoni",
            "Amir Zait",
            "Ilia Labzovsky",
            "Danny Karmon"
        ],
        "date": "2024/09/22",
        "pdf": "http://arxiv.org/pdf/2409.14371",
        "abstract": "Generative AI agents are often expected to respond to complex user requests that have No One Right Answer (NORA), e.g., &#34;design a vegetarian meal plan below 1800 calories&#34;. Such requests may entail a set of constraints that the agent should adhere to. To successfully develop agents for NORA scenarios, an accurate automatic evaluation framework is essential, and specifically - one capable of validating the satisfaction of constraints in the agent&#39;s response. Recently, large language models (LLMs) have been adopted as versatile evaluators for many NORA tasks, but their ability to evaluate constraint-satisfaction in generated text remains unclear. To study this, we develop and release a novel Arithmetic Constraint-Satisfaction (ACS) benchmarking dataset. The dataset consists of complex user requests with corresponding constraints, agent responses and human labels indicating each constraint&#39;s satisfaction level in the response. A unique property of this dataset is that validating many of its constraints requires reviewing the response as a whole (in contrast to many other benchmarks that require the validation of a single independent item). Moreover, it assesses LLMs in performing reasoning, in-context data extraction, arithmetic calculations, and counting. We then benchmark both open and proprietary LLMs on evaluating constraint-satisfaction, and show that most models still have a significant headroom for improvement, and that errors primarily stem from reasoning issues. In addition, most models exhibit a skewed constraint-satisfaction prediction pattern, with higher accuracy where the ground-truth label is &#34;satisfied&#34;. Lastly, few-shot prompting for our task proved to be rather challenging, since many of the studied models showed a degradation in performance when it was introduced.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2409.14371"
    },
    "34dede8cf1ee09ccdbfa52fade7357d0": {
        "title": "ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning",
        "authors": [
            "Yihong Tang",
            "Jiao Ou",
            "Che Liu",
            "Fuzheng Zhang",
            "Di Zhang",
            "Kun Gai"
        ],
        "date": "2024/09/23",
        "pdf": "http://arxiv.org/pdf/2409.14710",
        "abstract": "Role-playing is an emerging application in the field of Human-Computer Interaction (HCI), primarily implemented through the alignment training of a large language model (LLM) with assigned characters. Despite significant progress, role-playing agents (RPLAs) still struggle with maintaining role-consistency across conversations, particularly when confronted with boundary queries subtly related to character attributes. In this paper, we present ERABAL, a framework aimed at enhancing RPLAs&#39; role-playing capabilities through boundary-aware learning. ERABAL encompasses a generation pipeline for role-specific dialogues and a concomitant methodology for alignment training. Through comprehensive evaluations, we demonstrate that ERABAL is both efficient and effective. By training with significantly fewer dialogues than those used in leading approaches, ERABAL achieves notable improvements across WikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared to the generalist baseline models. Our code and datasets will be made publicly available to support further research.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.14710"
    },
    "dd00a0fbb2bee5fd3f114ae9def2f127": {
        "title": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents",
        "authors": [
            "Peter Mühlbacher",
            "Nikos I. Bosse",
            "Lawrence Phillips"
        ],
        "date": "2024/09/23",
        "pdf": "http://arxiv.org/pdf/2409.14913",
        "abstract": "We present initial results of a forthcoming benchmark for evaluating LLM agents on white-collar tasks of economic value. We evaluate agents on real-world &#34;messy&#34; open-web research tasks of the type that are routine in finance and consulting. In doing so, we lay the groundwork for an LLM agent evaluation suite where good performance directly corresponds to a large economic and societal impact. We built and tested several agent architectures with o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini. On average, LLM agents powered by Claude-3.5 Sonnet and o1-preview substantially outperformed agents using GPT-4o, with agents based on Llama 3.1 (405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct architecture with the ability to delegate subtasks to subagents performed best. In addition to quantitative evaluations, we qualitatively assessed the performance of the LLM agents by inspecting their traces and reflecting on their observations. Our evaluation represents the first in-depth assessment of agents&#39; abilities to conduct challenging, economically valuable analyst-style research on the real open web.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2409.14913"
    },
    "746409495d2ea05b1222f0cd8ea11fb4": {
        "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
        "authors": [
            "Bandhav Veluri",
            "Benjamin N Peloquin",
            "Bokai Yu",
            "Hongyu Gong",
            "Shyamnath Gollakota"
        ],
        "date": "2024/09/23",
        "pdf": "http://arxiv.org/pdf/2409.15594",
        "abstract": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently &#34;half-duplex&#34; -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is &#34;full-duplex&#34; allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of &#34;time&#34;. To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model&#39;s ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms. Webpage: https://syncllm.cs.washington.edu/.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.15594"
    },
    "5f1e635457185ba1426c76ebc3fe581f": {
        "title": "Automated test generation to evaluate tool-augmented LLMs as conversational AI agents",
        "authors": [
            "Samuel Arcadinho",
            "David Aparicio",
            "Mariana Almeida"
        ],
        "date": "2024/09/24",
        "pdf": "http://arxiv.org/pdf/2409.15934",
        "abstract": "Tool-augmented LLMs are a promising approach to create AI agents that can have realistic conversations, follow procedures, and call appropriate functions. However, evaluating them is challenging due to the diversity of possible conversations, and existing datasets focus only on single interactions and function-calling. We present a test generation pipeline to evaluate LLMs as conversational AI agents. Our framework uses LLMs to generate diverse tests grounded on user-defined procedures. For that, we use intermediate graphs to limit the LLM test generator&#39;s tendency to hallucinate content that is not grounded on input procedures, and enforces high coverage of the possible conversations. Additionally, we put forward ALMITA, a manually curated dataset for evaluating AI agents in customer support, and use it to evaluate existing LLMs. Our results show that while tool-augmented LLMs perform well in single interactions, they often struggle to handle complete conversations. While our focus is on customer support, our method is general and capable of AI agents for different domains.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2409.15934"
    },
    "fd47a9bf36e0f553ccfb797072d92bf9": {
        "title": "Plurals: A System for Guiding LLMs Via Simulated Social Ensembles",
        "authors": [
            "Joshua Ashkinaze",
            "Emily Fry",
            "Narendra Edara",
            "Eric Gilbert",
            "Ceren Budak"
        ],
        "date": "2024/09/25",
        "pdf": "http://arxiv.org/pdf/2409.17213",
        "abstract": "Recent debates raised concerns that language models may favor certain viewpoints. But what if the solution is not to aim for a &#39;view from nowhere&#39; but rather to leverage different viewpoints? We introduce Plurals, a system and Python library for pluralistic AI deliberation. Plurals consists of Agents (LLMs, optionally with personas) which deliberate within customizable Structures, with Moderators overseeing deliberation. Plurals is a generator of simulated social ensembles. Plurals integrates with government datasets to create nationally representative personas, includes deliberation templates inspired by deliberative democracy, and allows users to customize both information-sharing structures and deliberation behavior within Structures. Six case studies demonstrate fidelity to theoretical constructs and efficacy. Three randomized experiments show simulated focus groups produced output resonant with an online sample of the relevant audiences (chosen over zero-shot generation in 75% of trials). Plurals is both a paradigm and a concrete system for pluralistic AI. The Plurals library is available at https://github.com/josh-ashkinaze/plurals and will be continually updated.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.17213"
    },
    "64c171708415d40fa08c6bd9f6fc6d4f": {
        "title": "A Survey on Complex Tasks for Goal-Directed Interactive Agents",
        "authors": [
            "Mareike Hartmann",
            "Alexander Koller"
        ],
        "date": "2024/09/27",
        "pdf": "http://arxiv.org/pdf/2409.18538",
        "abstract": "Goal-directed interactive agents, which autonomously complete tasks through interactions with their environment, can assist humans in various domains of their daily lives. Recent advances in large language models (LLMs) led to a surge of new, more and more challenging tasks to evaluate such agents. To properly contextualize performance across these tasks, it is imperative to understand the different challenges they pose to agents. To this end, this survey compiles relevant tasks and environments for evaluating goal-directed interactive agents, structuring them along dimensions relevant for understanding current obstacles. An up-to-date compilation of relevant resources can be found on our project website: https://coli-saar.github.io/interactive-agents.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2409.18538"
    },
    "db92968cb99da099ce2f5a7d57b8938c": {
        "title": "Towards Automated Patent Workflows: AI-Orchestrated Multi-Agent Framework for Intellectual Property Management and Analysis",
        "authors": [
            "Sakhinana Sagar Srinivas",
            "Vijay Sri Vaikunth",
            "Venkataramana Runkana"
        ],
        "date": "2024/09/21",
        "pdf": "http://arxiv.org/pdf/2409.19006",
        "abstract": "Patents are the currency of innovation, and like any currency, they need to be managed and protected (Gavin Potenza). Patents, as legal documents that secure intellectual property rights, play a critical role in technological innovation. The growing complexity of patent documents and the surge in patent applications have created a need for automated solutions in patent analysis. In this work, we present PatExpert, an autonomous multi-agent conversational framework designed to streamline and optimize patent-related tasks. The framework consists of a metaagent that coordinates task-specific expert agents for various patent-related tasks and a critique agent for error handling and feedback provision. The meta-agent orchestrates specialized expert agents, each fine-tuned for specific tasks such as patent classification, acceptance, claim generation, abstractive summarization, multi-patent analysis, and scientific hypothesis generation. For multi-patent analysis, the framework incorporates advanced methods like Graph Retrieval-Augmented Generation (GRAG) to enhance response accuracy and relevance by combining semantic similarity with knowledge graphs. Error handling is managed by critique agents (Gold-LLM-as-a-Judge and Reward-LLM-as-a-Judge), which evaluate output responses for accuracy and provide iterative feedback. The framework also prioritizes explainability, ensuring transparent justifications for decisions made during patent analysis. Its comprehensive capabilities make it a valuable tool for automating complex patent workflows, enhancing efficiency, accuracy, and compliance in patent-related tasks. Empirical evidence demonstrates significant improvements in patent processing tasks, concluding that the framework offers a robust solution for automating and optimizing patent analysis.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.19006"
    },
    "4b5a3dd76f2e66f22a4adc7ac0054530": {
        "title": "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs",
        "authors": [
            "Zheng Wang",
            "Zhongyang Li",
            "Zeren Jiang",
            "Dandan Tu",
            "Wei Shi"
        ],
        "date": "2024/09/28",
        "pdf": "http://arxiv.org/pdf/2409.19401",
        "abstract": "In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user&#39;s smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2409.19401"
    },
    "82e7c720dafe7b45fbc53faddaf12374": {
        "title": "Co-Learning: Code Learning for Multi-Agent Reinforcement Collaborative Framework with Conversational Natural Language Interfaces",
        "authors": [
            "Jiapeng Yu",
            "Yuqian Wu",
            "Yajing Zhan",
            "Wenhao Guo",
            "Zhou Xu",
            "Raymond Lee"
        ],
        "date": "2024/09/02",
        "pdf": "http://arxiv.org/pdf/2409.00985",
        "abstract": "Online question-and-answer (Q\\&amp;A) systems based on the Large Language Model (LLM) have progressively diverged from recreational to professional use. This paper proposed a Multi-Agent framework with environmentally reinforcement learning (E-RL) for code correction called Code Learning (Co-Learning) community, assisting beginners to correct code errors independently. It evaluates the performance of multiple LLMs from an original dataset with 702 error codes, uses it as a reward or punishment criterion for E-RL; Analyzes input error codes by the current agent; selects the appropriate LLM-based agent to achieve optimal error correction accuracy and reduce correction time. Experiment results showed that 3\\% improvement in Precision score and 15\\% improvement in time cost as compared with no E-RL method respectively. Our source code is available at: https://github.com/yuqian2003/Co_Learning",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.00985"
    },
    "02a0b2fc91f2e9d34a8068697e17bd4c": {
        "title": "A Survey on Emergent Language",
        "authors": [
            "Jannik Peters",
            "Constantin Waubert de Puiseau",
            "Hasan Tercan",
            "Arya Gopikrishnan",
            "Gustavo Adolpho Lucas De Carvalho",
            "Christian Bitter",
            "Tobias Meisen"
        ],
        "date": "2024/09/04",
        "pdf": "http://arxiv.org/pdf/2409.02645",
        "abstract": "The field of emergent language represents a novel area of research within the domain of artificial intelligence, particularly within the context of multi-agent reinforcement learning. Although the concept of studying language emergence is not new, early approaches were primarily concerned with explaining human language formation, with little consideration given to its potential utility for artificial agents. In contrast, studies based on reinforcement learning aim to develop communicative capabilities in agents that are comparable to or even superior to human language. Thus, they extend beyond the learned statistical representations that are common in natural language processing research. This gives rise to a number of fundamental questions, from the prerequisites for language emergence to the criteria for measuring its success. This paper addresses these questions by providing a comprehensive review of 181 scientific publications on emergent language in artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the field. Consequently, the main contributions are the definition and overview of the prevailing terminology, the analysis of existing evaluation methods and metrics, and the description of the identified research gaps.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2409.02645"
    },
    "e993f041789c8f1e491acdea1ffd4aef": {
        "title": "From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents",
        "authors": [
            "Jifan Yu",
            "Zheyuan Zhang",
            "Daniel Zhang-li",
            "Shangqing Tu",
            "Zhanxin Hao",
            "Rui Miao Li",
            "Haoxuan Li",
            "Yuanchun Wang",
            "Hanming Li",
            "Linlu Gong",
            "Jie Cao",
            "Jiayin Lin",
            "Jinchang Zhou",
            "Fei Qin",
            "Haohua Wang",
            "Jianxiao Jiang",
            "Lijun Deng",
            "Yisi Zhan",
            "Chaojun Xiao",
            "Xusheng Dai",
            "Xuan Yan",
            "Nianyi Lin",
            "Nan Zhang",
            "Ruixin Ni",
            "Yang Dang",
            "Lei Hou",
            "Yu Zhang",
            "Xu Han",
            "Manli Li",
            "Juanzi Li",
            "Zhiyuan Liu",
            "Huiqin Liu",
            "Maosong Sun"
        ],
        "date": "2024/09/05",
        "pdf": "http://arxiv.org/pdf/2409.03512",
        "abstract": "Since the first instances of online education, where courses were uploaded to accessible and shared online platforms, this form of scaling the dissemination of human knowledge to reach a broader audience has sparked extensive discussion and widespread adoption. Recognizing that personalized learning still holds significant potential for improvement, new AI technologies have been continuously integrated into this learning format, resulting in a variety of educational AI applications such as educational recommendation and intelligent tutoring. The emergence of intelligence in large language models (LLMs) has allowed for these educational enhancements to be built upon a unified foundational model, enabling deeper integration. In this context, we propose MAIC (Massive AI-empowered Course), a new form of online education that leverages LLM-driven multi-agent systems to construct an AI-augmented classroom, balancing scalability with adaptivity. Beyond exploring the conceptual framework and technical innovations, we conduct preliminary experiments at Tsinghua University, one of China&#39;s leading universities. Drawing from over 100,000 learning records of more than 500 students, we obtain a series of valuable observations and initial analyses. This project will continue to evolve, ultimately aiming to establish a comprehensive open platform that supports and unifies research, technology, and applications in exploring the possibilities of online education in the era of large model AI. We envision this platform as a collaborative hub, bringing together educators, researchers, and innovators to collectively explore the future of AI-driven online education.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.03512"
    },
    "3e94d8511af8b7cb112bc3e3bdda8128": {
        "title": "Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets",
        "authors": [
            "Desiree Heim",
            "Christian Jilek",
            "Adrian Ulges",
            "Andreas Dengel"
        ],
        "date": "2024/09/06",
        "pdf": "http://arxiv.org/pdf/2409.04286",
        "abstract": "Current publicly available knowledge work data collections lack diversity, extensive annotations, and contextual information about the users and their documents. These issues hinder objective and comparable data-driven evaluations and optimizations of knowledge work assistance systems. Due to the considerable resources needed to collect such data in real-life settings and the necessity of data censorship, collecting such a dataset appears nearly impossible. For this reason, we propose a configurable, multi-agent knowledge work dataset generator. This system simulates collaborative knowledge work among agents producing Large Language Model-generated documents and accompanying data traces. Additionally, the generator captures all background information, given in its configuration or created during the simulation process, in a knowledge graph. Finally, the resulting dataset can be utilized and shared without privacy or confidentiality concerns. This paper introduces our approach&#39;s design and vision and focuses on generating authentic knowledge work documents using Large Language Models. Our study involving human raters who assessed 53% of the generated and 74% of the real documents as realistic demonstrates the potential of our approach. Furthermore, we analyze the authenticity criteria mentioned in the participants&#39; comments and elaborate on potential improvements for identified common issues.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.04286"
    },
    "4a59b5d4fb6f7293c6dbb7e6b050f812": {
        "title": "SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning",
        "authors": [
            "Alireza Ghafarollahi",
            "Markus J. Buehler"
        ],
        "date": "2024/09/09",
        "pdf": "http://arxiv.org/pdf/2409.05556",
        "abstract": "A key challenge in artificial intelligence is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. In this work, we present SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities. Applied to biologically inspired materials, SciAgents reveals hidden interdisciplinary relationships that were previously considered unrelated, achieving a scale, precision, and exploratory power that surpasses traditional human-driven research methods. The framework autonomously generates and refines research hypotheses, elucidating underlying mechanisms, design principles, and unexpected material properties. By integrating these capabilities in a modular fashion, the intelligent system yields material discoveries, critique and improve existing hypotheses, retrieve up-to-date data about existing research, and highlights their strengths and limitations. Our case studies demonstrate scalable capabilities to combine generative AI, ontological representations, and multi-agent modeling, harnessing a `swarm of intelligence&#39; similar to biological systems. This provides new avenues for materials discovery and accelerates the development of advanced materials by unlocking Nature&#39;s design principles.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.05556"
    },
    "6de7a5738c9e900612b380db5c07a079": {
        "title": "Using Generative Agents to Create Tip Sheets for Investigative Data Reporting",
        "authors": [
            "Joris Veerbeek",
            "Nicholas Diakopoulos"
        ],
        "date": "2024/09/11",
        "pdf": "http://arxiv.org/pdf/2409.07286",
        "abstract": "This paper introduces a system using generative AI agents to create tip sheets for investigative data reporting. Our system employs three specialized agents--an analyst, a reporter, and an editor--to collaboratively generate and refine tips from datasets. We validate this approach using real-world investigative stories, demonstrating that our agent-based system generally generates more newsworthy and accurate insights compared to a baseline model without agents, although some variability was noted between different stories. Our findings highlight the potential of generative AI to provide leads for investigative data reporting.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.07286"
    },
    "7a38cccb4e66d1f09fb40e7e40b39bd5": {
        "title": "SUPER: Evaluating Agents on Setting Up and Executing Tasks from Research Repositories",
        "authors": [
            "Ben Bogin",
            "Kejuan Yang",
            "Shashank Gupta",
            "Kyle Richardson",
            "Erin Bransom",
            "Peter Clark",
            "Ashish Sabharwal",
            "Tushar Khot"
        ],
        "date": "2024/09/11",
        "pdf": "http://arxiv.org/pdf/2409.07440",
        "abstract": "Given that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPERaims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 602 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2409.07440"
    },
    "1ae330f91317aca5507a022b64aae990": {
        "title": "DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?",
        "authors": [
            "Liqiang Jing",
            "Zhehui Huang",
            "Xiaoyang Wang",
            "Wenlin Yao",
            "Wenhao Yu",
            "Kaixin Ma",
            "Hongming Zhang",
            "Xinya Du",
            "Dong Yu"
        ],
        "date": "2024/09/12",
        "pdf": "http://arxiv.org/pdf/2409.07703",
        "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2409.07703"
    },
    "e8a07f983ea7cadc2f2eb54e4ab93f13": {
        "title": "TravelAgent: An AI Assistant for Personalized Travel Planning",
        "authors": [
            "Aili Chen",
            "Xuyang Ge",
            "Ziquan Fu",
            "Yanghua Xiao",
            "Jiangjie Chen"
        ],
        "date": "2024/09/12",
        "pdf": "http://arxiv.org/pdf/2409.08069",
        "abstract": "As global tourism expands and artificial intelligence technology advances, intelligent travel planning services have emerged as a significant research focus. Within dynamic real-world travel scenarios with multi-dimensional constraints, services that support users in automatically creating practical and customized travel itineraries must address three key objectives: Rationality, Comprehensiveness, and Personalization. However, existing systems with rule-based combinations or LLM-based planning methods struggle to fully satisfy these criteria. To overcome the challenges, we introduce TravelAgent, a travel planning system powered by large language models (LLMs) designed to provide reasonable, comprehensive, and personalized travel itineraries grounded in dynamic scenarios. TravelAgent comprises four modules: Tool-usage, Recommendation, Planning, and Memory Module. We evaluate TravelAgent&#39;s performance with human and simulated users, demonstrating its overall effectiveness in three criteria and confirming the accuracy of personalized recommendations.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.08069"
    },
    "6eaaee674eeb1b60d91b0e6e8062546a": {
        "title": "Self-Supervised Inference of Agents in Trustless Environments",
        "authors": [
            "Vladyslav Larin",
            "Ivan Nikitin",
            "Alexander Firsov"
        ],
        "date": "2024/09/12",
        "pdf": "http://arxiv.org/pdf/2409.08386",
        "abstract": "In this paper, we propose a novel approach where agents can form swarms to produce high-quality responses effectively. This is accomplished by utilizing agents capable of data inference and ranking, which can be effectively implemented using LLMs as response classifiers. We assess existing approaches for trustless agent inference, define our methodology, estimate practical parameters, and model various types of malicious agent attacks. Our method leverages the collective intelligence of swarms, ensuring robust and efficient decentralized AI inference with better accuracy, security, and reliability. We show that our approach is an order of magnitude faster than other trustless inference strategies reaching less than 125 ms validation latency.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.08386"
    },
    "17b6124b9435fc829993658201fb15dd": {
        "title": "Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance",
        "authors": [
            "Lucio La Cava",
            "Andrea Tagarelli"
        ],
        "date": "2024/09/13",
        "pdf": "http://arxiv.org/pdf/2409.08963",
        "abstract": "Ensuring content compliance with community guidelines is crucial for maintaining healthy online social environments. However, traditional human-based compliance checking struggles with scaling due to the increasing volume of user-generated content and a limited number of moderators. Recent advancements in Natural Language Understanding demonstrated by Large Language Models unlock new opportunities for automated content compliance verification. This work evaluates six AI-agents built on Open-LLMs for automated rule compliance checking in Decentralized Social Networks, a challenging environment due to heterogeneous community scopes and rules. Analyzing over 50,000 posts from hundreds of Mastodon servers, we find that AI-agents effectively detect non-compliant content, grasp linguistic subtleties, and adapt to diverse community contexts. Most agents also show high inter-rater reliability and consistency in score justification and suggestions for compliance. Human-based evaluation with domain experts confirmed the agents&#39; reliability and usefulness, rendering them promising tools for semi-automated or human-in-the-loop content moderation systems.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.08963"
    },
    "0a5626864f29951fcfe8434985f5e08c": {
        "title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents",
        "authors": [
            "Zhe Su",
            "Xuhui Zhou",
            "Sanketh Rangreji",
            "Anubha Kabra",
            "Julia Mendelsohn",
            "Faeze Brahman",
            "Maarten Sap"
        ],
        "date": "2024/09/13",
        "pdf": "http://arxiv.org/pdf/2409.09013",
        "abstract": "To be safely and successfully deployed, LLMs must simultaneously satisfy truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI agent assisting a used car salesman selling a car with flaws), partly due to ambiguous or misleading user instructions. We propose AI-LieDar, a framework to study how LLM-based agents navigate scenarios with utility-truthfulness conflicts in a multi-turn interactive setting. We design a set of realistic scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents&#39; responses. Our experiment demonstrates that all models are truthful less than 50% of the time, although truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models follow malicious instructions to deceive, and even truth-steered models can still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and AI agents.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.09013"
    },
    "7c5f7b7fe72c7d5d94f7648fddba9f49": {
        "title": "Agents in Software Engineering: Survey, Landscape, and Vision",
        "authors": [
            "Yanlin Wang",
            "Wanjun Zhong",
            "Yanxian Huang",
            "Ensheng Shi",
            "Min Yang",
            "Jiachi Chen",
            "Hui Li",
            "Yuchi Ma",
            "Qianxiang Wang",
            "Zibin Zheng"
        ],
        "date": "2024/09/13",
        "pdf": "http://arxiv.org/pdf/2409.09030",
        "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable success and have been widely used in various downstream tasks, especially in the tasks of the software engineering (SE) field. We find that many studies combining LLMs with SE have employed the concept of agents either explicitly or implicitly. However, there is a lack of an in-depth survey to sort out the development context of existing works, analyze how existing works combine the LLM-based agent technologies to optimize various tasks, and clarify the framework of LLM-based agents in SE. In this paper, we conduct the first survey of the studies on combining LLM-based agents with SE and present a framework of LLM-based agents in SE which includes three key modules: perception, memory, and action. We also summarize the current challenges in combining the two fields and propose future opportunities in response to existing challenges. We maintain a GitHub repository of the related papers at: https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2409.09030"
    },
    "ae719da0778628f332e07c03d27b4a42": {
        "title": "Automatic Control With Human-Like Reasoning: Exploring Language Model Embodied Air Traffic Agents",
        "authors": [
            "Justas Andriuškevičius",
            "Junzi Sun"
        ],
        "date": "2024/09/15",
        "pdf": "http://arxiv.org/pdf/2409.09717",
        "abstract": "Recent developments in language models have created new opportunities in air traffic control studies. The current focus is primarily on text and language-based use cases. However, these language models may offer a higher potential impact in the air traffic control domain, thanks to their ability to interact with air traffic environments in an embodied agent form. They also provide a language-like reasoning capability to explain their decisions, which has been a significant roadblock for the implementation of automatic air traffic control. This paper investigates the application of a language model-based agent with function-calling and learning capabilities to resolve air traffic conflicts without human intervention. The main components of this research are foundational large language models, tools that allow the agent to interact with the simulator, and a new concept, the experience library. An innovative part of this research, the experience library, is a vector database that stores synthesized knowledge that agents have learned from interactions with the simulations and language models. To evaluate the performance of our language model-based agent, both open-source and closed-source models were tested. The results of our study reveal significant differences in performance across various configurations of the language model-based agents. The best-performing configuration was able to solve almost all 120 but one imminent conflict scenarios, including up to four aircraft at the same time. Most importantly, the agents are able to provide human-level text explanations on traffic situations and conflict resolution strategies.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.09717"
    },
    "35b1e73fdc5f871eb4e4efe3bab90c0b": {
        "title": "Instigating Cooperation among LLM Agents Using Adaptive Information Modulation",
        "authors": [
            "Qiliang Chen",
            "Sepehr Ilami",
            "Nunzio Lore",
            "Babak Heydari"
        ],
        "date": "2024/09/16",
        "pdf": "http://arxiv.org/pdf/2409.10372",
        "abstract": "This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.10372"
    },
    "a17a18a53e355ae8901effacf15f0f65": {
        "title": "Agentic Society: Merging skeleton from real world and texture from Large Language Model",
        "authors": [
            "Yuqi Bai",
            "Kun Sun",
            "Huishi Yin"
        ],
        "date": "2024/09/02",
        "pdf": "http://arxiv.org/pdf/2409.10550",
        "abstract": "Recent advancements in large language models (LLMs) and agent technologies offer promising solutions to the simulation of social science experiments, but the availability of data of real-world population required by many of them still poses as a major challenge. This paper explores a novel framework that leverages census data and LLMs to generate virtual populations, significantly reducing resource requirements and bypassing privacy compliance issues associated with real-world data, while keeping a statistical truthfulness. Drawing on real-world census data, our approach first generates a persona that reflects demographic characteristics of the population. We then employ LLMs to enrich these personas with intricate details, using techniques akin to those in image generative models but applied to textual data. Additionally, we propose a framework for the evaluation of the feasibility of our method with respect to capability of LLMs based on personality trait tests, specifically the Big Five model, which also enhances the depth and realism of the generated personas. Through preliminary experiments and analysis, we demonstrate that our method produces personas with variability essential for simulating diverse human behaviors in social science experiments. But the evaluation result shows that only weak sign of statistical truthfulness can be produced due to limited capability of current LLMs. Insights from our study also highlight the tension within LLMs between aligning with human values and reflecting real-world complexities. Thorough and rigorous test call for further research. Our codes are released at https://github.com/baiyuqi/agentic-society.git",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.10550"
    },
    "3a5280ef45b05c2165fe7cbcb86cf3a2": {
        "title": "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy Leakage",
        "authors": [
            "Zeyi Liao",
            "Lingbo Mo",
            "Chejian Xu",
            "Mintong Kang",
            "Jiawei Zhang",
            "Chaowei Xiao",
            "Yuan Tian",
            "Bo Li",
            "Huan Sun"
        ],
        "date": "2024/09/17",
        "pdf": "http://arxiv.org/pdf/2409.11295",
        "abstract": "Generalist web agents have demonstrated remarkable potential in autonomously completing a wide range of tasks on real websites, significantly boosting human productivity. However, web tasks, such as booking flights, usually involve users&#39; PII, which may be exposed to potential privacy risks if web agents accidentally interact with compromised websites, a scenario that remains largely unexplored in the literature. In this work, we narrow this gap by conducting the first study on the privacy risks of generalist web agents in adversarial environments. First, we present a realistic threat model for attacks on the website, where we consider two adversarial targets: stealing users&#39; specific PII or the entire user request. Then, we propose a novel attack method, termed Environmental Injection Attack (EIA). EIA injects malicious content designed to adapt well to environments where the agents operate and our work instantiates EIA specifically for privacy scenarios in web environments. We collect 177 action steps that involve diverse PII categories on realistic websites from the Mind2Web, and conduct experiments using one of the most capable generalist web agent frameworks to date. The results demonstrate that EIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user request. Additionally, by accessing the stealthiness and experimenting with a defensive system prompt, we indicate that EIA is hard to detect and mitigate. Notably, attacks that are not well adapted for a webpage can be detected via human inspection, leading to our discussion about the trade-off between security and autonomy. However, extra attackers&#39; efforts can make EIA seamlessly adapted, rendering such supervision ineffective. Thus, we further discuss the defenses at the pre- and post-deployment stages of the websites without relying on human supervision and call for more advanced defense strategies.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2409.11295"
    },
    "33531e8e47fd8738cc3591a9a8cf5a76": {
        "title": "Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models",
        "authors": [
            "Asher Sprigler",
            "Alexander Drobek",
            "Keagan Weinstock",
            "Wendpanga Tapsoba",
            "Gavin Childress",
            "Andy Dao",
            "Lucas Gral"
        ],
        "date": "2024/09/14",
        "pdf": "http://arxiv.org/pdf/2409.13753",
        "abstract": "Large Language Models (LLMs) have increasingly demonstrated the ability to facilitate the development of multi-agent systems that allow the interpretation of thoughts and actions generated by each individual. Promising advancements have also been made in LLM-based interaction with existing worlds, particularly in interacting with simulated environments. This paper aims to integrate both aforementioned topics (agents &amp; world interaction) into a single simulation where multiple agents can work together to solve a problem, modeling how groups of humans can often solve problems better than individuals. By showing whether LLMs demonstrate the synergy of human collaboration, it could lead to advancements in the applications of LLMs. We implemented two simulations: a physical studio apartment with two roommates, and another where agents collaborate to complete a programming task. We provide a multi-agent framework, discuss the performance of the agents in each simulation, and discuss potential future additions.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.13753"
    },
    "cc7066d09977aaae68d18954675765ad": {
        "title": "MOSS: Enabling Code-Driven Evolution and Context Management for AI Agents",
        "authors": [
            "Ming Zhu",
            "Yi Zhou"
        ],
        "date": "2024/09/24",
        "pdf": "http://arxiv.org/pdf/2409.16120",
        "abstract": "Developing AI agents powered by large language models (LLMs) faces significant challenges in achieving true Turing completeness and adaptive, code-driven evolution. Current approaches often generate code independently of its runtime context, relying heavily on the LLM&#39;s memory, which results in inefficiencies and limits adaptability. Manual protocol development in sandbox environments further constrains the agent&#39;s autonomous adaptability. Crucially, achieving consistency in code and context across multi-turn interactions and ensuring isolation of local variables within each interaction remains an unsolved problem. We introduce MOSS (llM-oriented Operating System Simulation), a novel framework that addresses these challenges by integrating code generation with a dynamic context management system. MOSS ensures consistency and adaptability by using a mechanism that maintains the Python context across interactions, including isolation of local variables and preservation of runtime integrity. At its core, the framework employs an Inversion of Control (IoC) container in conjunction with decorators to enforce the least knowledge principle, allowing agents to focus on abstract interfaces rather than concrete implementations. This facilitates seamless integration of new tools and libraries, enables runtime instance replacement, and reduces prompt complexity, providing a &#34;what you see is what you get&#34; environment for the agent. Through a series of case studies, we show how this framework can enhance the efficiency and capabilities of agent development and highlight its advantages in moving towards Turing-complete agents capable of evolving through code.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.16120"
    },
    "515a5f1e4ee0ed085471aa5674483bf4": {
        "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making",
        "authors": [
            "Dayuan Fu",
            "Biqing Qi",
            "Yihuai Gao",
            "Che Jiang",
            "Guanting Dong",
            "Bowen Zhou"
        ],
        "date": "2024/09/25",
        "pdf": "http://arxiv.org/pdf/2409.16686",
        "abstract": "Long-term memory is significant for agents, in which insights play a crucial role. However, the emergence of irrelevant insight and the lack of general insight can greatly undermine the effectiveness of insight. To solve this problem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an embodied agent designed to improve LLMs&#39; planning and decision-making ability by summarizing and utilizing insight effectively across different scales. MSI achieves this through the experience selector, insight generator, and insight selector. Leveraging a three-part pipeline, MSI can generate task-specific and high-level insight, store it in a database, and then use relevant insight from it to aid in decision-making. Our experiments show that MSI outperforms another insight strategy when planning by GPT3.5. Moreover, We delve into the strategies for selecting seed experience and insight, aiming to provide LLM with more useful and relevant insight for better decision-making. Our observations also indicate that MSI exhibits better robustness when facing domain-shifting scenarios.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2409.16686"
    },
    "c41cde4a2e191810ef9b3b8ae9e35d10": {
        "title": "SELP: Generating Safe and Efficient Task Plans for Robot Agents with Large Language Models",
        "authors": [
            "Yi Wu",
            "Zikang Xiong",
            "Yiran Hu",
            "Shreyash S. Iyengar",
            "Nan Jiang",
            "Aniket Bera",
            "Lin Tan",
            "Suresh Jagannathan"
        ],
        "date": "2024/09/28",
        "pdf": "http://arxiv.org/pdf/2409.19471",
        "abstract": "Despite significant advancements in large language models (LLMs) that enhance robot agents&#39; understanding and execution of natural language (NL) commands, ensuring the agents adhere to user-specified constraints remains challenging, particularly for complex commands and long-horizon tasks. To address this challenge, we present three key insights, equivalence voting, constrained decoding, and domain-specific fine-tuning, which significantly enhance LLM planners&#39; capability in handling complex tasks. Equivalence voting ensures consistency by generating and sampling multiple Linear Temporal Logic (LTL) formulas from NL commands, grouping equivalent LTL formulas, and selecting the majority group of formulas as the final LTL formula. Constrained decoding then uses the generated LTL formula to enforce the autoregressive inference of plans, ensuring the generated plans conform to the LTL. Domain-specific fine-tuning customizes LLMs to produce safe and efficient plans within specific task domains. Our approach, Safe Efficient LLM Planner (SELP), combines these insights to create LLM planners to generate plans adhering to user commands with high confidence. We demonstrate the effectiveness and generalizability of SELP across different robot agents and tasks, including drone navigation and robot manipulation. For drone navigation tasks, SELP outperforms state-of-the-art planners by 10.8% in safety rate (i.e., finishing tasks conforming to NL commands) and by 19.8% in plan efficiency. For robot manipulation tasks, SELP achieves 20.4% improvement in safety rate. Our datasets for evaluating NL-to-LTL and robot task planning will be released in github.com/lt-asset/selp.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2409.19471"
    },
    "abaa1b3fbace483cafbaa6a1f843c901": {
        "title": "Automating Knowledge Discovery from Scientific Literature via LLMs: A Dual-Agent Approach with Progressive Ontology Prompting",
        "authors": [
            "Yuting Hu",
            "Dancheng Liu",
            "Qingyun Wang",
            "Charles Yu",
            "Heng Ji",
            "Jinjun Xiong"
        ],
        "date": "2024/08/20",
        "pdf": "http://arxiv.org/pdf/2409.00054",
        "abstract": "To address the challenge of automating knowledge discovery from a vast volume of literature, in this paper, we introduce a novel framework based on large language models (LLMs) that combines a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo, designed to enhance the automation of knowledge extraction from scientific articles. The POP algorithm utilizes a prioritized breadth-first search (BFS) across a predefined ontology to generate structured prompt templates and action orders, thereby guiding LLMs to discover knowledge in an automatic manner. Additionally, our LLM-Duo employs two specialized LLM agents: an explorer and an evaluator. These two agents work collaboratively and adversarially to enhance the reliability of the discovery and annotation processes. Experiments demonstrate that our method outperforms advanced baselines, enabling more accurate and complete annotations. To validate the effectiveness of our method in real-world scenarios, we employ our method in a case study of speech-language intervention discovery. Our method identifies 2,421 interventions from 64,177 research articles in the speech-language therapy domain. We curate these findings into a publicly accessible intervention knowledge base that holds significant potential to benefit the speech-language therapy community.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.00054"
    },
    "ef86d708558553ddd1b0e408140ab7f3": {
        "title": "Towards Human-Level Understanding of Complex Process Engineering Schematics: A Pedagogical, Introspective Multi-Agent Framework for Open-Domain Question Answering",
        "authors": [
            "Sagar Srinivas Sakhinana",
            "Geethan Sannidhi",
            "Venkataramana Runkana"
        ],
        "date": "2024/08/24",
        "pdf": "http://arxiv.org/pdf/2409.00082",
        "abstract": "In the chemical and process industries, Process Flow Diagrams (PFDs) and Piping and Instrumentation Diagrams (P&amp;IDs) are critical for design, construction, and maintenance. Recent advancements in Generative AI, such as Large Multimodal Models (LMMs) like GPT4 (Omni), have shown promise in understanding and interpreting process diagrams for Visual Question Answering (VQA). However, proprietary models pose data privacy risks, and their computational complexity prevents knowledge editing for domain-specific customization on consumer hardware. To overcome these challenges, we propose a secure, on-premises enterprise solution using a hierarchical, multi-agent Retrieval Augmented Generation (RAG) framework for open-domain question answering (ODQA) tasks, offering enhanced data privacy, explainability, and cost-effectiveness. Our novel multi-agent framework employs introspective and specialized sub-agents using open-source, small-scale multimodal models with the ReAct (Reason+Act) prompting technique for PFD and P&amp;ID analysis, integrating multiple information sources to provide accurate and contextually relevant answers. Our approach, supported by iterative self-correction, aims to deliver superior performance in ODQA tasks. We conducted rigorous experimental studies, and the empirical results validated the proposed approach effectiveness.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.00082"
    },
    "2c0e95b6ad33e2bb7ba8eb19c033ac96": {
        "title": "HoneyComb: A Flexible LLM-Based Agent System for Materials Science",
        "authors": [
            "Huan Zhang",
            "Yu Song",
            "Ziyu Hou",
            "Santiago Miret",
            "Bang Liu"
        ],
        "date": "2024/08/29",
        "pdf": "http://arxiv.org/pdf/2409.00135",
        "abstract": "The emergence of specialized large language models (LLMs) has shown promise in addressing complex tasks for materials science. Many LLMs, however, often struggle with distinct complexities of material science tasks, such as materials science computational tasks, and often rely heavily on outdated implicit knowledge, leading to inaccuracies and hallucinations. To address these challenges, we introduce HoneyComb, the first LLM-based agent system specifically designed for materials science. HoneyComb leverages a novel, high-quality materials science knowledge base (MatSciKB) and a sophisticated tool hub (ToolHub) to enhance its reasoning and computational capabilities tailored to materials science. MatSciKB is a curated, structured knowledge collection based on reliable literature, while ToolHub employs an Inductive Tool Construction method to generate, decompose, and refine API tools for materials science. Additionally, HoneyComb leverages a retriever module that adaptively selects the appropriate knowledge source or tools for specific tasks, thereby ensuring accuracy and relevance. Our results demonstrate that HoneyComb significantly outperforms baseline models across various tasks in materials science, effectively bridging the gap between current LLM capabilities and the specialized needs of this domain. Furthermore, our adaptable framework can be easily extended to other scientific domains, highlighting its potential for broad applicability in advancing scientific research and applications.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.00135"
    },
    "42d1e3398f13e4f11882987a8babf97a": {
        "title": "Self-evolving Agents with reflective and memory-augmented abilities",
        "authors": [
            "Xuechen Liang",
            "Meiling Tao",
            "Yinghui Xia",
            "Tianyu Shi",
            "Jun Wang",
            "JingSong Yang"
        ],
        "date": "2024/09/01",
        "pdf": "http://arxiv.org/pdf/2409.00872",
        "abstract": "Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making. In this research, we propose a novel framework by integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents&#39; capabilities in handling multi-tasking and long-span information.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2409.00872"
    },
    "3a06b3addc44479cccf3d5ac5f5aed02": {
        "title": "TinyAgent: Function Calling at the Edge",
        "authors": [
            "Lutfi Eren Erdogan",
            "Nicholas Lee",
            "Siddharth Jha",
            "Sehoon Kim",
            "Ryan Tabrizi",
            "Suhong Moon",
            "Coleman Hooper",
            "Gopala Anumanchipalli",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "date": "2024/09/01",
        "pdf": "http://arxiv.org/pdf/2409.00608",
        "abstract": "Recent large language models (LLMs) have enabled the development of advanced agentic systems that can integrate various tools and APIs to fulfill user queries through function calling. However, the deployment of these LLMs on the edge has not been explored since they typically require cloud-based infrastructure due to their substantial model size and computational demands. To this end, we present TinyAgent, an end-to-end framework for training and deploying task-specific small language model agents capable of function calling for driving agentic systems at the edge. We first show how to enable accurate function calling for open-source models via the LLMCompiler framework. We then systematically curate a high-quality dataset for function calling, which we use to fine-tune two small language models, TinyAgent-1.1B and 7B. For efficient inference, we introduce a novel tool retrieval method to reduce the input prompt length and utilize quantization to further accelerate the inference speed. As a driving application, we demonstrate a local Siri-like system for Apple&#39;s MacBook that can execute user commands through text or voice input. Our results show that our models can achieve, and even surpass, the function-calling capabilities of larger models like GPT-4-Turbo, while being fully deployed at the edge. We open-source our dataset, models, and installable package and provide a demo video for our MacBook assistant agent.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2409.00608"
    },
    "21daf20c45bc0a57adf77b16b315f99d": {
        "title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
        "authors": [
            "Xiangyuan Xue",
            "Zeyu Lu",
            "Di Huang",
            "Zidong Wang",
            "Wanli Ouyang",
            "Lei Bai"
        ],
        "date": "2024/09/02",
        "pdf": "http://arxiv.org/pdf/2409.01392",
        "abstract": "Much previous AI research has focused on developing monolithic models to maximize their intelligence, with the primary goal of enhancing performance on specific tasks. In contrast, this work attempts to study using LLM-based agents to design collaborative AI systems autonomously. To explore this problem, we first introduce ComfyBench to evaluate agents&#39;s ability to design collaborative AI systems in ComfyUI. ComfyBench is a comprehensive benchmark comprising 200 diverse tasks covering various instruction-following generation challenges, along with detailed annotations for 3,205 nodes and 20 workflows. Based on ComfyBench, we further develop ComfyAgent, a novel framework that empowers LLM-based agents to autonomously design collaborative AI systems by generating workflows. ComfyAgent is based on two core concepts. First, it represents workflows with code, which can be reversibly converted into workflows and executed as collaborative systems by the interpreter. Second, it constructs a multi-agent system that cooperates to learn from existing workflows and generate new workflows for a given task. While experimental results demonstrate that ComfyAgent achieves a comparable resolve rate to o1-preview and significantly surpasses other agents on ComfyBench, ComfyAgent has resolved only 15\\% of creative tasks. LLM-based agents still have a long way to go in autonomously designing collaborative AI systems. Progress with ComfyBench is paving the way for more intelligent and autonomous collaborative AI systems.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2409.01392"
    },
    "a8b1c34e922f35e9e1c58e604c9de741": {
        "title": "An Implementation of Werewolf Agent That does not Truly Trust LLMs",
        "authors": [
            "Takehiro Sato",
            "Shintaro Ozaki",
            "Daisaku Yokoyama"
        ],
        "date": "2024/09/03",
        "pdf": "http://arxiv.org/pdf/2409.01575",
        "abstract": "Werewolf is an incomplete information game, which has several challenges when creating a computer agent as a player given the lack of understanding of the situation and individuality of utterance (e.g., computer agents are not capable of characterful utterance or situational lying). We propose a werewolf agent that solves some of those difficulties by combining a Large Language Model (LLM) and a rule-based algorithm. In particular, our agent uses a rule-based algorithm to select an output either from an LLM or a template prepared beforehand based on the results of analyzing conversation history using an LLM. It allows the agent to refute in specific situations, identify when to end the conversation, and behave with persona. This approach mitigated conversational inconsistencies and facilitated logical utterance as a result. We also conducted a qualitative evaluation, which resulted in our agent being perceived as more human-like compared to an unmodified LLM. The agent is freely available for contributing to advance the research in the field of Werewolf game.",
        "code": "",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2409.01575"
    },
    "6cf03814bd2ba6b1c179299361564986": {
        "title": "AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction",
        "authors": [
            "Yuchen Shi",
            "Guochao Jiang",
            "Tian Qiu",
            "Deqing Yang"
        ],
        "date": "2024/09/03",
        "pdf": "http://arxiv.org/pdf/2409.01854",
        "abstract": "The relation extraction (RE) in complex scenarios faces challenges such as diverse relation types and ambiguous relations between entities within a single sentence, leading to the poor performance of pure &#34;text-in, text-out&#34; language models (LMs). To address these challenges, in this paper, we propose an agent-based RE framework, namely AgentRE, which fully leverages the potential of large language models (LLMs) including memory, retrieval and reflection, to achieve RE in complex scenarios. Specifically, three major modules are built in AgentRE serving as the tools to help the agent acquire and process various useful information, thereby obtaining improved RE performance. Our extensive experimental results upon two datasets in English and Chinese demonstrate our AgentRE&#39;s superior performance, especially in low-resource scenarios. Additionally, the trajectories generated by AgentRE can be refined to construct a high-quality training dataset incorporating different reasoning methods, which can be used to fine-tune smaller models. Code is available at https://github.com/Lightblues/AgentRE.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.01854"
    },
    "e0e4801519262a30c9546ffbdce264a6": {
        "title": "xLAM: A Family of Large Action Models to Empower AI Agent Systems",
        "authors": [
            "Jianguo Zhang",
            "Tian Lan",
            "Ming Zhu",
            "Zuxin Liu",
            "Thai Hoang",
            "Shirley Kokane",
            "Weiran Yao",
            "Juntao Tan",
            "Akshara Prabhakar",
            "Haolin Chen",
            "Zhiwei Liu",
            "Yihao Feng",
            "Tulika Awalgaonkar",
            "Rithesh Murthy",
            "Eric Hu",
            "Zeyuan Chen",
            "Ran Xu",
            "Juan Carlos Niebles",
            "Shelby Heinecke",
            "Huan Wang",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "date": "2024/09/05",
        "pdf": "http://arxiv.org/pdf/2409.03215",
        "abstract": "Autonomous agents powered by large language models (LLMs) have attracted significant research interest. However, the open-source community faces many challenges in developing specialized models for agent tasks, driven by the scarcity of high-quality agent datasets and the absence of standard protocols in this area. We introduce and publicly release xLAM, a series of large action models designed for AI agent tasks. The xLAM series includes five models with both dense and mixture-of-expert architectures, ranging from 1B to 8x22B parameters, trained using a scalable, flexible pipeline that unifies, augments, and synthesizes diverse datasets to enhance AI agents&#39; generalizability and performance across varied environments. Our experimental results demonstrate that xLAM consistently delivers exceptional performance across multiple agent ability benchmarks, notably securing the 1st position on the Berkeley Function-Calling Leaderboard, outperforming GPT-4, Claude-3, and many other models in terms of tool use. By releasing the xLAM series, we aim to advance the performance of open-source LLMs for autonomous AI agents, potentially accelerating progress and democratizing access to high-performance models for agent tasks. Models are available at https://huggingface.co/collections/Salesforce/xlam-models-65f00e2a0a63bbcd1c2dade4",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2409.03215"
    },
    "9b16501ae98cd4f3c793c785575dc780": {
        "title": "E2CL: Exploration-based Error Correction Learning for Embodied Agents",
        "authors": [
            "Hanlin Wang",
            "Chak Tou Leong",
            "Jian Wang",
            "Wenjie Li"
        ],
        "date": "2024/09/05",
        "pdf": "http://arxiv.org/pdf/2409.03256",
        "abstract": "Language models are exhibiting increasing capability in knowledge utilization and reasoning. However, when applied as agents in embodied environments, they often suffer from misalignment between their intrinsic knowledge and environmental knowledge, leading to infeasible actions. Traditional environment alignment methods, such as supervised learning on expert trajectories and reinforcement learning, encounter limitations in covering environmental knowledge and achieving efficient convergence, respectively. Inspired by human learning, we propose Exploration-based Error Correction Learning (E2CL), a novel framework that leverages exploration-induced errors and environmental feedback to enhance environment alignment for embodied agents. E2CL incorporates teacher-guided and teacher-free explorations to gather environmental feedback and correct erroneous actions. The agent learns to provide feedback and self-correct, thereby enhancing its adaptability to target environments. Extensive experiments in the VirtualHome environment demonstrate that E2CL-trained agents outperform those trained by baseline methods and exhibit superior self-correction capabilities.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2409.03256"
    },
    "4da89e6f01d92485331348c3011afd44": {
        "title": "Rx Strategist: Prescription Verification using LLM Agents System",
        "authors": [
            "Phuc Phan Van",
            "Dat Nguyen Minh",
            "An Dinh Ngoc",
            "Huy Phan Thanh"
        ],
        "date": "2024/09/05",
        "pdf": "http://arxiv.org/pdf/2409.03440",
        "abstract": "To protect patient safety, modern pharmaceutical complexity demands strict prescription verification. We offer a new approach - Rx Strategist - that makes use of knowledge graphs and different search strategies to enhance the power of Large Language Models (LLMs) inside an agentic framework. This multifaceted technique allows for a multi-stage LLM pipeline and reliable information retrieval from a custom-built active ingredient database. Different facets of prescription verification, such as indication, dose, and possible drug interactions, are covered in each stage of the pipeline. We alleviate the drawbacks of monolithic LLM techniques by spreading reasoning over these stages, improving correctness and reliability while reducing memory demands. Our findings demonstrate that Rx Strategist surpasses many current LLMs, achieving performance comparable to that of a highly experienced clinical pharmacist. In the complicated world of modern medications, this combination of LLMs with organized knowledge and sophisticated search methods presents a viable avenue for reducing prescription errors and enhancing patient outcomes.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.03440"
    },
    "d93fb53beb898aadb36fc058ce9e3497": {
        "title": "LLM-based multi-agent poetry generation in non-cooperative environments",
        "authors": [
            "Ran Zhang",
            "Steffen Eger"
        ],
        "date": "2024/09/05",
        "pdf": "http://arxiv.org/pdf/2409.03659",
        "abstract": "Despite substantial progress of large language models (LLMs) for automatic poetry generation, the generated poetry lacks diversity while the training process differs greatly from human learning. Under the rationale that the learning process of the poetry generation systems should be more human-like and their output more diverse and novel, we introduce a framework based on social learning where we emphasize non-cooperative interactions besides cooperative interactions to encourage diversity. Our experiments are the first attempt at LLM-based multi-agent systems in non-cooperative environments for poetry generation employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED agents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows that our framework benefits the poetry generation process for TRAINING-BASED agents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity and a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams. The generated poetry from TRAINING-BASED agents also exhibits group divergence in terms of lexicons, styles and semantics. PROMPTING-BASED agents in our framework also benefit from non-cooperative environments and a more diverse ensemble of models with non-homogeneous agents has the potential to further enhance diversity, with an increase of 7.0-17.5 pp according to our experiments. However, PROMPTING-BASED agents show a decrease in lexical diversity over time and do not exhibit the group-based divergence intended in the social network. Our paper argues for a paradigm shift in creative tasks such as automatic poetry generation to include social learning processes (via LLM-based agent modeling) similar to human interaction.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.03659"
    },
    "8e071e37350eccaf0e7fd83d1ebb3472": {
        "title": "Sparse Rewards Can Self-Train Dialogue Agents",
        "authors": [
            "Barrett Martin Lattimer",
            "Varun Gangal",
            "Ryan McDonald",
            "Yi Yang"
        ],
        "date": "2024/09/06",
        "pdf": "http://arxiv.org/pdf/2409.04617",
        "abstract": "Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM) agents, especially in multi-turn dialogue tasks, have been primarily driven by supervised fine-tuning and high-quality human feedback. However, as base LLM models continue to improve, acquiring meaningful human feedback has become increasingly challenging and costly. In certain domains, base LLM agents may eventually exceed human capabilities, making traditional feedback-driven methods impractical. In this paper, we introduce a novel self-improvement paradigm that empowers LLM agents to autonomously enhance their performance without external human feedback. Our method, Juxtaposed Outcomes for Simulation Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward simulation environment to extract ideal behaviors and further train the LLM on its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation environment derived from MultiWOZ. We demonstrate that models trained with JOSH, both small and frontier, significantly improve tool-based interactions while preserving general model capabilities across diverse benchmarks. Our code and data are publicly available on GitHub at https://github.com/asappresearch/josh-llm-simulation-training",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2409.04617"
    },
    "cd493b2016ded91e8c3522c4451d21ab": {
        "title": "Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs",
        "authors": [
            "Firoj Alam",
            "Md. Rafiul Biswas",
            "Uzair Shah",
            "Wajdi Zaghouani",
            "Georgios Mikros"
        ],
        "date": "2024/09/11",
        "pdf": "http://arxiv.org/pdf/2409.07246",
        "abstract": "In the past decade, social media platforms have been used for information dissemination and consumption. While a major portion of the content is posted to promote citizen journalism and public awareness, some content is posted to mislead users. Among different content types such as text, images, and videos, memes (text overlaid on images) are particularly prevalent and can serve as powerful vehicles for propaganda, hate, and humor. In the current literature, there have been efforts to individually detect such content in memes. However, the study of their intersection is very limited. In this study, we explore the intersection between propaganda and hate in memes using a multi-agent LLM-based approach. We extend the propagandistic meme dataset with coarse and fine-grained hate labels. Our finding suggests that there is an association between propaganda and hate in memes. We provide detailed experimental results that can serve as a baseline for future studies. We will make the experimental resources publicly available to the community (https://github.com/firojalam/propaganda-and-hateful-memes).",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.07246"
    },
    "17b0196512689b297676db675a040c9f": {
        "title": "Agent Workflow Memory",
        "authors": [
            "Zora Zhiruo Wang",
            "Jiayuan Mao",
            "Daniel Fried",
            "Graham Neubig"
        ],
        "date": "2024/09/11",
        "pdf": "http://arxiv.org/pdf/2409.07429",
        "abstract": "Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2409.07429"
    },
    "38e61751bacde32ac0c4114d29a893d5": {
        "title": "Knowledge Tagging with Large Language Model based Multi-Agent System",
        "authors": [
            "Hang Li",
            "Tianlong Xu",
            "Ethan Chang",
            "Qingsong Wen"
        ],
        "date": "2024/09/12",
        "pdf": "http://arxiv.org/pdf/2409.08406",
        "abstract": "Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.08406"
    },
    "46fcbbcb6a5e36b2331f5099cd3ab9ca": {
        "title": "The Art of Storytelling: Multi-Agent Generative AI for Dynamic Multimodal Narratives",
        "authors": [
            "Samee Arif",
            "Taimoor Arif",
            "Muhammad Saad Haroon",
            "Aamina Jamal Khan",
            "Agha Ali Raza",
            "Awais Athar"
        ],
        "date": "2024/09/17",
        "pdf": "http://arxiv.org/pdf/2409.11261",
        "abstract": "This paper introduces the concept of an education tool that utilizes Generative Artificial Intelligence (GenAI) to enhance storytelling for children. The system combines GenAI-driven narrative co-creation, text-to-speech conversion, and text-to-video generation to produce an engaging experience for learners. We describe the co-creation process, the adaptation of narratives into spoken words using text-to-speech models, and the transformation of these narratives into contextually relevant visuals through text-to-video technology. Our evaluation covers the linguistics of the generated stories, the text-to-speech conversion quality, and the accuracy of the generated visuals.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.11261"
    },
    "286fa2ac65574904a8059dc636a6de68": {
        "title": "CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark",
        "authors": [
            "Zachary S. Siegel",
            "Sayash Kapoor",
            "Nitya Nagdir",
            "Benedikt Stroebl",
            "Arvind Narayanan"
        ],
        "date": "2024/09/17",
        "pdf": "http://arxiv.org/pdf/2409.11363",
        "abstract": "AI agents have the potential to aid users on a variety of consequential tasks, including conducting scientific research. To spur the development of useful agents, we need benchmarks that are challenging, but more crucially, directly correspond to real-world tasks of interest. This paper introduces such a benchmark, designed to measure the accuracy of AI agents in tackling a crucial yet surprisingly challenging aspect of scientific research: computational reproducibility. This task, fundamental to the scientific process, involves reproducing the results of a study using the provided code and data. We introduce CORE-Bench (Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasks based on 90 scientific papers across three disciplines (computer science, social science, and medicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy of agents in a fast and parallelizable way, saving days of evaluation time for each run compared to a sequential implementation. We evaluated two baseline agents: the general-purpose AutoGPT and a task-specific agent called CORE-Agent. We tested both variants using two underlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracy of 21% on the hardest task, showing the vast scope for improvement in automating routine scientific tasks. Having agents that can reproduce existing work is a necessary step towards building agents that can conduct novel research and could verify and improve the performance of other research agents. We hope that CORE-Bench can improve the state of reproducibility and spur the development of future research agents.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2409.11363"
    },
    "8d33203149957afde0b5aa3c179bec89": {
        "title": "MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning",
        "authors": [
            "Justin Chih-Yao Chen",
            "Archiki Prasad",
            "Swarnadeep Saha",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "date": "2024/09/18",
        "pdf": "http://arxiv.org/pdf/2409.12147",
        "abstract": "Large Language Models&#39; (LLM) reasoning can be improved using test-time aggregation strategies, i.e., generating multiple samples and voting among generated samples. While these improve performance, they often reach a saturation point. Refinement offers an alternative by using LLM-generated feedback to improve solution quality. However, refinement introduces 3 key challenges: (1) Excessive refinement: Uniformly refining all instances can over-correct and reduce the overall performance. (2) Inability to localize and address errors: LLMs have a limited ability to self-correct and struggle to identify and correct their own mistakes. (3) Insufficient refinement: Deciding how many iterations of refinement are needed is non-trivial, and stopping too soon could leave errors unaddressed. To tackle these issues, we propose MAgICoRe, which avoids excessive refinement by categorizing problem difficulty as easy or hard, solving easy problems with coarse-grained aggregation and hard ones with fine-grained and iterative multi-agent refinement. To improve error localization, we incorporate external step-wise reward model (RM) scores. Moreover, to ensure effective refinement, we employ a multi-agent loop with three agents: Solver, Reviewer (which generates targeted feedback based on step-wise RM scores), and the Refiner (which incorporates feedback). To ensure sufficient refinement, we re-evaluate updated solutions, iteratively initiating further rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5 and show its effectiveness across 5 math datasets. Even one iteration of MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by 4.0% while using less than half the samples. Unlike iterative refinement with baselines, MAgICoRe continues to improve with more iterations. Finally, our ablations highlight the importance of MAgICoRe&#39;s RMs and multi-agent communication.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.12147"
    },
    "737ce2a7ac8b9939607d73146d29408e": {
        "title": "Textualized Agent-Style Reasoning for Complex Tasks by Multiple Round LLM Generation",
        "authors": [
            "Chen Liang",
            "Zhifan Feng",
            "Zihe Liu",
            "Wenbin Jiang",
            "Jinan Xu",
            "Yufeng Chen",
            "Yong Wang"
        ],
        "date": "2024/09/19",
        "pdf": "http://arxiv.org/pdf/2409.12411",
        "abstract": "Chain-of-thought prompting significantly boosts the reasoning ability of large language models but still faces three issues: hallucination problem, restricted interpretability, and uncontrollable generation. To address these challenges, we present AgentCOT, a llm-based autonomous agent framework, which can solve complex problems in an agent-style manner by multiple round LLM generation. At each step, AgentCOT selects an action and executes it to yield an intermediate result with supporting evidence. In addition, we integrate the step&#39;s index into the reasoning process to form a graph structure for complex inference logic. We introduce two new strategies to enhance the performance of AgentCOT.We conduct extensive experiments to verify the effectiveness of our method on six common benchmarks. Results exhibit that our method brings in substantial improvements over current competitive approaches.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.12411"
    },
    "b6d51b8f0e9a4d100cca5ea9a105c5eb": {
        "title": "FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists",
        "authors": [
            "Tenghao Huang",
            "Donghee Lee",
            "John Sweeney",
            "Jiatong Shi",
            "Emily Steliotes",
            "Matthew Lange",
            "Jonathan May",
            "Muhao Chen"
        ],
        "date": "2024/09/19",
        "pdf": "http://arxiv.org/pdf/2409.12832",
        "abstract": "Flavor development in the food industry is increasingly challenged by the need for rapid innovation and precise flavor profile creation. Traditional flavor research methods typically rely on iterative, subjective testing, which lacks the efficiency and scalability required for modern demands. This paper presents three contributions to address the challenges. Firstly, we define a new problem domain for scientific agents in flavor science, conceptualized as the generation of hypotheses for flavor profile sourcing and understanding. To facilitate research in this area, we introduce the FoodPuzzle, a challenging benchmark consisting of 978 food items and 1,766 flavor molecules profiles. We propose a novel Scientific Agent approach, integrating in-context learning and retrieval augmented techniques to generate grounded hypotheses in the domain of food science. Experimental results indicate that our model significantly surpasses traditional methods in flavor profile prediction tasks, demonstrating its potential to transform flavor development practices.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.12832"
    },
    "87ab004e36b06b4054d9d2b8d6a5d107": {
        "title": "Minstrel: Structural Prompt Generation with Multi-Agents Coordination for Non-AI Experts",
        "authors": [
            "Ming Wang",
            "Yuanzhong Liu",
            "Xiaoyu Liang",
            "Yijie Huang",
            "Daling Wang",
            "Xiaocui Yang",
            "Sijia Shen",
            "Shi Feng",
            "Xiaoming Zhang",
            "Chaofeng Guan",
            "Yifei Zhang"
        ],
        "date": "2024/09/20",
        "pdf": "http://arxiv.org/pdf/2409.13449",
        "abstract": "LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to assist them in their work poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structural design, incurring high learning costs and it is not conducive to the iterative updating of prompts, especially for non-AI experts. Inspired by structured reusable programming languages, we propose LangGPT, a structural prompt design framework. Furthermore, we introduce Minstrel, a multi-generative agent system with reflection to automate the generation of structural prompts. Experiments and the case study illustrate that structural prompts generated by Minstrel or written manually significantly enhance the performance of LLMs. Furthermore, we analyze the ease of use of structural prompts through a user survey in our online community.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.13449"
    },
    "11dda558774bbd22e20ed446273f9686": {
        "title": "Language agents achieve superhuman synthesis of scientific knowledge",
        "authors": [
            "Michael D. Skarlinski",
            "Sam Cox",
            "Jon M. Laurent",
            "James D. Braza",
            "Michaela Hinks",
            "Michael J. Hammerling",
            "Manvitha Ponnapati",
            "Samuel G. Rodriques",
            "Andrew D. White"
        ],
        "date": "2024/09/10",
        "pdf": "http://arxiv.org/pdf/2409.13740",
        "abstract": "Language models are known to hallucinate incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipedia-style summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 +/- 1.99 contradictions per paper in a random subset of biology papers, of which 70% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2409.13740"
    },
    "ab497c820fe14d1b7f6fa4350247b2bc": {
        "title": "GroupDebate: Enhancing the Efficiency of Multi-Agent Debate Using Group Discussion",
        "authors": [
            "Tongxuan Liu",
            "Xingyu Wang",
            "Weizhe Huang",
            "Wenjiang Xu",
            "Yuting Zeng",
            "Lei Jiang",
            "Hailong Yang",
            "Jing Li"
        ],
        "date": "2024/09/21",
        "pdf": "http://arxiv.org/pdf/2409.14051",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse NLP tasks. Extensive research has explored how to enhance the logical reasoning abilities such as Chain-of-Thought, Chain-of-Thought with Self-Consistency, Tree-Of-Thoughts, and multi-agent debates. In the context of multi-agent debates, significant performance improvements can be achieved with an increasing number of agents and debate rounds. However, the escalation in the number of agents and debate rounds can drastically raise the tokens cost of debates, thereby limiting the scalability of the multi-agent debate technique. To better harness the advantages of multi-agent debates in logical reasoning tasks, this paper proposes a method to significantly reduce token cost in multi-agent debates. This approach involves dividing all agents into multiple debate groups, with agents engaging in debates within their respective groups and sharing interim debate results between groups. Comparative experiments across multiple datasets have demonstrated that this method can reduce the total tokens by up to 51.7% during debates and while potentially enhancing accuracy by as much as 25%. Our method significantly enhances the performance and efficiency of interactions in the multi-agent debate.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2409.14051"
    },
    "dbbc7e22bf928151d7636f8a71b9d283": {
        "title": "AgentGen: Enhancing Planning Abilities for Large Language Model based Agent via Environment and Task Generation",
        "authors": [
            "Mengkang Hu",
            "Pu Zhao",
            "Can Xu",
            "Qingfeng Sun",
            "Jianguang Lou",
            "Qingwei Lin",
            "Ping Luo",
            "Saravan Rajmohan"
        ],
        "date": "2024/08/01",
        "pdf": "http://arxiv.org/pdf/2408.00764",
        "abstract": "Large Language Model-based agents have garnered significant attention and are becoming increasingly popular. Furthermore, planning ability is a crucial component of an LLM-based agent, which generally entails achieving a desired goal from an initial state. This paper investigates enhancing the planning abilities of LLMs through instruction tuning, referred to as agent training. Recent studies have demonstrated that utilizing expert-level trajectory for instruction-tuning LLMs effectively enhances their planning capabilities. However, existing work primarily focuses on synthesizing trajectories from manually designed planning tasks and environments. The labor-intensive nature of creating these environments and tasks impedes the generation of sufficiently varied and extensive trajectories. To address this limitation, this paper explores the automated synthesis of diverse environments and a gradual range of planning tasks, from easy to difficult. We introduce a framework, AgentGen, that leverages LLMs first to generate environments and subsequently generate planning tasks conditioned on these environments. Specifically, to improve environmental diversity, we propose using an inspiration corpus composed of various domain-specific text segments as the context for synthesizing environments. Moreover, to increase the difficulty diversity of generated planning tasks, we propose a bidirectional evolution method, Bi-Evol, that evolves planning tasks from easier and harder directions to synthesize a task set with a smoother difficulty curve. The evaluation results derived from AgentBoard show that AgentGen greatly improves LLMs&#39; planning ability, e.g., the AgentGen instruction-tuned Llama-3.1-8B surpasses GPT-3.5 in overall performance. Moreover, the AgentGen-tuned Llama-3.1-70B model achieves state-of-the-art results in planning tasks. Project page: https://agent-gen.github.io/.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2408.00764"
    },
    "c6b627be92ba318c26367a5983aed27f": {
        "title": "Coalitions of Large Language Models Increase the Robustness of AI Agents",
        "authors": [
            "Prattyush Mangal",
            "Carol Mak",
            "Theo Kanakis",
            "Timothy Donovan",
            "Dave Braines",
            "Edward Pyzer-Knapp"
        ],
        "date": "2024/08/02",
        "pdf": "http://arxiv.org/pdf/2408.01380",
        "abstract": "The emergence of Large Language Models (LLMs) have fundamentally altered the way we interact with digital systems and have led to the pursuit of LLM powered AI agents to assist in daily workflows. LLMs, whilst powerful and capable of demonstrating some emergent properties, are not logical reasoners and often struggle to perform well at all sub-tasks carried out by an AI agent to plan and execute a workflow. While existing studies tackle this lack of proficiency by generalised pretraining at a huge scale or by specialised fine-tuning for tool use, we assess if a system comprising of a coalition of pretrained LLMs, each exhibiting specialised performance at individual sub-tasks, can match the performance of single model agents. The coalition of models approach showcases its potential for building robustness and reducing the operational costs of these AI agents by leveraging traits exhibited by specific models. Our findings demonstrate that fine-tuning can be mitigated by considering a coalition of pretrained models and believe that this approach can be applied to other non-agentic systems which utilise LLMs.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.01380"
    },
    "a74c42560de6b2fef2972ca80f4c95b3": {
        "title": "MathLearner: A Large Language Model Agent Framework for Learning to Solve Mathematical Problems",
        "authors": [
            "Wenbei Xie",
            "Donglin Liu",
            "Haoran Yan",
            "Wenjie Wu",
            "Zongyang Liu"
        ],
        "date": "2024/08/03",
        "pdf": "http://arxiv.org/pdf/2408.01779",
        "abstract": "With the development of artificial intelligence (AI), large language models (LLM) are widely used in many fields. However, the reasoning ability of LLM is still very limited when it comes to mathematical reasoning. Mathematics plays an important role in all aspects of human society and is a technical guarantee in the fields of healthcare, transport and aerospace, for this reason, the development of AI big language models in the field of mathematics has great potential significance. To improve the mathematical reasoning ability of large language models, we proposed an agent framework for learning to solve mathematical problems based on inductive reasoning. By emulating the human learning process of generalization of learned information and effective application of previous knowledge in new reasoning tasks, this framework has great performance in the mathematical reasoning process. It improves global accuracy over the baseline method (chain-of-thought) by 20.96% and solves 17.54% of the mathematical problems that the baseline cannot solve. Benefiting from the efficient RETRIEVAL method, our model improves the ability of large language models to efficiently use external knowledge, i.e., the mathematical computation of the model can be based on written procedures. In education, our model can be used as a personalised learning aid, thus reducing the inequality of educational resources.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.01779"
    },
    "a2dd498870d0d8035b33a8c00b113d29": {
        "title": "MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance",
        "authors": [
            "Jihye Choi",
            "Nils Palumbo",
            "Prasad Chalasani",
            "Matthew M. Engelhard",
            "Somesh Jha",
            "Anivarya Kumar",
            "David Page"
        ],
        "date": "2024/08/03",
        "pdf": "http://arxiv.org/pdf/2408.01869",
        "abstract": "In the era of Large Language Models (LLMs), given their remarkable text understanding and generation abilities, there is an unprecedented opportunity to develop new, LLM-based methods for trustworthy medical knowledge synthesis, extraction and summarization. This paper focuses on the problem of Pharmacovigilance (PhV), where the significance and challenges lie in identifying Adverse Drug Events (ADEs) from diverse text sources, such as medical literature, clinical notes, and drug labels. Unfortunately, this task is hindered by factors including variations in the terminologies of drugs and outcomes, and ADE descriptions often being buried in large amounts of narrative text. We present MALADE, the first effective collaborative multi-agent system powered by LLM with Retrieval Augmented Generation for ADE extraction from drug label data. This technique involves augmenting a query to an LLM with relevant information extracted from text resources, and instructing the LLM to compose a response consistent with the augmented data. MALADE is a general LLM-agnostic architecture, and its unique capabilities are: (1) leveraging a variety of external sources, such as medical literature, drug labels, and FDA tools (e.g., OpenFDA drug information API), (2) extracting drug-outcome association in a structured format along with the strength of the association, and (3) providing explanations for established associations. Instantiated with GPT-4 Turbo or GPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area Under ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our implementation leverages the Langroid multi-agent LLM framework and can be found at https://github.com/jihyechoi77/malade.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.01869"
    },
    "6dcd3421d4242819d1932b7446ff91bb": {
        "title": "ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems",
        "authors": [
            "Andrew Zhu",
            "Liam Dugan",
            "Chris Callison-Burch"
        ],
        "date": "2024/08/05",
        "pdf": "http://arxiv.org/pdf/2408.02248",
        "abstract": "Recently, there has been increasing interest in using Large Language Models (LLMs) to construct complex multi-agent systems to perform tasks such as compiling literature reviews, drafting consumer reports, and planning vacations. Many tools and libraries exist for helping create such systems, however none support recursive multi-agent systems -- where the models themselves flexibly decide when to delegate tasks and how to organize their delegation structure. In this work, we introduce ReDel: a toolkit for recursive multi-agent systems that supports custom tool-use, delegation schemes, event-based logging, and interactive replay in an easy-to-use web interface. We show that, using ReDel, we are able to easily identify potential areas of improvements through the visualization and debugging tools. Our code, documentation, and PyPI package are open-source and free to use under the MIT license at https://github.com/zhudotexe/redel.",
        "code": "",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2408.02248"
    },
    "9aea28457ee0355851fe7ef5e72a81c4": {
        "title": "Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions",
        "authors": [
            "Xinbei Ma",
            "Yiting Wang",
            "Yao Yao",
            "Tongxin Yuan",
            "Aston Zhang",
            "Zhuosheng Zhang",
            "Hai Zhao"
        ],
        "date": "2024/08/05",
        "pdf": "http://arxiv.org/pdf/2408.02544",
        "abstract": "This paper investigates the faithfulness of multimodal large language model (MLLM) agents in the graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general setting is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using our simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness (i.e., action accuracy) of multimodal agents, our findings indicate that these agents are prone to environmental distractions, resulting in unfaithful behaviors. Furthermore, we switch to the adversarial perspective and implement environment injection, demonstrating that such unfaithfulness can be exploited, leading to unexpected risks.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2408.02544"
    },
    "a23eda8dbe937be4a146b51306a36f25": {
        "title": "Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information",
        "authors": [
            "Yauwai Yim",
            "Chunkit Chan",
            "Tianyu Shi",
            "Zheye Deng",
            "Wei Fan",
            "Tianshi Zheng",
            "Yangqiu Song"
        ],
        "date": "2024/08/05",
        "pdf": "http://arxiv.org/pdf/2408.02559",
        "abstract": "Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.",
        "code": "",
        "category": [
            "Game Playing",
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2408.02559"
    },
    "e5c53089674be9108d8ce4f00ed5ae24": {
        "title": "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate",
        "authors": [
            "Yiqun Zhang",
            "Xiaocui Yang",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang",
            "Kaisong Song"
        ],
        "date": "2024/08/08",
        "pdf": "http://arxiv.org/pdf/2408.04472",
        "abstract": "Competitive debate is a complex task of computational argumentation. Large Language Models (LLMs) suffer from hallucinations and lack competitiveness in this field. To address these challenges, we introduce Agent for Debate (Agent4Debate), a dynamic multi-agent framework based on LLMs designed to enhance their capabilities in competitive debate. Drawing inspiration from human behavior in debate preparation and execution, Agent4Debate employs a collaborative architecture where four specialized agents, involving Searcher, Analyzer, Writer, and Reviewer, dynamically interact and cooperate. These agents work throughout the debate process, covering multiple stages from initial research and argument formulation to rebuttal and summary. To comprehensively evaluate framework performance, we construct the Competitive Debate Arena, comprising 66 carefully selected Chinese debate motions. We recruit ten experienced human debaters and collect records of 200 debates involving Agent4Debate, baseline models, and humans. The evaluation employs the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking. Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans. Furthermore, ablation studies demonstrate the effectiveness of each component in the agent structure.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2408.04472"
    },
    "d24290009beedf9b92d7e629bc7e6c7d": {
        "title": "MAG-SQL: Multi-Agent Generative Approach with Soft Schema Linking and Iterative Sub-SQL Refinement for Text-to-SQL",
        "authors": [
            "Wenxuan Xie",
            "Gaochen Wu",
            "Bowen Zhou"
        ],
        "date": "2024/08/15",
        "pdf": "http://arxiv.org/pdf/2408.07930",
        "abstract": "Recent In-Context Learning based methods have achieved remarkable success in Text-to-SQL task. However, there is still a large gap between the performance of these models and human performance on datasets with complex database schema and difficult questions, such as BIRD. Besides, existing work has neglected to supervise intermediate steps when solving questions iteratively with question decomposition methods, and the schema linking methods used in these works are very rudimentary. To address these issues, we propose MAG-SQL, a multi-agent generative approach with soft schema linking and iterative Sub-SQL refinement. In our framework, an entity-based method with tables&#39; summary is used to select the columns in database, and a novel targets-conditions decomposition method is introduced to decompose those complex questions. Additionally, we build a iterative generating module which includes a Sub-SQL Generator and Sub-SQL Refiner, introducing external oversight for each step of generation. Through a series of ablation studies, the effectiveness of each agent in our framework has been demonstrated. When evaluated on the BIRD benchmark with GPT-4, MAG-SQL achieves an execution accuracy of 61.08%, compared to the baseline accuracy of 46.35% for vanilla GPT-4 and the baseline accuracy of 57.56% for MAC-SQL. Besides, our approach makes similar progress on Spider. The codes are available at https://github.com/LancelotXWX/MAG-SQL.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2408.07930"
    },
    "e1991b64c3d638744b55d2c647b160df": {
        "title": "AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents",
        "authors": [
            "Guhong Chen",
            "Liyang Fan",
            "Zihan Gong",
            "Nan Xie",
            "Zixuan Li",
            "Ziqiang Liu",
            "Chengming Li",
            "Qiang Qu",
            "Shiwen Ni",
            "Min Yang"
        ],
        "date": "2024/08/15",
        "pdf": "http://arxiv.org/pdf/2408.08089",
        "abstract": "In this paper, we present a simulation system called AgentCourt that simulates the entire courtroom process. The judge, plaintiff&#39;s lawyer, defense lawyer, and other participants are autonomous agents driven by large language models (LLMs). Our core goal is to enable lawyer agents to learn how to argue a case, as well as improving their overall legal skills, through courtroom process simulation. To achieve this goal, we propose an adversarial evolutionary approach for the lawyer-agent. Since AgentCourt can simulate the occurrence and development of court hearings based on a knowledge base and LLM, the lawyer agents can continuously learn and accumulate experience from real court cases. The simulation experiments show that after two lawyer-agents have engaged in a thousand adversarial legal cases in AgentCourt (which can take a decade for real-world lawyers), compared to their pre-evolutionary state, the evolved lawyer agents exhibit consistent improvement in their ability to handle legal tasks. To enhance the credibility of our experimental results, we enlisted a panel of professional lawyers to evaluate our simulations. The evaluation indicates that the evolved lawyer agents exhibit notable advancements in responsiveness, as well as expertise and logical rigor. This work paves the way for advancing LLM-driven agent technology in legal scenarios. Code is available at https://github.com/relic-yuexi/AgentCourt.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2408.08089"
    },
    "261b8e7b4a92a56a944db8d0e097cf90": {
        "title": "The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation",
        "authors": [
            "Samee Arif",
            "Sualeha Farid",
            "Abdul Hameed Azeemi",
            "Awais Athar",
            "Agha Ali Raza"
        ],
        "date": "2024/08/16",
        "pdf": "http://arxiv.org/pdf/2408.08688",
        "abstract": "This paper presents a novel methodology for generating synthetic Preference Optimization (PO) datasets using multi-agent workflows. We evaluate the effectiveness and potential of these workflows in automating and enhancing the dataset generation process. PO dataset generation requires two modules: (1) response evaluation, and (2) response generation. In the response evaluation module, the responses from Large Language Models (LLMs) are evaluated and ranked - a task typically carried out by human annotators that we automate using LLMs. We assess the response evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators using three distinct prompting strategies. In step 2, we apply the winning prompting strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across all datasets. For the response generation module, we use the identified LLM evaluator configuration and compare different configurations of the LLM Feedback Loop. We use the win rate to determine the best multi-agent configuration for generation. Experimenting with various configurations, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-agent Llama and Gemma, respectively. After identifying the best configurations for both modules, we generate our PO datasets using the above pipeline.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2408.08688"
    },
    "a941f35559c198b6edff0f12757d76c1": {
        "title": "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model",
        "authors": [
            "Mengkang Hu",
            "Tianxing Chen",
            "Qiguang Chen",
            "Yao Mu",
            "Wenqi Shao",
            "Ping Luo"
        ],
        "date": "2024/08/18",
        "pdf": "http://arxiv.org/pdf/2408.09559",
        "abstract": "Large Language Model (LLM)-based agents exhibit significant potential across various domains, operating as interactive systems that process environmental observations to generate executable actions for target tasks. The effectiveness of these agents is significantly influenced by their memory mechanism, which records historical experiences as sequences of action-observation pairs. We categorize memory into two types: cross-trial memory, accumulated across multiple attempts, and in-trial memory (working memory), accumulated within a single attempt. While considerable research has optimized performance through cross-trial memory, the enhancement of agent performance through improved working memory utilization remains underexplored. Instead, existing approaches often involve directly inputting entire historical action-observation pairs into LLMs, leading to redundancy in long-horizon tasks. Inspired by human problem-solving strategies, this paper introduces HiAgent, a framework that leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. Specifically, HiAgent prompts LLMs to formulate subgoals before generating executable actions and enables LLMs to decide proactively to replace previous subgoals with summarized observations, retaining only the action-observation pairs relevant to the current subgoal. Experimental results across five long-horizon tasks demonstrate that HiAgent achieves a twofold increase in success rate and reduces the average number of steps required by 3.8. Additionally, our analysis shows that HiAgent consistently improves performance across various steps, highlighting its robustness and generalizability. Project Page: https://github.com/HiAgent2024/HiAgent .",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2408.09559"
    },
    "660d685a188a016b40085cefcafdfcc1": {
        "title": "BLADE: Benchmarking Language Model Agents for Data-Driven Science",
        "authors": [
            "Ken Gu",
            "Ruoxi Shang",
            "Ruien Jiang",
            "Keying Kuang",
            "Richard-John Lin",
            "Donghe Lyu",
            "Yue Mao",
            "Youran Pan",
            "Teng Wu",
            "Jiaqian Yu",
            "Yikun Zhang",
            "Tianmai M. Zhang",
            "Lanyi Zhu",
            "Mike A. Merrill",
            "Jeffrey Heer",
            "Tim Althoff"
        ],
        "date": "2024/08/19",
        "pdf": "http://arxiv.org/pdf/2408.09667",
        "abstract": "Data-driven scientific discovery requires the iterative integration of scientific domain knowledge, statistical expertise, and an understanding of data semantics to make nuanced analytical decisions, e.g., about which variables, transformations, and statistical models to consider. LM-based agents equipped with planning, memory, and code execution capabilities have the potential to support data-driven science. However, evaluating agents on such open-ended tasks is challenging due to multiple valid approaches, partially correct steps, and different ways to express the same decisions. To address these challenges, we present BLADE, a benchmark to automatically evaluate agents&#39; multifaceted approaches to open-ended research questions. BLADE consists of 12 datasets and research questions drawn from existing scientific literature, with ground truth collected from independent analyses by expert data scientists and researchers. To automatically evaluate agent responses, we developed corresponding computational methods to match different representations of analyses to this ground truth. Though language models possess considerable world knowledge, our evaluation shows that they are often limited to basic analyses. However, agents capable of interacting with the underlying data demonstrate improved, but still non-optimal, diversity in their analytical decision making. Our work enables the evaluation of agents for data-driven science and provides researchers deeper insights into agents&#39; analysis approaches.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2408.09667"
    },
    "55cb02be863c4932e55605e3b0fba125": {
        "title": "Anim-Director: A Large Multimodal Model Powered Agent for Controllable Animation Video Generation",
        "authors": [
            "Yunxin Li",
            "Haoyuan Shi",
            "Baotian Hu",
            "Longyue Wang",
            "Jiashun Zhu",
            "Jinyi Xu",
            "Zhen Zhao",
            "Min Zhang"
        ],
        "date": "2024/08/19",
        "pdf": "http://arxiv.org/pdf/2408.09787",
        "abstract": "Traditional animation generation methods depend on training generative models with human-labelled data, entailing a sophisticated multi-stage pipeline that demands substantial human effort and incurs high training costs. Due to limited prompting plans, these methods typically produce brief, information-poor, and context-incoherent animations. To overcome these limitations and automate the animation process, we pioneer the introduction of large multimodal models (LMMs) as the core processor to build an autonomous animation-making agent, named Anim-Director. This agent mainly harnesses the advanced understanding and reasoning capabilities of LMMs and generative AI tools to create animated videos from concise narratives or simple instructions. Specifically, it operates in three main stages: Firstly, the Anim-Director generates a coherent storyline from user inputs, followed by a detailed director&#39;s script that encompasses settings of character profiles and interior/exterior descriptions, and context-coherent scene descriptions that include appearing characters, interiors or exteriors, and scene events. Secondly, we employ LMMs with the image generation tool to produce visual images of settings and scenes. These images are designed to maintain visual consistency across different scenes using a visual-language prompting method that combines scene descriptions and images of the appearing character and setting. Thirdly, scene images serve as the foundation for producing animated videos, with LMMs generating prompts to guide this process. The whole process is notably autonomous without manual intervention, as the LMMs interact seamlessly with generative tools to generate prompts, evaluate visual quality, and select the best one to optimize the final output.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.09787"
    },
    "7cbf5c951a172306cc98e3d5da05a873": {
        "title": "Athena: Safe Autonomous Agents with Verbal Contrastive Learning",
        "authors": [
            "Tanmana Sadhu",
            "Ali Pesaranghader",
            "Yanan Chen",
            "Dong Hoon Yi"
        ],
        "date": "2024/08/20",
        "pdf": "http://arxiv.org/pdf/2408.11021",
        "abstract": "Due to emergent capabilities, large language models (LLMs) have been utilized as language-based agents to perform a variety of tasks and make decisions with an increasing degree of autonomy. These autonomous agents can understand high-level instructions, interact with their environments, and execute complex tasks using a selection of tools available to them. As the capabilities of the agents expand, ensuring their safety and trustworthiness becomes more imperative. In this study, we introduce the Athena framework which leverages the concept of verbal contrastive learning where past safe and unsafe trajectories are used as in-context (contrastive) examples to guide the agent towards safety while fulfilling a given task. The framework also incorporates a critiquing mechanism to guide the agent to prevent risky actions at every step. Furthermore, due to the lack of existing benchmarks on the safety reasoning ability of LLM-based agents, we curate a set of 80 toolkits across 8 categories with 180 scenarios to provide a safety evaluation benchmark. Our experimental evaluation, with both closed- and open-source LLMs, indicates verbal contrastive learning and interaction-level critiquing improve the safety rate significantly.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.11021"
    },
    "cd5004147ad6ef1ced7635c17f8b0bd8": {
        "title": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents",
        "authors": [
            "Congchi Yin",
            "Feng Li",
            "Shu Zhang",
            "Zike Wang",
            "Jun Shao",
            "Piji Li",
            "Jianhua Chen",
            "Xun Jiang"
        ],
        "date": "2024/08/22",
        "pdf": "http://arxiv.org/pdf/2408.12142",
        "abstract": "The clinical diagnosis of most mental disorders primarily relies on the conversations between psychiatrist and patient. The creation of such diagnostic conversation datasets is promising to boost the AI mental healthcare community. However, directly collecting the conversations in real diagnosis scenarios is near impossible due to stringent privacy and ethical considerations. To address this issue, we seek to synthesize diagnostic conversation by exploiting anonymized patient cases that are easier to access. Specifically, we design a neuro-symbolic multi-agent framework for synthesizing the diagnostic conversation of mental disorders with large language models. It takes patient case as input and is capable of generating multiple diverse conversations with one single patient case. The framework basically involves the interaction between a doctor agent and a patient agent, and generates conversations under symbolic control via a dynamic diagnosis tree. By applying the proposed framework, we develop the largest Chinese mental disorders diagnosis dataset MDD-5k. This dataset is built upon 1000 real, anonymized patient cases by cooperating with Shanghai Mental Health Center and comprises 5000 high-quality long conversations with diagnosis results and treatment opinions as labels. To the best of our knowledge, it&#39;s also the first labeled dataset for Chinese mental disorders diagnosis. Human evaluation demonstrates the proposed MDD-5k dataset successfully simulates human-like diagnostic process of mental disorders.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2408.12142"
    },
    "69ac0fb45e46442c4063c4c3143bc62a": {
        "title": "AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems",
        "authors": [
            "Chi-Min Chan",
            "Jianxuan Yu",
            "Weize Chen",
            "Chunyang Jiang",
            "Xinyu Liu",
            "Weijie Shi",
            "Zhiyuan Liu",
            "Wei Xue",
            "Yike Guo"
        ],
        "date": "2024/08/27",
        "pdf": "http://arxiv.org/pdf/2408.14972",
        "abstract": "The rapid advancement of large language models (LLMs) has led to the rise of LLM-based agents. Recent research shows that multi-agent systems (MAS), where each agent plays a specific role, can outperform individual LLMs. However, configuring an MAS for a task remains challenging, with performance only observable post-execution. Inspired by scaling laws in LLM development, we investigate whether MAS performance can be predicted beforehand. We introduce AgentMonitor, a framework that integrates at the agent level to capture inputs and outputs, transforming them into statistics for training a regression model to predict task performance. Additionally, it can further apply real-time corrections to address security risks posed by malicious agents, mitigating negative impacts and enhancing MAS security. Experiments demonstrate that an XGBoost model achieves a Spearman correlation of 0.89 in-domain and 0.58 in more challenging scenarios. Furthermore, using AgentMonitor reduces harmful content by 6.2% and increases helpful content by 1.8% on average, enhancing safety and reliability. Code is available at \\url{https://github.com/chanchimin/AgentMonitor}.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2408.14972"
    },
    "a8c5757f567133bd8ed62e4cf3274497": {
        "title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
        "authors": [
            "Yucheng Jiang",
            "Yijia Shao",
            "Dekun Ma",
            "Sina J. Semnani",
            "Monica S. Lam"
        ],
        "date": "2024/08/27",
        "pdf": "http://arxiv.org/pdf/2408.15232",
        "abstract": "While language model (LM)-powered chatbots and generative search engines excel at answering concrete queries, discovering information in the terrain of unknown unknowns remains challenging for users. To emulate the common educational scenario where children/students learn by listening to and participating in conversations of their parents/teachers, we create Collaborative STORM (Co-STORM). Unlike QA systems that require users to ask all the questions, Co-STORM lets users observe and occasionally steer the discourse among several LM agents. The agents ask questions on the user&#39;s behalf, allowing the user to discover unknown unknowns serendipitously. To facilitate user interaction, Co-STORM assists users in tracking the discourse by organizing the uncovered information into a dynamic mind map, ultimately generating a comprehensive report as takeaways. For automatic evaluation, we construct the WildSeek dataset by collecting real information-seeking records with user goals. Co-STORM outperforms baseline methods on both discourse trace and report quality. In a further human evaluation, 70% of participants prefer Co-STORM over a search engine, and 78% favor it over a RAG chatbot.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.15232"
    },
    "fb8a5b2f3bb18288e5d2a9878fa08723": {
        "title": "Interactive Agents: Simulating Counselor-Client Psychological Counseling via Role-Playing LLM-to-LLM Interactions",
        "authors": [
            "Huachuan Qiu",
            "Zhenzhong Lan"
        ],
        "date": "2024/08/28",
        "pdf": "http://arxiv.org/pdf/2408.15787",
        "abstract": "Virtual counselors powered by large language models (LLMs) aim to create interactive support systems that effectively assist clients struggling with mental health challenges. To replicate counselor-client conversations, researchers have built an online mental health platform that allows professional counselors to provide clients with text-based counseling services for about an hour per session. Notwithstanding its effectiveness, challenges exist as human annotation is time-consuming, cost-intensive, privacy-protected, and not scalable. To address this issue and investigate the applicability of LLMs in psychological counseling conversation simulation, we propose a framework that employs two LLMs via role-playing for simulating counselor-client interactions. Our framework involves two LLMs, one acting as a client equipped with a specific and real-life user profile and the other playing the role of an experienced counselor, generating professional responses using integrative therapy techniques. We implement both the counselor and the client by zero-shot prompting the GPT-4 model. In order to assess the effectiveness of LLMs in simulating counselor-client interactions and understand the disparities between LLM- and human-generated conversations, we evaluate the synthetic data from various perspectives. We begin by assessing the client&#39;s performance through automatic evaluations. Next, we analyze and compare the disparities between dialogues generated by the LLM and those generated by professional counselors. Furthermore, we conduct extensive experiments to thoroughly examine the performance of our LLM-based counselor trained with synthetic interactive dialogues by benchmarking against state-of-the-art models for mental health.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.15787"
    },
    "65fca19276be73ed930286269796a862": {
        "title": "BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems",
        "authors": [
            "Wei Wang",
            "Dan Zhang",
            "Tao Feng",
            "Boyan Wang",
            "Jie Tang"
        ],
        "date": "2024/08/28",
        "pdf": "http://arxiv.org/pdf/2408.15971",
        "abstract": "Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, e.g., building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the collaboration capabilities of language models. Many benchmarks are proposed to evaluate their collaborative abilities. However, these benchmarks lack fine-grained evaluations of LLM collaborative capabilities. Additionally, multi-agent collaborative and competitive scenarios are ignored in existing works. To address these two problems, we propose a benchmark, called BattleAgentBench, which defines seven sub-stages of three varying difficulty levels and conducts a fine-grained evaluation of language models in terms of single-agent scenario navigation capabilities, paired-agent task execution abilities, and multi-agent collaboration and competition capabilities. We conducted extensive evaluations on leading four closed-source and seven open-source models. Experimental results indicate that API-based models perform excellently on simple tasks but open-source small models struggle with simple tasks. Regarding difficult tasks that require collaborative and competitive abilities, although API-based models have demonstrated some collaborative capabilities, there is still enormous room for improvement.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2408.15971"
    },
    "21beb16d5cb74aaced3f93b09ff8fee6": {
        "title": "Tool-Assisted Agent on SQL Inspection and Refinement in Real-World Scenarios",
        "authors": [
            "Zhongyuan Wang",
            "Richong Zhang",
            "Zhijie Nie",
            "Jaein Kim"
        ],
        "date": "2024/08/30",
        "pdf": "http://arxiv.org/pdf/2408.16991",
        "abstract": "Recent Text-to-SQL methods leverage large language models (LLMs) by incorporating feedback from the database management system. While these methods effectively address execution errors in SQL queries, they struggle with database mismatches -- errors that do not trigger execution exceptions. Database mismatches include issues such as condition mismatches and stricter constraint mismatches, both of which are more prevalent in real-world scenarios. To address these challenges, we propose a tool-assisted agent framework for SQL inspection and refinement, equipping the LLM-based agent with two specialized tools: a retriever and a detector, designed to diagnose and correct SQL queries with database mismatches. These tools enhance the capability of LLMs to handle real-world queries more effectively. We also introduce Spider-Mismatch, a new dataset specifically constructed to reflect the condition mismatch problems encountered in real-world scenarios. Experimental results demonstrate that our method achieves the highest performance on the averaged results of the Spider and Spider-Realistic datasets in few-shot settings, and it significantly outperforms baseline methods on the more realistic dataset, Spider-Mismatch.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2408.16991"
    },
    "1a3f39eece9285a119b607a90bd48082": {
        "title": "OmniParser for Pure Vision Based GUI Agent",
        "authors": [
            "Yadong Lu",
            "Jianwei Yang",
            "Yelong Shen",
            "Ahmed Awadallah"
        ],
        "date": "2024/08/01",
        "pdf": "http://arxiv.org/pdf/2408.00203",
        "abstract": "The recent success of large vision language models shows great potential in driving the agent system operating on user interfaces. However, we argue that the power multimodal models like GPT-4V as a general agent on multiple operating systems across different applications is largely underestimated due to the lack of a robust screen parsing technique capable of: 1) reliably identifying interactable icons within the user interface, and 2) understanding the semantics of various elements in a screenshot and accurately associate the intended action with the corresponding region on the screen. To fill these gaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user interface screenshots into structured elements, which significantly enhances the ability of GPT-4V to generate actions that can be accurately grounded in the corresponding regions of the interface. We first curated an interactable icon detection dataset using popular webpages and an icon description dataset. These datasets were utilized to fine-tune specialized models: a detection model to parse interactable regions on the screen and a caption model to extract the functional semantics of the detected elements. \\textsc{OmniParser} significantly improves GPT-4V&#39;s performance on ScreenSpot benchmark. And on Mind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input outperforms the GPT-4V baselines requiring additional information outside of screenshot.",
        "code": "",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2408.00203"
    },
    "fb9de19dc325901d191ebb8e3ace969f": {
        "title": "Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base",
        "authors": [
            "Zhiyu An",
            "Xianzhong Ding",
            "Yen-Chun Fu",
            "Cheng-Chung Chu",
            "Yan Li",
            "Wan Du"
        ],
        "date": "2024/07/20",
        "pdf": "http://arxiv.org/pdf/2408.00798",
        "abstract": "This paper introduces Golden-Retriever, designed to efficiently navigate vast industrial knowledge bases, overcoming challenges in traditional LLM fine-tuning and RAG frameworks with domain-specific jargon and context interpretation. Golden-Retriever incorporates a reflection-based question augmentation step before document retrieval, which involves identifying jargon, clarifying its meaning based on context, and augmenting the question accordingly. Specifically, our method extracts and lists all jargon and abbreviations in the input question, determines the context against a pre-defined list, and queries a jargon dictionary for extended definitions and descriptions. This comprehensive augmentation ensures the RAG framework retrieves the most relevant documents by providing clear context and resolving ambiguities, significantly improving retrieval accuracy. Evaluations using three open-source LLMs on a domain-specific question-answer dataset demonstrate Golden-Retriever&#39;s superior performance, providing a robust solution for efficiently integrating and querying industrial knowledge bases.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.00798"
    },
    "d573edd529e927b53c5bd5c11dce12ec": {
        "title": "AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment of Bullying and Joking in Peer Interactions in Schools",
        "authors": [
            "Aditya Paul",
            "Chi Lok Yu",
            "Eva Adelina Susanto",
            "Nicholas Wai Long Lau",
            "Gwenyth Isobel Meadows"
        ],
        "date": "2024/07/27",
        "pdf": "http://arxiv.org/pdf/2408.01459",
        "abstract": "Addressing school bullying effectively and promptly is crucial for the mental health of students. This study examined the potential of large language models (LLMs) to empower students by discerning between bullying and joking in school peer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus, evaluating their effectiveness through human review. Our results revealed that not all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the most promise. We observed variations in LLM outputs, possibly influenced by political overcorrectness, context window limitations, and pre-existing bias in their training data. ChatGPT-4 excelled in context-specific accuracy after implementing the agentic approach, highlighting its potential to provide continuous, real-time support to vulnerable students. This study underlines the significant social impact of using agentic AI in educational settings, offering a new avenue for reducing the negative consequences of bullying and enhancing student well-being.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.01459"
    },
    "764d26b7d815b1676b80b586bb776d93": {
        "title": "Self-Emotion Blended Dialogue Generation in Social Simulation Agents",
        "authors": [
            "Qiang Zhang",
            "Jason Naradowsky",
            "Yusuke Miyao"
        ],
        "date": "2024/08/03",
        "pdf": "http://arxiv.org/pdf/2408.01633",
        "abstract": "When engaging in conversations, dialogue agents in a virtual simulation environment may exhibit their own emotional states that are unrelated to the immediate conversational context, a phenomenon known as self-emotion. This study explores how such self-emotion affects the agents&#39; behaviors in dialogue strategies and decision-making within a large language model (LLM)-driven simulation framework. In a dialogue strategy prediction experiment, we analyze the dialogue strategy choices employed by agents both with and without self-emotion, comparing them to those of humans. The results show that incorporating self-emotion helps agents exhibit more human-like dialogue strategies. In an independent experiment comparing the performance of models fine-tuned on GPT-4 generated dialogue datasets, we demonstrate that self-emotion can lead to better overall naturalness and humanness. Finally, in a virtual simulation environment where agents have discussions on multiple topics, we show that self-emotion of agents can significantly influence the decision-making process of the agents, leading to approximately a 50% change in decisions.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.01633"
    },
    "cff7a01884eecfae46e3c6916e5e5203": {
        "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
        "authors": [
            "Haolin Jin",
            "Linghan Huang",
            "Haipeng Cai",
            "Jun Yan",
            "Bo Li",
            "Huaming Chen"
        ],
        "date": "2024/08/05",
        "pdf": "http://arxiv.org/pdf/2408.02479",
        "abstract": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2408.02479"
    },
    "086a5cb864eb76025e61465d55c23120": {
        "title": "OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents",
        "authors": [
            "Qiang Sun",
            "Yuanyi Luo",
            "Sirui Li",
            "Wenxiao Zhang",
            "Wei Liu"
        ],
        "date": "2024/08/06",
        "pdf": "http://arxiv.org/pdf/2408.03047",
        "abstract": "Multimodal conversational agents are highly desirable because they offer natural and human-like interaction. However, there is a lack of comprehensive end-to-end solutions to support collaborative development and benchmarking. While proprietary systems like GPT-4o and Gemini demonstrating impressive integration of audio, video, and text with response times of 200-250ms, challenges remain in balancing latency, accuracy, cost, and data privacy. To better understand and quantify these issues, we developed OpenOmni, an open-source, end-to-end pipeline benchmarking tool that integrates advanced technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented Generation, Large Language Models, along with the ability to integrate customized models. OpenOmni supports local and cloud deployment, ensuring data privacy and supporting latency and accuracy benchmarking. This flexible framework allows researchers to customize the pipeline, focusing on real bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can significantly enhance applications like indoor assistance for visually impaired individuals, advancing human-computer interaction. Our demonstration video is available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via https://openomni.ai4wa.com, code is available via https://github.com/AI4WA/OpenOmniFramework.",
        "code": "",
        "category": [
            "Environment&Platform"
        ],
        "url": "https://arxiv.org/abs/2408.03047"
    },
    "5d434acddc21b38bc2619646fa1fea05": {
        "title": "Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks",
        "authors": [
            "Zaijing Li",
            "Yuquan Xie",
            "Rui Shao",
            "Gongwei Chen",
            "Dongmei Jiang",
            "Liqiang Nie"
        ],
        "date": "2024/08/07",
        "pdf": "http://arxiv.org/pdf/2408.03615",
        "abstract": "Building a general-purpose agent is a long-standing vision in the field of artificial intelligence. Existing agents have made remarkable progress in many domains, yet they still struggle to complete long-horizon tasks in an open world. We attribute this to the lack of necessary world knowledge and multimodal experience that can guide agents through a variety of long-horizon tasks. In this paper, we propose a Hybrid Multimodal Memory module to address the above challenges. It 1) transforms knowledge into Hierarchical Directed Knowledge Graph that allows agents to explicitly represent and learn world knowledge, and 2) summarises historical information into Abstracted Multimodal Experience Pool that provide agents with rich references for in-context learning. On top of the Hybrid Multimodal Memory module, a multimodal agent, Optimus-1, is constructed with dedicated Knowledge-guided Planner and Experience-Driven Reflector, contributing to a better planning and reflection in the face of long-horizon tasks in Minecraft. Extensive experimental results show that Optimus-1 significantly outperforms all existing agents on challenging long-horizon task benchmarks, and exhibits near human-level performance on many tasks. In addition, we introduce various Multimodal Large Language Models (MLLMs) as the backbone of Optimus-1. Experimental results show that Optimus-1 exhibits strong generalization with the help of the Hybrid Multimodal Memory module, outperforming the GPT-4V baseline on many tasks.",
        "code": "",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2408.03615"
    },
    "1ef6b8889528920614a7762918a84a3f": {
        "title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents",
        "authors": [
            "Xiao Liu",
            "Tianjie Zhang",
            "Yu Gu",
            "Iat Long Iong",
            "Yifan Xu",
            "Xixuan Song",
            "Shudan Zhang",
            "Hanyu Lai",
            "Xinyi Liu",
            "Hanlin Zhao",
            "Jiadai Sun",
            "Xinyue Yang",
            "Yu Yang",
            "Zehan Qi",
            "Shuntian Yao",
            "Xueqiao Sun",
            "Siyi Cheng",
            "Qinkai Zheng",
            "Hao Yu",
            "Hanchen Zhang",
            "Wenyi Hong",
            "Ming Ding",
            "Lihang Pan",
            "Xiaotao Gu",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chan Hee Song",
            "Yu Su",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2024/08/12",
        "pdf": "http://arxiv.org/pdf/2408.06327",
        "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs&#39; understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train \\&amp; test data, and part of fine-tuned open LMMs are available at \\url{https://github.com/THUDM/VisualAgentBench}.",
        "code": "",
        "category": [
            "Benchmark&Evaluation"
        ],
        "url": "https://arxiv.org/abs/2408.06327"
    },
    "a640af362a345be1dda61463a08f66d5": {
        "title": "Large Language Model Agent in Financial Trading: A Survey",
        "authors": [
            "Han Ding",
            "Yinheng Li",
            "Junhao Wang",
            "Hang Chen"
        ],
        "date": "2024/07/26",
        "pdf": "http://arxiv.org/pdf/2408.06361",
        "abstract": "Trading is a highly competitive task that requires a combination of strategy, knowledge, and psychological fortitude. With the recent success of large language models(LLMs), it is appealing to apply the emerging intelligence of LLM agents in this competitive arena and understanding if they can outperform professional traders. In this survey, we provide a comprehensive review of the current research on using LLMs as agents in financial trading. We summarize the common architecture used in the agent, the data inputs, and the performance of LLM trading agents in backtesting as well as the challenges presented in these research. This survey aims to provide insights into the current state of LLM-based financial trading agents and outline future research directions in this field.",
        "code": "",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2408.06361"
    },
    "4cb0753ab613272080fe321459948459": {
        "title": "Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models",
        "authors": [
            "Abhishek Dutta",
            "Yen-Che Hsiao"
        ],
        "date": "2024/08/12",
        "pdf": "http://arxiv.org/pdf/2408.06458",
        "abstract": "We propose a novel in-context learning algorithm for building autonomous decision-making language agents. The language agent continuously attempts to solve the same task by self-correcting each time the task fails. Our selected language agent demonstrates the ability to solve tasks in a text-based game environment. Our results show that the gemma-2-9b-it language model, using our proposed method, can successfully complete two of six tasks that failed in the first attempt. This highlights the effectiveness of our approach in enhancing the problem-solving capabilities of a single language model through self-correction, paving the way for more advanced autonomous agents. The code is publicly available at https://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.",
        "code": "",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2408.06458"
    },
    "969057ef323acecff476cec539bc0e0e": {
        "title": "Causal Agent based on Large Language Model",
        "authors": [
            "Kairong Han",
            "Kun Kuang",
            "Ziyu Zhao",
            "Junjian Ye",
            "Fei Wu"
        ],
        "date": "2024/08/13",
        "pdf": "http://arxiv.org/pdf/2408.06849",
        "abstract": "Large language models (LLMs) have achieved significant success across various domains. However, the inherent complexity of causal problems and causal theory poses challenges in accurately describing them in natural language, making it difficult for LLMs to comprehend and use them effectively. Causal methods are not easily conveyed through natural language, which hinders LLMs&#39; ability to apply them accurately. Additionally, causal datasets are typically tabular, while LLMs excel in handling natural language data, creating a structural mismatch that impedes effective reasoning with tabular data. This lack of causal reasoning capability limits the development of LLMs. To address these challenges, we have equipped the LLM with causal tools within an agent framework, named the Causal Agent, enabling it to tackle causal problems. The causal agent comprises tools, memory, and reasoning modules. In the tools module, the causal agent applies causal methods to align tabular data with natural language. In the reasoning module, the causal agent employs the ReAct framework to perform reasoning through multiple iterations with the tools. In the memory module, the causal agent maintains a dictionary instance where the keys are unique names and the values are causal graphs. To verify the causal ability of the causal agent, we established a benchmark consisting of four levels of causal problems: variable level, edge level, causal graph level, and causal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for these four levels of issues and tested the causal agent on the datasets. Our methodology demonstrates remarkable efficacy on the four-level causal problems, with accuracy rates all above 80%. For further insights and implementation details, our code is accessible via the GitHub repository https://github.com/Kairong-Han/Causal_Agent.",
        "code": "",
        "category": [
            "Agent Framework"
        ],
        "url": "https://arxiv.org/abs/2408.06849"
    },
    "3f890b32a1d1cf216d84404918e7425e": {
        "title": "Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents",
        "authors": [
            "Kexun Zhang",
            "Weiran Yao",
            "Zuxin Liu",
            "Yihao Feng",
            "Zhiwei Liu",
            "Rithesh Murthy",
            "Tian Lan",
            "Lei Li",
            "Renze Lou",
            "Jiacheng Xu",
            "Bo Pang",
            "Yingbo Zhou",
            "Shelby Heinecke",
            "Silvio Savarese",
            "Huan Wang",
            "Caiming Xiong"
        ],
        "date": "2024/08/13",
        "pdf": "http://arxiv.org/pdf/2408.07060",
        "abstract": "Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise. DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent&#39;s performance by a large margin. For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions. Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.",
        "code": "",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2408.07060"
    },
    "38b8de6a2c2166ed9e53dc1569ece917": {
        "title": "Development of a Large Language Model-based Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments",
        "authors": [
            "Seungjun Han",
            "Wongyung Choi"
        ],
        "date": "2024/08/14",
        "pdf": "http://arxiv.org/pdf/2408.07531",
        "abstract": "Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management. We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management. The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation. Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2408.07531"
    },
    "faee3dc900333e58359ab8708b0c9128": {
        "title": "Text2BIM: Generating Building Models Using a Large Language Model-based Multi-Agent Framework",
        "authors": [
            "Changyu Du",
            "Sebastian Esser",
            "Stavros Nousias",
            "André Borrmann"
        ],
        "date": "2024/08/15",
        "pdf": "http://arxiv.org/pdf/2408.08054",
        "abstract": "The conventional BIM authoring process typically requires designers to master complex and tedious modeling commands in order to materialize their design intentions within BIM authoring tools. This additional cognitive burden complicates the design process and hinders the adoption of BIM and model-based design in the AEC (Architecture, Engineering, and Construction) industry. To facilitate the expression of design intentions more intuitively, we propose Text2BIM, an LLM-based multi-agent framework that can generate 3D building models from natural language instructions. This framework orchestrates multiple LLM agents to collaborate and reason, transforming textual user input into imperative code that invokes the BIM authoring tool&#39;s APIs, thereby generating editable BIM models with internal layouts, external envelopes, and semantic information directly in the software. Furthermore, a rule-based model checker is introduced into the agentic workflow, utilizing predefined domain knowledge to guide the LLM agents in resolving issues within the generated models and iteratively improving model quality. Extensive experiments were conducted to compare and analyze the performance of three different LLMs under the proposed framework. The evaluation results demonstrate that our approach can effectively generate high-quality, structurally rational building models that are aligned with the abstract concepts specified by user input. Finally, an interactive software prototype was developed to integrate the framework into the BIM authoring software Vectorworks, showcasing the potential of modeling by chatting.",
        "code": "",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2408.08054"
    }
}