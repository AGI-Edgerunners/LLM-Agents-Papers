{
    "59ce6dd642c3cee234d3e05856eef0cc": {
        "title": "Adapting LLM Agents Through Communication",
        "authors": [
            "Kuan Wang",
            "Yadong Lu",
            "Michael Santacroce",
            "Yeyun Gong",
            "Chao Zhang",
            "Yelong Shen"
        ],
        "date": "2023/10/01",
        "pdf": "https://arxiv.org/pdf/2310.01444.pdf",
        "abstract": " Recent advancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Recent\nadvancements in large language models (LLMs) have shown potential for\nhuman-like agents. To help these agents adapt to new tasks without extensive\nhuman supervision, we propose the Learning through Communication (LTC)\nparadigm, a novel training approach enabling LLM agents to improve continuously\nthrough interactions with their environments and other agents. Through\niterative exploration and PPO training, LTC empowers the agent to assimilate\nshort-term experiences into long-term memory. To optimize agent interactions\nfor task-specific learning, we introduce three structured communication\npatterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as\ndecision-making, knowledge-intensive reasoning, and numerical reasoning. We\nevaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA\n(knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld,\nit exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA,\nLTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it\noutperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k,\nLTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results\nshowcase the versatility and efficiency of the LTC approach across diverse\ndomains. We will open-source our code to promote further development of the\ncommunity.\n",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2310.01444v2"
    },
    "a48d6f115726758bb29c8fe22f9dffd9": {
        "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
        "authors": [
            "Aohan Zeng",
            "Mingdao Liu",
            "Rui Lu",
            "Bowen Wang",
            "Xiao Liu",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/10/19",
        "pdf": "https://arxiv.org/pdf/2310.12823.pdf",
        "code": "https://github.com/THUDM/AgentTuning",
        "abstract": " Open large language models (LLMs) with great performance in various tasks\nhave significantly advanced the development of LLMs. However, they are far\ninferior to commercial models such as ChatGPT and GPT-4 when acting as agents\nto tackle complex tasks in the real world. These agent tasks employ LLMs as the\ncentral controller responsible for planning, memorization, and tool\nutilization, necessitating both fine-grained prompting methods and robust LLMs\nto achieve satisfactory performance. Though many prompting methods have been\nproposed to complete particular agent tasks, there is lack of research focusing\non improving the agent capabilities of LLMs themselves without compromising\ntheir general abilities. In this work, we present AgentTuning, a simple and\ngeneral method to enhance the agent abilities of LLMs while maintaining their\ngeneral LLM capabilities. We construct AgentInstruct, a lightweight\ninstruction-tuning dataset containing high-quality interaction trajectories. We\nemploy a hybrid instruction-tuning strategy by combining AgentInstruct with\nopen-source instructions from general domains. AgentTuning is used to\ninstruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show\nthat AgentTuning enables LLMs&#39; agent capabilities without compromising general\nabilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent\ntasks, demonstrating generalized agent capabilities. We open source the\nAgentInstruct and AgentLM-7B, 13B, and 70B models at\nhttps://github.com/THUDM/AgentTuning, serving open and powerful alternatives to\ncommercial LLMs for agent tasks.\n",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2310.12823"
    },
    "cf1de36f3cf9d3bf1383a44f988ddf99": {
        "title": "FireAct: Toward Language Agent Fine-tuning",
        "authors": [
            "Baian Chen",
            "Chang Shu",
            "Ehsan Shareghi",
            "Nigel Collier",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "date": "2023/10/09",
        "pdf": "https://arxiv.org/pdf/2310.05915.pdf",
        "code": "https://github.com/anchen1011/FireAct",
        "abstract": " Recent efforts have augmented language models (LMs) with external tools or\nenvironments, leading to the development of language agents that can reason and\nact. However, most of these agents rely on few-shot prompting techniques with\noff-the-shelf LMs. In this paper, we investigate and argue for the overlooked\ndirection of fine-tuning LMs to obtain language agents. Using a setup of\nquestion answering (QA) with a Google search API, we explore a variety of base\nLMs, prompting methods, fine-tuning data, and QA tasks, and find language\nagents are consistently improved after fine-tuning their backbone LMs. For\nexample, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4\nleads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct,\na novel approach to fine-tuning LMs with trajectories from multiple tasks and\nprompting methods, and show having more diverse fine-tuning data can further\nimprove agents. Along with other findings regarding scaling effects,\nrobustness, generalization, efficiency and cost, our work establishes\ncomprehensive benefits of fine-tuning LMs for agents, and provides an initial\nset of experimental designs, insights, as well as open questions toward\nlanguage agent fine-tuning.\n",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2310.05915"
    },
    "661810d5074a67af6c355de8a64c87b0": {
        "title": "Machine Mindset: An MBTI Exploration of Large Language Models",
        "authors": [
            "Jiaxi Cui",
            "Liuzhenghao Lv",
            "Jing Wen",
            "Jing Tang",
            "YongHong Tian",
            "Li Yuan"
        ],
        "date": "2023/12/20",
        "pdf": "http://arxiv.org/pdf/2312.12999.pdf",
        "code": "https://github.com/PKU-YuanGroup/Machine-Mindset",
        "abstract": "We present a novel approach for integrating Myers-Briggs Type Indicator (MBTI) personality traits into large language models (LLMs), addressing the challenges of personality consistency in personalized AI. Our method, &#34;Machine Mindset,&#34; involves a two-phase fine-tuning and Direct Preference Optimization (DPO) to embed MBTI traits into LLMs. This approach ensures that models internalize these traits, offering a stable and consistent personality profile. We demonstrate the effectiveness of our models across various domains, showing alignment between model performance and their respective MBTI traits. The paper highlights significant contributions in the development of personality datasets and a new training methodology for personality integration in LLMs, enhancing the potential for personalized AI applications. We also open-sourced our model and part of the data at \\url{https://github.com/PKU-YuanGroup/Machine-Mindset}.",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2312.12999"
    },
    "c8c04c2c8e3e947e8cdee88e9ce28e21": {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "authors": [
            "Ehsan Kamalloo",
            "Aref Jafari",
            "Xinyu Zhang",
            "Nandan Thakur",
            "Jimmy Lin"
        ],
        "date": "2023/07/31",
        "pdf": "https://arxiv.org/pdf/2307.16883",
        "code": "https://github.com/project-miracl/hagrid",
        "abstract": " The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.\n",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2307.16883"
    },
    "a46f1651f62db660d022c495bbab01a4": {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "authors": [
            "Xiao Liu",
            "Hao Yu",
            "Hanchen Zhang",
            "Yifan Xu",
            "Xuanyu Lei",
            "Hanyu Lai",
            "Yu Gu",
            "Hangliang Ding",
            "Kaiwen Men",
            "Kejuan Yang",
            "Shudan Zhang",
            "Xiang Deng",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chenhui Zhang",
            "Sheng Shen",
            "Tianjun Zhang",
            "Yu Su",
            "Huan Sun",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/08/07",
        "pdf": "https://arxiv.org/pdf/2308.03688",
        "code": "https://github.com/THUDM/AgentBench",
        "abstract": " Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent&#39;s reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 25 LLMs (including APIs\nand open-sourced models) shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and open-sourced competitors. It also\nserves as a component of an ongoing project with wider coverage and deeper\nconsideration towards systematic LLM evaluation. Datasets, environments, and an\nintegrated evaluation package for AgentBench are released at\nhttps://github.com/THUDM/AgentBench\n",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2308.03688"
    },
    "2d8ee880fda520da34ed2cd55d7f8096": {
        "title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents",
        "authors": [
            "Zhiwei Liu",
            "Weiran Yao",
            "Jianguo Zhang",
            "Le Xue",
            "Shelby Heinecke",
            "Rithesh Murthy",
            "Yihao Feng",
            "Zeyuan Chen",
            "Juan Carlos Niebles",
            "Devansh Arpit",
            "Ran Xu",
            "Phil Mui",
            "Huan Wang",
            "Caiming Xiong",
            "Silvio Savarese"
        ],
        "date": "2023/08/11",
        "pdf": "https://arxiv.org/pdf/2308.05960",
        "code": "https://github.com/salesforce/BOLAA",
        "abstract": " The massive successes of large language models (LLMs) encourage the emerging\nexploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to\ngenerate actions with its core LLM and interact with environments, which\nfacilitates the ability to resolve complex tasks by conditioning on past\ninteractions such as observations and actions. Since the investigation of LAA\nis still very recent, limited explorations are available. Therefore, we provide\na comprehensive comparison of LAA in terms of both agent architectures and LLM\nbackbones. Additionally, we propose a new strategy to orchestrate multiple LAAs\nsuch that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA,\nwhere a controller manages the communication among multiple agents. We conduct\nsimulations on both decision-making and multi-step reasoning environments,\nwhich comprehensively justify the capacity of LAAs. Our performance results\nprovide quantitative suggestions for designing LAA architectures and the\noptimal choice of LLMs, as well as the compatibility of both. We release our\nimplementation code of LAAs to the public at\n\\url{https://github.com/salesforce/BOLAA}.\n",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2308.05960"
    },
    "ff1281e65bb2fcc4de561ef67dd08f7f": {
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "authors": [
            "Xiang Deng",
            "Yu Gu",
            "Boyuan Zheng",
            "Shijie Chen",
            "Samuel Stevens",
            "Boshi Wang",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2023/06/09",
        "pdf": "https://arxiv.org/pdf/2306.06070",
        "code": "https://github.com/OSU-NLP-Group/Mind2Web",
        "abstract": " We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.\n",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2306.06070"
    },
    "e551d82f612e1b1fabd139d6337197b9": {
        "title": "Agents: An Open-source Framework for Autonomous Language Agents",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Long Li",
            "Jialong Wu",
            "Tiannan Wang",
            "Shi Qiu",
            "Jintian Zhang",
            "Jing Chen",
            "Ruipu Wu",
            "Shuai Wang",
            "Shiding Zhu",
            "Jiyu Chen",
            "Wentao Zhang",
            "Ningyu Zhang",
            "Huajun Chen",
            "Peng Cui",
            "Mrinmaya Sachan"
        ],
        "date": "2023/09/14",
        "pdf": "https://arxiv.org/pdf/2309.07870",
        "code": "https://github.com/aiwaves-cn/agents",
        "abstract": " Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\n",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2309.07870"
    },
    "d10c223c151e7cd6de70ba79e82f36cc": {
        "title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena",
        "authors": [
            "Jiangjie Chen",
            "Siyu Yuan",
            "Rong Ye",
            "Bodhisattwa Prasad Majumder",
            "Kyle Richardson"
        ],
        "date": "2023/10/09",
        "pdf": "https://arxiv.org/pdf/2310.05746.pdf",
        "code": "https://github.com/jiangjiechen/auction-arena",
        "abstract": "Can Large Language Models (LLMs) simulate human behavior in complex environments? LLMs have recently been shown to exhibit advanced reasoning skills but much of NLP evaluation still relies on static benchmarks. Answering this requires evaluation environments that probe strategic reasoning in competitive, dynamic scenarios that involve long-term planning. We introduce AucArena, a novel simulation environment for evaluating LLMs within auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct several controlled simulations using state-of-the-art LLMs as bidding agents. We find that through simple prompting, LLMs do indeed demonstrate many of the skills needed for effectively engaging in auctions (e.g., managing budget, adhering to long-term goals and priorities), skills that we find can be sharpened by explicitly encouraging models to be adaptive and observe strategies in past auctions. These results are significant as they show the potential of using LLM agents to model intricate social dynamics, especially in competitive settings. However, we also observe considerable variability in the capabilities of individual LLMs. Notably, even our most advanced models (GPT-4) are occasionally surpassed by heuristic baselines and human agents, highlighting the potential for further improvements in the design of LLM agents and the important role that our simulation environment can play in further testing and refining agent architectures.",
        "category": [
            "Benchmark&Evaluation&Framework",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2310.05746"
    },
    "012d0edc2f0cd976178627ba1878a2ee": {
        "title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
        "authors": [
            "Yue Wu",
            "Xuan Tang",
            "Tom M. Mitchell",
            "Yuanzhi Li"
        ],
        "date": "2023/10/02",
        "pdf": "https://arxiv.org/pdf/2310.01557.pdf",
        "code": "https://github.com/microsoft/SmartPlay",
        "abstract": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs&#39; abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/microsoft/SmartPlay",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2310.01557"
    },
    "804f5942ec7bac44830ad6243a4d15c7": {
        "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",
        "authors": [
            "Zhihan Liu",
            "Hao Hu",
            "Shenao Zhang",
            "Hongyi Guo",
            "Shuqi Ke",
            "Boyi Liu",
            "Zhaoran Wang"
        ],
        "date": "2023/09/29",
        "pdf": "https://arxiv.org/pdf/2309.17382.pdf",
        "code": "https://github.com/agentification/RAFA_code",
        "abstract": " Large language models (LLMs) demonstrate impressive reasoning abilities, but\ntranslating reasoning into actions in the real world remains challenging. In\nparticular, it remains unclear how to complete a given task provably within a\nminimum number of interactions with the external environment, e.g., through an\ninternal mechanism of reasoning. To this end, we propose a principled framework\nwith provable regret guarantees to orchestrate reasoning and acting, which we\ncall &#34;reason for future, act for now&#34; (\\texttt{RAFA}). Specifically, we design\na prompt template for reasoning that learns from the memory buffer and plans a\nfuture trajectory over a long horizon (&#34;reason for future&#34;). At each step, the\nLLM agent takes the initial action of the planned trajectory (&#34;act for now&#34;),\nstores the collected feedback in the memory buffer, and reinvokes the reasoning\nroutine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in\nBayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt\nLLMs to form an updated posterior of the unknown environment from the memory\nbuffer (learning) and generate an optimal trajectory for multiple future steps\nthat maximizes a value function (planning). The learning and planning\nsubroutines are performed in an &#34;in-context&#34; manner to emulate the actor-critic\nupdate for MDPs. Our theoretical analysis proves that the novel combination of\nlong-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. In\nparticular, the regret bound highlights an intriguing interplay between the\nprior knowledge obtained through pretraining and the uncertainty reduction\nachieved by reasoning and acting. Our empirical validation shows that it\noutperforms various existing frameworks and achieves nearly perfect scores on a\nfew benchmarks.\n",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2309.17382"
    },
    "1035d55aceab0dd79e95634189726dd8": {
        "title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting",
        "authors": [
            "Nicholas Farn",
            "Richard Shin"
        ],
        "date": "2023/11/15",
        "pdf": "https://arxiv.org/pdf/2311.10775.pdf",
        "code": "https://github.com/microsoft/ToolTalk",
        "abstract": "Large language models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Many recent works seek to augment LLM-based assistants with external tools so they can access private or up-to-date information and carry out actions on behalf of users. To better measure the performance of these assistants, this paper introduces ToolTalk, a benchmark consisting of complex user intents requiring multi-step tool usage specified through dialogue. ToolTalk contains 28 tools grouped into 7 plugins, and includes a complete simulated implementation of each tool, allowing for fully automated evaluation of assistants that rely on execution feedback. ToolTalk also emphasizes tools that externally affect the world rather than only tools for referencing or searching information. We evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and 50% respectively. Our analysis of the errors reveals three major categories and suggests some future directions for improvement. We release ToolTalk at https://github.com/microsoft/ToolTalk.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2311.10775"
    },
    "54f679e78e32c2a097dbea9d822cd5b1": {
        "title": "ProAgent: From Robotic Process Automation to Agentic Process Automation",
        "authors": [
            "Yining Ye",
            "Xin Cong",
            "Shizuo Tian",
            "Jiannan Cao",
            "Hao Wang",
            "Yujia Qin",
            "Yaxi Lu",
            "Heyang Yu",
            "Huadong Wang",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2023/11/02",
        "pdf": "https://arxiv.org/pdf/2311.10751.pdf",
        "code": "https://github.com/OpenBMB/ProAgent",
        "abstract": "From ancient water wheels to robotic process automation (RPA), automation technology has evolved throughout history to liberate human beings from arduous tasks. Yet, RPA struggles with tasks needing human-like intelligence, especially in elaborate design of workflow construction and dynamic decision-making in workflow execution. As Large Language Models (LLMs) have emerged human-like intelligence, this paper introduces Agentic Process Automation (APA), a groundbreaking automation paradigm using LLM-based agents for advanced automation by offloading the human labor to agents associated with construction and execution. We then instantiate ProAgent, an LLM-based agent designed to craft workflows from human instructions and make intricate decisions by coordinating specialized agents. Empirical experiments are conducted to detail its construction and execution procedure of workflow, showcasing the feasibility of APA, unveiling the possibility of a new paradigm of automation driven by agents. Our code is public at https://github.com/OpenBMB/ProAgent.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2311.10751"
    },
    "1a40deb82fb0656c7b0275ac70641179": {
        "title": "Testing Language Model Agents Safely in the Wild",
        "authors": [
            "Silen Naihin",
            "David Atkinson",
            "Marc Green",
            "Merwane Hamadi",
            "Craig Swift",
            "Douglas Schonholtz",
            "Adam Tauman Kalai",
            "David Bau"
        ],
        "date": "2023/11/17",
        "pdf": "https://arxiv.org/pdf/2311.10538.pdf",
        "abstract": "A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We a design a basic safety monitor that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the safety monitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2311.10538"
    },
    "a395035cf81a5a9129beb1f8413fa648": {
        "title": "ML-Bench: Large Language Models Leverage Open-source Libraries for Machine Learning Tasks",
        "authors": [
            "Yuliang Liu",
            "Xiangru Tang",
            "Zefan Cai",
            "Junjie Lu",
            "Yichi Zhang",
            "Yanjun Shao",
            "Zexuan Deng",
            "Helan Hu",
            "Zengxian Yang",
            "Kaikai An",
            "Ruijun Huang",
            "Shuzheng Si",
            "Sheng Chen",
            "Haozhe Zhao",
            "Zhengliang Li",
            "Liang Chen",
            "Yiming Zong",
            "Yan Wang",
            "Tianyu Liu",
            "Zhiwei Jiang",
            "Baobao Chang",
            "Yujia Qin",
            "Wangchunshu Zhou",
            "Yilun Zhao",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "date": "2023/11/16",
        "pdf": "https://arxiv.org/pdf/2311.09835.pdf",
        "code": "https://ml-bench.github.io/",
        "abstract": "Large language models have shown promising performance in code generation benchmarks. However, a considerable divide exists between these benchmark achievements and their practical applicability, primarily attributed to real-world programming&#39;s reliance on pre-existing libraries. Instead of evaluating LLMs to code from scratch, this work aims to propose a new evaluation setup where LLMs use open-source libraries to finish machine learning tasks. Therefore, we propose ML-Bench, an expansive benchmark developed to assess the effectiveness of LLMs in leveraging existing functions in open-source libraries. Consisting of 10044 samples spanning 130 tasks over 14 notable machine learning GitHub repositories. In this setting, given a specific machine learning task instruction and the accompanying README in a codebase, an LLM is tasked to generate code to accomplish the task. This necessitates the comprehension of long and language-code interleaved documents, as well as the understanding of complex cross-file code structures, introducing new challenges. Notably, while GPT-4 exhibits remarkable improvement over other LLMs, it manages to accomplish only 39.73\\% of the tasks, leaving a huge space for improvement. We address these challenges by proposing ML-Agent, designed to effectively navigate the codebase, locate documentation, retrieve code, and generate executable code. Empirical results demonstrate that ML-Agent, built upon GPT-4, results in further improvements. Code, data, and models are available at \\url{https://ml-bench.github.io/}.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2311.09835"
    },
    "7d9efef02cf95a833624a14158e4ad39": {
        "title": "ProAgent: Building Proactive Cooperative AI with Large Language Models",
        "authors": [
            "Ceyao Zhang",
            "Kaijie Yang",
            "Siyi Hu",
            "Zihao Wang",
            "Guanghe Li",
            "Yihang Sun",
            "Cheng Zhang",
            "Zhaowei Zhang",
            "Anji Liu",
            "Song-Chun Zhu",
            "Xiaojun Chang",
            "Junge Zhang",
            "Feng Yin",
            "Yitao Liang",
            "Yaodong Yang"
        ],
        "date": "2023/08/22",
        "pdf": "http://arxiv.org/pdf/2308.11339.pdf",
        "code": "https://github.com/PKU-Alignment/ProAgent",
        "abstract": "Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent&#39;s capacity to recalibrate its strategy when confronted with novel teammates. We propose \\textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \\textit{pro}active \\textit{agent} empowered with the ability to anticipate teammates&#39; forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations conducted within the framework of \\textit{Overcook-AI} unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training in cooperation with AI agents. Further, when cooperating with human proxy models, its performance exhibits an average improvement exceeding 10\\% compared to the current state-of-the-art, COLE. The advancement was consistently observed across diverse scenarios involving interactions with both AI agents of varying characteristics and human counterparts. These findings inspire future research for human-robot collaborations. For a hands-on demonstration, please visit \\url{https://pku-proagent.github.io}.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2308.11339"
    },
    "7e2c67da344f12dc61a0a219ed1d7c98": {
        "title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models",
        "authors": [
            "Tianhao Shen",
            "Sun Li",
            "Deyi Xiong"
        ],
        "date": "2023/12/26",
        "pdf": "http://arxiv.org/pdf/2312.16132.pdf",
        "abstract": "The rapid evolution of large language models (LLMs) necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions. This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge. RoleEval comprises RoleEval-Global (including internationally recognized characters) and RoleEval-Chinese (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fiction. These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences of the characters. To maintain high standards, we perform a hybrid quality check process combining automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative. Our extensive evaluations of RoleEval across various open-source and proprietary large language models, under both the zero- and few-shot settings, reveal insightful findings. Notably, while GPT-4 outperforms other models on RoleEval-Global, Chinese LLMs excel on RoleEval-Chinese, highlighting significant knowledge distribution differences. We expect that RoleEval will highlight the significance of assessing role knowledge for foundation models across various languages and cultural settings.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2312.16132"
    },
    "d180677e0a021d9f02ea25bdaf8c9f9e": {
        "title": "How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation",
        "authors": [
            "Yang Xiao",
            "Yi Cheng",
            "Jinlan Fu",
            "Jiashuo Wang",
            "Wenjie Li",
            "Pengfei Liu"
        ],
        "date": "2023/12/28",
        "pdf": "http://arxiv.org/pdf/2312.17115.pdf",
        "abstract": "Human behavior simulation of AI agents necessitates the agents to possess a quality of believability, which is crucial as it facilitates users in establishing trust toward the agents and streamlines the fulfillment of the agents&#39; goal. While recent advancements in Large Language Model (LLM) based agents have improved human behavior simulation, challenges inherent to LLMs (e.g., long context modeling) can undermine their believability. Consequently, evaluating AI agent believability becomes imperative. Unfortunately, prior research often neglects the negative impacts of LLM deficiencies. To address these gaps, we introduce two metrics for assessing LLM-based agent believability: consistency, and robustness, together with a benchmark, SimulateBench, with which, we evaluate the consistency and robustness of agents implemented with popular LLMs. We find that agents (i) struggle to accurately depict character information when presented with lengthy profile inputs; (ii) exhibit vulnerability to profile perturbations; and (iii) are significantly affected by certain key factors that impact their overall believability. Code and SimulateBench are public at https://github.com/GAIR-NLP/GPTMan.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2312.17115"
    },
    "926e1a0151c4046b84210e6e81639f51": {
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
        "authors": [
            "Hyunwoo Kim",
            "Melanie Sclar",
            "Xuhui Zhou",
            "Ronan Le Bras",
            "Gunhee Kim",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "date": "2023/10/24",
        "pdf": "http://arxiv.org/pdf/2310.15421.pdf",
        "abstract": "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.",
        "category": [
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2310.15421"
    },
    "3f38895b5dc2bb39a5857ec9a34da3ac": {
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Wenxiang Jiao",
            "Xing Wang",
            "Yan Wang",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi"
        ],
        "date": "2023/05/30",
        "pdf": "https://arxiv.org/pdf/2305.19118",
        "abstract": " Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n&#34;tit for tat&#34; and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of &#34;tit for tat&#34; state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Codes:\nhttps://github.com/Skytliang/Multi-Agents-Debate\n",
        "code": "https://github.com/Skytliang/Multi-Agents-Debate",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2305.19118"
    },
    "27622956eb8adf45c1c26de87fedc9ff": {
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback",
        "authors": [
            "Nikhil Mehta",
            "Milagro Teruel",
            "Patricio Figueroa Sanz",
            "Xin Deng",
            "Ahmed Hassan Awadallah",
            "Julia Kiseleva"
        ],
        "date": "2023/04/21",
        "pdf": "https://arxiv.org/pdf/2304.10750",
        "abstract": " Many approaches to Natural Language Processing (NLP) tasks often treat them\nas single-step problems, where an agent receives an instruction, executes it,\nand is evaluated based on the final outcome. However, human language is\ninherently interactive, as evidenced by the back-and-forth nature of human\nconversations. In light of this, we posit that human-AI collaboration should\nalso be interactive, with humans monitoring the work of AI agents and providing\nfeedback that the agent can understand and utilize. Further, the AI agent\nshould be able to detect when it needs additional information and proactively\nask for help. Enabling this scenario would lead to more natural, efficient, and\nengaging human-AI collaborations. In this work, we explore these directions using the challenging task defined\nby the IGLU competition, an interactive grounded language understanding task in\na MineCraft-like world. We explore multiple types of help players can give to\nthe AI to guide it and analyze the impact of this help in AI behavior,\nresulting in performance improvements.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2304.10750"
    },
    "aa093a5d75fafc6139a1a2e4ea9ac2f8": {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
        "authors": [
            "Ning Miao",
            "Yee Whye Teh",
            "Tom Rainforth"
        ],
        "date": "2023/08/01",
        "pdf": "https://arxiv.org/pdf/2308.00436",
        "code": "https://github.com/ningmiao/selfcheck",
        "abstract": " The recent progress in large language models (LLMs), especially the invention\nof chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning\nproblems. However, even the strongest LLMs are still struggling with more\ncomplicated problems that require non-linear thinking and multi-step reasoning.\nIn this work, we explore whether LLMs have the ability to recognize their own\nerrors, without resorting to external resources. In particular, we investigate\nwhether they can be used to identify individual errors within a step-by-step\nreasoning. To this end, we propose a zero-shot verification scheme to recognize\nsuch errors. We then use this verification scheme to improve question-answering\nperformance, by using it to perform weighted voting on different generated\nanswers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and\nfind that it successfully recognizes errors and, in turn, increases final\npredictive performance.\n",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2308.00436"
    },
    "1b7a9bae65346bc13ade7d5f7ff8dfa7": {
        "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback",
        "authors": [
            "Bo Shen",
            "Jiaxin Zhang",
            "Taihong Chen",
            "Daoguang Zan",
            "Bing Geng",
            "An Fu",
            "Muhan Zeng",
            "Ailun Yu",
            "Jichuan Ji",
            "Jingyang Zhao",
            "Yuenan Guo",
            "Qianxiang Wang"
        ],
        "date": "2023/07/27",
        "pdf": "https://arxiv.org/pdf/2307.14936",
        "abstract": " Large Language Models for Code (Code LLM) are flourishing. New and powerful\nmodels are released on a weekly basis, demonstrating remarkable performance on\nthe code generation task. Various approaches have been proposed to boost the\ncode generation performance of pre-trained Code LLMs, such as supervised\nfine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we\npropose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework,\nwhich can effectively and efficiently boost pre-trained large language models\nfor code generation. Under this framework, we present PanGu-Coder2, which\nachieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through\nan extensive evaluation on CoderEval and LeetCode benchmarks, we show that\nPanGu-Coder2 consistently outperforms all previous Code LLMs.\n",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2307.14936"
    },
    "2f642f73495732f56d575ca9c0ecb9d2": {
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Prakhar Gupta",
            "Skyler Hallinan",
            "Luyu Gao",
            "Sarah Wiegreffe",
            "Uri Alon",
            "Nouha Dziri",
            "Shrimai Prabhumoye",
            "Yiming Yang",
            "Shashank Gupta",
            "Bodhisattwa Prasad Majumder",
            "Katherine Hermann",
            "Sean Welleck",
            "Amir Yazdanbakhsh",
            "Peter Clark"
        ],
        "date": "2023/03/30",
        "pdf": "https://arxiv.org/pdf/2303.17651",
        "code": "https://github.com/madaan/self-refine",
        "abstract": " Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.\n",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2303.17651"
    },
    "64c8f9b19403f164ca1c339b1522f6c8": {
        "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Lingkai Kong",
            "Bo Dai",
            "Chao Zhang"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16653",
        "code": "https://github.com/haotiansun14/adaplanner",
        "abstract": " Large language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.\n",
        "category": [
            "Feedback&Reflection",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2305.16653"
    },
    "915f6ab291da9364b0bf8b61410afa01": {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "authors": [
            "Shuofei Qiao",
            "Honghao Gui",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "date": "2023/05/22",
        "pdf": "https://arxiv.org/pdf/2305.13068",
        "code": "https://github.com/zjunlp/trice",
        "abstract": " Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model&#39;s dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.\n",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2305.13068"
    },
    "5554888e178a16e7d530118c0f7ad123": {
        "title": "Teaching Large Language Models to Self-Debug",
        "authors": [
            "Xinyun Chen",
            "Maxwell Lin",
            "Nathanael Schrli",
            "Denny Zhou"
        ],
        "date": "2023/04/11",
        "pdf": "https://arxiv.org/pdf/2304.05128",
        "abstract": " Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by explaining the\ngenerated code in natural language. Self-Debugging achieves the\nstate-of-the-art performance on several code generation benchmarks, including\nthe Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python\ntranslation, and MBPP for text-to-Python generation. On the Spider benchmark\nwhere there are no unit tests to verify the correctness of predictions,\nSelf-Debugging with code explanation consistently improves the baseline by\n2-3%, and improves the prediction accuracy on problems of the hardest label by\n9%. On TransCoder and MBPP where unit tests are available, Self-Debugging\nimproves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback\nmessages and reusing failed predictions, Self-Debugging notably improves sample\nefficiency, and can match or outperform baseline models that generate more than\n10x candidate programs.\n",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2304.05128"
    },
    "141e07821bd167bc5ce5902645300d40": {
        "title": "The ART of LLM Refinement: Ask, Refine, and Trust",
        "authors": [
            "Kumar Shridhar",
            "Koustuv Sinha",
            "Andrew Cohen",
            "Tianlu Wang",
            "Ping Yu",
            "Ram Pasunuru",
            "Mrinmaya Sachan",
            "Jason Weston",
            "Asli Celikyilmaz"
        ],
        "date": "2023/11/14",
        "pdf": "https://arxiv.org/pdf/2311.07961.pdf",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations? A popular concept, referred to as self-refinement, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with refinement objective called ART: Ask, Refine, and Trust, which asks necessary questions to decide when an LLM should refine its output, and either affirm or withhold trust in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), ART achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We also demonstrate the benefit of using smaller models to make refinement decisions as a cost-effective alternative to fine-tuning a larger model.",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2311.07961"
    },
    "20c18a2ace6082d92a1fe9e1185843ba": {
        "title": "Learning From Mistakes Makes LLM Better Reasoner",
        "authors": [
            "Shengnan An",
            "Zexiong Ma",
            "Zeqi Lin",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "date": "2023/10/31",
        "pdf": "https://arxiv.org/pdf/2310.20689.pdf",
        "code": "https://github.com/microsoft/LEMA",
        "abstract": "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning paths from various LLMs and then employ GPT-4 as a &#34;corrector&#34; to (1) identify the mistake step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the final answer. Experimental results demonstrate the effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning tasks, LeMa consistently improves the performance compared with fine-tuning on CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved by non-execution open-source models on these challenging tasks. Our code, data and models will be publicly available at https://github.com/microsoft/LEMA.",
        "category": [
            "Feedback&Reflection"
        ],
        "url": "https://arxiv.org/abs/2310.20689"
    },
    "3b37d03718c40fac09e90662da286074": {
        "title": "CB2: Collaborative Natural Language Interaction Research Platform",
        "authors": [
            "Jacob Sharf",
            "Mustafa Omer Gul",
            "Yoav Artzi"
        ],
        "date": "2023/03/14",
        "pdf": "https://arxiv.org/pdf/2303.08127",
        "code": "https://github.com/lil-lab/cb2",
        "abstract": " CB2 is a multi-agent platform to study collaborative natural language\ninteraction in a grounded task-oriented scenario. It includes a 3D game\nenvironment, a backend server designed to serve trained models to human agents,\nand various tools and processes to enable scalable studies. We deploy CB2 at\nhttps://cb2.ai as a system demonstration with a learned instruction following\nmodel.\n",
        "category": [
            "Game Platform"
        ],
        "url": "https://arxiv.org/abs/2303.08127"
    },
    "12e797c33d37e99a30aa9bd979aa2b31": {
        "title": "Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks",
        "authors": [
            "Haoqi Yuan",
            "Chi Zhang",
            "Hongcheng Wang",
            "Feiyang Xie",
            "Penglin Cai",
            "Hao Dong",
            "Zongqing Lu"
        ],
        "date": "2023/03/29",
        "pdf": "https://arxiv.org/pdf/2303.16563",
        "code": "https://sites.google.com/view/plan4mc",
        "abstract": " We study building a multi-task agent in Minecraft. Without human\ndemonstrations, solving long-horizon tasks in this open-ended environment with\nreinforcement learning (RL) is extremely sample inefficient. To tackle the\nchallenge, we decompose solving Minecraft tasks into learning basic skills and\nplanning over the skills. We propose three types of fine-grained basic skills\nin Minecraft, and use RL with intrinsic rewards to accomplish basic skills with\nhigh success rates. For skill planning, we use Large Language Models to find\nthe relationships between skills and build a skill graph in advance. When the\nagent is solving a task, our skill search algorithm walks on the skill graph\nand generates the proper skill plans for the agent. In experiments, our method\naccomplishes 24 diverse Minecraft tasks, where many tasks require sequentially\nexecuting for more than 10 skills. Our method outperforms baselines in most\ntasks by a large margin. The project&#39;s website and code can be found at\nhttps://sites.google.com/view/plan4mc.\n",
        "category": [
            "Game Playing",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2303.16563"
    },
    "4977a28eb34673b1a874b008ab45aa6a": {
        "title": "Knowledge-enhanced Agents for Interactive Text Games",
        "authors": [
            "Prateek Chhikara",
            "Jiarui Zhang",
            "Filip Ilievski",
            "Jonathan Francis",
            "Kaixin Ma"
        ],
        "date": "2023/05/08",
        "pdf": "https://arxiv.org/pdf/2305.05091",
        "abstract": " Communication via natural language is a crucial aspect of intelligence, and\nit requires computational models to learn and reason about world concepts, with\nvarying levels of supervision. While there has been significant progress made\non fully-supervised non-interactive tasks, such as question-answering and\nprocedural text understanding, much of the community has turned to various\nsequential interactive tasks, as in semi-Markov text-based games, which have\nrevealed limitations of existing approaches in terms of coherence, contextual\nawareness, and their ability to learn effectively from the environment. In this\npaper, we propose a framework for enabling improved functional grounding of\nagents in text-based games. Specifically, we consider two forms of domain\nknowledge that we inject into learning-based agents: memory of previous correct\nactions and affordances of relevant objects in the environment. Our framework\nsupports three representative model classes: `pure&#39; reinforcement learning (RL)\nagents, RL agents enhanced with knowledge graphs, and agents equipped with\nlanguage models. Furthermore, we devise multiple injection strategies for the\nabove domain knowledge types and agent architectures, including injection via\nknowledge graphs and augmentation of the existing input encoding strategies. We\nperform all experiments on the ScienceWorld text-based game environment, to\nillustrate the performance of various model configurations in challenging\nscience-related instruction-following tasks. Our findings provide crucial\ninsights on the development of effective natural language processing systems\nfor interactive contexts.\n",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2305.05091"
    },
    "d0aa2508a83304e8baf09ffdf76be63c": {
        "title": "Playing repeated games with Large Language Models",
        "authors": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16867",
        "abstract": " Large Language Models (LLMs) are transforming society and permeating into\ndiverse applications. As a result, LLMs will frequently interact with us and\nother agents. It is, therefore, of great societal value to understand how LLMs\nbehave in interactive social settings. Here, we propose to use behavioral game\ntheory to study LLM&#39;s cooperation and coordination behavior. To do so, we let\ndifferent LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with\neach other and with other, human-like strategies. Our results show that LLMs\ngenerally perform well in such tasks and also uncover persistent behavioral\nsignatures. In a large set of two players-two strategies games, we find that\nLLMs are particularly good at games where valuing their own self-interest pays\noff, like the iterated Prisoner&#39;s Dilemma family. However, they behave\nsub-optimally in games that require coordination. We, therefore, further focus\non two games from these distinct families. In the canonical iterated Prisoner&#39;s\nDilemma, we find that GPT-4 acts particularly unforgivingly, always defecting\nafter another agent has defected only once. In the Battle of the Sexes, we find\nthat GPT-4 cannot match the behavior of the simple convention to alternate\nbetween options. We verify that these behavioral signatures are stable across\nrobustness checks. Finally, we show how GPT-4&#39;s behavior can be modified by\nproviding further information about the other player as well as by asking it to\npredict the other player&#39;s actions before making a choice. These results enrich\nour understanding of LLM&#39;s social behavior and pave the way for a behavioral\ngame theory for machines.\n",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2305.16867"
    },
    "16ade9646e937f57019f60d7eecb4a2d": {
        "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Yixin Cao",
            "Ting Liu",
            "Bing Qin"
        ],
        "date": "2023/05/19",
        "pdf": "https://arxiv.org/pdf/2305.11595",
        "abstract": " Large Language Models (LLMs) have demonstrated human-like intelligence and\nare widely used in various applications. However, LLMs still exhibit various\nkinds of inconsistency problems. Existing works mainly focus on the\ninconsistency issues within a single LLM, while we investigate the\ninter-consistency among multiple LLMs, which is critical for collaborating to\nsolve a complex task. To examine whether LLMs can collaborate to ultimately\nachieve a consensus for the shared goal and whether LLMs easily change their\nviewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct\na three-stage debate aligned with real-world scenarios: fair debate, mismatched\ndebate, and roundtable debate. Through extensive experiments on the commonsense\nreasoning task, LLMs not only become more inter-consistent but also achieve\nhigher performance. Moreover, we observe that stronger LLMs tend to dominate\nthe debates by adhering to their perspectives, while weaker ones are more\nlikely to change viewpoints. Additionally, we highlight the importance of a\ncompetent judge, such as GPT-4, to draw more proper conclusions. Our work\ncontributes to understanding the inter-consistency among LLMs and lays the\nfoundation for the development of future collaboration methods.\n",
        "category": [
            "Game Playing"
        ]
    },
    "9746aebf3641e1f07a29559fa01a166b": {
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Tushar Khot",
            "Mirella Lapata"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10142",
        "code": "https://github.com/FranxYao/GPT-Bargaining",
        "abstract": " We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and\ncriticizing. We are interested in this question because if LLMs were able to\nimprove each other, it would imply the possibility of creating strong AI agents\nwith minimal human intervention. We ask two LLMs to negotiate with each other,\nplaying the roles of a buyer and a seller, respectively. They aim to reach a\ndeal with the buyer targeting a lower price and the seller a higher one. A\nthird language model, playing the critic, provides feedback to a player to\nimprove the player&#39;s negotiation strategies. We let the two agents play\nmultiple rounds, using previous negotiation history and AI feedback as\nin-context demonstrations to improve the model&#39;s negotiation strategy\niteratively. We use different LLMs (GPT and Claude) for different roles and use\nthe deal price as the evaluation metric. Our experiments reveal multiple\nintriguing findings: (1) Only a subset of the language models we consider can\nself-play and improve the deal price from AI feedback, weaker models either do\nnot understand the game&#39;s rules or cannot incorporate AI feedback for further\nimprovement. (2) Models&#39; abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to\nimprove as the buyer than as the seller. (3) When unrolling the game to\nmultiple rounds, stronger agents can consistently improve their performance by\nmeaningfully using previous experiences and iterative AI feedback, yet have a\nhigher risk of breaking the deal. We hope our work provides insightful initial\nexplorations of having models autonomously improve each other with game playing\nand AI feedback.\n",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2305.10142"
    },
    "4ad0c8c7e6154544a3aee3d3680d9765": {
        "title": "Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models",
        "authors": [
            "Jun Inukai",
            "Tadahiro Taniguchi",
            "Akira Taniguchi",
            "Yoshinobu Hagiwara"
        ],
        "date": "2023/05/31",
        "pdf": "https://arxiv.org/pdf/2305.19761",
        "abstract": " In the studies on symbol emergence and emergent communication in a population\nof agents, a computational model was employed in which agents participate in\nvarious language games. Among these, the Metropolis-Hastings naming game (MHNG)\npossesses a notable mathematical property: symbol emergence through MHNG is\nproven to be a decentralized Bayesian inference of representations shared by\nthe agents. However, the previously proposed MHNG is limited to a two-agent\nscenario. This paper extends MHNG to an N-agent scenario. The main\ncontributions of this paper are twofold: (1) we propose the recursive\nMetropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and\ndemonstrate that RMHNG is an approximate Bayesian inference method for the\nposterior distribution over a latent variable shared by agents, similar to\nMHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and\nreal image data, enabling multiple agents to develop and share a symbol system.\nFurthermore, we introduce two types of approximations -- one-sample and\nlimited-length -- to reduce computational complexity while maintaining the\nability to explain communication in a population of agents. The experimental\nfindings showcased the efficacy of RMHNG as a decentralized Bayesian inference\nfor approximating the posterior distribution concerning latent variables, which\nare jointly shared among agents, akin to MHNG. Moreover, the utilization of\nRMHNG elucidated the agents&#39; capacity to exchange symbols. Furthermore, the\nstudy discovered that even the computationally simplified version of RMHNG\ncould enable symbols to emerge among the agents.\n",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2305.19761"
    },
    "7faa5c7e074c69eacba3978164125eb2": {
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
        "authors": [
            "Guanzhi Wang",
            "Yuqi Xie",
            "Yunfan Jiang",
            "Ajay Mandlekar",
            "Chaowei Xiao",
            "Yuke Zhu",
            "Linxi Fan",
            "Anima Anandkumar"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16291",
        "code": "https://github.com/MineDojo/Voyager",
        "abstract": " We introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent&#39;s abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https://voyager.minedojo.org/.\n",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2305.16291"
    },
    "ec37317b3579158b46de6d811f54021b": {
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.17144",
        "code": "https://github.com/OpenGVLab/GITM",
        "abstract": " The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular &#34;ObtainDiamond&#34; task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe &#34;ObtainDiamond&#34; task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the &#34;ObtainDiamond&#34; task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.\n",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2305.17144"
    },
    "691bf33064019cb4442e3758f44807c8": {
        "title": "An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents",
        "authors": [
            "Maximilian Croissant",
            "Madeleine Frister",
            "Guy Schofield",
            "Cade McCall"
        ],
        "date": "2023/09/10",
        "pdf": "https://arxiv.org/pdf/2309.05076",
        "abstract": " The development of believable, natural, and interactive digital artificial\nagents is a field of growing interest. Theoretical uncertainties and technical\nbarriers present considerable challenges to the field, particularly with\nregards to developing agents that effectively simulate human emotions. Large\nlanguage models (LLMs) might address these issues by tapping common patterns in\nsituational appraisal. In three empirical experiments, this study tests the\ncapabilities of LLMs to solve emotional intelligence tasks and to simulate\nemotions. It presents and evaluates a new chain-of-emotion architecture for\nemotion simulation within video games, based on psychological appraisal\nresearch. Results show that it outperforms standard LLM architectures on a\nrange of user experience and content analysis metrics. This study therefore\nprovides early evidence of how to construct and test affective agents based on\ncognitive processes represented in language models.\n",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2309.05076"
    },
    "2784f2a2b88063c67286262773571a65": {
        "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf",
        "authors": [
            "Yuzhuang Xu",
            "Shuo Wang",
            "Peng Li",
            "Fuwen Luo",
            "Xiaolong Wang",
            "Weidong Liu",
            "Yang Liu"
        ],
        "date": "2023/09/09",
        "pdf": "https://arxiv.org/pdf/2309.04658.pdf",
        "abstract": "Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf&#39;&#39;, demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2309.04658"
    },
    "fa0d2b5c94ed1f859fe93477ed2d3f79": {
        "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
        "authors": [
            "Jiaxian Guo",
            "Bo Yang",
            "Paul Yoo",
            "Bill Yuchen Lin",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "date": "2023/09/29",
        "pdf": "https://arxiv.org/pdf/2309.17277.pdf",
        "code": "https://github.com/CR-Gjx/Suspicion-Agent",
        "abstract": "Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4&#39;s learned knowledge for imperfect information games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4&#39;s capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others&#39; behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of Suspicion-Agent across three different imperfect information games and then quantitatively evaluate it in Leduc Hold&#39;em. The results show that Suspicion-Agent can potentially outperform traditional algorithms designed for imperfect information games, without any specialized training or examples. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available.",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2309.17277"
    },
    "e0a2fda9dfc3609da9420d67063733e8": {
        "title": "Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis",
        "authors": [
            "Akshat Gupta"
        ],
        "date": "2023/08/23",
        "pdf": "https://arxiv.org/pdf/2308.12466.pdf",
        "abstract": "Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players. Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This exploration then sheds light on the divergent poker tactics of the two models: ChatGPT&#39;s conservativeness juxtaposed against GPT-4&#39;s aggression. In poker vernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which means that it has a propensity to only engage with premium hands and folds a majority of hands. When subjected to the same directive, GPT-4 plays like a maniac, showcasing a loose and aggressive style of play. Both strategies, although relatively advanced, are not game theory optimal.",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2308.12466"
    },
    "547060c869101463eaac2740b4922d60": {
        "title": "Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Jen-tse Huang",
            "Wenxuan Wang",
            "Wenxiang Jiao",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi",
            "Xing Wang"
        ],
        "date": "2023/10/31",
        "pdf": "https://arxiv.org/pdf/2310.20499.pdf",
        "abstract": "The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy&#39;&#39;, we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players&#39; descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs&#39; expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs&#39; intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs&#39; human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2310.20499"
    },
    "6ef9581ab6919de277369293f4d554ec": {
        "title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game",
        "authors": [
            "Zijing Shi",
            "Meng Fang",
            "Shunfeng Zheng",
            "Shilong Deng",
            "Ling Chen",
            "Yali Du"
        ],
        "date": "2023/12/29",
        "pdf": "http://arxiv.org/pdf/2312.17515.pdf",
        "abstract": "Multi-agent collaboration with Large Language Models (LLMs) demonstrates proficiency in basic tasks, yet its efficiency in more complex scenarios remains unexplored. In gaming environments, these agents often face situations without established coordination protocols, requiring them to make intelligent inferences about teammates from limited data. This problem motivates the area of ad hoc teamwork, in which an agent may potentially cooperate with a variety of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork problem where the agent operates in an environment driven by natural language. Our findings reveal the potential of LLM agents in team collaboration, highlighting issues related to hallucinations in communication. To address this issue, we develop CodeAct, a general agent that equips LLM with enhanced memory and code-driven reasoning, enabling the repurposing of partial information for rapid adaptation to new teammates.",
        "category": [
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2312.17515"
    },
    "d0e04ab2d37bcd9b9f9980cddd5901c6": {
        "title": "Emergent and Predictable Memorization in Large Language Models",
        "authors": [
            "Stella Biderman",
            "USVSN Sai Prashanth",
            "Lintang Sutawika",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Shivanshu Purohit",
            "Edward Raf"
        ],
        "date": "2023/04/21",
        "pdf": "https://arxiv.org/pdf/2304.11158",
        "abstract": " Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model&#39;s\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model&#39;s full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite, and find that intermediate checkpoints are better\npredictors of a model&#39;s memorization behavior than smaller fully-trained\nmodels. We additionally provide further novel discoveries on the distribution\nof memorization scores across models and data.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2304.11158"
    },
    "3670318a3140c7ef30bd05aba962ac17": {
        "title": "ChatLog: Recording and Analyzing ChatGPT Across Time",
        "authors": [
            "Shangqing Tu",
            "Chunyang Li",
            "Jifan Yu",
            "Xiaozhi Wang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2023/04/27",
        "pdf": "https://arxiv.org/pdf/2304.14106",
        "abstract": " While there are abundant researches about evaluating ChatGPT on natural\nlanguage understanding and generation tasks, few studies have investigated how\nChatGPT&#39;s behavior changes over time. In this paper, we collect a\ncoarse-to-fine temporal dataset called ChatLog, consisting of two parts that\nupdate monthly and daily: ChatLog-Monthly is a dataset of 38,730\nquestion-answer pairs collected every month including questions from both the\nreasoning and classification tasks. ChatLog-Daily, on the other hand, consists\nof ChatGPT&#39;s responses to 1000 identical questions for long-form generation\nevery day. We conduct comprehensive automatic and human evaluation to provide\nthe evidence for the existence of ChatGPT evolving patterns. We further analyze\nthe unchanged characteristics of ChatGPT over time by extracting its knowledge\nand linguistic features. We find some stable features to improve the robustness\nof a RoBERTa-based detector on new versions of ChatGPT. We will continuously\nmaintain our project at https://github.com/THU-KEG/ChatLog.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2304.14106"
    },
    "f859fcfade304101ad2d098deeee606f": {
        "title": "Learning to Reason and Memorize with Self-Notes",
        "authors": [
            "Jack Lanchantin",
            "Shubham Toshniwal",
            "Jason Weston",
            "Arthur Szlam",
            "Sainbayar Sukhbaatar"
        ],
        "date": "2023/05/01",
        "pdf": "https://arxiv.org/pdf/2305.00833",
        "abstract": " Large language models have been shown to struggle with limited context memory\nand multi-step reasoning. We propose a simple method for solving both of these\nproblems by allowing the model to take Self-Notes. Unlike recent scratchpad\napproaches, the model can deviate from the input context at any time to\nexplicitly think. This allows the model to recall information and perform\nreasoning on the fly as it reads the context, thus extending its memory and\nenabling multi-step reasoning. Our experiments on multiple tasks demonstrate\nthat our method can successfully generalize to longer and more complicated\ninstances from their training setup by taking Self-Notes at inference time.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.00833"
    },
    "c63eb48acb4856602198c3e3b36201fc": {
        "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
        "authors": [
            "Wanjun Zhong",
            "Lianghong Guo",
            "Qiqi Gao",
            "He Ye",
            "Yanlin Wang"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10250",
        "abstract": " Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.10250"
    },
    "445f517ae67cddfd2bd81c5e1921367c": {
        "title": "Monotonic Location Attention for Length Generalization",
        "authors": [
            "Jishnu Ray Chowdhury",
            "Cornelia Caragea"
        ],
        "date": "2023/05/31",
        "pdf": "https://arxiv.org/pdf/2305.20019",
        "abstract": " We explore different ways to utilize position-based cross-attention in\nseq2seq networks to enable length generalization in algorithmic tasks. We show\nthat a simple approach of interpolating the original and reversed encoded\nrepresentations combined with relative attention allows near-perfect length\ngeneralization for both forward and reverse lookup tasks or copy tasks that had\nbeen generally hard to tackle. We also devise harder diagnostic tasks where the\nrelative distance of the ideal attention position varies with timestep. In such\nsettings, the simple interpolation trick with relative attention is not\nsufficient. We introduce novel variants of location attention building on top\nof Dubois et al. (2020) to address the new diagnostic tasks. We also show the\nbenefits of our approaches for length generalization in SCAN (Lake &amp; Baroni,\n2018) and CFQ (Keysers et al., 2020). Our code is available on GitHub.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.20019"
    },
    "4d80a78327d3119427f0a965743474bc": {
        "title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
        "authors": [
            "Anian Ruoss",
            "Grgoire Deltang",
            "Tim Genewein",
            "Jordi Grau-Moya",
            "Rbert Csords",
            "Mehdi Bennani",
            "Shane Legg",
            "Joel Veness"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16843",
        "abstract": " Transformers have impressive generalization capabilities on tasks with a\nfixed context length. However, they fail to generalize to sequences of\narbitrary length, even for seemingly simple tasks such as duplicating a string.\nMoreover, simply training on longer sequences is inefficient due to the\nquadratic computation complexity of the global attention mechanism. In this\nwork, we demonstrate that this failure mode is linked to positional encodings\nbeing out-of-distribution for longer sequences (even for relative encodings)\nand introduce a novel family of positional encodings that can overcome this\nproblem. Concretely, our randomized positional encoding scheme simulates the\npositions of longer sequences and randomly selects an ordered subset to fit the\nsequence&#39;s length. Our large-scale empirical evaluation of 6000 models across\n15 algorithmic reasoning tasks shows that our method allows Transformers to\ngeneralize to sequences of unseen length (increasing test accuracy by 12.0% on\naverage).\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.16843"
    },
    "07ac78ab6ef07caa1f81f53f497849a2": {
        "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
        "authors": [
            "Joshua Ainslie",
            "Tao Lei",
            "Michiel de Jong",
            "Santiago Ontan",
            "Siddhartha Brahma",
            "Yury Zemlyanskiy",
            "David Uthus",
            "Mandy Guo",
            "James Lee-Thorp",
            "Yi Tay",
            "Yun-Hsuan Sung",
            "Sumit Sanghai"
        ],
        "date": "2023/03/17",
        "pdf": "https://arxiv.org/pdf/2303.09752",
        "abstract": " Many natural language processing tasks benefit from long inputs, but\nprocessing long documents with Transformers is expensive -- not only due to\nquadratic attention complexity but also from applying feedforward and\nprojection layers to every token. However, not all tokens are equally\nimportant, especially for longer documents. We propose CoLT5, a long-input\nTransformer model that builds on this intuition by employing conditional\ncomputation, devoting more resources to important tokens in both feedforward\nand attention layers. We show that CoLT5 achieves stronger performance than\nLongT5 with much faster training and inference, achieving SOTA on the\nlong-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably\nmake use of extremely long inputs, showing strong gains up to 64k input length.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2303.09752"
    },
    "87d1ff46ce8a6a6881737c6bb6077623": {
        "title": "Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System",
        "authors": [
            "Xinnian Liang",
            "Bing Wang",
            "Hui Huang",
            "Shuangzhi Wu",
            "Peihao Wu",
            "Lu Lu",
            "Zejun Ma",
            "Zhoujun Li"
        ],
        "date": "2023/04/26",
        "pdf": "https://arxiv.org/pdf/2304.13343",
        "abstract": " Large-scale Language Models (LLMs) are constrained by their inability to\nprocess lengthy inputs. To address this limitation, we propose the\nSelf-Controlled Memory (SCM) system to unleash infinite-length input capacity\nfor large-scale language models. Our SCM system is composed of three key\nmodules: the language model agent, the memory stream, and the memory\ncontroller. The language model agent iteratively processes ultra-long inputs\nand stores all historical information in the memory stream. The memory\ncontroller provides the agent with both long-term memory (archived memory) and\nshort-term memory (flash memory) to generate precise and coherent responses.\nThe controller determines which memories from archived memory should be\nactivated and how to incorporate them into the model input. Our SCM system can\nbe integrated with any LLMs to enable them to process ultra-long texts without\nany modification or fine-tuning. Experimental results show that our SCM system\nenables LLMs, which are not optimized for multi-turn dialogue, to achieve\nmulti-turn dialogue capabilities that are comparable to ChatGPT, and to\noutperform ChatGPT in scenarios involving ultra-long document summarization or\nlong-term conversations. Additionally, we will supply a test set, which covers\ncommon long-text input scenarios, for evaluating the abilities of LLMs in\nprocessing long documents.~\\footnote{Working in\nprogress.}\\footnote{\\url{https://github.com/wbbeyourself/SCM4LLMs}}\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2304.13343"
    },
    "820150c149f38e2bafe056cd3d4cb9d3": {
        "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
        "authors": [
            "Amanda Bertsch",
            "Uri Alon",
            "Graham Neubig",
            "Matthew R. Gormley"
        ],
        "date": "2023/05/02",
        "pdf": "https://arxiv.org/pdf/2305.01625",
        "abstract": " Since the proposal of transformers, these models have been limited to bounded\ninput lengths, because of their need to attend to every token in the input. In\nthis work, we propose Unlimiformer: a general approach that wraps any existing\npretrained encoder-decoder transformer, and offloads the cross-attention\ncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNN\ndistances are the attention dot-product scores. This kNN index can be kept on\neither the GPU or CPU memory and queried in sub-linear time; this way, we can\nindex practically unlimited input sequences, while every attention head in\nevery decoder layer retrieves its top-k keys, instead of attending to every\nkey. We evaluate Unlimiformer on several long-document and book-summarization\nbenchmarks, showing that it can process even 500k token-long inputs from the\nBookSum dataset, without any input truncation at test time. We demonstrate that\nUnlimiformer improves pretrained models such as BART and Longformer by\nextending them to unlimited inputs without additional learned weights and\nwithout modifying their code. We make our code and models publicly available at\nhttps://github.com/abertsch72/unlimiformer .\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.01625"
    },
    "fb989b6304075754f68b99032e82ddc8": {
        "title": "Small Models are Valuable Plug-ins for Large Language Models",
        "authors": [
            "Canwen Xu",
            "Yichong Xu",
            "Shuohang Wang",
            "Yang Liu",
            "Chenguang Zhu",
            "Julian McAuley"
        ],
        "date": "2023/05/15",
        "pdf": "https://arxiv.org/pdf/2305.08848",
        "abstract": " Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their\nweights are often publicly unavailable and their immense sizes make the models\ndifficult to be tuned with common hardware. As a result, effectively tuning\nthese models with large-scale supervised data can be challenging. As an\nalternative, In-Context Learning (ICL) can only use a small number of\nsupervised examples due to context length limits. In this paper, we propose\nSuper In-Context Learning (SuperICL) which allows black-box LLMs to work with\nlocally fine-tuned smaller models, resulting in superior performance on\nsupervised tasks. Our experiments demonstrate that SuperICL can improve\nperformance beyond state-of-the-art fine-tuned models while addressing the\ninstability problem of in-context learning. Furthermore, SuperICL can enhance\nthe capabilities of smaller models, such as multilinguality and\ninterpretability.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.08848"
    },
    "b20784f97a6a9622b19766fb4d07d924": {
        "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
        "authors": [
            "Shibo Hao",
            "Tianyang Liu",
            "Zhen Wang",
            "Zhiting Hu"
        ],
        "date": "2023/05/19",
        "pdf": "https://arxiv.org/pdf/2305.11554",
        "abstract": " Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.11554"
    },
    "2503cb91a58cd6d74bc5c8ddd9c13b64": {
        "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Peng Cui",
            "Tiannan Wang",
            "Zhenxin Xiao",
            "Yifan Hou",
            "Ryan Cotterell",
            "Mrinmaya Sachan"
        ],
        "date": "2023/05/22",
        "pdf": "https://arxiv.org/pdf/2305.13304",
        "abstract": " The fixed-size context of Transformer makes GPT models incapable of\ngenerating arbitrarily long text. In this paper, we introduce RecurrentGPT, a\nlanguage-based simulacrum of the recurrence mechanism in RNNs. RecurrentGPT is\nbuilt upon a large language model (LLM) such as ChatGPT and uses natural\nlanguage to simulate the Long Short-Term Memory mechanism in an LSTM. At each\ntimestep, RecurrentGPT generates a paragraph of text and updates its\nlanguage-based long-short term memory stored on the hard drive and the prompt,\nrespectively. This recurrence mechanism enables RecurrentGPT to generate texts\nof arbitrary length without forgetting. Since human users can easily observe\nand edit the natural language memories, RecurrentGPT is interpretable and\nenables interactive generation of long text. RecurrentGPT is an initial step\ntowards next-generation computer-assisted writing systems beyond local editing\nsuggestions. In addition to producing AI-generated content (AIGC), we also\ndemonstrate the possibility of using RecurrentGPT as an interactive fiction\nthat directly interacts with consumers. We call this usage of generative models\nby ``AI As Contents&#39;&#39; (AIAC), which we believe is the next form of conventional\nAIGC. We further demonstrate the possibility of using RecurrentGPT to create\npersonalized interactive fiction that directly interacts with readers instead\nof interacting with writers. More broadly, RecurrentGPT demonstrates the\nutility of borrowing ideas from popular model designs in cognitive science and\ndeep learning for prompting LLMs. Our code is available at\nhttps://github.com/aiwaves-cn/RecurrentGPT and an online demo is available at\nhttps://www.aiwaves.org/recurrentgpt.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.13304"
    },
    "388ef6a1b09b07108327b064a92d2da2": {
        "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration",
        "authors": [
            "Kejuan Yang",
            "Xiao Liu",
            "Kaiwen Men",
            "Aohan Zeng",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/05/24",
        "pdf": "https://arxiv.org/pdf/2305.15262",
        "abstract": " We identify two crucial limitations in the evaluation of recent\nparallel-integrated method Parallel Context Windows (PCW), which extends the\nmaximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing\nwindow-wise attention and positional embedding techniques. We first show that a\nsimple yet strong baseline, weighted sum ensemble, is missing for the\nin-context few-shot classification. Moreover, on more challenging\nChain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected\ndeterioration regarding question miscomprehension and false inference. Based on\nour findings, we suggest that the existing PCW design may not guarantee\nsufficient improvement and practicality in handling lengthy documents in\nreal-world applications. More community efforts on enabling language models&#39;\nlong context understanding ability should be paid.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.15262"
    },
    "3fa3b1b5c98f30923e7924f7ff3fd499": {
        "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
        "authors": [
            "Amirkeivan Mohtashami",
            "Martin Jaggi"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16300",
        "abstract": " While transformers have shown remarkable success in natural language\nprocessing, their attention mechanism&#39;s large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model&#39;s attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system&#39;s memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity up to 32k tokens, allowing for\ninference at the context lengths of GPT-4.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.16300"
    },
    "84c2e2e6ea4dc01094a9e1794f7560da": {
        "title": "Adapting Language Models to Compress Contexts",
        "authors": [
            "Alexis Chevalier",
            "Alexander Wettig",
            "Anirudh Ajith",
            "Danqi Chen"
        ],
        "date": "2023/05/24",
        "pdf": "https://arxiv.org/pdf/2305.14788",
        "abstract": " Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These models are capable of\ncompressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT models on sequences of up to 30,720 tokens and show that\nAutoCompressors can utilize long contexts to improve perplexity. We evaluate\nAutoCompressors on in-context learning by compressing task demonstrations. We\nfind that summary vectors are good substitutes for plain-text demonstrations,\nincreasing accuracy while reducing inference cost. Finally, we explore the\nbenefits of pre-computing summary vectors for large corpora by applying summary\nvectors to retrieval-augmented language modeling. Overall, AutoCompressors\nemerge as a simple and inexpensive solution for extending the context window of\nLMs while speeding up inference over long contexts.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.14788"
    },
    "6a6f1235f7ec4b77cf44a2ecefe0220f": {
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
        "authors": [
            "Ali Modarressi",
            "Ayyoob Imani",
            "Mohsen Fayyaz",
            "Hinrich Schtze"
        ],
        "date": "2023/05/23",
        "pdf": "https://arxiv.org/pdf/2305.14322",
        "abstract": " Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2305.14322"
    },
    "8c6217168c689b7097c12b1d55262252": {
        "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
        "authors": [
            "Chenxu Hu",
            "Jie Fu",
            "Chenzhuang Du",
            "Simian Luo",
            "Junbo Zhao",
            "Hang Zhao"
        ],
        "date": "2023/06/06",
        "pdf": "https://arxiv.org/pdf/2306.03901",
        "abstract": " Large language models (LLMs) with memory are computationally universal.\nHowever, mainstream LLMs are not taking full advantage of memory, and the\ndesigns are heavily influenced by biological brains. Due to their approximate\nnature and proneness to the accumulation of errors, conventional neural memory\nmechanisms cannot support LLMs to simulate complex reasoning. In this paper, we\nseek inspiration from modern computer architectures to augment LLMs with\nsymbolic memory for complex multi-hop reasoning. Such a symbolic memory\nframework is instantiated as an LLM and a set of SQL databases, where the LLM\ngenerates SQL instructions to manipulate the SQL databases. We validate the\neffectiveness of the proposed memory framework on a synthetic dataset requiring\ncomplex reasoning. The project website is available at\nhttps://chatdatabase.github.io/ .\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2306.03901"
    },
    "d701c2770312ea342e7de91540243687": {
        "title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
        "authors": [
            "Bodhisattwa Prasad Majumder",
            "Bhavana Dalvi Mishra",
            "Peter Jansen",
            "Oyvind Tafjord",
            "Niket Tandon",
            "Li Zhang",
            "Chris Callison-Burch",
            "Peter Clark"
        ],
        "date": "2023/10/16",
        "pdf": "https://arxiv.org/pdf/2310.10134.pdf",
        "code": "https://github.com/allenai/clin",
        "abstract": " Language agents have shown some ability to interact with an external\nenvironment, e.g., a virtual world such as ScienceWorld, to perform complex\ntasks, e.g., growing a plant, without the startup costs of reinforcement\nlearning. However, despite their zero-shot capabilities, these agents to date\ndo not continually improve over time beyond performance refinement on a\nspecific task. Here we present CLIN, the first language-based agent to achieve\nthis, so that it continually improves over multiple trials, including when both\nthe environment and task are varied, and without requiring parameter updates.\nOur approach is to use a persistent, dynamic, textual memory centered on causal\nabstractions (rather than general &#34;helpful hints&#34;) that is regularly updated\nafter each trial so that the agent gradually learns useful knowledge for new\ntrials. In the ScienceWorld benchmark, CLIN is able to continually improve on\nrepeated trials on the same task and environment, outperforming\nstate-of-the-art reflective language agents like Reflexion by 23 absolute\npoints. CLIN can also transfer its learning to new environments (or new tasks),\nimproving its zero-shot performance by 4 points (13 for new tasks) and can\nfurther improve performance there through continual memory updates, enhancing\nperformance by an additional 17 points (7 for new tasks). This suggests a new\narchitecture for agents built on frozen models that can still continually and\nrapidly improve over time.\n",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2310.10134"
    },
    "551fee4788f8898c8d1be8f00a632812": {
        "title": "Empowering Working Memory for Large Language Model Agents",
        "authors": [
            "Jing Guo",
            "Nan Li",
            "Jianchuan Qi",
            "Hang Yang",
            "Ruiqiao Li",
            "Yuzhen Feng",
            "Si Zhang",
            "Ming Xu"
        ],
        "date": "2023/12/22",
        "pdf": "http://arxiv.org/pdf/2312.17259.pdf",
        "abstract": "Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology&#39;s working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2312.17259"
    },
    "5f7a4f7a1540265fee692f647846c5db": {
        "title": "Evolving Large Language Model Assistant with Long-Term Conditional Memory",
        "authors": [
            "Ruifeng Yuan",
            "Shichao Sun",
            "Zili Wang",
            "Ziqiang Cao",
            "Wenjie Li"
        ],
        "date": "2023/12/22",
        "pdf": "http://arxiv.org/pdf/2312.17257.pdf",
        "abstract": "With the rapid development of large language models, AI assistants like ChatGPT have widely entered people&#39;s works and lives. In this paper, we present an evolving large language model assistant that utilizes verbal long-term memory. It focuses on preserving the knowledge and experience from the history dialogue between the user and AI assistant, which can be applied to future dialogue for generating a better response. The model generates a set of records for each finished dialogue and stores them in the memory. In later usage, given a new user input, the model uses it to retrieve its related memory to improve the quality of the response. To find the best form of memory, we explore different ways of constructing the memory and propose a new memorizing mechanism called conditional memory to solve the problems in previous methods. We also investigate the retrieval and usage of memory in the generation process. The assistant uses GPT-4 as the backbone and we evaluate it on three constructed test datasets focusing on different abilities required by an AI assistant with long-term memory.",
        "category": [
            "Memory Mechanism"
        ],
        "url": "https://arxiv.org/abs/2312.17257"
    },
    "5590ade4da00d9823d9b5cd9a26936eb": {
        "title": "InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent",
        "authors": [
            "Po-Lin Chen",
            "Cheng-Shang Chang"
        ],
        "date": "2023/08/03",
        "pdf": "https://arxiv.org/pdf/2308.01552",
        "abstract": " This research paper delves into the integration of OpenAI&#39;s ChatGPT into\nembodied agent systems, evaluating its influence on interactive decision-making\nbenchmark. Drawing a parallel to the concept of people assuming roles according\nto their unique strengths, we introduce InterAct. In this approach, we feed\nChatGPT with varied prompts, assigning it a numerous roles like a checker and a\nsorter, then integrating them with the original language model. Our research\nshows a remarkable success rate of 98% in AlfWorld, which consists of 6\ndifferent tasks in a simulated household environment, emphasizing the\nsignificance of proficient prompt engineering. The results highlight ChatGPT&#39;s\ncompetence in comprehending and performing intricate tasks effectively in\nreal-world settings, thus paving the way for further advancements in task\nplanning.\n",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2308.01552"
    },
    "9eff80228411b7637537c841822c6d9e": {
        "title": "MetaGPT: Meta Programming for Multi-Agent Collaborative Framework",
        "authors": [
            "Sirui Hong",
            "Xiawu Zheng",
            "Jonathan Chen",
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Zili Wang",
            "Steven Ka Shing Yau",
            "Zijuan Lin",
            "Liyang Zhou",
            "Chenyu Ran",
            "Lingfeng Xiao",
            "Chenglin Wu"
        ],
        "date": "2023/08/01",
        "pdf": "https://arxiv.org/pdf/2308.00352",
        "code": "https://github.com/geekan/MetaGPT",
        "abstract": " Recently, remarkable progress has been made in automated task-solving through\nthe use of multi-agents driven by large language models (LLMs). However,\nexisting works primarily focuses on simple tasks lacking exploration and\ninvestigation in complicated tasks mainly due to the hallucination problem.\nThis kind of hallucination gets amplified infinitely as multiple intelligent\nagents interact with each other, resulting in failures when tackling\ncomplicated problems.Therefore, we introduce MetaGPT, an innovative framework\nthat infuses effective human workflows as a meta programming approach into\nLLM-driven multi-agent collaboration. In particular, MetaGPT first encodes\nStandardized Operating Procedures (SOPs) into prompts, fostering structured\ncoordination. And then, it further mandates modular outputs, bestowing agents\nwith domain expertise paralleling human professionals to validate outputs and\nreduce compounded errors. In this way, MetaGPT leverages the assembly line work\nmodel to assign diverse roles to various agents, thus establishing a framework\nthat can effectively and cohesively deconstruct complex multi-agent\ncollaborative problems. Our experiments conducted on collaborative software\nengineering tasks illustrate MetaGPT&#39;s capability in producing comprehensive\nsolutions with higher coherence relative to existing conversational and\nchat-based multi-agent systems. This underscores the potential of incorporating\nhuman domain knowledge into multi-agents, thus opening up novel avenues for\ngrappling with intricate real-world challenges. The GitHub repository of this\nproject is made publicly available on: https://github.com/geekan/MetaGPT\n",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2308.00352"
    },
    "2eac0f4e2491bfdec954e0d07a70a650": {
        "title": "Communicative Agents for Software Development",
        "authors": [
            "Chen Qian",
            "Xin Cong",
            "Cheng Yang",
            "Weize Chen",
            "Yusheng Su",
            "Juyuan Xu",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2023/07/16",
        "pdf": "https://arxiv.org/pdf/2307.07924",
        "code": "https://github.com/openbmb/chatdev",
        "abstract": " Software engineering is a domain characterized by intricate decision-making\nprocesses, often relying on nuanced intuition and consultation. Recent\nadvancements in deep learning have started to revolutionize software\nengineering practices through elaborate designs implemented at various stages\nof software development. In this paper, we present an innovative paradigm that\nleverages large language models (LLMs) throughout the entire software\ndevelopment process, streamlining and unifying key processes through natural\nlanguage communication, thereby eliminating the need for specialized models at\neach phase. At the core of this paradigm lies ChatDev, a virtual chat-powered\nsoftware development company that mirrors the established waterfall model,\nmeticulously dividing the development process into four distinct chronological\nstages: designing, coding, testing, and documenting. Each stage engages a team\nof agents, such as programmers, code reviewers, and test engineers, fostering\ncollaborative dialogue and facilitating a seamless workflow. The chat chain\nacts as a facilitator, breaking down each stage into atomic subtasks. This\nenables dual roles, allowing for proposing and validating solutions through\ncontext-aware communication, leading to efficient resolution of specific\nsubtasks. The instrumental analysis of ChatDev highlights its remarkable\nefficacy in software generation, enabling the completion of the entire software\ndevelopment process in under seven minutes at a cost of less than one dollar.\nIt not only identifies and alleviates potential vulnerabilities but also\nrectifies potential hallucinations while maintaining commendable efficiency and\ncost-effectiveness. The potential of ChatDev unveils fresh possibilities for\nintegrating LLMs into the realm of software development.\n",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2307.07924"
    },
    "8b49604f02c96ae0a94969b9d0375cb9": {
        "title": "Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "authors": [
            "Zhenhailong Wang",
            "Shaoguang Mao",
            "Wenshan Wu",
            "Tao Ge",
            "Furu Wei",
            "Heng Ji"
        ],
        "date": "2023/07/11",
        "pdf": "https://arxiv.org/pdf/2307.05300",
        "code": "https://github.com/MikeWangWZHL/Solo-Performance-Prompting",
        "abstract": " Human intelligence thrives on the concept of cognitive synergy, where\ncollaboration and information integration among different cognitive processes\nyield superior outcomes compared to individual cognitive processes in\nisolation. Although Large Language Models (LLMs) have demonstrated promising\nperformance as general task-solving agents, they still struggle with tasks that\nrequire intensive domain knowledge and complex reasoning. In this work, we\npropose Solo Performance Prompting (SPP), which transforms a single LLM into a\ncognitive synergist by engaging in multi-turn self-collaboration with multiple\npersonas. A cognitive synergist refers to an intelligent agent that\ncollaborates with multiple minds, combining their individual strengths and\nknowledge, to enhance problem-solving and overall performance in complex tasks.\nBy dynamically identifying and simulating different personas based on task\ninputs, SPP unleashes the potential of cognitive synergy in LLMs. We have\ndiscovered that assigning multiple, fine-grained personas in LLMs elicits\nbetter problem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP\neffectively elicits internal knowledge acquisition abilities, reduces\nhallucination, and maintains strong reasoning capabilities. Code, data, and\nprompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2307.05300"
    },
    "35cb79b6dd5ae58f6f00de3fb5c70d29": {
        "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
        "authors": [
            "Hongxin Zhang",
            "Weihua Du",
            "Jiaming Shan",
            "Qinhong Zhou",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Tianmin Shu",
            "Chuang Gan"
        ],
        "date": "2023/07/05",
        "pdf": "https://arxiv.org/pdf/2307.02485",
        "code": "https://github.com/UMass-Foundation-Model/Co-LLM-Agents",
        "abstract": " Large Language Models (LLMs) have demonstrated impressive planning abilities\nin single-agent embodied tasks across various domains. However, their capacity\nfor planning and communication in multi-agent cooperation remains unclear, even\nthough these are crucial skills for intelligent embodied agents. In this paper,\nwe present a novel framework that utilizes LLMs for multi-agent cooperation and\ntests it in various embodied environments. Our framework enables embodied\nagents to plan, communicate, and cooperate with other embodied agents or humans\nto accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs,\nsuch as GPT-4, can surpass strong planning-based methods and exhibit emergent\neffective communication using our framework without requiring fine-tuning or\nfew-shot prompting. We also discover that LLM-based agents that communicate in\nnatural language can earn more trust and cooperate more effectively with\nhumans. Our research underscores the potential of LLMs for embodied AI and lays\nthe foundation for future research in multi-agent cooperation. Videos can be\nfound on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.\n",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2307.02485"
    },
    "8064cabdde186f6c901cd321e0c1e117": {
        "title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents",
        "authors": [
            "Yashar Talebirad",
            "Amirhossein Nadiri"
        ],
        "date": "2023/06/05",
        "pdf": "https://arxiv.org/pdf/2306.03314",
        "abstract": " In this paper, we present a novel framework for enhancing the capabilities of\nlarge language models (LLMs) by leveraging the power of multi-agent systems.\nOur framework introduces a collaborative environment where multiple intelligent\nagent components, each with distinctive attributes and roles, work together to\nhandle complex tasks more efficiently and effectively. We demonstrate the\npracticality and versatility of our framework through case studies in\nartificial general intelligence (AGI), specifically focusing on the Auto-GPT\nand BabyAGI models. We also examine the &#34;Gorilla&#34; model, which integrates\nexternal APIs into the LLM. Our framework addresses limitations and challenges\nsuch as looping issues, security risks, scalability, system evaluation, and\nethical considerations. By modeling various domains such as courtroom\nsimulations and software development scenarios, we showcase the potential\napplications and benefits of our proposed multi-agent system. Our framework\nprovides an avenue for advancing the capabilities and performance of LLMs\nthrough collaboration and knowledge exchange among intelligent agents.\n",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2306.03314"
    },
    "5073802d8fc8727b178df4d11b1f3cef": {
        "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents",
        "authors": [
            "Weize Chen",
            "Yusheng Su",
            "Jingwei Zuo",
            "Cheng Yang",
            "Chenfei Yuan",
            "Chen Qian",
            "Chi-Min Chan",
            "Yujia Qin",
            "Yaxi Lu",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "date": "2023/08/21",
        "pdf": "https://arxiv.org/pdf/2308.10848",
        "code": "https://github.com/OpenBMB/AgentVerse",
        "abstract": " Autonomous agents empowered by Large Language Models (LLMs) have undergone\nsignificant improvements, enabling them to generalize across a broad spectrum\nof tasks. However, in real-world scenarios, cooperation among individuals is\noften required to enhance the efficiency and effectiveness of task\naccomplishment. Hence, inspired by human group dynamics, we propose a\nmulti-agent framework \\framework that can collaboratively and dynamically\nadjust its composition as a greater-than-the-sum-of-its-parts system. Our\nexperiments demonstrate that \\framework framework can effectively deploy\nmulti-agent groups that outperform a single agent. Furthermore, we delve into\nthe emergence of social behaviors among individual agents within a group during\ncollaborative task accomplishment. In view of these behaviors, we discuss some\npossible strategies to leverage positive ones and mitigate negative ones for\nimproving the collaborative potential of multi-agent groups. Our codes for\n\\framework will soon be released at\n\\url{https://github.com/OpenBMB/AgentVerse}.\n",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2308.10848"
    },
    "eecbaf3e56b9f6a2e8c2abf1201ee2bb": {
        "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
        "authors": [
            "Yuan Li",
            "Yixuan Zhang",
            "Lichao Sun"
        ],
        "date": "2023/10/10",
        "pdf": "https://arxiv.org/pdf/2310.06500.pdf",
        "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2310.06500"
    },
    "6e9ff82716ae89e1a364697a663541a9": {
        "title": "Learning to Coordinate with Anyone",
        "authors": [
            "Lei Yuan",
            "Lihe Li",
            "Ziqian Zhang",
            "Feng Chen",
            "Tianyi Zhang",
            "Cong Guan",
            "Yang Yu",
            "Zhi-Hua Zhou"
        ],
        "date": "2023/09/22",
        "pdf": "https://arxiv.org/pdf/2309.12633.pdf",
        "abstract": "In open multi-agent environments, the agents may encounter unexpected teammates. Classical multi-agent learning approaches train agents that can only coordinate with seen teammates. Recent studies attempted to generate diverse teammates to enhance the generalizable coordination ability, but were restricted by pre-defined teammates. In this work, our aim is to train agents with strong coordination ability by generating teammates that fully cover the teammate policy space, so that agents can coordinate with any teammates. Since the teammate policy space is too huge to be enumerated, we find only dissimilar teammates that are incompatible with controllable agents, which highly reduces the number of teammates that need to be trained with. However, it is hard to determine the number of such incompatible teammates beforehand. We therefore introduce a continual multi-agent learning process, in which the agent learns to coordinate with different teammates until no more incompatible teammates can be found. The above idea is implemented in the proposed Macop (Multi-agent compatible policy learning) algorithm. We conduct experiments in 8 scenarios from 4 environments that have distinct coordination patterns. Experiments show that Macop generates training teammates with much lower compatibility than previous methods. As a result, in all scenarios Macop achieves the best overall coordination ability while never significantly worse than the baselines, showing strong generalization ability.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2309.12633"
    },
    "d8e54607635f25e1f80f96448dc3f72c": {
        "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
        "authors": [
            "Jintian Zhang",
            "Xin Xu",
            "Shumin Deng"
        ],
        "date": "2023/10/03",
        "pdf": "https://arxiv.org/pdf/2310.02124.pdf",
        "code": "https://github.com/zjunlp/MachineSoM",
        "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies&#39; comprised of LLM agents, where each agent is characterized by a specific `trait&#39; (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern&#39; (debate or reflection). Evaluating these multi-agent societies on three benchmark datasets, we discern that LLM agents navigate tasks by leveraging diverse social behaviors, from active debates to introspective reflections. Notably, certain collaborative strategies only optimize efficiency (using fewer API tokens), but also outshine previous top-tier approaches. Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity or majority rule, mirroring foundational Social Psychology theories. In conclusion, we integrate insights from Social Psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets (already submitted in supplementary materials), hoping to catalyze further research in this promising avenue (All code and data are available at \\url{https://github.com/zjunlp/MachineSoM}.).",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2310.02124"
    },
    "f5c8363d28a1236e6465845e8e6559d4": {
        "title": "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
        "authors": [
            "Dong Huang",
            "Qingwen Bu",
            "Jie M. Zhang",
            "Michael Luck",
            "Heming Cui"
        ],
        "date": "2023/12/20",
        "pdf": "http://arxiv.org/pdf/2312.13010.pdf",
        "abstract": "The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent&#39;s feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder&#39;s superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder achieves 77.4% and 89.1% pass@1 in HumanEval-ET and MBPP-ET with GPT-3.5, while SOTA baselines obtain only 69.5% and 63.0%.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2312.13010"
    },
    "de2f7afa9bbbc00faa0191f82c0cab68": {
        "title": "MultiPrompter: Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning",
        "authors": [
            "Dong-Ki Kim",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Dongsub Shim",
            "Honglak Lee"
        ],
        "date": "2023/10/25",
        "pdf": "http://arxiv.org/pdf/2310.16730.pdf",
        "abstract": "Recently, there has been an increasing interest in automated prompt optimization based on reinforcement learning (RL). This approach offers important advantages, such as generating interpretable prompts and being compatible with black-box foundation models. However, the substantial prompt space size poses challenges for RL-based methods, often leading to suboptimal policy convergence. This paper introduces MultiPrompter, a new framework that views prompt optimization as a cooperative game between prompters which take turns composing a prompt together. Our cooperative prompt optimization effectively reduces the problem size and helps prompters learn optimal prompts. We test our method on the text-to-image task and show its ability to generate higher-quality images than baselines.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2310.16730"
    },
    "2b83b0605caa5b7e0e3b46f73fbaa780": {
        "title": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games",
        "authors": [
            "Dekun Wu",
            "Haochen Shi",
            "Zhiyuan Sun",
            "Bang Liu"
        ],
        "date": "2023/12/01",
        "pdf": "http://arxiv.org/pdf/2312.00746.pdf",
        "abstract": "In this study, we explore the application of Large Language Models (LLMs) in &#34;Jubensha&#34; (Chinese murder mystery role-playing games), a novel area in AI-driven gaming. We introduce the first Chinese dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in the game, enhancing the dynamics of Jubensha gameplay. To evaluate these AI agents, we developed specialized methods targeting their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents&#39; performance in critical aspects like information gathering, murderer detection, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a fresh perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents to researchers in the field.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2312.00746"
    },
    "e8e2e43e8e9e8ce333cd2a84b3902e41": {
        "title": "Multi-Agent Consensus Seeking via Large Language Models",
        "authors": [
            "Huaben Chen",
            "Wenkang Ji",
            "Lufeng Xu",
            "Shiyu Zhao"
        ],
        "date": "2023/10/31",
        "pdf": "http://arxiv.org/pdf/2310.20151.pdf",
        "abstract": "Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: westlakeintelligentrobotics.github.io/ConsensusLLM/.",
        "category": [
            "Multi-Agent System"
        ],
        "url": "https://arxiv.org/abs/2310.20151"
    },
    "fefbb121e5e6daefd835e760d5f08117": {
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "authors": [
            "Joon Sung Park",
            "Joseph C. O&#39;Brien",
            "Carrie J. Cai",
            "Meredith Ringel Morris",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "date": "2023/04/07",
        "pdf": "https://arxiv.org/pdf/2304.03442",
        "code": "https://github.com/joonspk-research/generative_agents",
        "abstract": " Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents--computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent&#39;s experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine&#39;s\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture--observation, planning, and reflection--each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2304.03442"
    },
    "63dc16015de346991b87581ffc2f70a9": {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ],
        "date": "2023/04/19",
        "pdf": "https://arxiv.org/pdf/2304.09842",
        "abstract": " Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner.\n",
        "category": [
            "Role Playing"
        ]
    },
    "6346920757c54de4cfb53cbd57153400": {
        "title": "ChatLLM Network: More brains, More intelligence",
        "authors": [
            "Rui Hao",
            "Linmei Hu",
            "Weijian Qi",
            "Qingliu Wu",
            "Yirui Zhang",
            "Liqiang Nie"
        ],
        "date": "2023/04/24",
        "pdf": "https://arxiv.org/pdf/2304.12998",
        "abstract": " Dialogue-based language models mark a huge milestone in the field of\nartificial intelligence, by their impressive ability to interact with users, as\nwell as a series of challenging tasks prompted by customized instructions.\nHowever, the prevalent large-scale dialogue-based language models like ChatGPT\nstill have room for improvement, such as unstable responses to questions and\nthe inability to think cooperatively like humans. Considering the ability of\ndialogue-based language models in conversation and their inherent randomness in\nthinking, we propose ChatLLM network that allows multiple dialogue-based\nlanguage models to interact, provide feedback, and think together. We design\nthe network of ChatLLMs based on ChatGPT. Specifically, individual instances of\nChatGPT may possess distinct perspectives towards the same problem, and by\nconsolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM\nnetwork system can conduct decision-making more objectively and\ncomprehensively. In addition, a language-based feedback mechanism comparable to\nbackpropagation is devised to update the ChatGPTs within the network.\nExperiments on two datasets demonstrate that our network attains significant\nimprovements in problem-solving, leading to observable progress amongst each\nmember.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2304.12998"
    },
    "d4c1c4e1c46d89daf6890d5f71c62c63": {
        "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
        "authors": [
            "Jimmy Wei",
            "Kurt Shuster",
            "Arthur Szlam",
            "Jason Weston",
            "Jack Urbanek",
            "Mojtaba Komeili"
        ],
        "date": "2023/04/26",
        "pdf": "https://arxiv.org/pdf/2304.13835",
        "code": "https://github.com/facebookresearch/LIGHT",
        "abstract": " Current dialogue research primarily studies pairwise (two-party)\nconversations, and does not address the everyday setting where more than two\nspeakers converse together. In this work, we both collect and evaluate\nmulti-party conversations to study this more general case. We use the LIGHT\nenvironment to construct grounded conversations, where each participant has an\nassigned character to role-play. We thus evaluate the ability of language\nmodels to act as one or more characters in such conversations. Models require\ntwo skills that pairwise-trained models appear to lack: (1) being able to\ndecide when to talk; (2) producing coherent utterances grounded on multiple\ncharacters. We compare models trained on our new dataset to existing\npairwise-trained dialogue models, as well as large language models with\nfew-shot prompting. We find that our new dataset, MultiLIGHT, which we will\npublicly release, can help bring significant improvements in the group setting.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2304.13835"
    },
    "685611a174dfe4f335c66b3a972ad793": {
        "title": "The Role of Summarization in Generative Agents: A Preliminary Perspective",
        "authors": [
            "Xiachong Feng",
            "Xiaocheng Feng",
            "Bing Qin"
        ],
        "date": "2023/05/02",
        "pdf": "https://arxiv.org/pdf/2305.01253",
        "abstract": " Generative agents that simulate human society show tremendous potential for\nfurther research and practical applications. Specifically, the generative agent\narchitecture comprising several meticulously designed modules constitutes the\nmost critical component. To facilitate progress in this research, this report\npresents our integrated perspective on comprehending generative agents through\nsummarization, since we believe summarization is the most fundamental and\nindispensable capacity of generative agents manifested across diverse\nscenarios. We hope this report can provide insight into understanding the\nimportance of summarization capacity in generative agents and motivate future\nresearch.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2305.01253"
    },
    "98f7696b30c392052ea00d12d2d79918": {
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "authors": [
            "Jimmy Wu",
            "Rika Antonova",
            "Adam Kan",
            "Marion Lepert",
            "Andy Zeng",
            "Shuran Song",
            "Jeannette Bohg",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ],
        "date": "2023/05/09",
        "pdf": "https://arxiv.org/pdf/2305.05658",
        "code": "https://github.com/jimmyyhwu/tidybot",
        "homepage": "https://tidybot.cs.princeton.edu/",
        "abstract": " For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people&#39;s preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2305.05658"
    },
    "aff5e8a0a6be3cc3d84f669a5dc2cf7b": {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "Thomas L. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10601",
        "code": "https://github.com/ysymyth/tree-of-thought-llm",
        "abstract": " Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models&#39;\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/ysymyth/tree-of-thought-llm.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2305.10601"
    },
    "843b42ba9aa69a474fd90dd43404ee8a": {
        "title": "Role-Play with Large Language Models",
        "authors": [
            "Murray Shanahan",
            "Kyle McDonell",
            "Laria Reynolds"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16367",
        "abstract": " As dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2305.16367"
    },
    "2fe6d5bff84082b57a528b3f5b08131e": {
        "title": "Training Socially Aligned Language Models in Simulated Human Society",
        "authors": [
            "Ruibo Liu",
            "Ruixin Yang",
            "Chenyan Jia",
            "Ge Zhang",
            "Denny Zhou",
            "Andrew M. Dai",
            "Diyi Yang",
            "Soroush Vosoughi"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16960",
        "code": "https://github.com/agi-templar/Stable-Alignment",
        "abstract": " Social alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2305.16960"
    },
    "5ad397cd019ef8e7d6f8717d67b90261": {
        "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
        "authors": [
            "Bill Yuchen Lin",
            "Yicheng Fu",
            "Karina Yang",
            "Prithviraj Ammanabrolu",
            "Faeze Brahman",
            "Shiyu Huang",
            "Chandra Bhagavatula",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "date": "2023/05/27",
        "pdf": "https://arxiv.org/pdf/2305.17390",
        "code": "https://github.com/yuchenlin/swiftsage/",
        "homepage": "https://yuchenlin.xyz/swiftsage/",
        "abstract": " We introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent&#39;s action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\nreal-world tasks.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2305.17390"
    },
    "445a34d666cdf8017df2d3b28f3e2bcf": {
        "title": "CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Scale Language Model Society",
        "authors": [
            "Guohao Li",
            "Hasan Abed Al Kader Hammoud",
            "Hani Itani",
            "Dmitrii Khizbullin",
            "Bernard Ghanem"
        ],
        "date": "2023/03/31",
        "pdf": "https://arxiv.org/pdf/2303.17760",
        "abstract": " The rapid advancement of conversational and chat-based language models has\nled to remarkable progress in complex task-solving. However, their success\nheavily relies on human input to guide the conversation, which can be\nchallenging and time-consuming. This paper explores the potential of building\nscalable techniques to facilitate autonomous cooperation among communicative\nagents and provide insight into their &#34;cognitive&#34; processes. To address the\nchallenges of achieving autonomous cooperation, we propose a novel\ncommunicative agent framework named role-playing. Our approach involves using\ninception prompting to guide chat agents toward task completion while\nmaintaining consistency with human intentions. We showcase how role-playing can\nbe used to generate conversational data for studying the behaviors and\ncapabilities of chat agents, providing a valuable resource for investigating\nconversational language models. Our contributions include introducing a novel\ncommunicative agent framework, offering a scalable approach for studying the\ncooperative behaviors and capabilities of multi-agent systems, and\nopen-sourcing our library to support research on communicative agents and\nbeyond. The GitHub repository of this project is made publicly available on:\nhttps://github.com/lightaime/camel.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2303.17760"
    },
    "6728afeb589c1328ba967436092eeb98": {
        "title": "Self-collaboration Code Generation via ChatGPT",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Zhi Jin",
            "Ge Li"
        ],
        "date": "2023/04/15",
        "pdf": "https://arxiv.org/pdf/2304.07590",
        "abstract": " Although Large Language Models (LLMs) have demonstrated remarkable\ncode-generation ability, they still struggle with complex tasks. In real-world\nsoftware development, humans usually tackle complex tasks through collaborative\nteamwork, a strategy that significantly controls development complexity and\nenhances software quality. Inspired by this, we present a self-collaboration\nframework for code generation employing LLMs, exemplified by ChatGPT.\nSpecifically, through role instructions, 1) Multiple LLMs act as distinct\n``experts&#39;&#39;, each responsible for a specific subtask within a complex task; 2)\nSpecify the way to collaborate and interact, so that different roles form a\nvirtual team to facilitate each other&#39;s work, ultimately the virtual team\naddresses code generation tasks collaboratively without the need for human\nintervention. To effectively organize and manage this virtual team, we\nincorporate software-development methodology into the framework. Thus, we\nassemble an elementary team consisting of three ChatGPT roles (i.e., analyst,\ncoder, and tester) responsible for software development&#39;s analysis, coding, and\ntesting stages. We conduct comprehensive experiments on various code-generation\nbenchmarks. Experimental results indicate that self-collaboration code\ngeneration relatively improves 29.9%-47.1% Pass@1 compared to direct code\ngeneration, achieving state-of-the-art performance and even surpassing GPT-4.\nMoreover, we showcase that self-collaboration could potentially enable LLMs to\nefficiently handle complex real-world tasks that are not readily solved by\ndirect code generation, as evidenced in case study.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2304.07590"
    },
    "8fb53b2baa1560345d6a6abe0eb7961f": {
        "title": "Inferring the Goals of Communicating Agents from Actions and Instructions",
        "authors": [
            "Lance Ying",
            "Tan Zhi-Xuan",
            "Vikash Mansinghka",
            "Joshua B. Tenenbaum"
        ],
        "date": "2023/06/28",
        "pdf": "https://arxiv.org/pdf/2306.16207",
        "abstract": " When humans cooperate, they frequently coordinate their activity through both\nverbal communication and non-verbal actions, using this information to infer a\nshared goal and plan. How can we model this inferential ability? In this paper,\nwe introduce a model of a cooperative team where one agent, the principal, may\ncommunicate natural language instructions about their shared plan to another\nagent, the assistant, using GPT-3 as a likelihood function for instruction\nutterances. We then show how a third person observer can infer the team&#39;s goal\nvia multi-modal Bayesian inverse planning from actions and instructions,\ncomputing the posterior distribution over goals under the assumption that\nagents will act and communicate rationally to achieve them. We evaluate this\napproach by comparing it with human goal inferences in a multi-agent gridworld,\nfinding that our model&#39;s inferences closely correlate with human judgments (R =\n0.96). When compared to inference from actions alone, we also find that\ninstructions lead to more rapid and less uncertain goal inference, highlighting\nthe importance of verbal communication for cooperative agents.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2306.16207"
    },
    "b528b505331cd36fd5ba472f7bc6922c": {
        "title": "To Infinity and Beyond: SHOW-1 and Showrunner Agents in Multi-Agent Simulations",
        "authors": [
            "Philipp Maas",
            "Frank Carey",
            "Chris Wheeler",
            "Edward Saatchi",
            "Pete Billington",
            "Jessica Yaffa Shamash"
        ],
        "date": "2023/07/24",
        "pdf": "https://fablestudio.github.io/showrunner-agents/static/pdfs/To_Infinity_and_Beyond_SHOW-1_And_Showrunner_Agents_in_Multi_Agent_Simulations_v2.pdf",
        "abstract": "In this work we present our approach to generating high-quality episodic content for IP's (Intellectual Property) using large language models (LLMs), custom state-of-the art diffusion models and our multi-agent simulation for contextualization, story progression and behavioral control. Powerful LLMs such as GPT-4 were trained on a large corpus of TV show data which lets us believe that with the right guidance users will be able to rewrite entire seasons. \"That Is What Entertainment Will Look Like. Maybe people are still upset about the last season of Game of Thrones. Imagine if you could ask your A.I. to make a new ending that goes a different way and maybe even put yourself in there as a main character or something.",
        "category": [
            "Role Playing"
        ],
        "url": "https://fablestudio.github.io/showrunner-agents/"
    },
    "3447891c35f4bf8f904ab23534e84b9e": {
        "title": "TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents",
        "authors": [
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Zhiwei Xu",
            "Tianpeng Bao",
            "Guoqing Du",
            "Shiwei Shi",
            "Hangyu Mao",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "date": "2023/08/07",
        "pdf": "https://arxiv.org/pdf/2308.03427.pdf",
        "abstract": "With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2308.03427"
    },
    "b9f40007be7ffb09746b30ea4342e688": {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "authors": [
            "Chi-Min Chan",
            "Weize Chen",
            "Yusheng Su",
            "Jianxuan Yu",
            "Wei Xue",
            "Shanghang Zhang",
            "Jie Fu",
            "Zhiyuan Liu"
        ],
        "date": "2023/08/14",
        "pdf": "https://arxiv.org/pdf/2308.07201",
        "code": "https://github.com/thunlp/ChatEval",
        "abstract": " Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs&#39; potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2308.07201"
    },
    "99ab89a760611e1ee01b97ece9476d28": {
        "title": "LLM As DBA",
        "authors": [
            "Xuanhe Zhou",
            "Guoliang Li",
            "Zhiyuan Liu"
        ],
        "date": "2023/08/10",
        "pdf": "https://arxiv.org/pdf/2308.05481",
        "code": "https://github.com/TsinghuaDatabaseGroup/DB-GPT",
        "abstract": " Database administrators (DBAs) play a crucial role in managing, maintaining\nand optimizing a database system to ensure data availability, performance, and\nreliability. However, it is hard and tedious for DBAs to manage a large number\nof database instances (e.g., millions of instances on the cloud databases).\nRecently large language models (LLMs) have shown great potential to understand\nvaluable documents and accordingly generate reasonable answers. Thus, we\npropose D-Bot, a LLM-based database administrator that can continuously acquire\ndatabase maintenance experience from textual sources, and provide reasonable,\nwell-founded, in-time diagnosis and optimization advice for target databases.\nThis paper presents a revolutionary LLM-centric framework for database\nmaintenance, including (i) database maintenance knowledge detection from\ndocuments and tools, (ii) tree of thought reasoning for root cause analysis,\nand (iii) collaborative diagnosis among multiple LLMs. Our preliminary\nexperimental results that D-Bot can efficiently and effectively diagnose the\nroot causes and our code is available at\ngithub.com/TsinghuaDatabaseGroup/DB-GPT.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2308.05481"
    },
    "ccf3bc888537a9e8c3fd5a712c59e974": {
        "title": "Cognitive Architectures for Language Agents",
        "authors": [
            "Theodore Sumers",
            "Shunyu Yao",
            "Karthik Narasimhan",
            "Thomas L. Griffiths"
        ],
        "date": "2023/09/05",
        "pdf": "https://arxiv.org/pdf/2309.02427",
        "abstract": " Recent efforts have incorporated large language models (LLMs) with external\nresources (e.g., the Internet) or internal control flows (e.g., prompt\nchaining) for tasks requiring grounding or reasoning. However, these efforts\nhave largely been piecemeal, lacking a systematic framework for constructing a\nfully-fledged language agent. To address this challenge, we draw on the rich\nhistory of agent design in symbolic artificial intelligence to develop a\nblueprint for a new wave of cognitive language agents. We first show that LLMs\nhave many of the same properties as production systems, and recent efforts to\nimprove their grounding or reasoning mirror the development of cognitive\narchitectures built around production systems. We then propose Cognitive\nArchitectures for Language Agents (CoALA), a conceptual framework to\nsystematize diverse methods for LLM-based reasoning, grounding, learning, and\ndecision making as instantiations of language agents in the framework. Finally,\nwe use the CoALA framework to highlight gaps and propose actionable directions\ntoward more capable language agents in the future.\n",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2309.02427"
    },
    "649d55c8a7a65af48b7a6a279512302a": {
        "title": "Unleashing the Power of Graph Learning through LLM-based Autonomous Agents",
        "authors": [
            "Lanning Wei",
            "Zhiqiang He",
            "Huan Zhao",
            "Quanming Yao"
        ],
        "date": "2023/09/08",
        "pdf": "https://arxiv.org/pdf/2309.04565.pdf",
        "abstract": "Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph learning task is decomposed into three components following the agent planning, namely, detecting the learning intent, configuring solutions based on AutoGraph, and generating a response. The AutoGraph agents manage crucial procedures in automated graph learning, including data-processing, AutoML configuration, searching architectures, and hyper-parameter fine-tuning. With these agents, those components are processed by decomposing and completing step by step, thereby generating a solution for the given data automatically, regardless of the learning task on node or graph. The proposed method is dubbed Auto$^2$Graph, and the comparable performance on different datasets and learning tasks. Its effectiveness is demonstrated by its comparable performance on different datasets and learning tasks, as well as the human-like decisions made by the agents.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2309.04565"
    },
    "c80b679cf1da13cfa8781f22f67a9488": {
        "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models",
        "authors": [
            "Zekun Moore Wang",
            "Zhongyuan Peng",
            "Haoran Que",
            "Jiaheng Liu",
            "Wangchunshu Zhou",
            "Yuhan Wu",
            "Hongcheng Guo",
            "Ruitong Gan",
            "Zehao Ni",
            "Man Zhang",
            "Zhaoxiang Zhang",
            "Wanli Ouyang",
            "Ke Xu",
            "Wenhu Chen",
            "Jie Fu",
            "Junran Peng"
        ],
        "date": "2023/10/01",
        "pdf": "https://arxiv.org/pdf/2310.00746.pdf",
        "code": "https://github.com/InteractiveNLP-Team/RoleLLM-public",
        "abstract": "The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2310.00746"
    },
    "f46f37266200ef7081c636ebaf459a15": {
        "title": "Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations",
        "authors": [
            "Zengqing Wu",
            "Run Peng",
            "Xu Han",
            "Shuyuan Zheng",
            "Yixin Zhang",
            "Chuan Xiao"
        ],
        "date": "2023/11/10",
        "pdf": "http://arxiv.org/pdf/2311.06330.pdf",
        "code": "https://github.com/Roihn/SABM",
        "abstract": "Computer simulations offer a robust toolset for exploring complex systems across various disciplines. A particularly impactful approach within this realm is Agent-Based Modeling (ABM), which harnesses the interactions of individual agents to emulate intricate system dynamics. ABM&#39;s strength lies in its bottom-up methodology, illuminating emergent phenomena by modeling the behaviors of individual components of a system. Yet, ABM has its own set of challenges, notably its struggle with modeling natural language instructions and common sense in mathematical equations or rules. This paper seeks to transcend these boundaries by integrating Large Language Models (LLMs) like GPT into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based Modeling (SABM). Building upon the concept of smart agents -- entities characterized by their intelligence, adaptability, and computation ability -- we explore in the direction of utilizing LLM-powered agents to simulate real-world scenarios with increased nuance and realism. In this comprehensive exploration, we elucidate the state of the art of ABM, introduce SABM&#39;s potential and methodology, and present three case studies (source codes available at https://github.com/Roihn/SABM), demonstrating the SABM methodology and validating its effectiveness in modeling real-world systems. Furthermore, we cast a vision towards several aspects of the future of SABM, anticipating a broader horizon for its applications. Through this endeavor, we aspire to redefine the boundaries of computer simulations, enabling a more profound understanding of complex systems.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2311.06330"
    },
    "de513f1e7dfb0963d0a6da0cda7e8108": {
        "title": "ChatGPT as a commenter to the news: can LLMs generate human-like opinions?",
        "authors": [
            "Rayden Tseng",
            "Suzan Verberne",
            "Peter van der Putten"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13961.pdf",
        "abstract": "ChatGPT, GPT-3.5, and other large language models (LLMs) have drawn significant attention since their release, and the abilities of these models have been investigated for a wide variety of tasks. In this research we investigate to what extent GPT-3.5 can generate human-like comments on Dutch news articles. We define human likeness as `not distinguishable from human comments&#39;, approximated by the difficulty of automatic classification between human and GPT comments. We analyze human likeness across multiple prompting techniques. In particular, we utilize zero-shot, few-shot and context prompts, for two generated personas. We found that our fine-tuned BERT models can easily distinguish human-written comments from GPT-3.5 generated comments, with none of the used prompting methods performing noticeably better. We further analyzed that human comments consistently showed higher lexical diversity than GPT-generated comments. This indicates that although generative LLMs can generate fluent text, their capability to create human-like opinionated comments is still limited.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2312.13961"
    },
    "41a3472c1867c5546c3cac80da7266ef": {
        "title": "Can ChatGPT be Your Personal Medical Assistant?",
        "authors": [
            "Md. Rafiul Biswas",
            "Ashhadul Islam",
            "Zubair Shah",
            "Wajdi Zaghouani",
            "Samir Brahim Belhaouari"
        ],
        "date": "2023/12/19",
        "pdf": "http://arxiv.org/pdf/2312.12006.pdf",
        "abstract": "The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs. This study aims to evaluate the potential of using a fine-tuned ChatGPT model as a personal medical assistant in the Arabic language. To do so, this study uses publicly available online questions and answering datasets in Arabic language. There are almost 430K questions and answers for 20 disease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion of this dataset. The performance of this fine-tuned model was evaluated through automated and human evaluation. The automated evaluations include perplexity, coherence, similarity, and token count. Native Arabic speakers with medical knowledge evaluated the generated text by calculating relevance, accuracy, precision, logic, and originality. The overall result shows that ChatGPT has a bright future in medical assistance.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2312.12006"
    },
    "9a8576963b068bd4380c72b187026243": {
        "title": "Reasoning with Language Model is Planning with World Model",
        "authors": [
            "Shibo Hao",
            "Yi Gu",
            "Haodi Ma",
            "Joshua Jiahua Hong",
            "Zhen Wang",
            "Daisy Zhe Wang",
            "Zhiting Hu"
        ],
        "date": "2023/05/24",
        "pdf": "http://arxiv.org/pdf/2305.14992.pdf",
        "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2305.14992"
    },
    "05010b701c190badb6151c0c424f1d62": {
        "title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars",
        "authors": [
            "Wenyue Hua",
            "Lizhou Fan",
            "Lingyao Li",
            "Kai Mei",
            "Jianchao Ji",
            "Yingqiang Ge",
            "Libby Hemphill",
            "Yongfeng Zhang"
        ],
        "date": "2023/11/28",
        "pdf": "http://arxiv.org/pdf/2311.17227.pdf",
        "abstract": "Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems&#39; abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \\url{https://github.com/agiresearch/WarAgent}.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2311.17227"
    },
    "e1722153565c5e01fff23a692a6cfb06": {
        "title": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",
        "authors": [
            "Yingqiang Ge",
            "Yujie Ren",
            "Wenyue Hua",
            "Shuyuan Xu",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "date": "2023/12/06",
        "pdf": "http://arxiv.org/pdf/2312.03815.pdf",
        "abstract": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)--an operating system &#34;with soul&#34;. Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLM&#39;s impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts: LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level). We begin by introducing the architecture of traditional OS. Then we formalize a conceptual framework for AIOS through &#34;LLM as OS (LLMOS)&#34;, drawing analogies between AIOS and traditional OS: LLM is likened to OS kernel, context window to memory, external storage to file system, hardware tools to peripheral devices, software tools to programming libraries, and user prompts to user commands. Subsequently, we introduce the new AIOS-Agent Ecosystem, where users can easily program Agent Applications (AAPs) using natural language, democratizing the development of software, which is different from the traditional OS-APP ecosystem. Following this, we explore the diverse scope of Agent Applications. We delve into both single-agent and multi-agent systems, as well as human-agent interaction. Lastly, drawing on the insights from traditional OS-APP ecosystem, we propose a roadmap for the evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the future research and development, suggesting systematic progresses of AIOS and its Agent applications.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2312.03815"
    },
    "527cb71d35909520605f0d39509bfac8": {
        "title": "Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs",
        "authors": [
            "Sanjay Oruganti",
            "Sergei Nirenburg",
            "Jesse English",
            "Marjorie McShane"
        ],
        "date": "2023/12/27",
        "pdf": "http://arxiv.org/pdf/2312.16378.pdf",
        "abstract": "The paper describes a system that uses large language model (LLM) technology to support the automatic learning of new entries in an intelligent agent&#39;s semantic lexicon. The process is bootstrapped by an existing non-toy lexicon and a natural language generator that converts formal, ontologically-grounded representations of meaning into natural language sentences. The learning method involves a sequence of LLM requests and includes an automatic quality control step. To date, this learning method has been applied to learning multiword expressions whose meanings are equivalent to those of transitive verbs in the agent&#39;s lexicon. The experiment demonstrates the benefits of a hybrid learning architecture that integrates knowledge-based methods and resources with both traditional data analytics and LLMs.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2312.16378"
    },
    "3c5cb3f3c825afd95557058b3c655d7e": {
        "title": "Experiential Co-Learning of Software-Developing Agents",
        "authors": [
            "Chen Qian",
            "Yufan Dang",
            "Jiahao Li",
            "Wei Liu",
            "Weize Chen",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2023/12/28",
        "pdf": "http://arxiv.org/pdf/2312.17025.pdf",
        "abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. These agents are now capable of collaborating seamlessly, splitting tasks and enhancing accuracy, thus minimizing the need for human involvement. However, these agents often approach a diverse range of tasks in isolation, without benefiting from past experiences. This isolation can lead to repeated mistakes and inefficient trials in task solving. To this end, this paper introduces Experiential Co-Learning, a novel framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for mutual reasoning. This paradigm, enriched with previous experiences, equips agents to more effectively address unseen tasks.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2312.17025"
    },
    "1a7cb27152159d355d47bfa90274d0ab": {
        "title": "Towards an On-device Agent for Text Rewriting",
        "authors": [
            "Yun Zhu",
            "Yinxiao Liu",
            "Felix Stahlberg",
            "Shankar Kumar",
            "Yu-hui Chen",
            "Liangchen Luo",
            "Lei Shu",
            "Renjie Liu",
            "Jindong Chen",
            "Lei Meng"
        ],
        "date": "2023/08/22",
        "pdf": "http://arxiv.org/pdf/2308.11807.pdf",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference. Creating a smaller yet potent language model for text rewriting presents a formidable challenge because it requires balancing the need for a small size with the need to retain the emergent capabilities of the LLM, that requires costly data collection. To address the above challenge, we introduce a new instruction tuning approach for building a mobile-centric text rewriting model. Our strategies enable the generation of high quality training data without any human labeling. In addition, we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data. To further bridge the performance gap with the larger server-side model, we propose an effective approach that combines the mobile rewrite agent with the server model using a cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce MessageRewriteEval, a benchmark that focuses on text rewriting for messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size. Notably, we show that our proposed cascading approach improves model performance.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2308.11807"
    },
    "2243b2331fd2574d72aca0cb65e10376": {
        "title": "Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach",
        "authors": [
            "Bin Zhang",
            "Hangyu Mao",
            "Jingqing Ruan",
            "Ying Wen",
            "Yang Li",
            "Shao Zhang",
            "Zhiwei Xu",
            "Dapeng Li",
            "Ziyue Li",
            "Rui Zhao",
            "Lijuan Li",
            "Guoliang Fan"
        ],
        "date": "2023/11/23",
        "pdf": "http://arxiv.org/pdf/2311.13884.pdf",
        "abstract": "The significant advancements in large language models (LLMs) have presented novel opportunities for tackling planning and decision-making within multi-agent systems. However, as the number of agents increases, the issues of hallucination in LLMs and coordination in multi-agent systems (MAS) have become increasingly pronounced. Additionally, the efficient utilization of tokens becomes a critical consideration when employing LLMs to facilitate the interactions of large numbers of agents. In this paper, we present a novel framework aimed at enhancing coordination and decision-making capabilities of LLMs within large-scale multi-agent environments. Our approach draws inspiration from the actor-critic framework employed in multi-agent reinforcement learning, and we develop a modular and token-efficient solution that effectively addresses challenges presented by LLMs and MAS. Through evaluations conducted in experiments involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.",
        "category": [
            "Role Playing"
        ],
        "url": "https://arxiv.org/abs/2311.13884"
    },
    "28a8719ce1adca46dae998bec22518f1": {
        "title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
        "authors": [
            "Chen Feng Tsai",
            "Xiaochen Zhou",
            "Sierra S. Liu",
            "Jing Li",
            "Mo Yu",
            "Hongyuan Mei"
        ],
        "date": "2023/04/06",
        "pdf": "https://arxiv.org/pdf/2304.02868",
        "abstract": " Large language models (LLMs) such as ChatGPT and GPT-4 have recently\ndemonstrated their remarkable abilities of communicating with human users. In\nthis technical report, we take an initiative to investigate their capacities of\nplaying text games, in which a player has to understand the environment and\nrespond to situations by having dialogues with the game world. Our experiments\nshow that ChatGPT performs competitively compared to all the existing systems\nbut still exhibits a low level of intelligence. Precisely, ChatGPT can not\nconstruct the world model by playing the game or even reading the game manual;\nit may fail to leverage the world knowledge that it already has; it cannot\ninfer the goal of each step as the game progresses. Our results open up new\nresearch questions at the intersection of artificial intelligence, machine\nlearning, and natural language processing.\n",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2304.02868"
    },
    "26a0f2b5b4d2ff9c7cc8fad72d1a94ff": {
        "title": "Next Steps for Human-Centered Generative AI: A Technical Perspective",
        "authors": [
            "Xiang &#39;Anthony&#39; Chen",
            "Jeff Burke",
            "Ruofei Du",
            "Matthew K. Hong",
            "Jennifer Jacobs",
            "Philippe Laban",
            "Dingzeyu Li",
            "Nanyun Peng",
            "Karl D. D. Willis",
            "Chien-Sheng Wu",
            "Bolei Zhou"
        ],
        "date": "2023/06/27",
        "pdf": "https://arxiv.org/pdf/2306.15774",
        "abstract": " Through iterative, cross-disciplinary discussions, we define and propose\nnext-steps for Human-centered Generative AI (HGAI) from a technical\nperspective. We contribute a roadmap that lays out future directions of\nGenerative AI spanning three levels: Aligning with human values; Accommodating\nhumans&#39; expression of intents; and Augmenting humans&#39; abilities in a\ncollaborative workflow. This roadmap intends to draw interdisciplinary research\nteams to a comprehensive list of emergent ideas in HGAI, identifying their\ninterested topics while maintaining a coherent big picture of the future work\nlandscape.\n",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2306.15774"
    },
    "75288ef81bd454aa2c0c646d5fdc4101": {
        "title": "A Survey on Large Language Model based Autonomous Agents",
        "authors": [
            "Lei Wang",
            "Chen Ma",
            "Xueyang Feng",
            "Zeyu Zhang",
            "Hao Yang",
            "Jingsen Zhang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Xu Chen",
            "Yankai Lin",
            "Wayne Xin Zhao",
            "Zhewei Wei",
            "Ji-Rong Wen"
        ],
        "date": "2023/08/22",
        "pdf": "https://arxiv.org/pdf/2308.11432",
        "code": "https://github.com/Paitesanshi/LLM-Agent-Survey",
        "abstract": " Autonomous agents have long been a prominent research topic in the academic\ncommunity. Previous research in this field often focuses on training agents\nwith limited knowledge within isolated environments, which diverges\nsignificantly from the human learning processes, and thus makes the agents hard\nto achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating autonomous agents based on LLMs. To harness\nthe full potential of LLMs, researchers have devised diverse agent\narchitectures tailored to different applications. In this paper, we present a\ncomprehensive survey of these studies, delivering a systematic review of the\nfield of autonomous agents from a holistic perspective. More specifically, our\nfocus lies in the construction of LLM-based agents, for which we propose a\nunified framework that encompasses a majority of the previous work.\nAdditionally, we provide a summary of the various applications of LLM-based AI\nagents in the domains of social science, natural science, and engineering.\nLastly, we discuss the commonly employed evaluation strategies for LLM-based AI\nagents. Based on the previous studies, we also present several challenges and\nfuture directions in this field. To keep track of this field and continuously\nupdate our survey, we maintain a repository for the related references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2308.11432"
    },
    "bb0bbf6c3da5efdcd8f7acdce436c1f1": {
        "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Xin Guo",
            "Wei He",
            "Yiwen Ding",
            "Boyang Hong",
            "Ming Zhang",
            "Junzhe Wang",
            "Senjie Jin",
            "Enyu Zhou",
            "Rui Zheng",
            "Xiaoran Fan",
            "Xiao Wang",
            "Limao Xiong",
            "Yuhao Zhou",
            "Weiran Wang",
            "Changhao Jiang",
            "Yicheng Zou",
            "Xiangyang Liu",
            "Zhangyue Yin",
            "Shihan Dou",
            "Rongxiang Weng",
            "Wensen Cheng",
            "Qi Zhang",
            "Wenjuan Qin",
            "Yongyan Zheng",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "date": "2023/09/14",
        "pdf": "https://arxiv.org/pdf/2309.07864",
        "code": "https://github.com/woooodyy/llm-agent-paper-list",
        "abstract": " For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent AI agents since the mid-20th century. However, these\nefforts have mainly focused on advancement in algorithms or training strategies\nto enhance specific capabilities or performance on particular tasks. Actually,\nwhat the community lacks is a sufficiently general and powerful model to serve\nas a starting point for designing AI agents that can adapt to diverse\nscenarios. Due to the versatile and remarkable capabilities they demonstrate,\nlarge language models (LLMs) are regarded as potential sparks for Artificial\nGeneral Intelligence (AGI), offering hope for building general AI agents. Many\nresearch efforts have leveraged LLMs as the foundation to build AI agents and\nhave achieved significant progress. We start by tracing the concept of agents\nfrom its philosophical origins to its development in AI, and explain why LLMs\nare suitable foundations for AI agents. Building upon this, we present a\nconceptual framework for LLM-based agents, comprising three main components:\nbrain, perception, and action, and the framework can be tailored to suit\ndifferent applications. Subsequently, we explore the extensive applications of\nLLM-based agents in three aspects: single-agent scenarios, multi-agent\nscenarios, and human-agent cooperation. Following this, we delve into agent\nsocieties, exploring the behavior and personality of LLM-based agents, the\nsocial phenomena that emerge when they form societies, and the insights they\noffer for human society. Finally, we discuss a range of key topics and open\nproblems within the field.\n",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2309.07864"
    },
    "fa928d3d54acadb993f4b8fc3cb3acee": {
        "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
        "authors": [
            "Chen Gao",
            "Xiaochong Lan",
            "Nian Li",
            "Yuan Yuan",
            "Jingtao Ding",
            "Zhilun Zhou",
            "Fengli Xu",
            "Yong Li"
        ],
        "date": "2023/12/19",
        "pdf": "http://arxiv.org/pdf/2312.11970.pdf",
        "abstract": "Agent-based modeling and simulation has evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, examining their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions.",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2312.11970"
    },
    "2c30a38090198bd31c3053691e42f5f1": {
        "title": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots",
        "authors": [
            "Richard Sutcliffe"
        ],
        "date": "2023/12/31",
        "pdf": "http://arxiv.org/pdf/2401.00609.pdf",
        "abstract": "We present a review of personality in neural conversational agents (CAs), also called chatbots. First, we define Personality, Persona, and Profile. We explain all personality schemes which have been used in CAs, and list models under the scheme(s) which they use. Second we describe 21 datasets which have been developed in recent CA personality research. Third, we define the methods used to embody personality in a CA, and review recent models using them. Fourth, we survey some relevant reviews on CAs, personality, and related topics. Finally, we draw conclusions and identify some research challenges for this important emerging field.",
        "category": [
            "Survey"
        ],
        "url": "https://arxiv.org/abs/2401.00609"
    },
    "a89d7d064ee50b5f5d994406024db2df": {
        "title": "AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems",
        "authors": [
            "Junjie Zhang",
            "Yupeng Hou",
            "Ruobing Xie",
            "Wenqi Sun",
            "Julian McAuley",
            "Wayne Xin Zhao",
            "Leyu Lin",
            "Ji-Rong Wen"
        ],
        "date": "2023/10/13",
        "pdf": "https://arxiv.org/pdf/2310.09233.pdf",
        "abstract": "Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations. To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents&#39; decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2310.09233"
    },
    "6bf24ac7d0056e0bc0240714642d96f8": {
        "title": "When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm",
        "authors": [
            "Lei Wang",
            "Jingsen Zhang",
            "Hao Yang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Zeyu Zhang",
            "Xu Chen",
            "Yankai Lin",
            "Ruihua Song",
            "Wayne Xin Zhao",
            "Jun Xu",
            "Zhicheng Dou",
            "Jun Wang",
            "Ji-Rong Wen"
        ],
        "date": "2023/06/05",
        "pdf": "https://arxiv.org/pdf/2306.02552.pdf",
        "code": "https://github.com/RUC-GSAI/YuLan-Rec",
        "abstract": "User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a virtual simulator called RecAgent. For comprehensively simulation, we not only consider the behaviors within the recommender system (\\emph{e.g.}, item browsing and clicking), but also accounts for external influential factors, such as, friend chatting and social advertisement. Our simulator contains at most 1000 agents, and each agent is composed of a profiling module, a memory module and an action module, enabling it to behave consistently, reasonably and reliably. In addition, to more flexibly operate our simulator, we also design two global functions including real-human playing and system intervention. To evaluate the effectiveness of our simulator, we conduct extensive experiments from both agent and system perspectives. In order to advance this direction, we have released our project at {https://github.com/RUC-GSAI/YuLan-Rec}.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2306.02552"
    },
    "79e1c9fc13e42067576b1a2670c7134b": {
        "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
        "authors": [
            "Dingyao Yu",
            "Kaitao Song",
            "Peiling Lu",
            "Tianyu He",
            "Xu Tan",
            "Wei Ye",
            "Shikun Zhang",
            "Jiang Bian"
        ],
        "date": "2023/10/18",
        "pdf": "https://arxiv.org/pdf/2310.11954.pdf",
        "abstract": "AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2310.11954"
    },
    "73579681b0fcf709b1b58ba2083317fa": {
        "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
        "authors": [
            "Chenliang Li",
            "Hehong Chen",
            "Ming Yan",
            "Weizhou Shen",
            "Haiyang Xu",
            "Zhikai Wu",
            "Zhicheng Zhang",
            "Wenmeng Zhou",
            "Yingda Chen",
            "Chen Cheng",
            "Hongzhu Shi",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "date": "2023/09/02",
        "pdf": "https://arxiv.org/pdf/2309.00986.pdf",
        "code": "https://github.com/modelscope/modelscope-agent",
        "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2309.00986"
    },
    "a555d236f43f11398588858b6003f747": {
        "title": "A Zero-Shot Language Agent for Computer Control with Structured Reflection",
        "authors": [
            "Tao Li",
            "Gang Li",
            "Zhiwei Deng",
            "Bryan Wang",
            "Yang Li"
        ],
        "date": "2023/10/12",
        "pdf": "https://arxiv.org/pdf/2310.08740.pdf",
        "abstract": "Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior best models, even though previous works had the advantages of accessing expert traces or additional screen information.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2310.08740"
    },
    "2f63080abf12a0214d3f2cbe658b934c": {
        "title": "TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems",
        "authors": [
            "Yilun Kong",
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Tianpeng Bao",
            "Shiwei Shi",
            "Guoqing Du",
            "Xiaoru Hu",
            "Hangyu Mao",
            "Ziyue Li",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "date": "2023/11/19",
        "pdf": "https://arxiv.org/pdf/2311.11315.pdf",
        "abstract": "Large Language Models (LLMs) have demonstrated proficiency in addressing tasks that necessitate a combination of task planning and the usage of external tools that require a blend of task planning and the utilization of external tools, such as APIs. However, real-world complex systems present three prevalent challenges concerning task planning and tool usage: (1) The real system usually has a vast array of APIs, so it is impossible to feed the descriptions of all APIs to the prompt of LLMs as the token length is limited; (2) the real system is designed for handling complex tasks, and the base LLMs can hardly plan a correct sub-task order and API-calling order for such tasks; (3) Similar semantics and functionalities among APIs in real systems create challenges for both LLMs and even humans in distinguishing between them. In response, this paper introduces a comprehensive framework aimed at enhancing the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating within real-world systems. Our framework comprises three key components designed to address these challenges: (1) the API Retriever selects the most pertinent APIs for the user task among the extensive array available; (2) LLM Finetuner tunes a base LLM so that the finetuned LLM can be more capable for task planning and API calling; (3) the Demo Selector adaptively retrieves different demonstrations related to hard-to-distinguish APIs, which is further used for in-context learning to boost the final performance. We validate our methods using a real-world commercial system as well as an open-sourced academic dataset, and the outcomes clearly showcase the efficacy of each individual component as well as the integrated framework.",
        "category": [
            "Tool Usage&Human-Agent Interaction",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2311.11315"
    },
    "32379c096b4ff85ab67a07fc51cbd301": {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Qingsong Lv",
            "Jiazheng Xu",
            "Wenmeng Yu",
            "Junhui Ji",
            "Yan Wang",
            "Zihan Wang",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "date": "2023/12/14",
        "pdf": "https://arxiv.org/pdf/2312.08914.pdf",
        "abstract": "People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at \\url{https://github.com/THUDM/CogVLM}.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2312.08914"
    },
    "07b43c95e66ae36855f382f95cea8e41": {
        "title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System",
        "authors": [
            "Ryu Hirai",
            "Shinya Iizuka",
            "Haruhisa Iseno",
            "Ao Guo",
            "Jingjing Jiang",
            "Atsumoto Ohashi",
            "Ryuichiro Higashinaka"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13816.pdf",
        "abstract": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve the capability of dialogue robots, our team developed a system that could build common ground and take more natural turns based on user utterance texts. Our system generated queries for sightseeing spot searches using the common ground and engaged in dialogue while waiting for user comprehension.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2312.13816"
    },
    "a268d9a93ef4c6dfe9e15a988ec88129": {
        "title": "AppAgent: Multimodal Agents as Smartphone Users",
        "authors": [
            "Chi Zhang",
            "Zhao Yang",
            "Jiaxuan Liu",
            "Yucheng Han",
            "Xin Chen",
            "Zebiao Huang",
            "Bin Fu",
            "Gang Yu"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13771.pdf",
        "code": "https://github.com/mnotgod96/AppAgent",
        "abstract": "Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent&#39;s functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent&#39;s proficiency in handling a diverse array of high-level tasks.",
        "category": [
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2312.13771"
    },
    "870815e42db4a9b074915ce5a5cd06d8": {
        "title": "MindAgent: Emergent Gaming Interaction",
        "authors": [
            "Ran Gong",
            "Qiuyuan Huang",
            "Xiaojian Ma",
            "Hoi Vo",
            "Zane Durante",
            "Yusuke Noda",
            "Zilong Zheng",
            "Song-Chun Zhu",
            "Demetri Terzopoulos",
            "Li Fei-Fei",
            "Jianfeng Gao"
        ],
        "date": "2023/09/18",
        "pdf": "http://arxiv.org/pdf/2309.09971.pdf",
        "abstract": "Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.",
        "code": "https://mindagent.github.io/",
        "category": [
            "Multi-Agent System",
            "Game Playing",
            "Benchmark&Evaluation&Framework"
        ],
        "url": "https://arxiv.org/abs/2309.09971"
    },
    "e8069ea17b87bc2381127ebd0b0e49cc": {
        "title": "CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update",
        "authors": [
            "Zhi Gao",
            "Yuntao Du",
            "Xintong Zhang",
            "Xiaojian Ma",
            "Wenjuan Han",
            "Song-Chun Zhu",
            "Qing Li"
        ],
        "date": "2023/12/18",
        "pdf": "http://arxiv.org/pdf/2312.10908.pdf",
        "abstract": "Leveraging large language models (LLMs) to integrate off-the-shelf tools (e.g., visual models and image processing functions) is a promising research direction to build powerful visual assistants for solving diverse visual tasks. However, the learning capability is rarely explored in existing methods, as they freeze the used tools after deployment, thereby limiting the generalization to new environments requiring specific knowledge. In this paper, we propose CLOVA, a Closed-LOop Visual Assistant to address this limitation, which encompasses inference, reflection, and learning phases in a closed-loop framework. During inference, LLMs generate programs and execute corresponding tools to accomplish given tasks. The reflection phase introduces a multimodal global-local reflection scheme to analyze whether and which tool needs to be updated based on environmental feedback. Lastly, the learning phase uses three flexible manners to collect training data in real-time and introduces a novel prompt tuning scheme to update the tools, enabling CLOVA to efficiently learn specific knowledge for new environments without human involvement. Experiments show that CLOVA outperforms tool-usage methods by 5% in visual question answering and multiple-image reasoning tasks, by 10% in knowledge tagging tasks, and by 20% in image editing tasks, highlighting the significance of the learning capability for general visual assistants.",
        "code": "https://clova-tool.github.io/",
        "category": [
            "Feedback&Reflection",
            "Tool Usage&Human-Agent Interaction"
        ],
        "url": "https://arxiv.org/abs/2312.10908"
    },
    "38db43d88f116a51413cbb3dbf6dde12": {
        "title": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models",
        "authors": [
            "Zihao Wang",
            "Shaofei Cai",
            "Anji Liu",
            "Yonggang Jin",
            "Jinbing Hou",
            "Bowei Zhang",
            "Haowei Lin",
            "Zhaofeng He",
            "Zilong Zheng",
            "Yaodong Yang",
            "Xiaojian Ma",
            "Yitao Liang"
        ],
        "date": "2023/11/10",
        "pdf": "http://arxiv.org/pdf/2311.05997.pdf",
        "abstract": "Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. These tasks range from short-horizon tasks, e.g., &#34;chopping trees&#34; to long-horizon tasks, e.g., &#34;obtaining a diamond pickaxe&#34;. JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly perfect performance. In the classic long-term task of $\\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. The project page is available at https://craftjarvis.org/JARVIS-1",
        "code": "https://github.com/CraftJarvis/JARVIS-1",
        "category": [
            "Memory Mechanism",
            "Game Playing"
        ],
        "url": "https://arxiv.org/abs/2311.05997"
    },
    "b1ce54ca409d3fedd20b7d6b04b4aaa8": {
        "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
        "authors": [
            "Zihao Wang",
            "Shaofei Cai",
            "Guanzhou Chen",
            "Anji Liu",
            "Xiaojian Ma",
            "Yitao Liang"
        ],
        "date": "2023/02/03",
        "pdf": "http://arxiv.org/pdf/2302.01560.pdf",
        "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose &#34;$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect&#34; ($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method&#39;s general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
        "code": "",
        "category": [
            "Feedback&Reflection",
            "Game Playing",
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2302.01560"
    },
    "eef6165c96965f04155ff5cc3d1b393e": {
        "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
        "authors": [
            "Lin Guan",
            "Karthik Valmeekam",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "date": "2023/05/24",
        "pdf": "http://arxiv.org/pdf/2305.14909.pdf",
        "abstract": "There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.",
        "code": "https://github.com/GuanSuns/LLMs-World-Models-for-Planning",
        "category": [
            "Planning"
        ],
        "url": "https://arxiv.org/abs/2305.14909"
    },
    "37c843ffe31783b0915c083bee24ed9d": {
        "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
        "authors": [
            "Filippos Christianos",
            "Georgios Papoudakis",
            "Matthieu Zimmer",
            "Thomas Coste",
            "Zhihao Wu",
            "Jingxuan Chen",
            "Khyati Khandelwal",
            "James Doran",
            "Xidong Feng",
            "Jiacheng Liu",
            "Zheng Xiong",
            "Yicheng Luo",
            "Jianye Hao",
            "Kun Shao",
            "Haitham Bou-Ammar",
            "Jun Wang"
        ],
        "date": "2023/12/22",
        "pdf": "http://arxiv.org/pdf/2312.14878",
        "abstract": "A key method for creating Artificial Intelligence (AI) agents is Reinforcement Learning (RL). However, constructing a standalone RL policy that maps perception to action directly encounters severe problems, chief among them being its lack of generality across multiple tasks and the need for a large amount of training data. The leading cause is that it cannot effectively integrate prior information into the perception-action cycle when devising the policy. Large language models (LLMs) emerged as a fundamental way to incorporate cross-domain knowledge into AI agents but lack crucial learning and adaptation toward specific decision problems. This paper presents a general framework model for integrating and learning structured reasoning into AI agents&#39; policies. Our methodology is motivated by the modularity found in the human brain. The framework utilises the construction of intrinsic and extrinsic functions to add previous understandings of reasoning structures. It also provides the adaptive ability to learn models inside every module or function, consistent with the modular structure of cognitive processes. We describe the framework in-depth and compare it with other AI pipelines and existing frameworks. The paper explores practical applications, covering experiments that show the effectiveness of our method. Our results indicate that AI agents perform and adapt far better when organised reasoning and prior knowledge are embedded. This opens the door to more resilient and general AI agent systems.",
        "code": "",
        "category": [
            "Agent Fine-tuning"
        ],
        "url": "https://arxiv.org/abs/2312.14878"
    }
}