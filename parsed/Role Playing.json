{
    "fefbb121e5e6daefd835e760d5f08117": {
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "authors": [
            "Joon Sung Park",
            "Joseph C. O&#39;Brien",
            "Carrie J. Cai",
            "Meredith Ringel Morris",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "date": "2023/04/07",
        "pdf": "https://arxiv.org/pdf/2304.03442",
        "code": "https://github.com/mkturkcan/generative-agents",
        "abstract": " Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents--computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent&#39;s experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine&#39;s\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture--observation, planning, and reflection--each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.\n"
    },
    "63dc16015de346991b87581ffc2f70a9": {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ],
        "date": "2023/04/19",
        "pdf": "https://arxiv.org/pdf/2304.09842",
        "abstract": " Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner.\n"
    },
    "27622956eb8adf45c1c26de87fedc9ff": {
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback",
        "authors": [
            "Nikhil Mehta",
            "Milagro Teruel",
            "Patricio Figueroa Sanz",
            "Xin Deng",
            "Ahmed Hassan Awadallah",
            "Julia Kiseleva"
        ],
        "date": "2023/04/21",
        "pdf": "https://arxiv.org/pdf/2304.10750",
        "abstract": " Many approaches to Natural Language Processing (NLP) tasks often treat them\nas single-step problems, where an agent receives an instruction, executes it,\nand is evaluated based on the final outcome. However, human language is\ninherently interactive, as evidenced by the back-and-forth nature of human\nconversations. In light of this, we posit that human-AI collaboration should\nalso be interactive, with humans monitoring the work of AI agents and providing\nfeedback that the agent can understand and utilize. Further, the AI agent\nshould be able to detect when it needs additional information and proactively\nask for help. Enabling this scenario would lead to more natural, efficient, and\nengaging human-AI collaborations. In this work, we explore these directions using the challenging task defined\nby the IGLU competition, an interactive grounded language understanding task in\na MineCraft-like world. We explore multiple types of help players can give to\nthe AI to guide it and analyze the impact of this help in AI behavior,\nresulting in performance improvements.\n"
    },
    "6346920757c54de4cfb53cbd57153400": {
        "title": "ChatLLM Network: More brains, More intelligence",
        "authors": [
            "Rui Hao",
            "Linmei Hu",
            "Weijian Qi",
            "Qingliu Wu",
            "Yirui Zhang",
            "Liqiang Nie"
        ],
        "date": "2023/04/24",
        "pdf": "https://arxiv.org/pdf/2304.12998",
        "abstract": " Dialogue-based language models mark a huge milestone in the field of\nartificial intelligence, by their impressive ability to interact with users, as\nwell as a series of challenging tasks prompted by customized instructions.\nHowever, the prevalent large-scale dialogue-based language models like ChatGPT\nstill have room for improvement, such as unstable responses to questions and\nthe inability to think cooperatively like humans. Considering the ability of\ndialogue-based language models in conversation and their inherent randomness in\nthinking, we propose ChatLLM network that allows multiple dialogue-based\nlanguage models to interact, provide feedback, and think together. We design\nthe network of ChatLLMs based on ChatGPT. Specifically, individual instances of\nChatGPT may possess distinct perspectives towards the same problem, and by\nconsolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM\nnetwork system can conduct decision-making more objectively and\ncomprehensively. In addition, a language-based feedback mechanism comparable to\nbackpropagation is devised to update the ChatGPTs within the network.\nExperiments on two datasets demonstrate that our network attains significant\nimprovements in problem-solving, leading to observable progress amongst each\nmember.\n"
    },
    "d4c1c4e1c46d89daf6890d5f71c62c63": {
        "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
        "authors": [
            "Jimmy Wei",
            "Kurt Shuster",
            "Arthur Szlam",
            "Jason Weston",
            "Jack Urbanek",
            "Mojtaba Komeili"
        ],
        "date": "2023/04/26",
        "pdf": "https://arxiv.org/pdf/2304.13835",
        "code": "https://github.com/facebookresearch/LIGHT",
        "abstract": " Current dialogue research primarily studies pairwise (two-party)\nconversations, and does not address the everyday setting where more than two\nspeakers converse together. In this work, we both collect and evaluate\nmulti-party conversations to study this more general case. We use the LIGHT\nenvironment to construct grounded conversations, where each participant has an\nassigned character to role-play. We thus evaluate the ability of language\nmodels to act as one or more characters in such conversations. Models require\ntwo skills that pairwise-trained models appear to lack: (1) being able to\ndecide when to talk; (2) producing coherent utterances grounded on multiple\ncharacters. We compare models trained on our new dataset to existing\npairwise-trained dialogue models, as well as large language models with\nfew-shot prompting. We find that our new dataset, MultiLIGHT, which we will\npublicly release, can help bring significant improvements in the group setting.\n"
    },
    "685611a174dfe4f335c66b3a972ad793": {
        "title": "The Role of Summarization in Generative Agents: A Preliminary Perspective",
        "authors": [
            "Xiachong Feng",
            "Xiaocheng Feng",
            "Bing Qin"
        ],
        "date": "2023/05/02",
        "pdf": "https://arxiv.org/pdf/2305.01253",
        "abstract": " Generative agents that simulate human society show tremendous potential for\nfurther research and practical applications. Specifically, the generative agent\narchitecture comprising several meticulously designed modules constitutes the\nmost critical component. To facilitate progress in this research, this report\npresents our integrated perspective on comprehending generative agents through\nsummarization, since we believe summarization is the most fundamental and\nindispensable capacity of generative agents manifested across diverse\nscenarios. We hope this report can provide insight into understanding the\nimportance of summarization capacity in generative agents and motivate future\nresearch.\n"
    },
    "98f7696b30c392052ea00d12d2d79918": {
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "authors": [
            "Jimmy Wu",
            "Rika Antonova",
            "Adam Kan",
            "Marion Lepert",
            "Andy Zeng",
            "Shuran Song",
            "Jeannette Bohg",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ],
        "date": "2023/05/09",
        "pdf": "https://arxiv.org/pdf/2305.05658",
        "code": "https://github.com/jimmyyhwu/tidybot",
        "homepage": "https://tidybot.cs.princeton.edu/",
        "abstract": " For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people&#39;s preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n"
    },
    "aff5e8a0a6be3cc3d84f669a5dc2cf7b": {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "Thomas L. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10601",
        "code": "https://github.com/ysymyth/tree-of-thought-llm",
        "abstract": " Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models&#39;\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/ysymyth/tree-of-thought-llm.\n"
    },
    "843b42ba9aa69a474fd90dd43404ee8a": {
        "title": "Role-Play with Large Language Models",
        "authors": [
            "Murray Shanahan",
            "Kyle McDonell",
            "Laria Reynolds"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16367",
        "abstract": " As dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.\n"
    },
    "2fe6d5bff84082b57a528b3f5b08131e": {
        "title": "Training Socially Aligned Language Models in Simulated Human Society",
        "authors": [
            "Ruibo Liu",
            "Ruixin Yang",
            "Chenyan Jia",
            "Ge Zhang",
            "Denny Zhou",
            "Andrew M. Dai",
            "Diyi Yang",
            "Soroush Vosoughi"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16960",
        "code": "https://github.com/agi-templar/Stable-Alignment",
        "abstract": " Social alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.\n"
    },
    "5ad397cd019ef8e7d6f8717d67b90261": {
        "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
        "authors": [
            "Bill Yuchen Lin",
            "Yicheng Fu",
            "Karina Yang",
            "Prithviraj Ammanabrolu",
            "Faeze Brahman",
            "Shiyu Huang",
            "Chandra Bhagavatula",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "date": "2023/05/27",
        "pdf": "https://arxiv.org/pdf/2305.17390",
        "code": "https://github.com/yuchenlin/swiftsage/",
        "homepage": "https://yuchenlin.xyz/swiftsage/",
        "abstract": " We introduce SwiftSage, a novel agent framework inspired by the dual-process\ntheory of human cognition, designed to excel in action planning for complex\ninteractive reasoning tasks. SwiftSage integrates the strengths of behavior\ncloning and prompting large language models (LLMs) to enhance task completion\nperformance. The framework comprises two primary modules: the Swift module,\nrepresenting fast and intuitive thinking, and the Sage module, emulating\ndeliberate thought processes. The Swift module is a small encoder-decoder LM\nfine-tuned on the oracle agent&#39;s action trajectories, while the Sage module\nemploys LLMs such as GPT-4 for subgoal planning and grounding. We develop a\nheuristic method to harmoniously integrate the two modules, resulting in a more\nefficient and robust problem-solving process. In 30 tasks from the ScienceWorld\nbenchmark, SwiftSage significantly outperforms other methods such as SayCan,\nReAct, and Reflexion, demonstrating its effectiveness in solving complex\nreal-world tasks.\n"
    },
    "445a34d666cdf8017df2d3b28f3e2bcf": {
        "title": "CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Scale Language Model Society",
        "authors": [
            "Guohao Li",
            "Hasan Abed Al Kader Hammoud",
            "Hani Itani",
            "Dmitrii Khizbullin",
            "Bernard Ghanem"
        ],
        "date": "2023/03/31",
        "pdf": "https://arxiv.org/pdf/2303.17760",
        "abstract": " The rapid advancement of conversational and chat-based language models has\nled to remarkable progress in complex task-solving. However, their success\nheavily relies on human input to guide the conversation, which can be\nchallenging and time-consuming. This paper explores the potential of building\nscalable techniques to facilitate autonomous cooperation among communicative\nagents and provide insight into their &#34;cognitive&#34; processes. To address the\nchallenges of achieving autonomous cooperation, we propose a novel\ncommunicative agent framework named role-playing. Our approach involves using\ninception prompting to guide chat agents toward task completion while\nmaintaining consistency with human intentions. We showcase how role-playing can\nbe used to generate conversational data for studying the behaviors and\ncapabilities of chat agents, providing a valuable resource for investigating\nconversational language models. Our contributions include introducing a novel\ncommunicative agent framework, offering a scalable approach for studying the\ncooperative behaviors and capabilities of multi-agent systems, and\nopen-sourcing our library to support research on communicative agents and\nbeyond. The GitHub repository of this project is made publicly available on:\nhttps://github.com/lightaime/camel.\n"
    },
    "6728afeb589c1328ba967436092eeb98": {
        "title": "Self-collaboration Code Generation via ChatGPT",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Zhi Jin",
            "Ge Li"
        ],
        "date": "2023/04/15",
        "pdf": "https://arxiv.org/pdf/2304.07590",
        "abstract": " Although Large Language Models (LLMs) have demonstrated remarkable\ncode-generation ability, they still struggle with complex tasks. In real-world\nsoftware development, humans usually tackle complex tasks through collaborative\nteamwork, a strategy that significantly controls development complexity and\nenhances software quality. Inspired by this, we present a self-collaboration\nframework for code generation employing LLMs, exemplified by ChatGPT.\nSpecifically, through role instructions, 1) Multiple LLMs act as distinct\n``experts&#39;&#39;, each responsible for a specific subtask within a complex task; 2)\nSpecify the way to collaborate and interact, so that different roles form a\nvirtual team to facilitate each other&#39;s work, ultimately the virtual team\naddresses code generation tasks collaboratively without the need for human\nintervention. To effectively organize and manage this virtual team, we\nincorporate software-development methodology into the framework. Thus, we\nassemble an elementary team consisting of three ChatGPT roles (i.e., analyst,\ncoder, and tester) responsible for software development&#39;s analysis, coding, and\ntesting stages. We conduct comprehensive experiments on various code-generation\nbenchmarks. Experimental results indicate that self-collaboration code\ngeneration relatively improves 29.9%-47.1% Pass@1 compared to direct code\ngeneration, achieving state-of-the-art performance and even surpassing GPT-4.\nMoreover, we showcase that self-collaboration could potentially enable LLMs to\nefficiently handle complex real-world tasks that are not readily solved by\ndirect code generation, as evidenced in case study.\n"
    },
    "3f38895b5dc2bb39a5857ec9a34da3ac": {
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Wenxiang Jiao",
            "Xing Wang",
            "Yan Wang",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi"
        ],
        "date": "2023/05/30",
        "pdf": "https://arxiv.org/pdf/2305.19118",
        "abstract": " Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n&#34;tit for tat&#34; and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of &#34;tit for tat&#34; state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Codes:\nhttps://github.com/Skytliang/Multi-Agents-Debate\n"
    }
}