{
    "a89d7d064ee50b5f5d994406024db2df": {
        "title": "AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems",
        "authors": [
            "Junjie Zhang",
            "Yupeng Hou",
            "Ruobing Xie",
            "Wenqi Sun",
            "Julian McAuley",
            "Wayne Xin Zhao",
            "Leyu Lin",
            "Ji-Rong Wen"
        ],
        "date": "2023/10/13",
        "pdf": "https://arxiv.org/pdf/2310.09233.pdf",
        "abstract": "Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations. To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents&#39; decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation."
    },
    "6bf24ac7d0056e0bc0240714642d96f8": {
        "title": "When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm",
        "authors": [
            "Lei Wang",
            "Jingsen Zhang",
            "Hao Yang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Zeyu Zhang",
            "Xu Chen",
            "Yankai Lin",
            "Ruihua Song",
            "Wayne Xin Zhao",
            "Jun Xu",
            "Zhicheng Dou",
            "Jun Wang",
            "Ji-Rong Wen"
        ],
        "date": "2023/06/05",
        "pdf": "https://arxiv.org/pdf/2306.02552.pdf",
        "code": "https://github.com/RUC-GSAI/YuLan-Rec",
        "abstract": "User behavior analysis is crucial in human-centered AI applications. In this field, the collection of sufficient and high-quality user behavior data has always been a fundamental yet challenging problem. An intuitive idea to address this problem is automatically simulating the user behaviors. However, due to the subjective and complex nature of human cognitive processes, reliably simulating the user behavior is difficult. Recently, large language models (LLM) have obtained remarkable successes, showing great potential to achieve human-like intelligence. We argue that these models present significant opportunities for reliable user simulation, and have the potential to revolutionize traditional study paradigms in user behavior analysis. In this paper, we take recommender system as an example to explore the potential of using LLM for user simulation. Specifically, we regard each user as an LLM-based autonomous agent, and let different agents freely communicate, behave and evolve in a virtual simulator called RecAgent. For comprehensively simulation, we not only consider the behaviors within the recommender system (\\emph{e.g.}, item browsing and clicking), but also accounts for external influential factors, such as, friend chatting and social advertisement. Our simulator contains at most 1000 agents, and each agent is composed of a profiling module, a memory module and an action module, enabling it to behave consistently, reasonably and reliably. In addition, to more flexibly operate our simulator, we also design two global functions including real-human playing and system intervention. To evaluate the effectiveness of our simulator, we conduct extensive experiments from both agent and system perspectives. In order to advance this direction, we have released our project at {https://github.com/RUC-GSAI/YuLan-Rec}."
    },
    "79e1c9fc13e42067576b1a2670c7134b": {
        "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
        "authors": [
            "Dingyao Yu",
            "Kaitao Song",
            "Peiling Lu",
            "Tianyu He",
            "Xu Tan",
            "Wei Ye",
            "Shikun Zhang",
            "Jiang Bian"
        ],
        "date": "2023/10/18",
        "pdf": "https://arxiv.org/pdf/2310.11954.pdf",
        "abstract": "AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience."
    },
    "3447891c35f4bf8f904ab23534e84b9e": {
        "title": "TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents",
        "authors": [
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Zhiwei Xu",
            "Tianpeng Bao",
            "Guoqing Du",
            "Shiwei Shi",
            "Hangyu Mao",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "date": "2023/08/07",
        "pdf": "https://arxiv.org/pdf/2308.03427.pdf",
        "abstract": "With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement."
    },
    "73579681b0fcf709b1b58ba2083317fa": {
        "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
        "authors": [
            "Chenliang Li",
            "Hehong Chen",
            "Ming Yan",
            "Weizhou Shen",
            "Haiyang Xu",
            "Zhikai Wu",
            "Zhicheng Zhang",
            "Wenmeng Zhou",
            "Yingda Chen",
            "Chen Cheng",
            "Hongzhu Shi",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "date": "2023/09/02",
        "pdf": "https://arxiv.org/pdf/2309.00986.pdf",
        "code": "https://github.com/modelscope/modelscope-agent",
        "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available."
    },
    "a555d236f43f11398588858b6003f747": {
        "title": "A Zero-Shot Language Agent for Computer Control with Structured Reflection",
        "authors": [
            "Tao Li",
            "Gang Li",
            "Zhiwei Deng",
            "Bryan Wang",
            "Yang Li"
        ],
        "date": "2023/10/12",
        "pdf": "https://arxiv.org/pdf/2310.08740.pdf",
        "abstract": "Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior best models, even though previous works had the advantages of accessing expert traces or additional screen information."
    },
    "2f63080abf12a0214d3f2cbe658b934c": {
        "title": "TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems",
        "authors": [
            "Yilun Kong",
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Tianpeng Bao",
            "Shiwei Shi",
            "Guoqing Du",
            "Xiaoru Hu",
            "Hangyu Mao",
            "Ziyue Li",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "date": "2023/11/19",
        "pdf": "https://arxiv.org/pdf/2311.11315.pdf",
        "abstract": "Large Language Models (LLMs) have demonstrated proficiency in addressing tasks that necessitate a combination of task planning and the usage of external tools that require a blend of task planning and the utilization of external tools, such as APIs. However, real-world complex systems present three prevalent challenges concerning task planning and tool usage: (1) The real system usually has a vast array of APIs, so it is impossible to feed the descriptions of all APIs to the prompt of LLMs as the token length is limited; (2) the real system is designed for handling complex tasks, and the base LLMs can hardly plan a correct sub-task order and API-calling order for such tasks; (3) Similar semantics and functionalities among APIs in real systems create challenges for both LLMs and even humans in distinguishing between them. In response, this paper introduces a comprehensive framework aimed at enhancing the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating within real-world systems. Our framework comprises three key components designed to address these challenges: (1) the API Retriever selects the most pertinent APIs for the user task among the extensive array available; (2) LLM Finetuner tunes a base LLM so that the finetuned LLM can be more capable for task planning and API calling; (3) the Demo Selector adaptively retrieves different demonstrations related to hard-to-distinguish APIs, which is further used for in-context learning to boost the final performance. We validate our methods using a real-world commercial system as well as an open-sourced academic dataset, and the outcomes clearly showcase the efficacy of each individual component as well as the integrated framework."
    },
    "32379c096b4ff85ab67a07fc51cbd301": {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Qingsong Lv",
            "Jiazheng Xu",
            "Wenmeng Yu",
            "Junhui Ji",
            "Yan Wang",
            "Zihan Wang",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "date": "2023/12/14",
        "pdf": "https://arxiv.org/pdf/2312.08914.pdf",
        "abstract": "People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at \\url{https://github.com/THUDM/CogVLM}."
    },
    "07b43c95e66ae36855f382f95cea8e41": {
        "title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System",
        "authors": [
            "Ryu Hirai",
            "Shinya Iizuka",
            "Haruhisa Iseno",
            "Ao Guo",
            "Jingjing Jiang",
            "Atsumoto Ohashi",
            "Ryuichiro Higashinaka"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13816.pdf",
        "abstract": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve the capability of dialogue robots, our team developed a system that could build common ground and take more natural turns based on user utterance texts. Our system generated queries for sightseeing spot searches using the common ground and engaged in dialogue while waiting for user comprehension."
    },
    "a268d9a93ef4c6dfe9e15a988ec88129": {
        "title": "AppAgent: Multimodal Agents as Smartphone Users",
        "authors": [
            "Chi Zhang",
            "Zhao Yang",
            "Jiaxuan Liu",
            "Yucheng Han",
            "Xin Chen",
            "Zebiao Huang",
            "Bin Fu",
            "Gang Yu"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13771.pdf",
        "code": "https://github.com/mnotgod96/AppAgent",
        "abstract": "Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent&#39;s functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent&#39;s proficiency in handling a diverse array of high-level tasks."
    },
    "039db49f2e19de35f5de8b42670347a3": {
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "authors": [
            "Boyuan Zheng",
            "Boyu Gou",
            "Jihyung Kil",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2024/01/03",
        "pdf": "http://arxiv.org/pdf/2401.01614.pdf",
        "abstract": "The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out not effective for web agents, and the best grounding strategy we develop in this paper leverages both the HTML text and visuals. Yet, there is still a substantial gap with oracle grounding, leaving ample room for further improvement."
    }
}