{
    "28a8719ce1adca46dae998bec22518f1": {
        "title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
        "authors": [
            "Chen Feng Tsai",
            "Xiaochen Zhou",
            "Sierra S. Liu",
            "Jing Li",
            "Mo Yu",
            "Hongyuan Mei"
        ],
        "date": "2023/04/06",
        "pdf": "https://arxiv.org/pdf/2304.02868",
        "abstract": " Large language models (LLMs) such as ChatGPT and GPT-4 have recently\ndemonstrated their remarkable abilities of communicating with human users. In\nthis technical report, we take an initiative to investigate their capacities of\nplaying text games, in which a player has to understand the environment and\nrespond to situations by having dialogues with the game world. Our experiments\nshow that ChatGPT performs competitively compared to all the existing systems\nbut still exhibits a low level of intelligence. Precisely, ChatGPT can not\nconstruct the world model by playing the game or even reading the game manual;\nit may fail to leverage the world knowledge that it already has; it cannot\ninfer the goal of each step as the game progresses. Our results open up new\nresearch questions at the intersection of artificial intelligence, machine\nlearning, and natural language processing.\n"
    },
    "26a0f2b5b4d2ff9c7cc8fad72d1a94ff": {
        "title": "Next Steps for Human-Centered Generative AI: A Technical Perspective",
        "authors": [
            "Xiang &#39;Anthony&#39; Chen",
            "Jeff Burke",
            "Ruofei Du",
            "Matthew K. Hong",
            "Jennifer Jacobs",
            "Philippe Laban",
            "Dingzeyu Li",
            "Nanyun Peng",
            "Karl D. D. Willis",
            "Chien-Sheng Wu",
            "Bolei Zhou"
        ],
        "date": "2023/06/27",
        "pdf": "https://arxiv.org/pdf/2306.15774",
        "abstract": " Through iterative, cross-disciplinary discussions, we define and propose\nnext-steps for Human-centered Generative AI (HGAI) from a technical\nperspective. We contribute a roadmap that lays out future directions of\nGenerative AI spanning three levels: Aligning with human values; Accommodating\nhumans&#39; expression of intents; and Augmenting humans&#39; abilities in a\ncollaborative workflow. This roadmap intends to draw interdisciplinary research\nteams to a comprehensive list of emergent ideas in HGAI, identifying their\ninterested topics while maintaining a coherent big picture of the future work\nlandscape.\n"
    },
    "75288ef81bd454aa2c0c646d5fdc4101": {
        "title": "A Survey on Large Language Model based Autonomous Agents",
        "authors": [
            "Lei Wang",
            "Chen Ma",
            "Xueyang Feng",
            "Zeyu Zhang",
            "Hao Yang",
            "Jingsen Zhang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Xu Chen",
            "Yankai Lin",
            "Wayne Xin Zhao",
            "Zhewei Wei",
            "Ji-Rong Wen"
        ],
        "date": "2023/08/22",
        "pdf": "https://arxiv.org/pdf/2308.11432",
        "code": "https://github.com/Paitesanshi/LLM-Agent-Survey",
        "abstract": " Autonomous agents have long been a prominent research topic in the academic\ncommunity. Previous research in this field often focuses on training agents\nwith limited knowledge within isolated environments, which diverges\nsignificantly from the human learning processes, and thus makes the agents hard\nto achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating autonomous agents based on LLMs. To harness\nthe full potential of LLMs, researchers have devised diverse agent\narchitectures tailored to different applications. In this paper, we present a\ncomprehensive survey of these studies, delivering a systematic review of the\nfield of autonomous agents from a holistic perspective. More specifically, our\nfocus lies in the construction of LLM-based agents, for which we propose a\nunified framework that encompasses a majority of the previous work.\nAdditionally, we provide a summary of the various applications of LLM-based AI\nagents in the domains of social science, natural science, and engineering.\nLastly, we discuss the commonly employed evaluation strategies for LLM-based AI\nagents. Based on the previous studies, we also present several challenges and\nfuture directions in this field. To keep track of this field and continuously\nupdate our survey, we maintain a repository for the related references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n"
    },
    "bb0bbf6c3da5efdcd8f7acdce436c1f1": {
        "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Xin Guo",
            "Wei He",
            "Yiwen Ding",
            "Boyang Hong",
            "Ming Zhang",
            "Junzhe Wang",
            "Senjie Jin",
            "Enyu Zhou",
            "Rui Zheng",
            "Xiaoran Fan",
            "Xiao Wang",
            "Limao Xiong",
            "Yuhao Zhou",
            "Weiran Wang",
            "Changhao Jiang",
            "Yicheng Zou",
            "Xiangyang Liu",
            "Zhangyue Yin",
            "Shihan Dou",
            "Rongxiang Weng",
            "Wensen Cheng",
            "Qi Zhang",
            "Wenjuan Qin",
            "Yongyan Zheng",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "date": "2023/09/14",
        "pdf": "https://arxiv.org/pdf/2309.07864",
        "code": "https://github.com/woooodyy/llm-agent-paper-list",
        "abstract": " For a long time, humanity has pursued artificial intelligence (AI) equivalent\nto or surpassing the human level, with AI agents considered a promising vehicle\nfor this pursuit. AI agents are artificial entities that sense their\nenvironment, make decisions, and take actions. Many efforts have been made to\ndevelop intelligent AI agents since the mid-20th century. However, these\nefforts have mainly focused on advancement in algorithms or training strategies\nto enhance specific capabilities or performance on particular tasks. Actually,\nwhat the community lacks is a sufficiently general and powerful model to serve\nas a starting point for designing AI agents that can adapt to diverse\nscenarios. Due to the versatile and remarkable capabilities they demonstrate,\nlarge language models (LLMs) are regarded as potential sparks for Artificial\nGeneral Intelligence (AGI), offering hope for building general AI agents. Many\nresearch efforts have leveraged LLMs as the foundation to build AI agents and\nhave achieved significant progress. We start by tracing the concept of agents\nfrom its philosophical origins to its development in AI, and explain why LLMs\nare suitable foundations for AI agents. Building upon this, we present a\nconceptual framework for LLM-based agents, comprising three main components:\nbrain, perception, and action, and the framework can be tailored to suit\ndifferent applications. Subsequently, we explore the extensive applications of\nLLM-based agents in three aspects: single-agent scenarios, multi-agent\nscenarios, and human-agent cooperation. Following this, we delve into agent\nsocieties, exploring the behavior and personality of LLM-based agents, the\nsocial phenomena that emerge when they form societies, and the insights they\noffer for human society. Finally, we discuss a range of key topics and open\nproblems within the field.\n"
    },
    "fa928d3d54acadb993f4b8fc3cb3acee": {
        "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
        "authors": [
            "Chen Gao",
            "Xiaochong Lan",
            "Nian Li",
            "Yuan Yuan",
            "Jingtao Ding",
            "Zhilun Zhou",
            "Fengli Xu",
            "Yong Li"
        ],
        "date": "2023/12/19",
        "pdf": "http://arxiv.org/pdf/2312.11970.pdf",
        "abstract": "Agent-based modeling and simulation has evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, examining their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions."
    },
    "c7716e44d8d1c20e6e1fa9922dfc75d7": {
        "title": "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents",
        "authors": [
            "Ke Yang",
            "Jiateng Liu",
            "John Wu",
            "Chaoqi Yang",
            "Yi R. Fung",
            "Sha Li",
            "Zixuan Huang",
            "Xu Cao",
            "Xingyao Wang",
            "Yiquan Wang",
            "Heng Ji",
            "Chengxiang Zhai"
        ],
        "date": "2024/01/01",
        "pdf": "http://arxiv.org/pdf/2401.00812.pdf",
        "abstract": "The prominent large language models (LLMs) of today differ from past language models not only in size, but also in the fact that they are trained on a combination of natural language and formal language (code). As a medium between humans and computers, code translates high-level goals into executable steps, featuring standard syntax, logical consistency, abstraction, and modularity. In this survey, we present an overview of the various benefits of integrating code into LLMs&#39; training data. Specifically, beyond enhancing LLMs in code generation, we observe that these unique properties of code help (i) unlock the reasoning ability of LLMs, enabling their applications to a range of more complex natural language tasks; (ii) steer LLMs to produce structured and precise intermediate steps, which can then be connected to external execution ends through function calls; and (iii) take advantage of code compilation and execution environment, which also provides diverse feedback for model improvement. In addition, we trace how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents (IAs) in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks. Finally, we present several key challenges and future directions of empowering LLMs with code."
    },
    "2c30a38090198bd31c3053691e42f5f1": {
        "title": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots",
        "authors": [
            "Richard Sutcliffe"
        ],
        "date": "2023/12/31",
        "pdf": "http://arxiv.org/pdf/2401.00609.pdf",
        "abstract": "We present a review of personality in neural conversational agents (CAs), also called chatbots. First, we define Personality, Persona, and Profile. We explain all personality schemes which have been used in CAs, and list models under the scheme(s) which they use. Second we describe 21 datasets which have been developed in recent CA personality research. Third, we define the methods used to embody personality in a CA, and review recent models using them. Fourth, we survey some relevant reviews on CAs, personality, and related topics. Finally, we draw conclusions and identify some research challenges for this important emerging field."
    }
}