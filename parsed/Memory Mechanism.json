{
    "d0e04ab2d37bcd9b9f9980cddd5901c6": {
        "title": "Emergent and Predictable Memorization in Large Language Models",
        "authors": [
            "Stella Biderman",
            "USVSN Sai Prashanth",
            "Lintang Sutawika",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Shivanshu Purohit",
            "Edward Raf"
        ],
        "date": "2023/04/21",
        "pdf": "https://arxiv.org/pdf/2304.11158",
        "abstract": " Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model&#39;s\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model&#39;s full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite, and find that intermediate checkpoints are better\npredictors of a model&#39;s memorization behavior than smaller fully-trained\nmodels. We additionally provide further novel discoveries on the distribution\nof memorization scores across models and data.\n"
    },
    "3670318a3140c7ef30bd05aba962ac17": {
        "title": "ChatLog: Recording and Analyzing ChatGPT Across Time",
        "authors": [
            "Shangqing Tu",
            "Chunyang Li",
            "Jifan Yu",
            "Xiaozhi Wang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2023/04/27",
        "pdf": "https://arxiv.org/pdf/2304.14106",
        "abstract": " While there are abundant researches about evaluating ChatGPT on natural\nlanguage understanding and generation tasks, few studies have investigated how\nChatGPT&#39;s behavior changes over time. In this paper, we collect a\ncoarse-to-fine temporal dataset called ChatLog, consisting of two parts that\nupdate monthly and daily: ChatLog-Monthly is a dataset of 38,730\nquestion-answer pairs collected every month including questions from both the\nreasoning and classification tasks. ChatLog-Daily, on the other hand, consists\nof ChatGPT&#39;s responses to 1000 identical questions for long-form generation\nevery day. We conduct comprehensive automatic and human evaluation to provide\nthe evidence for the existence of ChatGPT evolving patterns. We further analyze\nthe unchanged characteristics of ChatGPT over time by extracting its knowledge\nand linguistic features. We find some stable features to improve the robustness\nof a RoBERTa-based detector on new versions of ChatGPT. We will continuously\nmaintain our project at https://github.com/THU-KEG/ChatLog.\n"
    },
    "f859fcfade304101ad2d098deeee606f": {
        "title": "Learning to Reason and Memorize with Self-Notes",
        "authors": [
            "Jack Lanchantin",
            "Shubham Toshniwal",
            "Jason Weston",
            "Arthur Szlam",
            "Sainbayar Sukhbaatar"
        ],
        "date": "2023/05/01",
        "pdf": "https://arxiv.org/pdf/2305.00833",
        "abstract": " Large language models have been shown to struggle with limited context memory\nand multi-step reasoning. We propose a simple method for solving both of these\nproblems by allowing the model to take Self-Notes. Unlike recent scratchpad\napproaches, the model can deviate from the input context at any time to\nexplicitly think. This allows the model to recall information and perform\nreasoning on the fly as it reads the context, thus extending its memory and\nenabling multi-step reasoning. Our experiments on multiple tasks demonstrate\nthat our method can successfully generalize to longer and more complicated\ninstances from their training setup by taking Self-Notes at inference time.\n"
    },
    "c63eb48acb4856602198c3e3b36201fc": {
        "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
        "authors": [
            "Wanjun Zhong",
            "Lianghong Guo",
            "Qiqi Gao",
            "He Ye",
            "Yanlin Wang"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10250",
        "abstract": " Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.\n"
    },
    "445f517ae67cddfd2bd81c5e1921367c": {
        "title": "Monotonic Location Attention for Length Generalization",
        "authors": [
            "Jishnu Ray Chowdhury",
            "Cornelia Caragea"
        ],
        "date": "2023/05/31",
        "pdf": "https://arxiv.org/pdf/2305.20019",
        "abstract": " We explore different ways to utilize position-based cross-attention in\nseq2seq networks to enable length generalization in algorithmic tasks. We show\nthat a simple approach of interpolating the original and reversed encoded\nrepresentations combined with relative attention allows near-perfect length\ngeneralization for both forward and reverse lookup tasks or copy tasks that had\nbeen generally hard to tackle. We also devise harder diagnostic tasks where the\nrelative distance of the ideal attention position varies with timestep. In such\nsettings, the simple interpolation trick with relative attention is not\nsufficient. We introduce novel variants of location attention building on top\nof Dubois et al. (2020) to address the new diagnostic tasks. We also show the\nbenefits of our approaches for length generalization in SCAN (Lake &amp; Baroni,\n2018) and CFQ (Keysers et al., 2020). Our code is available on GitHub.\n"
    },
    "4d80a78327d3119427f0a965743474bc": {
        "title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
        "authors": [
            "Anian Ruoss",
            "Grégoire Delétang",
            "Tim Genewein",
            "Jordi Grau-Moya",
            "Róbert Csordás",
            "Mehdi Bennani",
            "Shane Legg",
            "Joel Veness"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16843",
        "abstract": " Transformers have impressive generalization capabilities on tasks with a\nfixed context length. However, they fail to generalize to sequences of\narbitrary length, even for seemingly simple tasks such as duplicating a string.\nMoreover, simply training on longer sequences is inefficient due to the\nquadratic computation complexity of the global attention mechanism. In this\nwork, we demonstrate that this failure mode is linked to positional encodings\nbeing out-of-distribution for longer sequences (even for relative encodings)\nand introduce a novel family of positional encodings that can overcome this\nproblem. Concretely, our randomized positional encoding scheme simulates the\npositions of longer sequences and randomly selects an ordered subset to fit the\nsequence&#39;s length. Our large-scale empirical evaluation of 6000 models across\n15 algorithmic reasoning tasks shows that our method allows Transformers to\ngeneralize to sequences of unseen length (increasing test accuracy by 12.0% on\naverage).\n"
    },
    "07ac78ab6ef07caa1f81f53f497849a2": {
        "title": "CoLT5: Faster Long-Range Transformers with Conditional Computation",
        "authors": [
            "Joshua Ainslie",
            "Tao Lei",
            "Michiel de Jong",
            "Santiago Ontañón",
            "Siddhartha Brahma",
            "Yury Zemlyanskiy",
            "David Uthus",
            "Mandy Guo",
            "James Lee-Thorp",
            "Yi Tay",
            "Yun-Hsuan Sung",
            "Sumit Sanghai"
        ],
        "date": "2023/03/17",
        "pdf": "https://arxiv.org/pdf/2303.09752",
        "abstract": " Many natural language processing tasks benefit from long inputs, but\nprocessing long documents with Transformers is expensive -- not only due to\nquadratic attention complexity but also from applying feedforward and\nprojection layers to every token. However, not all tokens are equally\nimportant, especially for longer documents. We propose CoLT5, a long-input\nTransformer model that builds on this intuition by employing conditional\ncomputation, devoting more resources to important tokens in both feedforward\nand attention layers. We show that CoLT5 achieves stronger performance than\nLongT5 with much faster training and inference, achieving SOTA on the\nlong-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably\nmake use of extremely long inputs, showing strong gains up to 64k input length.\n"
    },
    "87d1ff46ce8a6a6881737c6bb6077623": {
        "title": "Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System",
        "authors": [
            "Xinnian Liang",
            "Bing Wang",
            "Hui Huang",
            "Shuangzhi Wu",
            "Peihao Wu",
            "Lu Lu",
            "Zejun Ma",
            "Zhoujun Li"
        ],
        "date": "2023/04/26",
        "pdf": "https://arxiv.org/pdf/2304.13343",
        "abstract": " Large-scale Language Models (LLMs) are constrained by their inability to\nprocess lengthy inputs. To address this limitation, we propose the\nSelf-Controlled Memory (SCM) system to unleash infinite-length input capacity\nfor large-scale language models. Our SCM system is composed of three key\nmodules: the language model agent, the memory stream, and the memory\ncontroller. The language model agent iteratively processes ultra-long inputs\nand stores all historical information in the memory stream. The memory\ncontroller provides the agent with both long-term memory (archived memory) and\nshort-term memory (flash memory) to generate precise and coherent responses.\nThe controller determines which memories from archived memory should be\nactivated and how to incorporate them into the model input. Our SCM system can\nbe integrated with any LLMs to enable them to process ultra-long texts without\nany modification or fine-tuning. Experimental results show that our SCM system\nenables LLMs, which are not optimized for multi-turn dialogue, to achieve\nmulti-turn dialogue capabilities that are comparable to ChatGPT, and to\noutperform ChatGPT in scenarios involving ultra-long document summarization or\nlong-term conversations. Additionally, we will supply a test set, which covers\ncommon long-text input scenarios, for evaluating the abilities of LLMs in\nprocessing long documents.~\\footnote{Working in\nprogress.}\\footnote{\\url{https://github.com/wbbeyourself/SCM4LLMs}}\n"
    },
    "820150c149f38e2bafe056cd3d4cb9d3": {
        "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
        "authors": [
            "Amanda Bertsch",
            "Uri Alon",
            "Graham Neubig",
            "Matthew R. Gormley"
        ],
        "date": "2023/05/02",
        "pdf": "https://arxiv.org/pdf/2305.01625",
        "abstract": " Since the proposal of transformers, these models have been limited to bounded\ninput lengths, because of their need to attend to every token in the input. In\nthis work, we propose Unlimiformer: a general approach that wraps any existing\npretrained encoder-decoder transformer, and offloads the cross-attention\ncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNN\ndistances are the attention dot-product scores. This kNN index can be kept on\neither the GPU or CPU memory and queried in sub-linear time; this way, we can\nindex practically unlimited input sequences, while every attention head in\nevery decoder layer retrieves its top-k keys, instead of attending to every\nkey. We evaluate Unlimiformer on several long-document and book-summarization\nbenchmarks, showing that it can process even 500k token-long inputs from the\nBookSum dataset, without any input truncation at test time. We demonstrate that\nUnlimiformer improves pretrained models such as BART and Longformer by\nextending them to unlimited inputs without additional learned weights and\nwithout modifying their code. We make our code and models publicly available at\nhttps://github.com/abertsch72/unlimiformer .\n"
    },
    "fb989b6304075754f68b99032e82ddc8": {
        "title": "Small Models are Valuable Plug-ins for Large Language Models",
        "authors": [
            "Canwen Xu",
            "Yichong Xu",
            "Shuohang Wang",
            "Yang Liu",
            "Chenguang Zhu",
            "Julian McAuley"
        ],
        "date": "2023/05/15",
        "pdf": "https://arxiv.org/pdf/2305.08848",
        "abstract": " Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their\nweights are often publicly unavailable and their immense sizes make the models\ndifficult to be tuned with common hardware. As a result, effectively tuning\nthese models with large-scale supervised data can be challenging. As an\nalternative, In-Context Learning (ICL) can only use a small number of\nsupervised examples due to context length limits. In this paper, we propose\nSuper In-Context Learning (SuperICL) which allows black-box LLMs to work with\nlocally fine-tuned smaller models, resulting in superior performance on\nsupervised tasks. Our experiments demonstrate that SuperICL can improve\nperformance beyond state-of-the-art fine-tuned models while addressing the\ninstability problem of in-context learning. Furthermore, SuperICL can enhance\nthe capabilities of smaller models, such as multilinguality and\ninterpretability.\n"
    },
    "b20784f97a6a9622b19766fb4d07d924": {
        "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
        "authors": [
            "Shibo Hao",
            "Tianyang Liu",
            "Zhen Wang",
            "Zhiting Hu"
        ],
        "date": "2023/05/19",
        "pdf": "https://arxiv.org/pdf/2305.11554",
        "abstract": " Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.\n"
    },
    "2503cb91a58cd6d74bc5c8ddd9c13b64": {
        "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Peng Cui",
            "Tiannan Wang",
            "Zhenxin Xiao",
            "Yifan Hou",
            "Ryan Cotterell",
            "Mrinmaya Sachan"
        ],
        "date": "2023/05/22",
        "pdf": "https://arxiv.org/pdf/2305.13304",
        "abstract": " The fixed-size context of Transformer makes GPT models incapable of\ngenerating arbitrarily long text. In this paper, we introduce RecurrentGPT, a\nlanguage-based simulacrum of the recurrence mechanism in RNNs. RecurrentGPT is\nbuilt upon a large language model (LLM) such as ChatGPT and uses natural\nlanguage to simulate the Long Short-Term Memory mechanism in an LSTM. At each\ntimestep, RecurrentGPT generates a paragraph of text and updates its\nlanguage-based long-short term memory stored on the hard drive and the prompt,\nrespectively. This recurrence mechanism enables RecurrentGPT to generate texts\nof arbitrary length without forgetting. Since human users can easily observe\nand edit the natural language memories, RecurrentGPT is interpretable and\nenables interactive generation of long text. RecurrentGPT is an initial step\ntowards next-generation computer-assisted writing systems beyond local editing\nsuggestions. In addition to producing AI-generated content (AIGC), we also\ndemonstrate the possibility of using RecurrentGPT as an interactive fiction\nthat directly interacts with consumers. We call this usage of generative models\nby ``AI As Contents&#39;&#39; (AIAC), which we believe is the next form of conventional\nAIGC. We further demonstrate the possibility of using RecurrentGPT to create\npersonalized interactive fiction that directly interacts with readers instead\nof interacting with writers. More broadly, RecurrentGPT demonstrates the\nutility of borrowing ideas from popular model designs in cognitive science and\ndeep learning for prompting LLMs. Our code is available at\nhttps://github.com/aiwaves-cn/RecurrentGPT and an online demo is available at\nhttps://www.aiwaves.org/recurrentgpt.\n"
    },
    "388ef6a1b09b07108327b064a92d2da2": {
        "title": "Revisiting Parallel Context Windows: A Frustratingly Simple Alternative and Chain-of-Thought Deterioration",
        "authors": [
            "Kejuan Yang",
            "Xiao Liu",
            "Kaiwen Men",
            "Aohan Zeng",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/05/24",
        "pdf": "https://arxiv.org/pdf/2305.15262",
        "abstract": " We identify two crucial limitations in the evaluation of recent\nparallel-integrated method Parallel Context Windows (PCW), which extends the\nmaximum context lengths of language models, e.g., 2048 for LLaMA, by harnessing\nwindow-wise attention and positional embedding techniques. We first show that a\nsimple yet strong baseline, weighted sum ensemble, is missing for the\nin-context few-shot classification. Moreover, on more challenging\nChain-of-Thought (CoT) reasoning (e.g., HotpotQA), PCW would present unexpected\ndeterioration regarding question miscomprehension and false inference. Based on\nour findings, we suggest that the existing PCW design may not guarantee\nsufficient improvement and practicality in handling lengthy documents in\nreal-world applications. More community efforts on enabling language models&#39;\nlong context understanding ability should be paid.\n"
    },
    "3fa3b1b5c98f30923e7924f7ff3fd499": {
        "title": "Landmark Attention: Random-Access Infinite Context Length for Transformers",
        "authors": [
            "Amirkeivan Mohtashami",
            "Martin Jaggi"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16300",
        "abstract": " While transformers have shown remarkable success in natural language\nprocessing, their attention mechanism&#39;s large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model&#39;s attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system&#39;s memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity up to 32k tokens, allowing for\ninference at the context lengths of GPT-4.\n"
    },
    "84c2e2e6ea4dc01094a9e1794f7560da": {
        "title": "Adapting Language Models to Compress Contexts",
        "authors": [
            "Alexis Chevalier",
            "Alexander Wettig",
            "Anirudh Ajith",
            "Danqi Chen"
        ],
        "date": "2023/05/24",
        "pdf": "https://arxiv.org/pdf/2305.14788",
        "abstract": " Transformer-based language models (LMs) are powerful and widely-applicable\ntools, but their usefulness is constrained by a finite context window and the\nexpensive computational cost of processing long text documents. We propose to\nadapt pre-trained LMs into AutoCompressors. These models are capable of\ncompressing long contexts into compact summary vectors, which are then\naccessible to the model as soft prompts. Summary vectors are trained with an\nunsupervised objective, whereby long documents are processed in segments and\nsummary vectors from all previous segments are used in language modeling. We\nfine-tune OPT models on sequences of up to 30,720 tokens and show that\nAutoCompressors can utilize long contexts to improve perplexity. We evaluate\nAutoCompressors on in-context learning by compressing task demonstrations. We\nfind that summary vectors are good substitutes for plain-text demonstrations,\nincreasing accuracy while reducing inference cost. Finally, we explore the\nbenefits of pre-computing summary vectors for large corpora by applying summary\nvectors to retrieval-augmented language modeling. Overall, AutoCompressors\nemerge as a simple and inexpensive solution for extending the context window of\nLMs while speeding up inference over long contexts.\n"
    },
    "6a6f1235f7ec4b77cf44a2ecefe0220f": {
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
        "authors": [
            "Ali Modarressi",
            "Ayyoob Imani",
            "Mohsen Fayyaz",
            "Hinrich Schütze"
        ],
        "date": "2023/05/23",
        "pdf": "https://arxiv.org/pdf/2305.14322",
        "abstract": " Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.\n"
    },
    "8c6217168c689b7097c12b1d55262252": {
        "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
        "authors": [
            "Chenxu Hu",
            "Jie Fu",
            "Chenzhuang Du",
            "Simian Luo",
            "Junbo Zhao",
            "Hang Zhao"
        ],
        "date": "2023/06/06",
        "pdf": "https://arxiv.org/pdf/2306.03901",
        "abstract": " Large language models (LLMs) with memory are computationally universal.\nHowever, mainstream LLMs are not taking full advantage of memory, and the\ndesigns are heavily influenced by biological brains. Due to their approximate\nnature and proneness to the accumulation of errors, conventional neural memory\nmechanisms cannot support LLMs to simulate complex reasoning. In this paper, we\nseek inspiration from modern computer architectures to augment LLMs with\nsymbolic memory for complex multi-hop reasoning. Such a symbolic memory\nframework is instantiated as an LLM and a set of SQL databases, where the LLM\ngenerates SQL instructions to manipulate the SQL databases. We validate the\neffectiveness of the proposed memory framework on a synthetic dataset requiring\ncomplex reasoning. The project website is available at\nhttps://chatdatabase.github.io/ .\n"
    }
}