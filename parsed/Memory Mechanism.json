{
    "d0e04ab2d37bcd9b9f9980cddd5901c6": {
        "title": "Emergent and Predictable Memorization in Large Language Models",
        "authors": [
            "Stella Biderman",
            "USVSN Sai Prashanth",
            "Lintang Sutawika",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Shivanshu Purohit",
            "Edward Raf"
        ],
        "date": "2023/04/21",
        "pdf": "https://arxiv.org/pdf/2304.11158",
        "abstract": " Memorization, or the tendency of large language models (LLMs) to output\nentire sequences from their training data verbatim, is a key concern for safely\ndeploying language models. In particular, it is vital to minimize a model&#39;s\nmemorization of sensitive datapoints such as those containing personal\nidentifiable information (PII). The prevalence of such undesirable memorization\ncan pose issues for model trainers, and may even require discarding an\notherwise functional model. We therefore seek to predict which sequences will\nbe memorized before a large model&#39;s full train-time by extrapolating the\nmemorization behavior of lower-compute trial runs. We measure memorization of\nthe Pythia model suite, and find that intermediate checkpoints are better\npredictors of a model&#39;s memorization behavior than smaller fully-trained\nmodels. We additionally provide further novel discoveries on the distribution\nof memorization scores across models and data.\n"
    },
    "3670318a3140c7ef30bd05aba962ac17": {
        "title": "ChatLog: Recording and Analyzing ChatGPT Across Time",
        "authors": [
            "Shangqing Tu",
            "Chunyang Li",
            "Jifan Yu",
            "Xiaozhi Wang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2023/04/27",
        "pdf": "https://arxiv.org/pdf/2304.14106",
        "abstract": " While there are abundant researches about evaluating ChatGPT on natural\nlanguage understanding and generation tasks, few studies have investigated how\nChatGPT&#39;s behavior changes over time. In this paper, we collect a\ncoarse-to-fine temporal dataset called ChatLog, consisting of two parts that\nupdate monthly and daily: ChatLog-Monthly is a dataset of 38,730\nquestion-answer pairs collected every month including questions from both the\nreasoning and classification tasks. ChatLog-Daily, on the other hand, consists\nof ChatGPT&#39;s responses to 1000 identical questions for long-form generation\nevery day. We conduct comprehensive automatic and human evaluation to provide\nthe evidence for the existence of ChatGPT evolving patterns. We further analyze\nthe unchanged characteristics of ChatGPT over time by extracting its knowledge\nand linguistic features. We find some stable features to improve the robustness\nof a RoBERTa-based detector on new versions of ChatGPT. We will continuously\nmaintain our project at https://github.com/THU-KEG/ChatLog.\n"
    },
    "f859fcfade304101ad2d098deeee606f": {
        "title": "Learning to Reason and Memorize with Self-Notes",
        "authors": [
            "Jack Lanchantin",
            "Shubham Toshniwal",
            "Jason Weston",
            "Arthur Szlam",
            "Sainbayar Sukhbaatar"
        ],
        "date": "2023/05/01",
        "pdf": "https://arxiv.org/pdf/2305.00833",
        "abstract": " Large language models have been shown to struggle with limited context memory\nand multi-step reasoning. We propose a simple method for solving both of these\nproblems by allowing the model to take Self-Notes. Unlike recent scratchpad\napproaches, the model can deviate from the input context at any time to\nexplicitly think. This allows the model to recall information and perform\nreasoning on the fly as it reads the context, thus extending its memory and\nenabling multi-step reasoning. Our experiments on multiple tasks demonstrate\nthat our method can successfully generalize to longer and more complicated\ninstances from their training setup by taking Self-Notes at inference time.\n"
    },
    "c63eb48acb4856602198c3e3b36201fc": {
        "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
        "authors": [
            "Wanjun Zhong",
            "Lianghong Guo",
            "Qiqi Gao",
            "He Ye",
            "Yanlin Wang"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10250",
        "abstract": " Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.\n"
    },
    "6a6f1235f7ec4b77cf44a2ecefe0220f": {
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
        "authors": [
            "Ali Modarressi",
            "Ayyoob Imani",
            "Mohsen Fayyaz",
            "Hinrich Sch√ºtze"
        ],
        "date": "2023/05/23",
        "pdf": "https://arxiv.org/pdf/2305.14322",
        "abstract": " Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.\n"
    }
}