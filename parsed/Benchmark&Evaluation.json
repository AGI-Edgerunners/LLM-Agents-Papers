{
    "c8c04c2c8e3e947e8cdee88e9ce28e21": {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "authors": [
            "Ehsan Kamalloo",
            "Aref Jafari",
            "Xinyu Zhang",
            "Nandan Thakur",
            "Jimmy Lin"
        ],
        "date": "2023/07/31",
        "pdf": "https://arxiv.org/pdf/2307.16883",
        "code": "https://github.com/project-miracl/hagrid",
        "abstract": " The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.\n"
    },
    "a46f1651f62db660d022c495bbab01a4": {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "authors": [
            "Xiao Liu",
            "Hao Yu",
            "Hanchen Zhang",
            "Yifan Xu",
            "Xuanyu Lei",
            "Hanyu Lai",
            "Yu Gu",
            "Hangliang Ding",
            "Kaiwen Men",
            "Kejuan Yang",
            "Shudan Zhang",
            "Xiang Deng",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chenhui Zhang",
            "Sheng Shen",
            "Tianjun Zhang",
            "Yu Su",
            "Huan Sun",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/08/07",
        "pdf": "https://arxiv.org/pdf/2308.03688",
        "code": "https://github.com/THUDM/AgentBench",
        "abstract": " Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent&#39;s reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 25 LLMs (including APIs\nand open-sourced models) shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and open-sourced competitors. It also\nserves as a component of an ongoing project with wider coverage and deeper\nconsideration towards systematic LLM evaluation. Datasets, environments, and an\nintegrated evaluation package for AgentBench are released at\nhttps://github.com/THUDM/AgentBench\n"
    }
}