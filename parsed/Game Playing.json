{
    "12e797c33d37e99a30aa9bd979aa2b31": {
        "title": "Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks",
        "authors": [
            "Haoqi Yuan",
            "Chi Zhang",
            "Hongcheng Wang",
            "Feiyang Xie",
            "Penglin Cai",
            "Hao Dong",
            "Zongqing Lu"
        ],
        "date": "2023/03/29",
        "pdf": "https://arxiv.org/pdf/2303.16563",
        "code": "https://sites.google.com/view/plan4mc",
        "abstract": " We study building a multi-task agent in Minecraft. Without human\ndemonstrations, solving long-horizon tasks in this open-ended environment with\nreinforcement learning (RL) is extremely sample inefficient. To tackle the\nchallenge, we decompose solving Minecraft tasks into learning basic skills and\nplanning over the skills. We propose three types of fine-grained basic skills\nin Minecraft, and use RL with intrinsic rewards to accomplish basic skills with\nhigh success rates. For skill planning, we use Large Language Models to find\nthe relationships between skills and build a skill graph in advance. When the\nagent is solving a task, our skill search algorithm walks on the skill graph\nand generates the proper skill plans for the agent. In experiments, our method\naccomplishes 24 diverse Minecraft tasks, where many tasks require sequentially\nexecuting for more than 10 skills. Our method outperforms baselines in most\ntasks by a large margin. The project&#39;s website and code can be found at\nhttps://sites.google.com/view/plan4mc.\n"
    },
    "4977a28eb34673b1a874b008ab45aa6a": {
        "title": "Knowledge-enhanced Agents for Interactive Text Games",
        "authors": [
            "Prateek Chhikara",
            "Jiarui Zhang",
            "Filip Ilievski",
            "Jonathan Francis",
            "Kaixin Ma"
        ],
        "date": "2023/05/08",
        "pdf": "https://arxiv.org/pdf/2305.05091",
        "abstract": " Communication via natural language is a crucial aspect of intelligence, and\nit requires computational models to learn and reason about world concepts, with\nvarying levels of supervision. While there has been significant progress made\non fully-supervised non-interactive tasks, such as question-answering and\nprocedural text understanding, much of the community has turned to various\nsequential interactive tasks, as in semi-Markov text-based games, which have\nrevealed limitations of existing approaches in terms of coherence, contextual\nawareness, and their ability to learn effectively from the environment. In this\npaper, we propose a framework for enabling improved functional grounding of\nagents in text-based games. Specifically, we consider two forms of domain\nknowledge that we inject into learning-based agents: memory of previous correct\nactions and affordances of relevant objects in the environment. Our framework\nsupports three representative model classes: `pure&#39; reinforcement learning (RL)\nagents, RL agents enhanced with knowledge graphs, and agents equipped with\nlanguage models. Furthermore, we devise multiple injection strategies for the\nabove domain knowledge types and agent architectures, including injection via\nknowledge graphs and augmentation of the existing input encoding strategies. We\nperform all experiments on the ScienceWorld text-based game environment, to\nillustrate the performance of various model configurations in challenging\nscience-related instruction-following tasks. Our findings provide crucial\ninsights on the development of effective natural language processing systems\nfor interactive contexts.\n"
    },
    "d0aa2508a83304e8baf09ffdf76be63c": {
        "title": "Playing repeated games with Large Language Models",
        "authors": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16867",
        "abstract": " Large Language Models (LLMs) are transforming society and permeating into\ndiverse applications. As a result, LLMs will frequently interact with us and\nother agents. It is, therefore, of great societal value to understand how LLMs\nbehave in interactive social settings. Here, we propose to use behavioral game\ntheory to study LLM&#39;s cooperation and coordination behavior. To do so, we let\ndifferent LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with\neach other and with other, human-like strategies. Our results show that LLMs\ngenerally perform well in such tasks and also uncover persistent behavioral\nsignatures. In a large set of two players-two strategies games, we find that\nLLMs are particularly good at games where valuing their own self-interest pays\noff, like the iterated Prisoner&#39;s Dilemma family. However, they behave\nsub-optimally in games that require coordination. We, therefore, further focus\non two games from these distinct families. In the canonical iterated Prisoner&#39;s\nDilemma, we find that GPT-4 acts particularly unforgivingly, always defecting\nafter another agent has defected only once. In the Battle of the Sexes, we find\nthat GPT-4 cannot match the behavior of the simple convention to alternate\nbetween options. We verify that these behavioral signatures are stable across\nrobustness checks. Finally, we show how GPT-4&#39;s behavior can be modified by\nproviding further information about the other player as well as by asking it to\npredict the other player&#39;s actions before making a choice. These results enrich\nour understanding of LLM&#39;s social behavior and pave the way for a behavioral\ngame theory for machines.\n"
    },
    "16ade9646e937f57019f60d7eecb4a2d": {
        "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Yixin Cao",
            "Ting Liu",
            "Bing Qin"
        ],
        "date": "2023/05/19",
        "pdf": "https://arxiv.org/pdf/2305.11595",
        "abstract": " Large Language Models (LLMs) have demonstrated human-like intelligence and\nare widely used in various applications. However, LLMs still exhibit various\nkinds of inconsistency problems. Existing works mainly focus on the\ninconsistency issues within a single LLM, while we investigate the\ninter-consistency among multiple LLMs, which is critical for collaborating to\nsolve a complex task. To examine whether LLMs can collaborate to ultimately\nachieve a consensus for the shared goal and whether LLMs easily change their\nviewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct\na three-stage debate aligned with real-world scenarios: fair debate, mismatched\ndebate, and roundtable debate. Through extensive experiments on the commonsense\nreasoning task, LLMs not only become more inter-consistent but also achieve\nhigher performance. Moreover, we observe that stronger LLMs tend to dominate\nthe debates by adhering to their perspectives, while weaker ones are more\nlikely to change viewpoints. Additionally, we highlight the importance of a\ncompetent judge, such as GPT-4, to draw more proper conclusions. Our work\ncontributes to understanding the inter-consistency among LLMs and lays the\nfoundation for the development of future collaboration methods.\n"
    },
    "9746aebf3641e1f07a29559fa01a166b": {
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Tushar Khot",
            "Mirella Lapata"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10142",
        "abstract": " We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and\ncriticizing. We are interested in this question because if LLMs were able to\nimprove each other, it would imply the possibility of creating strong AI agents\nwith minimal human intervention. We ask two LLMs to negotiate with each other,\nplaying the roles of a buyer and a seller, respectively. They aim to reach a\ndeal with the buyer targeting a lower price and the seller a higher one. A\nthird language model, playing the critic, provides feedback to a player to\nimprove the player&#39;s negotiation strategies. We let the two agents play\nmultiple rounds, using previous negotiation history and AI feedback as\nin-context demonstrations to improve the model&#39;s negotiation strategy\niteratively. We use different LLMs (GPT and Claude) for different roles and use\nthe deal price as the evaluation metric. Our experiments reveal multiple\nintriguing findings: (1) Only a subset of the language models we consider can\nself-play and improve the deal price from AI feedback, weaker models either do\nnot understand the game&#39;s rules or cannot incorporate AI feedback for further\nimprovement. (2) Models&#39; abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to\nimprove as the buyer than as the seller. (3) When unrolling the game to\nmultiple rounds, stronger agents can consistently improve their performance by\nmeaningfully using previous experiences and iterative AI feedback, yet have a\nhigher risk of breaking the deal. We hope our work provides insightful initial\nexplorations of having models autonomously improve each other with game playing\nand AI feedback.\n"
    },
    "4ad0c8c7e6154544a3aee3d3680d9765": {
        "title": "Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models",
        "authors": [
            "Jun Inukai",
            "Tadahiro Taniguchi",
            "Akira Taniguchi",
            "Yoshinobu Hagiwara"
        ],
        "date": "2023/05/31",
        "pdf": "https://arxiv.org/pdf/2305.19761",
        "abstract": " In the studies on symbol emergence and emergent communication in a population\nof agents, a computational model was employed in which agents participate in\nvarious language games. Among these, the Metropolis-Hastings naming game (MHNG)\npossesses a notable mathematical property: symbol emergence through MHNG is\nproven to be a decentralized Bayesian inference of representations shared by\nthe agents. However, the previously proposed MHNG is limited to a two-agent\nscenario. This paper extends MHNG to an N-agent scenario. The main\ncontributions of this paper are twofold: (1) we propose the recursive\nMetropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and\ndemonstrate that RMHNG is an approximate Bayesian inference method for the\nposterior distribution over a latent variable shared by agents, similar to\nMHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and\nreal image data, enabling multiple agents to develop and share a symbol system.\nFurthermore, we introduce two types of approximations -- one-sample and\nlimited-length -- to reduce computational complexity while maintaining the\nability to explain communication in a population of agents. The experimental\nfindings showcased the efficacy of RMHNG as a decentralized Bayesian inference\nfor approximating the posterior distribution concerning latent variables, which\nare jointly shared among agents, akin to MHNG. Moreover, the utilization of\nRMHNG elucidated the agents&#39; capacity to exchange symbols. Furthermore, the\nstudy discovered that even the computationally simplified version of RMHNG\ncould enable symbols to emerge among the agents.\n"
    },
    "7faa5c7e074c69eacba3978164125eb2": {
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
        "authors": [
            "Guanzhi Wang",
            "Yuqi Xie",
            "Yunfan Jiang",
            "Ajay Mandlekar",
            "Chaowei Xiao",
            "Yuke Zhu",
            "Linxi Fan",
            "Anima Anandkumar"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16291",
        "code": "https://github.com/MineDojo/Voyager",
        "abstract": " We introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent&#39;s abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https://voyager.minedojo.org/.\n"
    },
    "ec37317b3579158b46de6d811f54021b": {
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.17144",
        "code": "https://github.com/OpenGVLab/GITM",
        "abstract": " The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular &#34;ObtainDiamond&#34; task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe &#34;ObtainDiamond&#34; task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the &#34;ObtainDiamond&#34; task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.\n"
    }
}