{
    "12e797c33d37e99a30aa9bd979aa2b31": {
        "title": "Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks",
        "authors": [
            "Haoqi Yuan",
            "Chi Zhang",
            "Hongcheng Wang",
            "Feiyang Xie",
            "Penglin Cai",
            "Hao Dong",
            "Zongqing Lu"
        ],
        "date": "2023/03/29",
        "pdf": "https://arxiv.org/pdf/2303.16563",
        "code": "https://sites.google.com/view/plan4mc",
        "abstract": " We study building a multi-task agent in Minecraft. Without human\ndemonstrations, solving long-horizon tasks in this open-ended environment with\nreinforcement learning (RL) is extremely sample inefficient. To tackle the\nchallenge, we decompose solving Minecraft tasks into learning basic skills and\nplanning over the skills. We propose three types of fine-grained basic skills\nin Minecraft, and use RL with intrinsic rewards to accomplish basic skills with\nhigh success rates. For skill planning, we use Large Language Models to find\nthe relationships between skills and build a skill graph in advance. When the\nagent is solving a task, our skill search algorithm walks on the skill graph\nand generates the proper skill plans for the agent. In experiments, our method\naccomplishes 24 diverse Minecraft tasks, where many tasks require sequentially\nexecuting for more than 10 skills. Our method outperforms baselines in most\ntasks by a large margin. The project&#39;s website and code can be found at\nhttps://sites.google.com/view/plan4mc.\n"
    },
    "4977a28eb34673b1a874b008ab45aa6a": {
        "title": "Knowledge-enhanced Agents for Interactive Text Games",
        "authors": [
            "Prateek Chhikara",
            "Jiarui Zhang",
            "Filip Ilievski",
            "Jonathan Francis",
            "Kaixin Ma"
        ],
        "date": "2023/05/08",
        "pdf": "https://arxiv.org/pdf/2305.05091",
        "abstract": " Communication via natural language is a crucial aspect of intelligence, and\nit requires computational models to learn and reason about world concepts, with\nvarying levels of supervision. While there has been significant progress made\non fully-supervised non-interactive tasks, such as question-answering and\nprocedural text understanding, much of the community has turned to various\nsequential interactive tasks, as in semi-Markov text-based games, which have\nrevealed limitations of existing approaches in terms of coherence, contextual\nawareness, and their ability to learn effectively from the environment. In this\npaper, we propose a framework for enabling improved functional grounding of\nagents in text-based games. Specifically, we consider two forms of domain\nknowledge that we inject into learning-based agents: memory of previous correct\nactions and affordances of relevant objects in the environment. Our framework\nsupports three representative model classes: `pure&#39; reinforcement learning (RL)\nagents, RL agents enhanced with knowledge graphs, and agents equipped with\nlanguage models. Furthermore, we devise multiple injection strategies for the\nabove domain knowledge types and agent architectures, including injection via\nknowledge graphs and augmentation of the existing input encoding strategies. We\nperform all experiments on the ScienceWorld text-based game environment, to\nillustrate the performance of various model configurations in challenging\nscience-related instruction-following tasks. Our findings provide crucial\ninsights on the development of effective natural language processing systems\nfor interactive contexts.\n"
    },
    "d0aa2508a83304e8baf09ffdf76be63c": {
        "title": "Playing repeated games with Large Language Models",
        "authors": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16867",
        "abstract": " Large Language Models (LLMs) are transforming society and permeating into\ndiverse applications. As a result, LLMs will frequently interact with us and\nother agents. It is, therefore, of great societal value to understand how LLMs\nbehave in interactive social settings. Here, we propose to use behavioral game\ntheory to study LLM&#39;s cooperation and coordination behavior. To do so, we let\ndifferent LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with\neach other and with other, human-like strategies. Our results show that LLMs\ngenerally perform well in such tasks and also uncover persistent behavioral\nsignatures. In a large set of two players-two strategies games, we find that\nLLMs are particularly good at games where valuing their own self-interest pays\noff, like the iterated Prisoner&#39;s Dilemma family. However, they behave\nsub-optimally in games that require coordination. We, therefore, further focus\non two games from these distinct families. In the canonical iterated Prisoner&#39;s\nDilemma, we find that GPT-4 acts particularly unforgivingly, always defecting\nafter another agent has defected only once. In the Battle of the Sexes, we find\nthat GPT-4 cannot match the behavior of the simple convention to alternate\nbetween options. We verify that these behavioral signatures are stable across\nrobustness checks. Finally, we show how GPT-4&#39;s behavior can be modified by\nproviding further information about the other player as well as by asking it to\npredict the other player&#39;s actions before making a choice. These results enrich\nour understanding of LLM&#39;s social behavior and pave the way for a behavioral\ngame theory for machines.\n"
    },
    "16ade9646e937f57019f60d7eecb4a2d": {
        "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate",
        "authors": [
            "Kai Xiong",
            "Xiao Ding",
            "Yixin Cao",
            "Ting Liu",
            "Bing Qin"
        ],
        "date": "2023/05/19",
        "pdf": "https://arxiv.org/pdf/2305.11595",
        "abstract": " Large Language Models (LLMs) have demonstrated human-like intelligence and\nare widely used in various applications. However, LLMs still exhibit various\nkinds of inconsistency problems. Existing works mainly focus on the\ninconsistency issues within a single LLM, while we investigate the\ninter-consistency among multiple LLMs, which is critical for collaborating to\nsolve a complex task. To examine whether LLMs can collaborate to ultimately\nachieve a consensus for the shared goal and whether LLMs easily change their\nviewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct\na three-stage debate aligned with real-world scenarios: fair debate, mismatched\ndebate, and roundtable debate. Through extensive experiments on the commonsense\nreasoning task, LLMs not only become more inter-consistent but also achieve\nhigher performance. Moreover, we observe that stronger LLMs tend to dominate\nthe debates by adhering to their perspectives, while weaker ones are more\nlikely to change viewpoints. Additionally, we highlight the importance of a\ncompetent judge, such as GPT-4, to draw more proper conclusions. Our work\ncontributes to understanding the inter-consistency among LLMs and lays the\nfoundation for the development of future collaboration methods.\n"
    },
    "9746aebf3641e1f07a29559fa01a166b": {
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Tushar Khot",
            "Mirella Lapata"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10142",
        "code": "https://github.com/FranxYao/GPT-Bargaining",
        "abstract": " We study whether multiple large language models (LLMs) can autonomously\nimprove each other in a negotiation game by playing, reflecting, and\ncriticizing. We are interested in this question because if LLMs were able to\nimprove each other, it would imply the possibility of creating strong AI agents\nwith minimal human intervention. We ask two LLMs to negotiate with each other,\nplaying the roles of a buyer and a seller, respectively. They aim to reach a\ndeal with the buyer targeting a lower price and the seller a higher one. A\nthird language model, playing the critic, provides feedback to a player to\nimprove the player&#39;s negotiation strategies. We let the two agents play\nmultiple rounds, using previous negotiation history and AI feedback as\nin-context demonstrations to improve the model&#39;s negotiation strategy\niteratively. We use different LLMs (GPT and Claude) for different roles and use\nthe deal price as the evaluation metric. Our experiments reveal multiple\nintriguing findings: (1) Only a subset of the language models we consider can\nself-play and improve the deal price from AI feedback, weaker models either do\nnot understand the game&#39;s rules or cannot incorporate AI feedback for further\nimprovement. (2) Models&#39; abilities to learn from the feedback differ when\nplaying different roles. For example, it is harder for Claude-instant to\nimprove as the buyer than as the seller. (3) When unrolling the game to\nmultiple rounds, stronger agents can consistently improve their performance by\nmeaningfully using previous experiences and iterative AI feedback, yet have a\nhigher risk of breaking the deal. We hope our work provides insightful initial\nexplorations of having models autonomously improve each other with game playing\nand AI feedback.\n"
    },
    "4ad0c8c7e6154544a3aee3d3680d9765": {
        "title": "Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models",
        "authors": [
            "Jun Inukai",
            "Tadahiro Taniguchi",
            "Akira Taniguchi",
            "Yoshinobu Hagiwara"
        ],
        "date": "2023/05/31",
        "pdf": "https://arxiv.org/pdf/2305.19761",
        "abstract": " In the studies on symbol emergence and emergent communication in a population\nof agents, a computational model was employed in which agents participate in\nvarious language games. Among these, the Metropolis-Hastings naming game (MHNG)\npossesses a notable mathematical property: symbol emergence through MHNG is\nproven to be a decentralized Bayesian inference of representations shared by\nthe agents. However, the previously proposed MHNG is limited to a two-agent\nscenario. This paper extends MHNG to an N-agent scenario. The main\ncontributions of this paper are twofold: (1) we propose the recursive\nMetropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and\ndemonstrate that RMHNG is an approximate Bayesian inference method for the\nposterior distribution over a latent variable shared by agents, similar to\nMHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and\nreal image data, enabling multiple agents to develop and share a symbol system.\nFurthermore, we introduce two types of approximations -- one-sample and\nlimited-length -- to reduce computational complexity while maintaining the\nability to explain communication in a population of agents. The experimental\nfindings showcased the efficacy of RMHNG as a decentralized Bayesian inference\nfor approximating the posterior distribution concerning latent variables, which\nare jointly shared among agents, akin to MHNG. Moreover, the utilization of\nRMHNG elucidated the agents&#39; capacity to exchange symbols. Furthermore, the\nstudy discovered that even the computationally simplified version of RMHNG\ncould enable symbols to emerge among the agents.\n"
    },
    "7faa5c7e074c69eacba3978164125eb2": {
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
        "authors": [
            "Guanzhi Wang",
            "Yuqi Xie",
            "Yunfan Jiang",
            "Ajay Mandlekar",
            "Chaowei Xiao",
            "Yuke Zhu",
            "Linxi Fan",
            "Anima Anandkumar"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16291",
        "code": "https://github.com/MineDojo/Voyager",
        "abstract": " We introduce Voyager, the first LLM-powered embodied lifelong learning agent\nin Minecraft that continuously explores the world, acquires diverse skills, and\nmakes novel discoveries without human intervention. Voyager consists of three\nkey components: 1) an automatic curriculum that maximizes exploration, 2) an\never-growing skill library of executable code for storing and retrieving\ncomplex behaviors, and 3) a new iterative prompting mechanism that incorporates\nenvironment feedback, execution errors, and self-verification for program\nimprovement. Voyager interacts with GPT-4 via blackbox queries, which bypasses\nthe need for model parameter fine-tuning. The skills developed by Voyager are\ntemporally extended, interpretable, and compositional, which compounds the\nagent&#39;s abilities rapidly and alleviates catastrophic forgetting. Empirically,\nVoyager shows strong in-context lifelong learning capability and exhibits\nexceptional proficiency in playing Minecraft. It obtains 3.3x more unique\nitems, travels 2.3x longer distances, and unlocks key tech tree milestones up\nto 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill\nlibrary in a new Minecraft world to solve novel tasks from scratch, while other\ntechniques struggle to generalize. We open-source our full codebase and prompts\nat https://voyager.minedojo.org/.\n"
    },
    "ec37317b3579158b46de6d811f54021b": {
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.17144",
        "code": "https://github.com/OpenGVLab/GITM",
        "abstract": " The captivating realm of Minecraft has attracted substantial research\ninterest in recent years, serving as a rich platform for developing intelligent\nagents capable of functioning in open-world environments. However, the current\nresearch landscape predominantly focuses on specific objectives, such as the\npopular &#34;ObtainDiamond&#34; task, and has not yet shown effective generalization to\na broader spectrum of tasks. Furthermore, the current leading success rate for\nthe &#34;ObtainDiamond&#34; task stands at around 20%, highlighting the limitations of\nReinforcement Learning (RL) based controllers used in existing methods. To\ntackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel\nframework integrates Large Language Models (LLMs) with text-based knowledge and\nmemory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These\nagents, equipped with the logic and common sense capabilities of LLMs, can\nskillfully navigate complex, sparse-reward environments with text-based\ninteractions. We develop a set of structured actions and leverage LLMs to\ngenerate action plans for the agents to execute. The resulting LLM-based agent\nmarkedly surpasses previous methods, achieving a remarkable improvement of\n+47.5% in success rate on the &#34;ObtainDiamond&#34; task, demonstrating superior\nrobustness compared to traditional RL-based controllers. Notably, our agent is\nthe first to procure all items in the Minecraft Overworld technology tree,\ndemonstrating its extensive capabilities. GITM does not need any GPU for\ntraining, but a single CPU node with 32 CPU cores is enough. This research\nshows the potential of LLMs in developing capable agents for handling\nlong-horizon, complex tasks and adapting to uncertainties in open-world\nenvironments. See the project website at https://github.com/OpenGVLab/GITM.\n"
    },
    "691bf33064019cb4442e3758f44807c8": {
        "title": "An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents",
        "authors": [
            "Maximilian Croissant",
            "Madeleine Frister",
            "Guy Schofield",
            "Cade McCall"
        ],
        "date": "2023/09/10",
        "pdf": "https://arxiv.org/pdf/2309.05076",
        "abstract": " The development of believable, natural, and interactive digital artificial\nagents is a field of growing interest. Theoretical uncertainties and technical\nbarriers present considerable challenges to the field, particularly with\nregards to developing agents that effectively simulate human emotions. Large\nlanguage models (LLMs) might address these issues by tapping common patterns in\nsituational appraisal. In three empirical experiments, this study tests the\ncapabilities of LLMs to solve emotional intelligence tasks and to simulate\nemotions. It presents and evaluates a new chain-of-emotion architecture for\nemotion simulation within video games, based on psychological appraisal\nresearch. Results show that it outperforms standard LLM architectures on a\nrange of user experience and content analysis metrics. This study therefore\nprovides early evidence of how to construct and test affective agents based on\ncognitive processes represented in language models.\n"
    },
    "2784f2a2b88063c67286262773571a65": {
        "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf",
        "authors": [
            "Yuzhuang Xu",
            "Shuo Wang",
            "Peng Li",
            "Fuwen Luo",
            "Xiaolong Wang",
            "Weidong Liu",
            "Yang Liu"
        ],
        "date": "2023/09/09",
        "pdf": "https://arxiv.org/pdf/2309.04658.pdf",
        "abstract": "Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf&#39;&#39;, demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains."
    },
    "fa0d2b5c94ed1f859fe93477ed2d3f79": {
        "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
        "authors": [
            "Jiaxian Guo",
            "Bo Yang",
            "Paul Yoo",
            "Bill Yuchen Lin",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "date": "2023/09/29",
        "pdf": "https://arxiv.org/pdf/2309.17277.pdf",
        "code": "https://github.com/CR-Gjx/Suspicion-Agent",
        "abstract": "Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4&#39;s learned knowledge for imperfect information games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4&#39;s capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others&#39; behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of Suspicion-Agent across three different imperfect information games and then quantitatively evaluate it in Leduc Hold&#39;em. The results show that Suspicion-Agent can potentially outperform traditional algorithms designed for imperfect information games, without any specialized training or examples. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available."
    },
    "e0a2fda9dfc3609da9420d67063733e8": {
        "title": "Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis",
        "authors": [
            "Akshat Gupta"
        ],
        "date": "2023/08/23",
        "pdf": "https://arxiv.org/pdf/2308.12466.pdf",
        "abstract": "Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players. Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This exploration then sheds light on the divergent poker tactics of the two models: ChatGPT&#39;s conservativeness juxtaposed against GPT-4&#39;s aggression. In poker vernacular, when tasked to play GTO poker, ChatGPT plays like a Nit, which means that it has a propensity to only engage with premium hands and folds a majority of hands. When subjected to the same directive, GPT-4 plays like a maniac, showcasing a loose and aggressive style of play. Both strategies, although relatively advanced, are not game theory optimal."
    },
    "547060c869101463eaac2740b4922d60": {
        "title": "Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Jen-tse Huang",
            "Wenxuan Wang",
            "Wenxiang Jiao",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi",
            "Xing Wang"
        ],
        "date": "2023/10/31",
        "pdf": "https://arxiv.org/pdf/2310.20499.pdf",
        "abstract": "The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy&#39;&#39;, we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players&#39; descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs&#39; expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs&#39; intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs&#39; human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication."
    },
    "6ef9581ab6919de277369293f4d554ec": {
        "title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game",
        "authors": [
            "Zijing Shi",
            "Meng Fang",
            "Shunfeng Zheng",
            "Shilong Deng",
            "Ling Chen",
            "Yali Du"
        ],
        "date": "2023/12/29",
        "pdf": "http://arxiv.org/pdf/2312.17515.pdf",
        "abstract": "Multi-agent collaboration with Large Language Models (LLMs) demonstrates proficiency in basic tasks, yet its efficiency in more complex scenarios remains unexplored. In gaming environments, these agents often face situations without established coordination protocols, requiring them to make intelligent inferences about teammates from limited data. This problem motivates the area of ad hoc teamwork, in which an agent may potentially cooperate with a variety of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork problem where the agent operates in an environment driven by natural language. Our findings reveal the potential of LLM agents in team collaboration, highlighting issues related to hallucinations in communication. To address this issue, we develop CodeAct, a general agent that equips LLM with enhanced memory and code-driven reasoning, enabling the repurposing of partial information for rapid adaptation to new teammates."
    }
}