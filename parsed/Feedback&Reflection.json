{
    "3f38895b5dc2bb39a5857ec9a34da3ac": {
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Wenxiang Jiao",
            "Xing Wang",
            "Yan Wang",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi"
        ],
        "date": "2023/05/30",
        "pdf": "https://arxiv.org/pdf/2305.19118",
        "code": "https://github.com/Skytliang/Multi-Agents-Debate",
        "abstract": " Modern large language models (LLMs) like ChatGPT have shown remarkable\nperformance on general language tasks but still struggle on complex reasoning\ntasks, which drives the research on cognitive behaviors of LLMs to explore\nhuman-like problem-solving strategies. Along this direction, one representative\nstrategy is self-reflection, which asks an LLM to refine the solution with the\nfeedback generated by itself iteratively. However, our study shows that such\nreflection-style methods suffer from the Degeneration-of-Thought (DoT) problem:\nonce the LLM has established confidence in its solutions, it is unable to\ngenerate novel thoughts later through reflection even if its initial stance is\nincorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD)\nframework, in which multiple agents express their arguments in the state of\n&#34;tit for tat&#34; and a judge manages the debate process to obtain a final\nsolution. Clearly, our MAD framework encourages divergent thinking in LLMs\nwhich would be helpful for tasks that require deep levels of contemplation.\nExperiment results on two challenging datasets, commonsense machine translation\nand counter-intuitive arithmetic reasoning, demonstrate the effectiveness of\nour MAD framework. Extensive analyses suggest that the adaptive break of debate\nand the modest level of &#34;tit for tat&#34; state are required for MAD to obtain good\nperformance. Moreover, we find that LLMs might not be a fair judge if different\nLLMs are used for agents. Codes:\nhttps://github.com/Skytliang/Multi-Agents-Debate\n"
    },
    "27622956eb8adf45c1c26de87fedc9ff": {
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback",
        "authors": [
            "Nikhil Mehta",
            "Milagro Teruel",
            "Patricio Figueroa Sanz",
            "Xin Deng",
            "Ahmed Hassan Awadallah",
            "Julia Kiseleva"
        ],
        "date": "2023/04/21",
        "pdf": "https://arxiv.org/pdf/2304.10750",
        "abstract": " Many approaches to Natural Language Processing (NLP) tasks often treat them\nas single-step problems, where an agent receives an instruction, executes it,\nand is evaluated based on the final outcome. However, human language is\ninherently interactive, as evidenced by the back-and-forth nature of human\nconversations. In light of this, we posit that human-AI collaboration should\nalso be interactive, with humans monitoring the work of AI agents and providing\nfeedback that the agent can understand and utilize. Further, the AI agent\nshould be able to detect when it needs additional information and proactively\nask for help. Enabling this scenario would lead to more natural, efficient, and\nengaging human-AI collaborations. In this work, we explore these directions using the challenging task defined\nby the IGLU competition, an interactive grounded language understanding task in\na MineCraft-like world. We explore multiple types of help players can give to\nthe AI to guide it and analyze the impact of this help in AI behavior,\nresulting in performance improvements.\n"
    },
    "aa093a5d75fafc6139a1a2e4ea9ac2f8": {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
        "authors": [
            "Ning Miao",
            "Yee Whye Teh",
            "Tom Rainforth"
        ],
        "date": "2023/08/01",
        "pdf": "https://arxiv.org/pdf/2308.00436",
        "code": "https://github.com/ningmiao/selfcheck",
        "abstract": " The recent progress in large language models (LLMs), especially the invention\nof chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning\nproblems. However, even the strongest LLMs are still struggling with more\ncomplicated problems that require non-linear thinking and multi-step reasoning.\nIn this work, we explore whether LLMs have the ability to recognize their own\nerrors, without resorting to external resources. In particular, we investigate\nwhether they can be used to identify individual errors within a step-by-step\nreasoning. To this end, we propose a zero-shot verification scheme to recognize\nsuch errors. We then use this verification scheme to improve question-answering\nperformance, by using it to perform weighted voting on different generated\nanswers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and\nfind that it successfully recognizes errors and, in turn, increases final\npredictive performance.\n"
    },
    "1b7a9bae65346bc13ade7d5f7ff8dfa7": {
        "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback",
        "authors": [
            "Bo Shen",
            "Jiaxin Zhang",
            "Taihong Chen",
            "Daoguang Zan",
            "Bing Geng",
            "An Fu",
            "Muhan Zeng",
            "Ailun Yu",
            "Jichuan Ji",
            "Jingyang Zhao",
            "Yuenan Guo",
            "Qianxiang Wang"
        ],
        "date": "2023/07/27",
        "pdf": "https://arxiv.org/pdf/2307.14936",
        "abstract": " Large Language Models for Code (Code LLM) are flourishing. New and powerful\nmodels are released on a weekly basis, demonstrating remarkable performance on\nthe code generation task. Various approaches have been proposed to boost the\ncode generation performance of pre-trained Code LLMs, such as supervised\nfine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we\npropose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework,\nwhich can effectively and efficiently boost pre-trained large language models\nfor code generation. Under this framework, we present PanGu-Coder2, which\nachieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through\nan extensive evaluation on CoderEval and LeetCode benchmarks, we show that\nPanGu-Coder2 consistently outperforms all previous Code LLMs.\n"
    },
    "2f642f73495732f56d575ca9c0ecb9d2": {
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Prakhar Gupta",
            "Skyler Hallinan",
            "Luyu Gao",
            "Sarah Wiegreffe",
            "Uri Alon",
            "Nouha Dziri",
            "Shrimai Prabhumoye",
            "Yiming Yang",
            "Shashank Gupta",
            "Bodhisattwa Prasad Majumder",
            "Katherine Hermann",
            "Sean Welleck",
            "Amir Yazdanbakhsh",
            "Peter Clark"
        ],
        "date": "2023/03/30",
        "pdf": "https://arxiv.org/pdf/2303.17651",
        "code": "https://github.com/madaan/self-refine",
        "abstract": " Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.\n"
    },
    "64c8f9b19403f164ca1c339b1522f6c8": {
        "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Lingkai Kong",
            "Bo Dai",
            "Chao Zhang"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16653",
        "code": "https://github.com/haotiansun14/adaplanner",
        "abstract": " Large language models (LLMs) have recently demonstrated the potential in\nacting as autonomous agents for sequential decision-making tasks. However, most\nexisting methods either take actions greedily without planning or rely on\nstatic plans that are not adaptable to environmental feedback. Consequently,\nthe sequential decision-making performance of LLM agents degenerates with\nproblem complexity and plan horizons increase. We propose a closed-loop\napproach, AdaPlanner, which allows the LLM agent to refine its self-generated\nplan adaptively in response to environmental feedback. In AdaPlanner, the LLM\nagent adaptively refines its plan from feedback with both in-plan and\nout-of-plan refinement strategies. To mitigate hallucination, we develop a\ncode-style LLM prompt structure that facilitates plan generation across a\nvariety of tasks, environments, and agent capabilities. Furthermore, we propose\na skill discovery mechanism that leverages successful plans as few-shot\nexemplars, enabling the agent to plan and refine with fewer task\ndemonstrations. Our experiments in the ALFWorld and MiniWoB++ environments\ndemonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and\n4.11% while utilizing 2x and 600x fewer samples, respectively.\n"
    },
    "915f6ab291da9364b0bf8b61410afa01": {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "authors": [
            "Shuofei Qiao",
            "Honghao Gui",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "date": "2023/05/22",
        "pdf": "https://arxiv.org/pdf/2305.13068",
        "code": "https://github.com/zjunlp/trice",
        "abstract": " Tools serve as pivotal interfaces that enable humans to understand and\nreshape the world. With the advent of foundational models, AI systems can\nutilize tools to expand their capabilities and interact with the world.\nExisting tool learning methodologies, encompassing supervised fine-tuning and\nprompt engineering approaches, often induce language models to utilize tools\nindiscriminately, as complex problems often exceed their own competencies.\nHowever, introducing tools for simple tasks, which the models themselves can\nreadily resolve, can inadvertently propagate errors rather than enhance\nperformance. This leads to the research question: can we teach language models\nwhen and how to use tools? To meet this need, we propose Tool leaRning wIth\nexeCution fEedback (TRICE), a two-stage end-to-end framework that enables the\nmodel to continually learn through feedback derived from tool execution,\nthereby learning when and how to use tools effectively. Experimental results,\nbacked by further analysis, show that TRICE can make the language model to\nselectively use tools by decreasing the model&#39;s dependency on tools while\nenhancing the performance. Code and datasets will be available in\nhttps://github.com/zjunlp/trice.\n"
    },
    "5554888e178a16e7d530118c0f7ad123": {
        "title": "Teaching Large Language Models to Self-Debug",
        "authors": [
            "Xinyun Chen",
            "Maxwell Lin",
            "Nathanael Schärli",
            "Denny Zhou"
        ],
        "date": "2023/04/11",
        "pdf": "https://arxiv.org/pdf/2304.05128",
        "abstract": " Large language models (LLMs) have achieved impressive performance on code\ngeneration. However, for complex programming tasks, generating the correct\nsolution in one go becomes challenging, thus some prior works have designed\nprogram repair approaches to improve code generation performance. In this work,\nwe propose Self-Debugging, which teaches a large language model to debug its\npredicted program via few-shot demonstrations. In particular, we demonstrate\nthat Self-Debugging can teach the large language model to perform rubber duck\ndebugging; i.e., without any feedback on the code correctness or error\nmessages, the model is able to identify its mistakes by explaining the\ngenerated code in natural language. Self-Debugging achieves the\nstate-of-the-art performance on several code generation benchmarks, including\nthe Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python\ntranslation, and MBPP for text-to-Python generation. On the Spider benchmark\nwhere there are no unit tests to verify the correctness of predictions,\nSelf-Debugging with code explanation consistently improves the baseline by\n2-3%, and improves the prediction accuracy on problems of the hardest label by\n9%. On TransCoder and MBPP where unit tests are available, Self-Debugging\nimproves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback\nmessages and reusing failed predictions, Self-Debugging notably improves sample\nefficiency, and can match or outperform baseline models that generate more than\n10x candidate programs.\n"
    },
    "141e07821bd167bc5ce5902645300d40": {
        "title": "The ART of LLM Refinement: Ask, Refine, and Trust",
        "authors": [
            "Kumar Shridhar",
            "Koustuv Sinha",
            "Andrew Cohen",
            "Tianlu Wang",
            "Ping Yu",
            "Ram Pasunuru",
            "Mrinmaya Sachan",
            "Jason Weston",
            "Asli Celikyilmaz"
        ],
        "date": "2023/11/14",
        "pdf": "https://arxiv.org/pdf/2311.07961.pdf",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations? A popular concept, referred to as self-refinement, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with refinement objective called ART: Ask, Refine, and Trust, which asks necessary questions to decide when an LLM should refine its output, and either affirm or withhold trust in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), ART achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We also demonstrate the benefit of using smaller models to make refinement decisions as a cost-effective alternative to fine-tuning a larger model."
    },
    "20c18a2ace6082d92a1fe9e1185843ba": {
        "title": "Learning From Mistakes Makes LLM Better Reasoner",
        "authors": [
            "Shengnan An",
            "Zexiong Ma",
            "Zeqi Lin",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "date": "2023/10/31",
        "pdf": "https://arxiv.org/pdf/2310.20689.pdf",
        "code": "https://github.com/microsoft/LEMA",
        "abstract": "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning paths from various LLMs and then employ GPT-4 as a &#34;corrector&#34; to (1) identify the mistake step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the final answer. Experimental results demonstrate the effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning tasks, LeMa consistently improves the performance compared with fine-tuning on CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved by non-execution open-source models on these challenging tasks. Our code, data and models will be publicly available at https://github.com/microsoft/LEMA."
    }
}