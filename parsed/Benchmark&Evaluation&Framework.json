{
    "c8c04c2c8e3e947e8cdee88e9ce28e21": {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "authors": [
            "Ehsan Kamalloo",
            "Aref Jafari",
            "Xinyu Zhang",
            "Nandan Thakur",
            "Jimmy Lin"
        ],
        "date": "2023/07/31",
        "pdf": "https://arxiv.org/pdf/2307.16883",
        "code": "https://github.com/project-miracl/hagrid",
        "abstract": " The rise of large language models (LLMs) had a transformative impact on\nsearch, ushering in a new era of search engines that are capable of generating\nsearch results in natural language text, imbued with citations for supporting\nsources. Building generative information-seeking models demands openly\naccessible datasets, which currently remain lacking. In this paper, we\nintroduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative\nRetrieval for Information-seeking Dataset) for building end-to-end generative\ninformation-seeking models that are capable of retrieving candidate quotes and\ngenerating attributed explanations. Unlike recent efforts that focus on human\nevaluation of black-box proprietary search engines, we built our dataset atop\nthe English subset of MIRACL, a publicly available information retrieval\ndataset. HAGRID is constructed based on human and LLM collaboration. We first\nautomatically collect attributed explanations that follow an in-context\ncitation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to\nevaluate the LLM explanations based on two criteria: informativeness and\nattributability. HAGRID serves as a catalyst for the development of\ninformation-seeking models with better attribution capabilities.\n"
    },
    "a46f1651f62db660d022c495bbab01a4": {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "authors": [
            "Xiao Liu",
            "Hao Yu",
            "Hanchen Zhang",
            "Yifan Xu",
            "Xuanyu Lei",
            "Hanyu Lai",
            "Yu Gu",
            "Hangliang Ding",
            "Kaiwen Men",
            "Kejuan Yang",
            "Shudan Zhang",
            "Xiang Deng",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chenhui Zhang",
            "Sheng Shen",
            "Tianjun Zhang",
            "Yu Su",
            "Huan Sun",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/08/07",
        "pdf": "https://arxiv.org/pdf/2308.03688",
        "code": "https://github.com/THUDM/AgentBench",
        "abstract": " Large Language Models (LLMs) are becoming increasingly smart and autonomous,\ntargeting real-world pragmatic missions beyond traditional NLP tasks. As a\nresult, there has been an urgent need to evaluate LLMs as agents on challenging\ntasks in interactive environments. We present AgentBench, a multi-dimensional\nevolving benchmark that currently consists of 8 distinct environments to assess\nLLM-as-Agent&#39;s reasoning and decision-making abilities in a multi-turn\nopen-ended generation setting. Our extensive test over 25 LLMs (including APIs\nand open-sourced models) shows that, while top commercial LLMs present a strong\nability of acting as agents in complex environments, there is a significant\ndisparity in performance between them and open-sourced competitors. It also\nserves as a component of an ongoing project with wider coverage and deeper\nconsideration towards systematic LLM evaluation. Datasets, environments, and an\nintegrated evaluation package for AgentBench are released at\nhttps://github.com/THUDM/AgentBench\n"
    },
    "2d8ee880fda520da34ed2cd55d7f8096": {
        "title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents",
        "authors": [
            "Zhiwei Liu",
            "Weiran Yao",
            "Jianguo Zhang",
            "Le Xue",
            "Shelby Heinecke",
            "Rithesh Murthy",
            "Yihao Feng",
            "Zeyuan Chen",
            "Juan Carlos Niebles",
            "Devansh Arpit",
            "Ran Xu",
            "Phil Mui",
            "Huan Wang",
            "Caiming Xiong",
            "Silvio Savarese"
        ],
        "date": "2023/08/11",
        "pdf": "https://arxiv.org/pdf/2308.05960",
        "abstract": " The massive successes of large language models (LLMs) encourage the emerging\nexploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to\ngenerate actions with its core LLM and interact with environments, which\nfacilitates the ability to resolve complex tasks by conditioning on past\ninteractions such as observations and actions. Since the investigation of LAA\nis still very recent, limited explorations are available. Therefore, we provide\na comprehensive comparison of LAA in terms of both agent architectures and LLM\nbackbones. Additionally, we propose a new strategy to orchestrate multiple LAAs\nsuch that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA,\nwhere a controller manages the communication among multiple agents. We conduct\nsimulations on both decision-making and multi-step reasoning environments,\nwhich comprehensively justify the capacity of LAAs. Our performance results\nprovide quantitative suggestions for designing LAA architectures and the\noptimal choice of LLMs, as well as the compatibility of both. We release our\nimplementation code of LAAs to the public at\n\\url{https://github.com/salesforce/BOLAA}.\n"
    },
    "ff1281e65bb2fcc4de561ef67dd08f7f": {
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "authors": [
            "Xiang Deng",
            "Yu Gu",
            "Boyuan Zheng",
            "Shijie Chen",
            "Samuel Stevens",
            "Boshi Wang",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2023/06/09",
        "pdf": "https://arxiv.org/pdf/2306.06070",
        "code": "https://github.com/OSU-NLP-Group/Mind2Web",
        "abstract": " We introduce Mind2Web, the first dataset for developing and evaluating\ngeneralist agents for the web that can follow language instructions to complete\ncomplex tasks on any website. Existing datasets for web agents either use\nsimulated websites or only cover a limited set of websites and tasks, thus not\nsuitable for generalist web agents. With over 2,000 open-ended tasks collected\nfrom 137 websites spanning 31 domains and crowdsourced action sequences for the\ntasks, Mind2Web provides three necessary ingredients for building generalist\nweb agents: 1) diverse domains, websites, and tasks, 2) use of real-world\nwebsites instead of simulated and simplified ones, and 3) a broad spectrum of\nuser interaction patterns. Based on Mind2Web, we conduct an initial exploration\nof using large language models (LLMs) for building generalist web agents. While\nthe raw HTML of real-world websites are often too large to be fed to LLMs, we\nshow that first filtering it with a small LM significantly improves the\neffectiveness and efficiency of LLMs. Our solution demonstrates a decent level\nof performance, even on websites or entire domains the model has never seen\nbefore, but there is still a substantial room to improve towards truly\ngeneralizable agents. We open-source our dataset, model implementation, and\ntrained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further\nresearch on building a generalist agent for the web.\n"
    },
    "e551d82f612e1b1fabd139d6337197b9": {
        "title": "Agents: An Open-source Framework for Autonomous Language Agents",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Long Li",
            "Jialong Wu",
            "Tiannan Wang",
            "Shi Qiu",
            "Jintian Zhang",
            "Jing Chen",
            "Ruipu Wu",
            "Shuai Wang",
            "Shiding Zhu",
            "Jiyu Chen",
            "Wentao Zhang",
            "Ningyu Zhang",
            "Huajun Chen",
            "Peng Cui",
            "Mrinmaya Sachan"
        ],
        "date": "2023/09/14",
        "pdf": "https://arxiv.org/pdf/2309.07870",
        "code": "https://github.com/aiwaves-cn/agents",
        "abstract": " Recent advances on large language models (LLMs) enable researchers and\ndevelopers to build autonomous language agents that can automatically solve\nvarious tasks and interact with environments, humans, and other agents using\nnatural language interfaces. We consider language agents as a promising\ndirection towards artificial general intelligence and release Agents, an\nopen-source library with the goal of opening up these advances to a wider\nnon-specialist audience. Agents is carefully engineered to support important\nfeatures including planning, memory, tool usage, multi-agent communication, and\nfine-grained symbolic control. Agents is user-friendly as it enables\nnon-specialists to build, customize, test, tune, and deploy state-of-the-art\nautonomous language agents without much coding. The library is also\nresearch-friendly as its modularized design makes it easily extensible for\nresearchers. Agents is available at https://github.com/aiwaves-cn/agents.\n"
    },
    "d10c223c151e7cd6de70ba79e82f36cc": {
        "title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena",
        "authors": [
            "Jiangjie Chen",
            "Siyu Yuan",
            "Rong Ye",
            "Bodhisattwa Prasad Majumder",
            "Kyle Richardson"
        ],
        "date": "2023/10/09",
        "pdf": "https://arxiv.org/pdf/2310.05746.pdf",
        "abstract": "Can Large Language Models (LLMs) simulate human behavior in complex environments? LLMs have recently been shown to exhibit advanced reasoning skills but much of NLP evaluation still relies on static benchmarks. Answering this requires evaluation environments that probe strategic reasoning in competitive, dynamic scenarios that involve long-term planning. We introduce AucArena, a novel simulation environment for evaluating LLMs within auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct several controlled simulations using state-of-the-art LLMs as bidding agents. We find that through simple prompting, LLMs do indeed demonstrate many of the skills needed for effectively engaging in auctions (e.g., managing budget, adhering to long-term goals and priorities), skills that we find can be sharpened by explicitly encouraging models to be adaptive and observe strategies in past auctions. These results are significant as they show the potential of using LLM agents to model intricate social dynamics, especially in competitive settings. However, we also observe considerable variability in the capabilities of individual LLMs. Notably, even our most advanced models (GPT-4) are occasionally surpassed by heuristic baselines and human agents, highlighting the potential for further improvements in the design of LLM agents and the important role that our simulation environment can play in further testing and refining agent architectures."
    }
}