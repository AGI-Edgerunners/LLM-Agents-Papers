{
    "fefbb121e5e6daefd835e760d5f08117": {
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "authors": [
            "Joon Sung Park",
            "Joseph C. O&#39;Brien",
            "Carrie J. Cai",
            "Meredith Ringel Morris",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "date": "2023/04/07",
        "pdf": "https://arxiv.org/pdf/2304.03442",
        "abstract": " Believable proxies of human behavior can empower interactive applications\nranging from immersive environments to rehearsal spaces for interpersonal\ncommunication to prototyping tools. In this paper, we introduce generative\nagents--computational software agents that simulate believable human behavior.\nGenerative agents wake up, cook breakfast, and head to work; artists paint,\nwhile authors write; they form opinions, notice each other, and initiate\nconversations; they remember and reflect on days past as they plan the next\nday. To enable generative agents, we describe an architecture that extends a\nlarge language model to store a complete record of the agent&#39;s experiences\nusing natural language, synthesize those memories over time into higher-level\nreflections, and retrieve them dynamically to plan behavior. We instantiate\ngenerative agents to populate an interactive sandbox environment inspired by\nThe Sims, where end users can interact with a small town of twenty five agents\nusing natural language. In an evaluation, these generative agents produce\nbelievable individual and emergent social behaviors: for example, starting with\nonly a single user-specified notion that one agent wants to throw a Valentine&#39;s\nDay party, the agents autonomously spread invitations to the party over the\nnext two days, make new acquaintances, ask each other out on dates to the\nparty, and coordinate to show up for the party together at the right time. We\ndemonstrate through ablation that the components of our agent\narchitecture--observation, planning, and reflection--each contribute critically\nto the believability of agent behavior. By fusing large language models with\ncomputational, interactive agents, this work introduces architectural and\ninteraction patterns for enabling believable simulations of human behavior.\n"
    },
    "63dc16015de346991b87581ffc2f70a9": {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "authors": [
            "Pan Lu",
            "Baolin Peng",
            "Hao Cheng",
            "Michel Galley",
            "Kai-Wei Chang",
            "Ying Nian Wu",
            "Song-Chun Zhu",
            "Jianfeng Gao"
        ],
        "date": "2023/04/19",
        "pdf": "https://arxiv.org/pdf/2304.09842",
        "abstract": " Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner.\n"
    },
    "27622956eb8adf45c1c26de87fedc9ff": {
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback",
        "authors": [
            "Nikhil Mehta",
            "Milagro Teruel",
            "Patricio Figueroa Sanz",
            "Xin Deng",
            "Ahmed Hassan Awadallah",
            "Julia Kiseleva"
        ],
        "date": "2023/04/21",
        "pdf": "https://arxiv.org/pdf/2304.10750",
        "abstract": " Many approaches to Natural Language Processing (NLP) tasks often treat them\nas single-step problems, where an agent receives an instruction, executes it,\nand is evaluated based on the final outcome. However, human language is\ninherently interactive, as evidenced by the back-and-forth nature of human\nconversations. In light of this, we posit that human-AI collaboration should\nalso be interactive, with humans monitoring the work of AI agents and providing\nfeedback that the agent can understand and utilize. Further, the AI agent\nshould be able to detect when it needs additional information and proactively\nask for help. Enabling this scenario would lead to more natural, efficient, and\nengaging human-AI collaborations. In this work, we explore these directions using the challenging task defined\nby the IGLU competition, an interactive grounded language understanding task in\na MineCraft-like world. We explore multiple types of help players can give to\nthe AI to guide it and analyze the impact of this help in AI behavior,\nresulting in performance improvements.\n"
    },
    "6346920757c54de4cfb53cbd57153400": {
        "title": "ChatLLM Network: More brains, More intelligence",
        "authors": [
            "Rui Hao",
            "Linmei Hu",
            "Weijian Qi",
            "Qingliu Wu",
            "Yirui Zhang",
            "Liqiang Nie"
        ],
        "date": "2023/04/24",
        "pdf": "https://arxiv.org/pdf/2304.12998",
        "abstract": " Dialogue-based language models mark a huge milestone in the field of\nartificial intelligence, by their impressive ability to interact with users, as\nwell as a series of challenging tasks prompted by customized instructions.\nHowever, the prevalent large-scale dialogue-based language models like ChatGPT\nstill have room for improvement, such as unstable responses to questions and\nthe inability to think cooperatively like humans. Considering the ability of\ndialogue-based language models in conversation and their inherent randomness in\nthinking, we propose ChatLLM network that allows multiple dialogue-based\nlanguage models to interact, provide feedback, and think together. We design\nthe network of ChatLLMs based on ChatGPT. Specifically, individual instances of\nChatGPT may possess distinct perspectives towards the same problem, and by\nconsolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM\nnetwork system can conduct decision-making more objectively and\ncomprehensively. In addition, a language-based feedback mechanism comparable to\nbackpropagation is devised to update the ChatGPTs within the network.\nExperiments on two datasets demonstrate that our network attains significant\nimprovements in problem-solving, leading to observable progress amongst each\nmember.\n"
    },
    "d4c1c4e1c46d89daf6890d5f71c62c63": {
        "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
        "authors": [
            "Jimmy Wei",
            "Kurt Shuster",
            "Arthur Szlam",
            "Jason Weston",
            "Jack Urbanek",
            "Mojtaba Komeili"
        ],
        "date": "2023/04/26",
        "pdf": "https://arxiv.org/pdf/2304.13835",
        "abstract": " Current dialogue research primarily studies pairwise (two-party)\nconversations, and does not address the everyday setting where more than two\nspeakers converse together. In this work, we both collect and evaluate\nmulti-party conversations to study this more general case. We use the LIGHT\nenvironment to construct grounded conversations, where each participant has an\nassigned character to role-play. We thus evaluate the ability of language\nmodels to act as one or more characters in such conversations. Models require\ntwo skills that pairwise-trained models appear to lack: (1) being able to\ndecide when to talk; (2) producing coherent utterances grounded on multiple\ncharacters. We compare models trained on our new dataset to existing\npairwise-trained dialogue models, as well as large language models with\nfew-shot prompting. We find that our new dataset, MultiLIGHT, which we will\npublicly release, can help bring significant improvements in the group setting.\n"
    },
    "685611a174dfe4f335c66b3a972ad793": {
        "title": "The Role of Summarization in Generative Agents: A Preliminary Perspective",
        "authors": [
            "Xiachong Feng",
            "Xiaocheng Feng",
            "Bing Qin"
        ],
        "date": "2023/05/02",
        "pdf": "https://arxiv.org/pdf/2305.01253",
        "abstract": " Generative agents that simulate human society show tremendous potential for\nfurther research and practical applications. Specifically, the generative agent\narchitecture comprising several meticulously designed modules constitutes the\nmost critical component. To facilitate progress in this research, this report\npresents our integrated perspective on comprehending generative agents through\nsummarization, since we believe summarization is the most fundamental and\nindispensable capacity of generative agents manifested across diverse\nscenarios. We hope this report can provide insight into understanding the\nimportance of summarization capacity in generative agents and motivate future\nresearch.\n"
    },
    "98f7696b30c392052ea00d12d2d79918": {
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "authors": [
            "Jimmy Wu",
            "Rika Antonova",
            "Adam Kan",
            "Marion Lepert",
            "Andy Zeng",
            "Shuran Song",
            "Jeannette Bohg",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ],
        "date": "2023/05/09",
        "pdf": "https://arxiv.org/pdf/2305.05658",
        "abstract": " For a robot to personalize physical assistance effectively, it must learn\nuser preferences that can be generally reapplied to future scenarios. In this\nwork, we investigate personalization of household cleanup with robots that can\ntidy up rooms by picking up objects and putting them away. A key challenge is\ndetermining the proper place to put each object, as people&#39;s preferences can\nvary greatly depending on personal taste or cultural background. For instance,\none person may prefer storing shirts in the drawer, while another may prefer\nthem on the shelf. We aim to build systems that can learn such preferences from\njust a handful of examples via prior interactions with a particular person. We\nshow that robots can combine language-based planning and perception with the\nfew-shot summarization capabilities of large language models (LLMs) to infer\ngeneralized user preferences that are broadly applicable to future\ninteractions. This approach enables fast adaptation and achieves 91.2% accuracy\non unseen objects in our benchmark dataset. We also demonstrate our approach on\na real-world mobile manipulator called TidyBot, which successfully puts away\n85.0% of objects in real-world test scenarios.\n"
    },
    "aff5e8a0a6be3cc3d84f669a5dc2cf7b": {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "Thomas L. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "date": "2023/05/17",
        "pdf": "https://arxiv.org/pdf/2305.10601",
        "abstract": " Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models&#39;\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/ysymyth/tree-of-thought-llm.\n"
    },
    "843b42ba9aa69a474fd90dd43404ee8a": {
        "title": "Role-Play with Large Language Models",
        "authors": [
            "Murray Shanahan",
            "Kyle McDonell",
            "Laria Reynolds"
        ],
        "date": "2023/05/25",
        "pdf": "https://arxiv.org/pdf/2305.16367",
        "abstract": " As dialogue agents become increasingly human-like in their performance, it is\nimperative that we develop effective ways to describe their behaviour in\nhigh-level terms without falling into the trap of anthropomorphism. In this\npaper, we foreground the concept of role-play. Casting dialogue agent behaviour\nin terms of role-play allows us to draw on familiar folk psychological terms,\nwithout ascribing human characteristics to language models they in fact lack.\nTwo important cases of dialogue agent behaviour are addressed this way, namely\n(apparent) deception and (apparent) self-awareness.\n"
    },
    "2fe6d5bff84082b57a528b3f5b08131e": {
        "title": "Training Socially Aligned Language Models in Simulated Human Society",
        "authors": [
            "Ruibo Liu",
            "Ruixin Yang",
            "Chenyan Jia",
            "Ge Zhang",
            "Denny Zhou",
            "Andrew M. Dai",
            "Diyi Yang",
            "Soroush Vosoughi"
        ],
        "date": "2023/05/26",
        "pdf": "https://arxiv.org/pdf/2305.16960",
        "abstract": " Social alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.\n"
    }
}