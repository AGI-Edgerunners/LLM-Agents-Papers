{
    "59ce6dd642c3cee234d3e05856eef0cc": {
        "title": "Adapting LLM Agents Through Communication",
        "authors": [
            "Kuan Wang",
            "Yadong Lu",
            "Michael Santacroce",
            "Yeyun Gong",
            "Chao Zhang",
            "Yelong Shen"
        ],
        "date": "2023/10/01",
        "pdf": "http://arxiv.org/pdf/2310.01444",
        "abstract": "Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue, and Analogue-tailored for common tasks such as decision-making, knowledge-intensive reasoning, and numerical reasoning. We evaluated LTC on three datasets: ALFWorld (decision-making), HotpotQA (knowledge-intensive reasoning), and GSM8k (numerical reasoning). On ALFWorld, it exceeds the instruction tuning baseline by 12% in success rate. On HotpotQA, LTC surpasses the instruction-tuned LLaMA-7B agent by 5.1% in EM score, and it outperforms the instruction-tuned 9x larger PaLM-62B agent by 0.6%. On GSM8k, LTC outperforms the CoT-Tuning baseline by 3.6% in accuracy. The results showcase the versatility and efficiency of the LTC approach across diverse domains. We will open-source our code to promote further development of the community.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.01444v2"
    },
    "a48d6f115726758bb29c8fe22f9dffd9": {
        "title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs",
        "authors": [
            "Aohan Zeng",
            "Mingdao Liu",
            "Rui Lu",
            "Bowen Wang",
            "Xiao Liu",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/10/19",
        "pdf": "http://arxiv.org/pdf/2310.12823",
        "abstract": "Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining AgentInstruct with open-source instructions from general domains. AgentTuning is used to instruction-tune the Llama 2 series, resulting in AgentLM. Our evaluations show that AgentTuning enables LLMs&#39; agent capabilities without compromising general abilities. The AgentLM-70B is comparable to GPT-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the AgentInstruct and AgentLM-7B, 13B, and 70B models at https://github.com/THUDM/AgentTuning, serving open and powerful alternatives to commercial LLMs for agent tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.12823"
    },
    "cf1de36f3cf9d3bf1383a44f988ddf99": {
        "title": "FireAct: Toward Language Agent Fine-tuning",
        "authors": [
            "Baian Chen",
            "Chang Shu",
            "Ehsan Shareghi",
            "Nigel Collier",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "date": "2023/10/09",
        "pdf": "http://arxiv.org/pdf/2310.05915",
        "abstract": "Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.05915"
    },
    "661810d5074a67af6c355de8a64c87b0": {
        "title": "Machine Mindset: An MBTI Exploration of Large Language Models",
        "authors": [
            "Jiaxi Cui",
            "Liuzhenghao Lv",
            "Jing Wen",
            "Rongsheng Wang",
            "Jing Tang",
            "YongHong Tian",
            "Li Yuan"
        ],
        "date": "2023/12/20",
        "pdf": "http://arxiv.org/pdf/2312.12999",
        "abstract": "We present a novel approach for integrating Myers-Briggs Type Indicator (MBTI) personality traits into large language models (LLMs), addressing the challenges of personality consistency in personalized AI. Our method, &#34;Machine Mindset,&#34; involves a two-phase fine-tuning and Direct Preference Optimization (DPO) to embed MBTI traits into LLMs. This approach ensures that models internalize these traits, offering a stable and consistent personality profile. We demonstrate the effectiveness of our models across various domains, showing alignment between model performance and their respective MBTI traits. The paper highlights significant contributions in the development of personality datasets and a new training methodology for personality integration in LLMs, enhancing the potential for personalized AI applications. We also open-sourced our model and part of the data at \\url{https://github.com/PKU-YuanGroup/Machine-Mindset}.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.12999"
    },
    "c8c04c2c8e3e947e8cdee88e9ce28e21": {
        "title": "HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution",
        "authors": [
            "Ehsan Kamalloo",
            "Aref Jafari",
            "Xinyu Zhang",
            "Nandan Thakur",
            "Jimmy Lin"
        ],
        "date": "2023/07/31",
        "pdf": "http://arxiv.org/pdf/2307.16883",
        "abstract": "The rise of large language models (LLMs) had a transformative impact on search, ushering in a new era of search engines that are capable of generating search results in natural language text, imbued with citations for supporting sources. Building generative information-seeking models demands openly accessible datasets, which currently remain lacking. In this paper, we introduce a new dataset, HAGRID (Human-in-the-loop Attributable Generative Retrieval for Information-seeking Dataset) for building end-to-end generative information-seeking models that are capable of retrieving candidate quotes and generating attributed explanations. Unlike recent efforts that focus on human evaluation of black-box proprietary search engines, we built our dataset atop the English subset of MIRACL, a publicly available information retrieval dataset. HAGRID is constructed based on human and LLM collaboration. We first automatically collect attributed explanations that follow an in-context citation style using an LLM, i.e. GPT-3.5. Next, we ask human annotators to evaluate the LLM explanations based on two criteria: informativeness and attributability. HAGRID serves as a catalyst for the development of information-seeking models with better attribution capabilities.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2307.16883"
    },
    "a46f1651f62db660d022c495bbab01a4": {
        "title": "AgentBench: Evaluating LLMs as Agents",
        "authors": [
            "Xiao Liu",
            "Hao Yu",
            "Hanchen Zhang",
            "Yifan Xu",
            "Xuanyu Lei",
            "Hanyu Lai",
            "Yu Gu",
            "Hangliang Ding",
            "Kaiwen Men",
            "Kejuan Yang",
            "Shudan Zhang",
            "Xiang Deng",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Chenhui Zhang",
            "Sheng Shen",
            "Tianjun Zhang",
            "Yu Su",
            "Huan Sun",
            "Minlie Huang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2023/08/07",
        "pdf": "http://arxiv.org/pdf/2308.03688",
        "abstract": "Large Language Models (LLMs) are becoming increasingly smart and autonomous, targeting real-world pragmatic missions beyond traditional NLP tasks. As a result, there has been an urgent need to evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional evolving benchmark that currently consists of 8 distinct environments to assess LLM-as-Agent&#39;s reasoning and decision-making abilities in a multi-turn open-ended generation setting. Our extensive test over 27 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and OSS competitors. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Training on code and high quality multi-turn alignment data could improve agent performance. Datasets, environments, and an integrated evaluation package for AgentBench are released at \\url{https://github.com/THUDM/AgentBench}.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.03688"
    },
    "2d8ee880fda520da34ed2cd55d7f8096": {
        "title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents",
        "authors": [
            "Zhiwei Liu",
            "Weiran Yao",
            "Jianguo Zhang",
            "Le Xue",
            "Shelby Heinecke",
            "Rithesh Murthy",
            "Yihao Feng",
            "Zeyuan Chen",
            "Juan Carlos Niebles",
            "Devansh Arpit",
            "Ran Xu",
            "Phil Mui",
            "Huan Wang",
            "Caiming Xiong",
            "Silvio Savarese"
        ],
        "date": "2023/08/11",
        "pdf": "http://arxiv.org/pdf/2308.05960",
        "abstract": "The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \\url{https://github.com/salesforce/BOLAA}.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.05960"
    },
    "ff1281e65bb2fcc4de561ef67dd08f7f": {
        "title": "Mind2Web: Towards a Generalist Agent for the Web",
        "authors": [
            "Xiang Deng",
            "Yu Gu",
            "Boyuan Zheng",
            "Shijie Chen",
            "Samuel Stevens",
            "Boshi Wang",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2023/06/09",
        "pdf": "http://arxiv.org/pdf/2306.06070",
        "abstract": "We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2306.06070"
    },
    "e551d82f612e1b1fabd139d6337197b9": {
        "title": "Agents: An Open-source Framework for Autonomous Language Agents",
        "authors": [
            "Wangchunshu Zhou",
            "Yuchen Eleanor Jiang",
            "Long Li",
            "Jialong Wu",
            "Tiannan Wang",
            "Shi Qiu",
            "Jintian Zhang",
            "Jing Chen",
            "Ruipu Wu",
            "Shuai Wang",
            "Shiding Zhu",
            "Jiyu Chen",
            "Wentao Zhang",
            "Xiangru Tang",
            "Ningyu Zhang",
            "Huajun Chen",
            "Peng Cui",
            "Mrinmaya Sachan"
        ],
        "date": "2023/09/14",
        "pdf": "http://arxiv.org/pdf/2309.07870",
        "abstract": "Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. Agents is available at https://github.com/aiwaves-cn/agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.07870"
    },
    "d10c223c151e7cd6de70ba79e82f36cc": {
        "title": "Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena",
        "authors": [
            "Jiangjie Chen",
            "Siyu Yuan",
            "Rong Ye",
            "Bodhisattwa Prasad Majumder",
            "Kyle Richardson"
        ],
        "date": "2023/10/09",
        "pdf": "http://arxiv.org/pdf/2310.05746",
        "abstract": "Recent advancements in Large Language Models (LLMs) showcase advanced reasoning, yet NLP evaluations often depend on static benchmarks. Evaluating this necessitates environments that test strategic reasoning in dynamic, competitive scenarios requiring long-term planning. We introduce AucArena, a novel evaluation suite that simulates auctions, a setting chosen for being highly unpredictable and involving many skills related to resource and risk management, while also being easy to evaluate. We conduct controlled experiments using state-of-the-art LLMs to power bidding agents to benchmark their planning and execution skills. Our research demonstrates that LLMs, such as GPT-4, possess key skills for auction participation, such as budget management and goal adherence, which improve with adaptive strategies. This highlights LLMs&#39; potential in modeling complex social interactions in competitive contexts. However, variability in LLM performance and occasional outperformance by simpler methods indicate opportunities for further advancements in LLM design and the value of our simulation environment for ongoing testing and refinement.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.05746"
    },
    "012d0edc2f0cd976178627ba1878a2ee": {
        "title": "SmartPlay: A Benchmark for LLMs as Intelligent Agents",
        "authors": [
            "Yue Wu",
            "Xuan Tang",
            "Tom M. Mitchell",
            "Yuanzhi Li"
        ],
        "date": "2023/10/02",
        "pdf": "http://arxiv.org/pdf/2310.01557",
        "abstract": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs&#39; abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/Microsoft/SmartPlay",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.01557"
    },
    "804f5942ec7bac44830ad6243a4d15c7": {
        "title": "Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency",
        "authors": [
            "Zhihan Liu",
            "Hao Hu",
            "Shenao Zhang",
            "Hongyi Guo",
            "Shuqi Ke",
            "Boyi Liu",
            "Zhaoran Wang"
        ],
        "date": "2023/09/29",
        "pdf": "http://arxiv.org/pdf/2309.17382",
        "abstract": "Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call &#34;reason for future, act for now&#34; (\\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon (&#34;reason for future&#34;). At each step, the LLM agent takes the initial action of the planned trajectory (&#34;act for now&#34;), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state. The key idea is to cast reasoning in LLMs as learning and planning in Bayesian adaptive Markov decision processes (MDPs). Correspondingly, we prompt LLMs to form an updated posterior of the unknown environment from the memory buffer (learning) and generate an optimal trajectory for multiple future steps that maximizes a value function (planning). The learning and planning subroutines are performed in an &#34;in-context&#34; manner to emulate the actor-critic update for MDPs. Our theoretical analysis proves that the novel combination of long-term reasoning and short-term acting achieves a $\\sqrt{T}$ regret. Here, $T$ denotes the number of online interactions. In particular, the regret bound highlights an intriguing interplay between the prior knowledge obtained through pretraining and the uncertainty reduction achieved by reasoning and acting. Our empirical validation shows that it outperforms various existing frameworks and achieves nearly perfect scores on a few benchmarks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.17382"
    },
    "1035d55aceab0dd79e95634189726dd8": {
        "title": "ToolTalk: Evaluating Tool-Usage in a Conversational Setting",
        "authors": [
            "Nicholas Farn",
            "Richard Shin"
        ],
        "date": "2023/11/15",
        "pdf": "http://arxiv.org/pdf/2311.10775",
        "abstract": "Large language models (LLMs) have displayed massive improvements in reasoning and decision-making skills and can hold natural conversations with users. Many recent works seek to augment LLM-based assistants with external tools so they can access private or up-to-date information and carry out actions on behalf of users. To better measure the performance of these assistants, this paper introduces ToolTalk, a benchmark consisting of complex user intents requiring multi-step tool usage specified through dialogue. ToolTalk contains 28 tools grouped into 7 plugins, and includes a complete simulated implementation of each tool, allowing for fully automated evaluation of assistants that rely on execution feedback. ToolTalk also emphasizes tools that externally affect the world rather than only tools for referencing or searching information. We evaluate GPT-3.5 and GPT-4 on ToolTalk resulting in success rates of 26% and 50% respectively. Our analysis of the errors reveals three major categories and suggests some future directions for improvement. We release ToolTalk at https://github.com/microsoft/ToolTalk.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.10775"
    },
    "54f679e78e32c2a097dbea9d822cd5b1": {
        "title": "ProAgent: From Robotic Process Automation to Agentic Process Automation",
        "authors": [
            "Yining Ye",
            "Xin Cong",
            "Shizuo Tian",
            "Jiannan Cao",
            "Hao Wang",
            "Yujia Qin",
            "Yaxi Lu",
            "Heyang Yu",
            "Huadong Wang",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2023/11/02",
        "pdf": "http://arxiv.org/pdf/2311.10751",
        "abstract": "From ancient water wheels to robotic process automation (RPA), automation technology has evolved throughout history to liberate human beings from arduous tasks. Yet, RPA struggles with tasks needing human-like intelligence, especially in elaborate design of workflow construction and dynamic decision-making in workflow execution. As Large Language Models (LLMs) have emerged human-like intelligence, this paper introduces Agentic Process Automation (APA), a groundbreaking automation paradigm using LLM-based agents for advanced automation by offloading the human labor to agents associated with construction and execution. We then instantiate ProAgent, an LLM-based agent designed to craft workflows from human instructions and make intricate decisions by coordinating specialized agents. Empirical experiments are conducted to detail its construction and execution procedure of workflow, showcasing the feasibility of APA, unveiling the possibility of a new paradigm of automation driven by agents. Our code is public at https://github.com/OpenBMB/ProAgent.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.10751"
    },
    "1a40deb82fb0656c7b0275ac70641179": {
        "title": "Testing Language Model Agents Safely in the Wild",
        "authors": [
            "Silen Naihin",
            "David Atkinson",
            "Marc Green",
            "Merwane Hamadi",
            "Craig Swift",
            "Douglas Schonholtz",
            "Adam Tauman Kalai",
            "David Bau"
        ],
        "date": "2023/11/17",
        "pdf": "http://arxiv.org/pdf/2311.10538",
        "abstract": "A prerequisite for safe autonomy-in-the-wild is safe testing-in-the-wild. Yet real-world autonomous tests face several unique safety challenges, both due to the possibility of causing harm during a test, as well as the risk of encountering new unsafe agent behavior through interactions with real-world and potentially malicious actors. We propose a framework for conducting safe autonomous agent tests on the open internet: agent actions are audited by a context-sensitive monitor that enforces a stringent safety boundary to stop an unsafe test, with suspect behavior ranked and logged to be examined by humans. We design a basic safety monitor (AgentMonitor) that is flexible enough to monitor existing LLM agents, and, using an adversarial simulated agent, we measure its ability to identify and stop unsafe situations. Then we apply the AgentMonitor on a battery of real-world tests of AutoGPT, and we identify several limitations and challenges that will face the creation of safe in-the-wild tests as autonomous agents grow more capable.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.10538"
    },
    "a395035cf81a5a9129beb1f8413fa648": {
        "title": "ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code",
        "authors": [
            "Xiangru Tang",
            "Yuliang Liu",
            "Zefan Cai",
            "Yanjun Shao",
            "Junjie Lu",
            "Yichi Zhang",
            "Zexuan Deng",
            "Helan Hu",
            "Kaikai An",
            "Ruijun Huang",
            "Shuzheng Si",
            "Sheng Chen",
            "Haozhe Zhao",
            "Liang Chen",
            "Yan Wang",
            "Tianyu Liu",
            "Zhiwei Jiang",
            "Baobao Chang",
            "Yin Fang",
            "Yujia Qin",
            "Wangchunshu Zhou",
            "Yilun Zhao",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "date": "2023/11/16",
        "pdf": "http://arxiv.org/pdf/2311.09835",
        "abstract": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs&#39; text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution. Our code, dataset, and models are available at https://github.com/gersteinlab/ML-bench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.09835"
    },
    "7d9efef02cf95a833624a14158e4ad39": {
        "title": "ProAgent: Building Proactive Cooperative Agents with Large Language Models",
        "authors": [
            "Ceyao Zhang",
            "Kaijie Yang",
            "Siyi Hu",
            "Zihao Wang",
            "Guanghe Li",
            "Yihang Sun",
            "Cheng Zhang",
            "Zhaowei Zhang",
            "Anji Liu",
            "Song-Chun Zhu",
            "Xiaojun Chang",
            "Junge Zhang",
            "Feng Yin",
            "Yitao Liang",
            "Yaodong Yang"
        ],
        "date": "2023/08/22",
        "pdf": "http://arxiv.org/pdf/2308.11339",
        "abstract": "Building agents with adaptive behavior in cooperative tasks stands as a paramount goal in the realm of multi-agent systems. Current approaches to developing cooperative agents rely primarily on learning-based methods, whose policy generalization depends heavily on the diversity of teammates they interact with during the training phase. Such reliance, however, constrains the agents&#39; capacity for strategic adaptation when cooperating with unfamiliar teammates, which becomes a significant challenge in zero-shot coordination scenarios. To address this challenge, we propose ProAgent, a novel framework that harnesses large language models (LLMs) to create proactive agents capable of dynamically adapting their behavior to enhance cooperation with teammates. ProAgent can analyze the present state, and infer the intentions of teammates from observations. It then updates its beliefs in alignment with the teammates&#39; subsequent actual behaviors. Moreover, ProAgent exhibits a high degree of modularity and interpretability, making it easily integrated into various of coordination scenarios. Experimental evaluations conducted within the Overcooked-AI environment unveil the remarkable performance superiority of ProAgent, outperforming five methods based on self-play and population-based training when cooperating with AI agents. Furthermore, in partnered with human proxy models, its performance exhibits an average improvement exceeding 10% compared to the current state-of-the-art method. For more information about our project, please visit~\\url{https://pku-proagent.github.io}.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.11339"
    },
    "7e2c67da344f12dc61a0a219ed1d7c98": {
        "title": "RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models",
        "authors": [
            "Tianhao Shen",
            "Sun Li",
            "Quan Tu",
            "Deyi Xiong"
        ],
        "date": "2023/12/26",
        "pdf": "http://arxiv.org/pdf/2312.16132",
        "abstract": "The rapid evolution of large language models necessitates effective benchmarks for evaluating their role knowledge, which is essential for establishing connections with the real world and providing more immersive interactions. This paper introduces RoleEval, a bilingual benchmark designed to assess the memorization, utilization, and reasoning capabilities of role knowledge. RoleEval comprises RoleEval-Global (including internationally recognized characters) and RoleEval-Chinese (including characters popular in China), with 6,000 Chinese-English parallel multiple-choice questions focusing on 300 influential people and fictional characters drawn from a variety of domains including celebrities, anime, comics, movies, TV series, games, and fictions. These questions cover basic knowledge and multi-hop reasoning abilities, aiming to systematically probe various aspects such as personal information, relationships, abilities, and experiences of the characters. To maintain high standards, we perform a hybrid quality check process combining both automatic and human verification, ensuring that the questions are diverse, challenging, and discriminative. Our extensive evaluations with RoleEval across various open-source and proprietary large language models, under both the zero- and few-shot settings, reveal insightful findings. Notably, while GPT-4 outperforms other models on RoleEval-Global, Chinese large language models excel on RoleEval-Chinese, highlighting significant knowledge distribution differences. We expect that RoleEval would highlight the significance of assessing role knowledge for large language models across various languages and cultural settings.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.16132"
    },
    "d180677e0a021d9f02ea25bdaf8c9f9e": {
        "title": "How Far Are LLMs from Believable AI? A Benchmark for Evaluating the Believability of Human Behavior Simulation",
        "authors": [
            "Yang Xiao",
            "Yi Cheng",
            "Jinlan Fu",
            "Jiashuo Wang",
            "Wenjie Li",
            "Pengfei Liu"
        ],
        "date": "2023/12/28",
        "pdf": "http://arxiv.org/pdf/2312.17115",
        "abstract": "In recent years, AI has demonstrated remarkable capabilities in simulating human behaviors, particularly those implemented with large language models (LLMs). However, due to the lack of systematic evaluation of LLMs&#39; simulated behaviors, the believability of LLMs among humans remains ambiguous, i.e., it is unclear which behaviors of LLMs are convincingly human-like and which need further improvements. In this work, we design SimulateBench to evaluate the believability of LLMs when simulating human behaviors. In specific, we evaluate the believability of LLMs based on two critical dimensions: 1) consistency: the extent to which LLMs can behave consistently with the given information of a human to simulate; and 2) robustness: the ability of LLMs&#39; simulated behaviors to remain robust when faced with perturbations. SimulateBench includes 65 character profiles and a total of 8,400 questions to examine LLMs&#39; simulated behaviors. Based on SimulateBench, we evaluate the performances of 10 widely used LLMs when simulating characters. The experimental results reveal that current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations in certain factors.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.17115"
    },
    "926e1a0151c4046b84210e6e81639f51": {
        "title": "FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions",
        "authors": [
            "Hyunwoo Kim",
            "Melanie Sclar",
            "Xuhui Zhou",
            "Ronan Le Bras",
            "Gunhee Kim",
            "Yejin Choi",
            "Maarten Sap"
        ],
        "date": "2023/10/24",
        "pdf": "http://arxiv.org/pdf/2310.15421",
        "abstract": "Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.15421"
    },
    "3f38895b5dc2bb39a5857ec9a34da3ac": {
        "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Wenxiang Jiao",
            "Xing Wang",
            "Yan Wang",
            "Rui Wang",
            "Yujiu Yang",
            "Shuming Shi",
            "Zhaopeng Tu"
        ],
        "date": "2023/05/30",
        "pdf": "http://arxiv.org/pdf/2305.19118",
        "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of &#34;tit for tat&#34; and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of &#34;tit for tat&#34; state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at https://github.com/Skytliang/Multi-Agents-Debate.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.19118"
    },
    "27622956eb8adf45c1c26de87fedc9ff": {
        "title": "Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback",
        "authors": [
            "Nikhil Mehta",
            "Milagro Teruel",
            "Patricio Figueroa Sanz",
            "Xin Deng",
            "Ahmed Hassan Awadallah",
            "Julia Kiseleva"
        ],
        "date": "2023/04/21",
        "pdf": "http://arxiv.org/pdf/2304.10750",
        "abstract": "Many approaches to Natural Language Processing (NLP) tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, human language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaborations. In this work, we explore these directions using the challenging task defined by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We explore multiple types of help players can give to the AI to guide it and analyze the impact of this help in AI behavior, resulting in performance improvements.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.10750"
    },
    "aa093a5d75fafc6139a1a2e4ea9ac2f8": {
        "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
        "authors": [
            "Ning Miao",
            "Yee Whye Teh",
            "Tom Rainforth"
        ],
        "date": "2023/08/01",
        "pdf": "http://arxiv.org/pdf/2308.00436",
        "abstract": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.00436"
    },
    "1b7a9bae65346bc13ade7d5f7ff8dfa7": {
        "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback",
        "authors": [
            "Bo Shen",
            "Jiaxin Zhang",
            "Taihong Chen",
            "Daoguang Zan",
            "Bing Geng",
            "An Fu",
            "Muhan Zeng",
            "Ailun Yu",
            "Jichuan Ji",
            "Jingyang Zhao",
            "Yuenan Guo",
            "Qianxiang Wang"
        ],
        "date": "2023/07/27",
        "pdf": "http://arxiv.org/pdf/2307.14936",
        "abstract": "Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&amp;Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2307.14936"
    },
    "2f642f73495732f56d575ca9c0ecb9d2": {
        "title": "Self-Refine: Iterative Refinement with Self-Feedback",
        "authors": [
            "Aman Madaan",
            "Niket Tandon",
            "Prakhar Gupta",
            "Skyler Hallinan",
            "Luyu Gao",
            "Sarah Wiegreffe",
            "Uri Alon",
            "Nouha Dziri",
            "Shrimai Prabhumoye",
            "Yiming Yang",
            "Shashank Gupta",
            "Bodhisattwa Prasad Majumder",
            "Katherine Hermann",
            "Sean Welleck",
            "Amir Yazdanbakhsh",
            "Peter Clark"
        ],
        "date": "2023/03/30",
        "pdf": "http://arxiv.org/pdf/2303.17651",
        "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2303.17651"
    },
    "64c8f9b19403f164ca1c339b1522f6c8": {
        "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
        "authors": [
            "Haotian Sun",
            "Yuchen Zhuang",
            "Lingkai Kong",
            "Bo Dai",
            "Chao Zhang"
        ],
        "date": "2023/05/26",
        "pdf": "http://arxiv.org/pdf/2305.16653",
        "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.16653"
    },
    "915f6ab291da9364b0bf8b61410afa01": {
        "title": "Making Language Models Better Tool Learners with Execution Feedback",
        "authors": [
            "Shuofei Qiao",
            "Honghao Gui",
            "Chengfei Lv",
            "Qianghuai Jia",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "date": "2023/05/22",
        "pdf": "http://arxiv.org/pdf/2305.13068",
        "abstract": "Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed by further analysis, show that TRICE can make the large language model selectively use tools by improving the accuracy of tool usage while enhancing insufficient tool learning and mitigating excessive reliance on tools. Code is available at https://github.com/zjunlp/TRICE.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.13068"
    },
    "5554888e178a16e7d530118c0f7ad123": {
        "title": "Teaching Large Language Models to Self-Debug",
        "authors": [
            "Xinyun Chen",
            "Maxwell Lin",
            "Nathanael Schärli",
            "Denny Zhou"
        ],
        "date": "2023/04/11",
        "pdf": "http://arxiv.org/pdf/2304.05128",
        "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.05128"
    },
    "141e07821bd167bc5ce5902645300d40": {
        "title": "The ART of LLM Refinement: Ask, Refine, and Trust",
        "authors": [
            "Kumar Shridhar",
            "Koustuv Sinha",
            "Andrew Cohen",
            "Tianlu Wang",
            "Ping Yu",
            "Ram Pasunuru",
            "Mrinmaya Sachan",
            "Jason Weston",
            "Asli Celikyilmaz"
        ],
        "date": "2023/11/14",
        "pdf": "http://arxiv.org/pdf/2311.07961",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations? A popular concept, referred to as self-refinement, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with refinement objective called ART: Ask, Refine, and Trust, which asks necessary questions to decide when an LLM should refine its output, and either affirm or withhold trust in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), ART achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We also demonstrate the benefit of using smaller models to make refinement decisions as a cost-effective alternative to fine-tuning a larger model.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.07961"
    },
    "20c18a2ace6082d92a1fe9e1185843ba": {
        "title": "Learning From Mistakes Makes LLM Better Reasoner",
        "authors": [
            "Shengnan An",
            "Zexiong Ma",
            "Zeqi Lin",
            "Nanning Zheng",
            "Jian-Guang Lou",
            "Weizhu Chen"
        ],
        "date": "2023/10/31",
        "pdf": "http://arxiv.org/pdf/2310.20689",
        "abstract": "Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a &#39;&#39;corrector&#39;&#39; to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that LEMA effectively improves CoT-alone fine-tuning. Our further ablations shed light on the non-homogeneous effectiveness between CoT data and correction data. These results suggest a significant potential for LLMs to improve through learning from their mistakes. Our code, models and prompts are publicly available at https://github.com/microsoft/LEMA.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.20689"
    },
    "3b37d03718c40fac09e90662da286074": {
        "title": "CB2: Collaborative Natural Language Interaction Research Platform",
        "authors": [
            "Jacob Sharf",
            "Mustafa Omer Gul",
            "Yoav Artzi"
        ],
        "date": "2023/03/14",
        "pdf": "http://arxiv.org/pdf/2303.08127",
        "abstract": "CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2303.08127"
    },
    "12e797c33d37e99a30aa9bd979aa2b31": {
        "title": "Skill Reinforcement Learning and Planning for Open-World Long-Horizon Tasks",
        "authors": [
            "Haoqi Yuan",
            "Chi Zhang",
            "Hongcheng Wang",
            "Feiyang Xie",
            "Penglin Cai",
            "Hao Dong",
            "Zongqing Lu"
        ],
        "date": "2023/03/29",
        "pdf": "http://arxiv.org/pdf/2303.16563",
        "abstract": "We study building multi-task agents in open-world environments. Without human demonstrations, learning to accomplish long-horizon tasks in a large open-world environment with reinforcement learning (RL) is extremely inefficient. To tackle this challenge, we convert the multi-task learning problem into learning basic skills and planning over the skills. Using the popular open-world game Minecraft as the testbed, we propose three types of fine-grained basic skills, and use RL with intrinsic rewards to acquire skills. A novel Finding-skill that performs exploration to find diverse items provides better initialization for other skills, improving the sample efficiency for skill learning. In skill planning, we leverage the prior knowledge in Large Language Models to find the relationships between skills and build a skill graph. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 40 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines by a large margin and is the most sample-efficient demonstration-free RL method to solve Minecraft Tech Tree tasks. The project&#39;s website and code can be found at https://sites.google.com/view/plan4mc.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2303.16563"
    },
    "4977a28eb34673b1a874b008ab45aa6a": {
        "title": "Knowledge-enhanced Agents for Interactive Text Games",
        "authors": [
            "Prateek Chhikara",
            "Jiarui Zhang",
            "Filip Ilievski",
            "Jonathan Francis",
            "Kaixin Ma"
        ],
        "date": "2023/05/08",
        "pdf": "http://arxiv.org/pdf/2305.05091",
        "abstract": "Communication via natural language is a key aspect of machine intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. Significant progress has been made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding. Yet, various sequential interactive tasks, as in text-based games, have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a knowledge-injection framework for improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports two representative model classes: reinforcement learning agents and language model agents. Furthermore, we devise multiple injection strategies for the above domain knowledge types and agent architectures, including injection via knowledge graphs and augmentation of the existing input encoding strategies. We experiment with four models on the 10 tasks in the ScienceWorld text-based game environment, to illustrate the impact of knowledge injection on various model configurations and challenging task settings. Our findings provide crucial insights into the interplay between task properties, model architectures, and domain knowledge for interactive contexts.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.05091"
    },
    "ec37317b3579158b46de6d811f54021b": {
        "title": "Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory",
        "authors": [
            "Xizhou Zhu",
            "Yuntao Chen",
            "Hao Tian",
            "Chenxin Tao",
            "Weijie Su",
            "Chenyu Yang",
            "Gao Huang",
            "Bin Li",
            "Lewei Lu",
            "Xiaogang Wang",
            "Yu Qiao",
            "Zhaoxiang Zhang",
            "Jifeng Dai"
        ],
        "date": "2023/05/25",
        "pdf": "http://arxiv.org/pdf/2305.17144",
        "abstract": "The captivating realm of Minecraft has attracted substantial research interest in recent years, serving as a rich platform for developing intelligent agents capable of functioning in open-world environments. However, the current research landscape predominantly focuses on specific objectives, such as the popular &#34;ObtainDiamond&#34; task, and has not yet shown effective generalization to a broader spectrum of tasks. Furthermore, the current leading success rate for the &#34;ObtainDiamond&#34; task stands at around 20%, highlighting the limitations of Reinforcement Learning (RL) based controllers used in existing methods. To tackle these challenges, we introduce Ghost in the Minecraft (GITM), a novel framework integrates Large Language Models (LLMs) with text-based knowledge and memory, aiming to create Generally Capable Agents (GCAs) in Minecraft. These agents, equipped with the logic and common sense capabilities of LLMs, can skillfully navigate complex, sparse-reward environments with text-based interactions. We develop a set of structured actions and leverage LLMs to generate action plans for the agents to execute. The resulting LLM-based agent markedly surpasses previous methods, achieving a remarkable improvement of +47.5% in success rate on the &#34;ObtainDiamond&#34; task, demonstrating superior robustness compared to traditional RL-based controllers. Notably, our agent is the first to procure all items in the Minecraft Overworld technology tree, demonstrating its extensive capabilities. GITM does not need any GPU for training, but a single CPU node with 32 CPU cores is enough. This research shows the potential of LLMs in developing capable agents for handling long-horizon, complex tasks and adapting to uncertainties in open-world environments. See the project website at https://github.com/OpenGVLab/GITM.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.17144"
    },
    "d0aa2508a83304e8baf09ffdf76be63c": {
        "title": "Playing repeated games with Large Language Models",
        "authors": [
            "Elif Akata",
            "Lion Schulz",
            "Julian Coda-Forno",
            "Seong Joon Oh",
            "Matthias Bethge",
            "Eric Schulz"
        ],
        "date": "2023/05/26",
        "pdf": "http://arxiv.org/pdf/2305.16867",
        "abstract": "Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM&#39;s cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures. In a large set of two players-two strategies games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner&#39;s Dilemma family. However, they behave sub-optimally in games that require coordination. We, therefore, further focus on two games from these distinct families. In the canonical iterated Prisoner&#39;s Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behavior of the simple convention to alternate between options. We verify that these behavioral signatures are stable across robustness checks. Finally, we show how GPT-4&#39;s behavior can be modified by providing further information about the other player as well as by asking it to predict the other player&#39;s actions before making a choice. These results enrich our understanding of LLM&#39;s social behavior and pave the way for a behavioral game theory for machines.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.16867"
    },
    "9746aebf3641e1f07a29559fa01a166b": {
        "title": "Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback",
        "authors": [
            "Yao Fu",
            "Hao Peng",
            "Tushar Khot",
            "Mirella Lapata"
        ],
        "date": "2023/05/17",
        "pdf": "http://arxiv.org/pdf/2305.10142",
        "abstract": "We study whether multiple large language models (LLMs) can autonomously improve each other in a negotiation game by playing, reflecting, and criticizing. We are interested in this question because if LLMs were able to improve each other, it would imply the possibility of creating strong AI agents with minimal human intervention. We ask two LLMs to negotiate with each other, playing the roles of a buyer and a seller, respectively. They aim to reach a deal with the buyer targeting a lower price and the seller a higher one. A third language model, playing the critic, provides feedback to a player to improve the player&#39;s negotiation strategies. We let the two agents play multiple rounds, using previous negotiation history and AI feedback as in-context demonstrations to improve the model&#39;s negotiation strategy iteratively. We use different LLMs (GPT and Claude) for different roles and use the deal price as the evaluation metric. Our experiments reveal multiple intriguing findings: (1) Only a subset of the language models we consider can self-play and improve the deal price from AI feedback, weaker models either do not understand the game&#39;s rules or cannot incorporate AI feedback for further improvement. (2) Models&#39; abilities to learn from the feedback differ when playing different roles. For example, it is harder for Claude-instant to improve as the buyer than as the seller. (3) When unrolling the game to multiple rounds, stronger agents can consistently improve their performance by meaningfully using previous experiences and iterative AI feedback, yet have a higher risk of breaking the deal. We hope our work provides insightful initial explorations of having models autonomously improve each other with game playing and AI feedback.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.10142"
    },
    "4ad0c8c7e6154544a3aee3d3680d9765": {
        "title": "Recursive Metropolis-Hastings Naming Game: Symbol Emergence in a Multi-agent System based on Probabilistic Generative Models",
        "authors": [
            "Jun Inukai",
            "Tadahiro Taniguchi",
            "Akira Taniguchi",
            "Yoshinobu Hagiwara"
        ],
        "date": "2023/05/31",
        "pdf": "http://arxiv.org/pdf/2305.19761",
        "abstract": "In the studies on symbol emergence and emergent communication in a population of agents, a computational model was employed in which agents participate in various language games. Among these, the Metropolis-Hastings naming game (MHNG) possesses a notable mathematical property: symbol emergence through MHNG is proven to be a decentralized Bayesian inference of representations shared by the agents. However, the previously proposed MHNG is limited to a two-agent scenario. This paper extends MHNG to an N-agent scenario. The main contributions of this paper are twofold: (1) we propose the recursive Metropolis-Hastings naming game (RMHNG) as an N-agent version of MHNG and demonstrate that RMHNG is an approximate Bayesian inference method for the posterior distribution over a latent variable shared by agents, similar to MHNG; and (2) we empirically evaluate the performance of RMHNG on synthetic and real image data, enabling multiple agents to develop and share a symbol system. Furthermore, we introduce two types of approximations -- one-sample and limited-length -- to reduce computational complexity while maintaining the ability to explain communication in a population of agents. The experimental findings showcased the efficacy of RMHNG as a decentralized Bayesian inference for approximating the posterior distribution concerning latent variables, which are jointly shared among agents, akin to MHNG. Moreover, the utilization of RMHNG elucidated the agents&#39; capacity to exchange symbols. Furthermore, the study discovered that even the computationally simplified version of RMHNG could enable symbols to emerge among the agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.19761"
    },
    "7faa5c7e074c69eacba3978164125eb2": {
        "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
        "authors": [
            "Guanzhi Wang",
            "Yuqi Xie",
            "Yunfan Jiang",
            "Ajay Mandlekar",
            "Chaowei Xiao",
            "Yuke Zhu",
            "Linxi Fan",
            "Anima Anandkumar"
        ],
        "date": "2023/05/25",
        "pdf": "http://arxiv.org/pdf/2305.16291",
        "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent&#39;s abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.16291"
    },
    "691bf33064019cb4442e3758f44807c8": {
        "title": "An Appraisal-Based Chain-Of-Emotion Architecture for Affective Language Model Game Agents",
        "authors": [
            "Maximilian Croissant",
            "Madeleine Frister",
            "Guy Schofield",
            "Cade McCall"
        ],
        "date": "2023/09/10",
        "pdf": "http://arxiv.org/pdf/2309.05076",
        "abstract": "The development of believable, natural, and interactive digital artificial agents is a field of growing interest. Theoretical uncertainties and technical barriers present considerable challenges to the field, particularly with regards to developing agents that effectively simulate human emotions. Large language models (LLMs) might address these issues by tapping common patterns in situational appraisal. In three empirical experiments, this study tests the capabilities of LLMs to solve emotional intelligence tasks and to simulate emotions. It presents and evaluates a new chain-of-emotion architecture for emotion simulation within video games, based on psychological appraisal research. Results show that it outperforms standard LLM architectures on a range of user experience and content analysis metrics. This study therefore provides early evidence of how to construct and test affective agents based on cognitive processes represented in language models.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.05076"
    },
    "2784f2a2b88063c67286262773571a65": {
        "title": "Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf",
        "authors": [
            "Yuzhuang Xu",
            "Shuo Wang",
            "Peng Li",
            "Fuwen Luo",
            "Xiaolong Wang",
            "Weidong Liu",
            "Yang Liu"
        ],
        "date": "2023/09/09",
        "pdf": "http://arxiv.org/pdf/2309.04658",
        "abstract": "Communication games, which we refer to as incomplete information games that heavily depend on natural language communication, hold significant research value in fields such as economics, social science, and artificial intelligence. In this work, we explore the problem of how to engage large language models (LLMs) in communication games, and in response, propose a tuning-free framework. Our approach keeps LLMs frozen, and relies on the retrieval and reflection on past communications and experiences for improvement. An empirical study on the representative and widely-studied communication game, ``Werewolf&#39;&#39;, demonstrates that our framework can effectively play Werewolf game without tuning the parameters of the LLMs. More importantly, strategic behaviors begin to emerge in our experiments, suggesting that it will be a fruitful journey to engage LLMs in communication games and associated domains.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.04658"
    },
    "fa0d2b5c94ed1f859fe93477ed2d3f79": {
        "title": "Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4",
        "authors": [
            "Jiaxian Guo",
            "Bo Yang",
            "Paul Yoo",
            "Bill Yuchen Lin",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "date": "2023/09/29",
        "pdf": "http://arxiv.org/pdf/2309.17277",
        "abstract": "Unlike perfect information games, where all elements are known to every player, imperfect information games emulate the real-world complexities of decision-making under uncertain or incomplete information. GPT-4, the recent breakthrough in large language models (LLMs) trained on massive passive data, is notable for its knowledge retrieval and reasoning abilities. This paper delves into the applicability of GPT-4&#39;s learned knowledge for imperfect information games. To achieve this, we introduce \\textbf{Suspicion-Agent}, an innovative agent that leverages GPT-4&#39;s capabilities for performing in imperfect information games. With proper prompt engineering to achieve different functions, Suspicion-Agent based on GPT-4 demonstrates remarkable adaptability across a range of imperfect information card games. Importantly, GPT-4 displays a strong high-order theory of mind (ToM) capacity, meaning it can understand others and intentionally impact others&#39; behavior. Leveraging this, we design a planning strategy that enables GPT-4 to competently play against different opponents, adapting its gameplay style as needed, while requiring only the game rules and descriptions of observations as input. In the experiments, we qualitatively showcase the capabilities of Suspicion-Agent across three different imperfect information games and then quantitatively evaluate it in Leduc Hold&#39;em. The results show that Suspicion-Agent can potentially outperform traditional algorithms designed for imperfect information games, without any specialized training or examples. In order to encourage and foster deeper insights within the community, we make our game-related data publicly available.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.17277"
    },
    "e0a2fda9dfc3609da9420d67063733e8": {
        "title": "Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis",
        "authors": [
            "Akshat Gupta"
        ],
        "date": "2023/08/23",
        "pdf": "http://arxiv.org/pdf/2308.12466",
        "abstract": "Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games, and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players. Profitable strategies in poker are evaluated in expectations over large samples. Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is a more advanced poker player than ChatGPT. This exploration then sheds light on the divergent poker tactics of the two models: ChatGPT&#39;s conservativeness juxtaposed against GPT-4&#39;s aggression. In poker vernacular, when tasked to play GTO poker, ChatGPT plays like a nit, which means that it has a propensity to only engage with premium hands and folds a majority of hands. When subjected to the same directive, GPT-4 plays like a maniac, showcasing a loose and aggressive style of play. Both strategies, although relatively advanced, are not game theory optimal.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.12466"
    },
    "547060c869101463eaac2740b4922d60": {
        "title": "Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models",
        "authors": [
            "Tian Liang",
            "Zhiwei He",
            "Jen-tse Huang",
            "Wenxuan Wang",
            "Wenxiang Jiao",
            "Rui Wang",
            "Yujiu Yang",
            "Zhaopeng Tu",
            "Shuming Shi",
            "Xing Wang"
        ],
        "date": "2023/10/31",
        "pdf": "http://arxiv.org/pdf/2310.20499",
        "abstract": "The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy&#39;&#39;, we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players&#39; descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs&#39; expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs&#39; intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs&#39; human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.20499"
    },
    "6ef9581ab6919de277369293f4d554ec": {
        "title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game",
        "authors": [
            "Zijing Shi",
            "Meng Fang",
            "Shunfeng Zheng",
            "Shilong Deng",
            "Ling Chen",
            "Yali Du"
        ],
        "date": "2023/12/29",
        "pdf": "http://arxiv.org/pdf/2312.17515",
        "abstract": "Multi-agent collaboration with Large Language Models (LLMs) demonstrates proficiency in basic tasks, yet its efficiency in more complex scenarios remains unexplored. In gaming environments, these agents often face situations without established coordination protocols, requiring them to make intelligent inferences about teammates from limited data. This problem motivates the area of ad hoc teamwork, in which an agent may potentially cooperate with a variety of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork problem where the agent operates in an environment driven by natural language. Our findings reveal the potential of LLM agents in team collaboration, highlighting issues related to hallucinations in communication. To address this issue, we develop CodeAct, a general agent that equips LLM with enhanced memory and code-driven reasoning, enabling the repurposing of partial information for rapid adaptation to new teammates.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.17515"
    },
    "d0e04ab2d37bcd9b9f9980cddd5901c6": {
        "title": "Emergent and Predictable Memorization in Large Language Models",
        "authors": [
            "Stella Biderman",
            "USVSN Sai Prashanth",
            "Lintang Sutawika",
            "Hailey Schoelkopf",
            "Quentin Anthony",
            "Shivanshu Purohit",
            "Edward Raff"
        ],
        "date": "2023/04/21",
        "pdf": "http://arxiv.org/pdf/2304.11158",
        "abstract": "Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model&#39;s memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model&#39;s full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.11158"
    },
    "3670318a3140c7ef30bd05aba962ac17": {
        "title": "ChatLog: Carefully Evaluating the Evolution of ChatGPT Across Time",
        "authors": [
            "Shangqing Tu",
            "Chunyang Li",
            "Jifan Yu",
            "Xiaozhi Wang",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2023/04/27",
        "pdf": "http://arxiv.org/pdf/2304.14106",
        "abstract": "ChatGPT has achieved great success and can be considered to have acquired an infrastructural status. There are abundant works for evaluating ChatGPT on benchmarks. However, existing benchmarks encounter two challenges: (1) Disregard for periodical evaluation and (2) Lack of fine-grained features. In this paper, we construct ChatLog, an ever-updating dataset with large-scale records of diverse long-form ChatGPT responses for 21 NLP benchmarks from March, 2023 to now. We conduct a comprehensive performance evaluation to find that most capabilities of ChatGPT improve over time except for some abilities, and there exists a step-wise evolving pattern of ChatGPT. We further analyze the inherent characteristics of ChatGPT by extracting the knowledge and linguistic features. We find some stable features that stay unchanged and apply them on the detection of ChatGPT-generated texts to improve the robustness of cross-version detection. We will continuously maintain our project at \\url{https://github.com/THU-KEG/ChatLog/}.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.14106"
    },
    "f859fcfade304101ad2d098deeee606f": {
        "title": "Learning to Reason and Memorize with Self-Notes",
        "authors": [
            "Jack Lanchantin",
            "Shubham Toshniwal",
            "Jason Weston",
            "Arthur Szlam",
            "Sainbayar Sukhbaatar"
        ],
        "date": "2023/05/01",
        "pdf": "http://arxiv.org/pdf/2305.00833",
        "abstract": "Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.00833"
    },
    "c63eb48acb4856602198c3e3b36201fc": {
        "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
        "authors": [
            "Wanjun Zhong",
            "Lianghong Guo",
            "Qiqi Gao",
            "He Ye",
            "Yanlin Wang"
        ],
        "date": "2023/05/17",
        "pdf": "http://arxiv.org/pdf/2305.10250",
        "abstract": "Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a human-like memory mechanism. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models like ChatGLM. We exemplify application of MemoryBank through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialogs, SiliconFriend displays heightened empathy in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.10250"
    },
    "6a6f1235f7ec4b77cf44a2ecefe0220f": {
        "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
        "authors": [
            "Ali Modarressi",
            "Ayyoob Imani",
            "Mohsen Fayyaz",
            "Hinrich Schütze"
        ],
        "date": "2023/05/23",
        "pdf": "http://arxiv.org/pdf/2305.14322",
        "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP) through their extensive parameters and comprehensive data utilization. However, existing LLMs lack a dedicated memory unit, limiting their ability to explicitly store and retrieve knowledge for various tasks. In this paper, we propose RET-LLM a novel framework that equips LLMs with a general write-read memory unit, allowing them to extract, store, and recall knowledge from the text as needed for task performance. Inspired by Davidsonian semantics theory, we extract and save knowledge in the form of triplets. The memory unit is designed to be scalable, aggregatable, updatable, and interpretable. Through qualitative evaluations, we demonstrate the superiority of our proposed framework over baseline approaches in question answering tasks. Moreover, our framework exhibits robust performance in handling temporal-based question answering tasks, showcasing its ability to effectively manage time-dependent information.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.14322"
    },
    "87d1ff46ce8a6a6881737c6bb6077623": {
        "title": "Enhancing Large Language Model with Self-Controlled Memory Framework",
        "authors": [
            "Bing Wang",
            "Xinnian Liang",
            "Jian Yang",
            "Hui Huang",
            "Shuangzhi Wu",
            "Peihao Wu",
            "Lu Lu",
            "Zejun Ma",
            "Zhoujun Li"
        ],
        "date": "2023/04/26",
        "pdf": "http://arxiv.org/pdf/2304.13343",
        "abstract": "Large Language Models (LLMs) are constrained by their inability to process lengthy inputs, resulting in the loss of critical historical information. To address this limitation, in this paper, we propose the Self-Controlled Memory (SCM) framework to enhance the ability of LLMs to maintain long-term memory and recall relevant information. Our SCM framework comprises three key components: an LLM-based agent serving as the backbone of the framework, a memory stream storing agent memories, and a memory controller updating memories and determining when and how to utilize memories from memory stream. Additionally, the proposed SCM is able to process ultra-long texts without any modification or fine-tuning, which can integrate with any instruction following LLMs in a plug-and-play paradigm. Furthermore, we annotate a dataset to evaluate the effectiveness of SCM for handling lengthy inputs. The annotated dataset covers three tasks: long-term dialogues, book summarization, and meeting summarization. Experimental results demonstrate that our method achieves better retrieval recall and generates more informative responses compared to competitive baselines in long-term dialogues. (https://github.com/wbbeyourself/SCM4LLMs)",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.13343"
    },
    "b20784f97a6a9622b19766fb4d07d924": {
        "title": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings",
        "authors": [
            "Shibo Hao",
            "Tianyang Liu",
            "Zhen Wang",
            "Zhiting Hu"
        ],
        "date": "2023/05/19",
        "pdf": "http://arxiv.org/pdf/2305.11554",
        "abstract": "Augmenting large language models (LLMs) with external tools has emerged as a promising approach to solving complex problems. However, traditional methods, which finetune LLMs with tool demonstration data, can be both costly and restricted to a predefined set of tools. Recent in-context learning paradigm alleviates these issues, but the limited context length only allows for a few shots of demonstrations, leading to suboptimal understandings of the tools. Moreover, when there are numerous tools to choose from, in-context learning could completely fail to work. In this paper, we propose an alternative approach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our approach represents each $\\underline{tool}$ as a to$\\underline{ken}$ ($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the same way as generating a regular word token. Once a toolken is triggered, the LLM is prompted to complete arguments for the tool to execute. ToolkenGPT offers the flexibility to plug in an arbitrary number of tools by expanding the set of toolkens on the fly. In addition, it improves tool use by allowing extensive demonstration data for learning the toolken embeddings. In diverse domains, including numerical reasoning, knowledge-based question answering, and embodied plan generation, our approach effectively augments LLMs with tools and substantially outperforms various latest baselines. ToolkenGPT demonstrates the promising ability to use relevant tools from a large tool set in complex scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.11554"
    },
    "8c6217168c689b7097c12b1d55262252": {
        "title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
        "authors": [
            "Chenxu Hu",
            "Jie Fu",
            "Chenzhuang Du",
            "Simian Luo",
            "Junbo Zhao",
            "Hang Zhao"
        ],
        "date": "2023/06/06",
        "pdf": "http://arxiv.org/pdf/2306.03901",
        "abstract": "Large language models (LLMs) with memory are computationally universal. However, mainstream LLMs are not taking full advantage of memory, and the designs are heavily influenced by biological brains. Due to their approximate nature and proneness to the accumulation of errors, conventional neural memory mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we seek inspiration from modern computer architectures to augment LLMs with symbolic memory for complex multi-hop reasoning. Such a symbolic memory framework is instantiated as an LLM and a set of SQL databases, where the LLM generates SQL instructions to manipulate the SQL databases. We validate the effectiveness of the proposed memory framework on a synthetic dataset requiring complex reasoning. The project website is available at https://chatdatabase.github.io/ .",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2306.03901"
    },
    "d701c2770312ea342e7de91540243687": {
        "title": "CLIN: A Continually Learning Language Agent for Rapid Task Adaptation and Generalization",
        "authors": [
            "Bodhisattwa Prasad Majumder",
            "Bhavana Dalvi Mishra",
            "Peter Jansen",
            "Oyvind Tafjord",
            "Niket Tandon",
            "Li Zhang",
            "Chris Callison-Burch",
            "Peter Clark"
        ],
        "date": "2023/10/16",
        "pdf": "http://arxiv.org/pdf/2310.10134",
        "abstract": "Language agents have shown some ability to interact with an external environment, e.g., a virtual world such as ScienceWorld, to perform complex tasks, e.g., growing a plant, without the startup costs of reinforcement learning. However, despite their zero-shot capabilities, these agents to date do not continually improve over time beyond performance refinement on a specific task. Here we present CLIN, the first language-based agent to achieve this, so that it continually improves over multiple trials, including when both the environment and task are varied, and without requiring parameter updates. Our approach is to use a persistent, dynamic, textual memory centered on causal abstractions (rather than general &#34;helpful hints&#34;) that is regularly updated after each trial so that the agent gradually learns useful knowledge for new trials. In the ScienceWorld benchmark, CLIN is able to continually improve on repeated trials on the same task and environment, outperforming state-of-the-art reflective language agents like Reflexion by 23 absolute points. CLIN can also transfer its learning to new environments (or new tasks), improving its zero-shot performance by 4 points (13 for new tasks) and can further improve performance there through continual memory updates, enhancing performance by an additional 17 points (7 for new tasks). This suggests a new architecture for agents built on frozen models that can still continually and rapidly improve over time.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.10134"
    },
    "551fee4788f8898c8d1be8f00a632812": {
        "title": "Empowering Working Memory for Large Language Model Agents",
        "authors": [
            "Jing Guo",
            "Nan Li",
            "Jianchuan Qi",
            "Hang Yang",
            "Ruiqiao Li",
            "Yuzhen Feng",
            "Si Zhang",
            "Ming Xu"
        ],
        "date": "2023/12/22",
        "pdf": "http://arxiv.org/pdf/2312.17259",
        "abstract": "Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology&#39;s working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.17259"
    },
    "5f7a4f7a1540265fee692f647846c5db": {
        "title": "Personalized Large Language Model Assistant with Evolving Conditional Memory",
        "authors": [
            "Ruifeng Yuan",
            "Shichao Sun",
            "Yongqi Li",
            "Zili Wang",
            "Ziqiang Cao",
            "Wenjie Li"
        ],
        "date": "2023/12/22",
        "pdf": "http://arxiv.org/pdf/2312.17257",
        "abstract": "With the rapid development of large language models, AI assistants like ChatGPT have become increasingly integrated into people&#39;s works and lives but are limited in personalized services. In this paper, we present a plug-and-play framework that could facilitate personalized large language model assistants with evolving conditional memory. The personalized assistant focuses on intelligently preserving the knowledge and experience from the history dialogue with the user, which can be applied to future tailored responses that better align with the user&#39;s preferences. Generally, the assistant generates a set of records from the dialogue dialogue, stores them in a memory bank, and retrieves related memory to improve the quality of the response. For the crucial memory design, we explore different ways of constructing the memory and propose a new memorizing mechanism named conditional memory. We also investigate the retrieval and usage of memory in the generation process. We build the first benchmark to evaluate personalized assistants&#39; ability from three aspects. The experimental results illustrate the effectiveness of our method.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.17257"
    },
    "5590ade4da00d9823d9b5cd9a26936eb": {
        "title": "InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent",
        "authors": [
            "Po-Lin Chen",
            "Cheng-Shang Chang"
        ],
        "date": "2023/08/03",
        "pdf": "http://arxiv.org/pdf/2308.01552",
        "abstract": "This research paper delves into the integration of OpenAI&#39;s ChatGPT into embodied agent systems, evaluating its influence on interactive decision-making benchmark. Drawing a parallel to the concept of people assuming roles according to their unique strengths, we introduce InterAct. In this approach, we feed ChatGPT with varied prompts, assigning it a numerous roles like a checker and a sorter, then integrating them with the original language model. Our research shows a remarkable success rate of 98% in AlfWorld, which consists of 6 different tasks in a simulated household environment, emphasizing the significance of proficient prompt engineering. The results highlight ChatGPT&#39;s competence in comprehending and performing intricate tasks effectively in real-world settings, thus paving the way for further advancements in task planning.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.01552"
    },
    "9eff80228411b7637537c841822c6d9e": {
        "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
        "authors": [
            "Sirui Hong",
            "Mingchen Zhuge",
            "Jiaqi Chen",
            "Xiawu Zheng",
            "Yuheng Cheng",
            "Ceyao Zhang",
            "Jinlin Wang",
            "Zili Wang",
            "Steven Ka Shing Yau",
            "Zijuan Lin",
            "Liyang Zhou",
            "Chenyu Ran",
            "Lingfeng Xiao",
            "Chenglin Wu",
            "Jürgen Schmidhuber"
        ],
        "date": "2023/08/01",
        "pdf": "http://arxiv.org/pdf/2308.00352",
        "abstract": "Remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Existing LLM-based multi-agent systems can already solve simple dialogue tasks. Solutions to more complex tasks, however, are complicated through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems. Our project can be found at https://github.com/geekan/MetaGPT",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.00352"
    },
    "2eac0f4e2491bfdec954e0d07a70a650": {
        "title": "ChatDev: Communicative Agents for Software Development",
        "authors": [
            "Chen Qian",
            "Wei Liu",
            "Hongzhang Liu",
            "Nuo Chen",
            "Yufan Dang",
            "Jiahao Li",
            "Cheng Yang",
            "Weize Chen",
            "Yusheng Su",
            "Xin Cong",
            "Juyuan Xu",
            "Dahai Li",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2023/07/16",
        "pdf": "http://arxiv.org/pdf/2307.07924",
        "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper, we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models (LLMs) are guided in what to communicate (via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishing language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2307.07924"
    },
    "8b49604f02c96ae0a94969b9d0375cb9": {
        "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
        "authors": [
            "Zhenhailong Wang",
            "Shaoguang Mao",
            "Wenshan Wu",
            "Tao Ge",
            "Furu Wei",
            "Heng Ji"
        ],
        "date": "2023/07/11",
        "pdf": "http://arxiv.org/pdf/2307.05300",
        "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds&#39; strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2307.05300"
    },
    "35cb79b6dd5ae58f6f00de3fb5c70d29": {
        "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
        "authors": [
            "Hongxin Zhang",
            "Weihua Du",
            "Jiaming Shan",
            "Qinhong Zhou",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Tianmin Shu",
            "Chuang Gan"
        ],
        "date": "2023/07/05",
        "pdf": "http://arxiv.org/pdf/2307.02485",
        "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2307.02485"
    },
    "8064cabdde186f6c901cd321e0c1e117": {
        "title": "Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents",
        "authors": [
            "Yashar Talebirad",
            "Amirhossein Nadiri"
        ],
        "date": "2023/06/05",
        "pdf": "http://arxiv.org/pdf/2306.03314",
        "abstract": "In this paper, we present a novel framework for enhancing the capabilities of large language models (LLMs) by leveraging the power of multi-agent systems. Our framework introduces a collaborative environment where multiple intelligent agent components, each with distinctive attributes and roles, work together to handle complex tasks more efficiently and effectively. We demonstrate the practicality and versatility of our framework through case studies in artificial general intelligence (AGI), specifically focusing on the Auto-GPT and BabyAGI models. We also examine the &#34;Gorilla&#34; model, which integrates external APIs into the LLM. Our framework addresses limitations and challenges such as looping issues, security risks, scalability, system evaluation, and ethical considerations. By modeling various domains such as courtroom simulations and software development scenarios, we showcase the potential applications and benefits of our proposed multi-agent system. Our framework provides an avenue for advancing the capabilities and performance of LLMs through collaboration and knowledge exchange among intelligent agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2306.03314"
    },
    "5073802d8fc8727b178df4d11b1f3cef": {
        "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
        "authors": [
            "Weize Chen",
            "Yusheng Su",
            "Jingwei Zuo",
            "Cheng Yang",
            "Chenfei Yuan",
            "Chi-Min Chan",
            "Heyang Yu",
            "Yaxi Lu",
            "Yi-Hsin Hung",
            "Chen Qian",
            "Yujia Qin",
            "Xin Cong",
            "Ruobing Xie",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Jie Zhou"
        ],
        "date": "2023/08/21",
        "pdf": "http://arxiv.org/pdf/2308.10848",
        "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \\framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \\framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \\framework will soon be released at \\url{https://github.com/OpenBMB/AgentVerse}.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.10848"
    },
    "eecbaf3e56b9f6a2e8c2abf1201ee2bb": {
        "title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
        "authors": [
            "Yuan Li",
            "Yixuan Zhang",
            "Lichao Sun"
        ],
        "date": "2023/10/10",
        "pdf": "http://arxiv.org/pdf/2310.06500",
        "abstract": "Significant advancements have occurred in the application of Large Language Models (LLMs) for various tasks and social simulations. Despite this, their capacities to coordinate within task-oriented social contexts are under-explored. Such capabilities are crucial if LLMs are to effectively mimic human-like social behavior and produce meaningful results. To bridge this gap, we introduce collaborative generative agents, endowing LLM-based Agents with consistent behavior patterns and task-solving abilities. We situate these agents in a simulated job fair environment as a case study to scrutinize their coordination skills. We propose a novel framework that equips collaborative generative agents with human-like reasoning abilities and specialized skills. Our evaluation demonstrates that these agents show promising performance. However, we also uncover limitations that hinder their effectiveness in more complex coordination tasks. Our work provides valuable insights into the role and evolution of LLMs in task-oriented social simulations.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.06500"
    },
    "6e9ff82716ae89e1a364697a663541a9": {
        "title": "Learning to Coordinate with Anyone",
        "authors": [
            "Lei Yuan",
            "Lihe Li",
            "Ziqian Zhang",
            "Feng Chen",
            "Tianyi Zhang",
            "Cong Guan",
            "Yang Yu",
            "Zhi-Hua Zhou"
        ],
        "date": "2023/09/22",
        "pdf": "http://arxiv.org/pdf/2309.12633",
        "abstract": "In open multi-agent environments, the agents may encounter unexpected teammates. Classical multi-agent learning approaches train agents that can only coordinate with seen teammates. Recent studies attempted to generate diverse teammates to enhance the generalizable coordination ability, but were restricted by pre-defined teammates. In this work, our aim is to train agents with strong coordination ability by generating teammates that fully cover the teammate policy space, so that agents can coordinate with any teammates. Since the teammate policy space is too huge to be enumerated, we find only dissimilar teammates that are incompatible with controllable agents, which highly reduces the number of teammates that need to be trained with. However, it is hard to determine the number of such incompatible teammates beforehand. We therefore introduce a continual multi-agent learning process, in which the agent learns to coordinate with different teammates until no more incompatible teammates can be found. The above idea is implemented in the proposed Macop (Multi-agent compatible policy learning) algorithm. We conduct experiments in 8 scenarios from 4 environments that have distinct coordination patterns. Experiments show that Macop generates training teammates with much lower compatibility than previous methods. As a result, in all scenarios Macop achieves the best overall coordination ability while never significantly worse than the baselines, showing strong generalization ability.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.12633"
    },
    "d8e54607635f25e1f80f96448dc3f72c": {
        "title": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
        "authors": [
            "Jintian Zhang",
            "Xin Xu",
            "Ningyu Zhang",
            "Ruibo Liu",
            "Bryan Hooi",
            "Shumin Deng"
        ],
        "date": "2023/10/03",
        "pdf": "http://arxiv.org/pdf/2310.02124",
        "abstract": "As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies&#39; comprised of LLM agents, where each agent is characterized by a specific `trait&#39; (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern&#39; (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches, but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest human-like social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We commit to sharing our code and datasets\\footnote{\\url{https://github.com/zjunlp/MachineSoM}.}, hoping to catalyze further research in this promising avenue.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.02124"
    },
    "f5c8363d28a1236e6465845e8e6559d4": {
        "title": "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
        "authors": [
            "Dong Huang",
            "Jie M. Zhang",
            "Michael Luck",
            "Qingwen Bu",
            "Yuhao Qing",
            "Heming Cui"
        ],
        "date": "2023/12/20",
        "pdf": "http://arxiv.org/pdf/2312.13010",
        "abstract": "The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent&#39;s feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder&#39;s superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder (GPT-4) achieves 96.3\\% and 91.8\\% pass@1 in HumanEval and MBPP datasets with an overall token overhead of 56.9K and 66.3K, while state-of-the-art obtains only 90.2\\% and 78.9\\% pass@1 with an overall token overhead of 138.2K and 206.5K.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.13010"
    },
    "de2f7afa9bbbc00faa0191f82c0cab68": {
        "title": "MultiPrompter: Cooperative Prompt Optimization with Multi-Agent Reinforcement Learning",
        "authors": [
            "Dong-Ki Kim",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Dongsub Shim",
            "Honglak Lee"
        ],
        "date": "2023/10/25",
        "pdf": "http://arxiv.org/pdf/2310.16730",
        "abstract": "Recently, there has been an increasing interest in automated prompt optimization based on reinforcement learning (RL). This approach offers important advantages, such as generating interpretable prompts and being compatible with black-box foundation models. However, the substantial prompt space size poses challenges for RL-based methods, often leading to suboptimal policy convergence. This paper introduces MultiPrompter, a new framework that views prompt optimization as a cooperative game between prompters which take turns composing a prompt together. Our cooperative prompt optimization effectively reduces the problem size and helps prompters learn optimal prompts. We test our method on the text-to-image task and show its ability to generate higher-quality images than baselines.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.16730"
    },
    "2b83b0605caa5b7e0e3b46f73fbaa780": {
        "title": "Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games",
        "authors": [
            "Dekun Wu",
            "Haochen Shi",
            "Zhiyuan Sun",
            "Bang Liu"
        ],
        "date": "2023/12/01",
        "pdf": "http://arxiv.org/pdf/2312.00746",
        "abstract": "In this study, we explore the application of Large Language Models (LLMs) in \\textit{Jubensha}, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in this game. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in in-context learning to improve the agents&#39; performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.00746"
    },
    "e8e2e43e8e9e8ce333cd2a84b3902e41": {
        "title": "Multi-Agent Consensus Seeking via Large Language Models",
        "authors": [
            "Huaben Chen",
            "Wenkang Ji",
            "Lufeng Xu",
            "Shiyu Zhao"
        ],
        "date": "2023/10/31",
        "pdf": "http://arxiv.org/pdf/2310.20151",
        "abstract": "Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: windylab.github.io/ConsensusLLM/.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.20151"
    },
    "fefbb121e5e6daefd835e760d5f08117": {
        "title": "Generative Agents: Interactive Simulacra of Human Behavior",
        "authors": [
            "Joon Sung Park",
            "Joseph C. O&#39;Brien",
            "Carrie J. Cai",
            "Meredith Ringel Morris",
            "Percy Liang",
            "Michael S. Bernstein"
        ],
        "date": "2023/04/07",
        "pdf": "http://arxiv.org/pdf/2304.03442",
        "abstract": "Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent&#39;s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine&#39;s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.03442"
    },
    "6346920757c54de4cfb53cbd57153400": {
        "title": "ChatLLM Network: More brains, More intelligence",
        "authors": [
            "Rui Hao",
            "Linmei Hu",
            "Weijian Qi",
            "Qingliu Wu",
            "Yirui Zhang",
            "Liqiang Nie"
        ],
        "date": "2023/04/24",
        "pdf": "http://arxiv.org/pdf/2304.12998",
        "abstract": "Dialogue-based language models mark a huge milestone in the field of artificial intelligence, by their impressive ability to interact with users, as well as a series of challenging tasks prompted by customized instructions. However, the prevalent large-scale dialogue-based language models like ChatGPT still have room for improvement, such as unstable responses to questions and the inability to think cooperatively like humans. Considering the ability of dialogue-based language models in conversation and their inherent randomness in thinking, we propose ChatLLM network that allows multiple dialogue-based language models to interact, provide feedback, and think together. We design the network of ChatLLMs based on ChatGPT. Specifically, individual instances of ChatGPT may possess distinct perspectives towards the same problem, and by consolidating these diverse viewpoints via a separate ChatGPT, the ChatLLM network system can conduct decision-making more objectively and comprehensively. In addition, a language-based feedback mechanism comparable to backpropagation is devised to update the ChatGPTs within the network. Experiments on two datasets demonstrate that our network attains significant improvements in problem-solving, leading to observable progress amongst each member.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.12998"
    },
    "d4c1c4e1c46d89daf6890d5f71c62c63": {
        "title": "Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models",
        "authors": [
            "Jimmy Wei",
            "Kurt Shuster",
            "Arthur Szlam",
            "Jason Weston",
            "Jack Urbanek",
            "Mojtaba Komeili"
        ],
        "date": "2023/04/26",
        "pdf": "http://arxiv.org/pdf/2304.13835",
        "abstract": "Current dialogue research primarily studies pairwise (two-party) conversations, and does not address the everyday setting where more than two speakers converse together. In this work, we both collect and evaluate multi-party conversations to study this more general case. We use the LIGHT environment to construct grounded conversations, where each participant has an assigned character to role-play. We thus evaluate the ability of language models to act as one or more characters in such conversations. Models require two skills that pairwise-trained models appear to lack: (1) being able to decide when to talk; (2) producing coherent utterances grounded on multiple characters. We compare models trained on our new dataset to existing pairwise-trained dialogue models, as well as large language models with few-shot prompting. We find that our new dataset, MultiLIGHT, which we will publicly release, can help bring significant improvements in the group setting.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.13835"
    },
    "685611a174dfe4f335c66b3a972ad793": {
        "title": "The Role of Summarization in Generative Agents: A Preliminary Perspective",
        "authors": [
            "Xiachong Feng",
            "Xiaocheng Feng",
            "Bing Qin"
        ],
        "date": "2023/05/02",
        "pdf": "http://arxiv.org/pdf/2305.01253",
        "abstract": "Generative agents that simulate human society show tremendous potential for further research and practical applications. Specifically, the generative agent architecture comprising several meticulously designed modules constitutes the most critical component. To facilitate progress in this research, this report presents our integrated perspective on comprehending generative agents through summarization, since we believe summarization is the most fundamental and indispensable capacity of generative agents manifested across diverse scenarios. We hope this report can provide insight into understanding the importance of summarization capacity in generative agents and motivate future research.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.01253"
    },
    "98f7696b30c392052ea00d12d2d79918": {
        "title": "TidyBot: Personalized Robot Assistance with Large Language Models",
        "authors": [
            "Jimmy Wu",
            "Rika Antonova",
            "Adam Kan",
            "Marion Lepert",
            "Andy Zeng",
            "Shuran Song",
            "Jeannette Bohg",
            "Szymon Rusinkiewicz",
            "Thomas Funkhouser"
        ],
        "date": "2023/05/09",
        "pdf": "http://arxiv.org/pdf/2305.05658",
        "abstract": "For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people&#39;s preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accuracy on unseen objects in our benchmark dataset. We also demonstrate our approach on a real-world mobile manipulator called TidyBot, which successfully puts away 85.0% of objects in real-world test scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.05658"
    },
    "aff5e8a0a6be3cc3d84f669a5dc2cf7b": {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "authors": [
            "Shunyu Yao",
            "Dian Yu",
            "Jeffrey Zhao",
            "Izhak Shafran",
            "Thomas L. Griffiths",
            "Yuan Cao",
            "Karthik Narasimhan"
        ],
        "date": "2023/05/17",
        "pdf": "http://arxiv.org/pdf/2305.10601",
        "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models&#39; problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.10601"
    },
    "843b42ba9aa69a474fd90dd43404ee8a": {
        "title": "Role-Play with Large Language Models",
        "authors": [
            "Murray Shanahan",
            "Kyle McDonell",
            "Laria Reynolds"
        ],
        "date": "2023/05/25",
        "pdf": "http://arxiv.org/pdf/2305.16367",
        "abstract": "As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.16367"
    },
    "2fe6d5bff84082b57a528b3f5b08131e": {
        "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
        "authors": [
            "Ruibo Liu",
            "Ruixin Yang",
            "Chenyan Jia",
            "Ge Zhang",
            "Denny Zhou",
            "Andrew M. Dai",
            "Diyi Yang",
            "Soroush Vosoughi"
        ],
        "date": "2023/05/26",
        "pdf": "http://arxiv.org/pdf/2305.16960",
        "abstract": "Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.16960"
    },
    "5ad397cd019ef8e7d6f8717d67b90261": {
        "title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
        "authors": [
            "Bill Yuchen Lin",
            "Yicheng Fu",
            "Karina Yang",
            "Faeze Brahman",
            "Shiyu Huang",
            "Chandra Bhagavatula",
            "Prithviraj Ammanabrolu",
            "Yejin Choi",
            "Xiang Ren"
        ],
        "date": "2023/05/27",
        "pdf": "http://arxiv.org/pdf/2305.17390",
        "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent&#39;s action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.17390"
    },
    "445a34d666cdf8017df2d3b28f3e2bcf": {
        "title": "CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Language Model Society",
        "authors": [
            "Guohao Li",
            "Hasan Abed Al Kader Hammoud",
            "Hani Itani",
            "Dmitrii Khizbullin",
            "Bernard Ghanem"
        ],
        "date": "2023/03/31",
        "pdf": "http://arxiv.org/pdf/2303.17760",
        "abstract": "The rapid advancement of chat-based language models has led to remarkable progress in complex task-solving. However, their success heavily relies on human input to guide the conversation, which can be challenging and time-consuming. This paper explores the potential of building scalable techniques to facilitate autonomous cooperation among communicative agents, and provides insight into their &#34;cognitive&#34; processes. To address the challenges of achieving autonomous cooperation, we propose a novel communicative agent framework named role-playing. Our approach involves using inception prompting to guide chat agents toward task completion while maintaining consistency with human intentions. We showcase how role-playing can be used to generate conversational data for studying the behaviors and capabilities of a society of agents, providing a valuable resource for investigating conversational language models. In particular, we conduct comprehensive studies on instruction-following cooperation in multi-agent settings. Our contributions include introducing a novel communicative agent framework, offering a scalable approach for studying the cooperative behaviors and capabilities of multi-agent systems, and open-sourcing our library to support research on communicative agents and beyond: https://github.com/camel-ai/camel.",
        "code": "https://github.com/camel-ai/camel",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2303.17760"
    },
    "6728afeb589c1328ba967436092eeb98": {
        "title": "Self-collaboration Code Generation via ChatGPT",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Zhi Jin",
            "Ge Li"
        ],
        "date": "2023/04/15",
        "pdf": "http://arxiv.org/pdf/2304.07590",
        "abstract": "Although Large Language Models (LLMs) have demonstrated remarkable code-generation ability, they still struggle with complex tasks. In real-world software development, humans usually tackle complex tasks through collaborative teamwork, a strategy that significantly controls development complexity and enhances software quality. Inspired by this, we present a self-collaboration framework for code generation employing LLMs, exemplified by ChatGPT. Specifically, through role instructions, 1) Multiple LLM agents act as distinct `experts&#39;, each responsible for a specific subtask within a complex task; 2) Specify the way to collaborate and interact, so that different roles form a virtual team to facilitate each other&#39;s work, ultimately the virtual team addresses code generation tasks collaboratively without the need for human intervention. To effectively organize and manage this virtual team, we incorporate software-development methodology into the framework. Thus, we assemble an elementary team consisting of three LLM roles (i.e., analyst, coder, and tester) responsible for software development&#39;s analysis, coding, and testing stages. We conduct comprehensive experiments on various code-generation benchmarks. Experimental results indicate that self-collaboration code generation relatively improves 29.9%-47.1% Pass@1 compared to the base LLM agent. Moreover, we showcase that self-collaboration could potentially enable LLMs to efficiently handle complex repository-level tasks that are not readily solved by the single LLM agent.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.07590"
    },
    "8fb53b2baa1560345d6a6abe0eb7961f": {
        "title": "Inferring the Goals of Communicating Agents from Actions and Instructions",
        "authors": [
            "Lance Ying",
            "Tan Zhi-Xuan",
            "Vikash Mansinghka",
            "Joshua B. Tenenbaum"
        ],
        "date": "2023/06/28",
        "pdf": "http://arxiv.org/pdf/2306.16207",
        "abstract": "When humans cooperate, they frequently coordinate their activity through both verbal communication and non-verbal actions, using this information to infer a shared goal and plan. How can we model this inferential ability? In this paper, we introduce a model of a cooperative team where one agent, the principal, may communicate natural language instructions about their shared plan to another agent, the assistant, using GPT-3 as a likelihood function for instruction utterances. We then show how a third person observer can infer the team&#39;s goal via multi-modal Bayesian inverse planning from actions and instructions, computing the posterior distribution over goals under the assumption that agents will act and communicate rationally to achieve them. We evaluate this approach by comparing it with human goal inferences in a multi-agent gridworld, finding that our model&#39;s inferences closely correlate with human judgments (R = 0.96). When compared to inference from actions alone, we also find that instructions lead to more rapid and less uncertain goal inference, highlighting the importance of verbal communication for cooperative agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2306.16207"
    },
    "3447891c35f4bf8f904ab23534e84b9e": {
        "title": "TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage",
        "authors": [
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Zhiwei Xu",
            "Tianpeng Bao",
            "Guoqing Du",
            "Shiwei Shi",
            "Hangyu Mao",
            "Ziyue Li",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "date": "2023/08/07",
        "pdf": "http://arxiv.org/pdf/2308.03427",
        "abstract": "With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.03427"
    },
    "b9f40007be7ffb09746b30ea4342e688": {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "authors": [
            "Chi-Min Chan",
            "Weize Chen",
            "Yusheng Su",
            "Jianxuan Yu",
            "Wei Xue",
            "Shanghang Zhang",
            "Jie Fu",
            "Zhiyuan Liu"
        ],
        "date": "2023/08/14",
        "pdf": "http://arxiv.org/pdf/2308.07201",
        "abstract": "Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs&#39; potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.07201"
    },
    "99ab89a760611e1ee01b97ece9476d28": {
        "title": "LLM As DBA",
        "authors": [
            "Xuanhe Zhou",
            "Guoliang Li",
            "Zhiyuan Liu"
        ],
        "date": "2023/08/10",
        "pdf": "http://arxiv.org/pdf/2308.05481",
        "abstract": "Database administrators (DBAs) play a crucial role in managing, maintaining and optimizing a database system to ensure data availability, performance, and reliability. However, it is hard and tedious for DBAs to manage a large number of database instances (e.g., millions of instances on the cloud databases). Recently large language models (LLMs) have shown great potential to understand valuable documents and accordingly generate reasonable answers. Thus, we propose D-Bot, a LLM-based database administrator that can continuously acquire database maintenance experience from textual sources, and provide reasonable, well-founded, in-time diagnosis and optimization advice for target databases. This paper presents a revolutionary LLM-centric framework for database maintenance, including (i) database maintenance knowledge detection from documents and tools, (ii) tree of thought reasoning for root cause analysis, and (iii) collaborative diagnosis among multiple LLMs. Our preliminary experimental results that D-Bot can efficiently and effectively diagnose the root causes and our code is available at github.com/TsinghuaDatabaseGroup/DB-GPT.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.05481"
    },
    "ccf3bc888537a9e8c3fd5a712c59e974": {
        "title": "Cognitive Architectures for Language Agents",
        "authors": [
            "Theodore R. Sumers",
            "Shunyu Yao",
            "Karthik Narasimhan",
            "Thomas L. Griffiths"
        ],
        "date": "2023/09/05",
        "pdf": "http://arxiv.org/pdf/2309.02427",
        "abstract": "Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today&#39;s language agents within the broader history of AI and outlines a path towards language-based general intelligence.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.02427"
    },
    "649d55c8a7a65af48b7a6a279512302a": {
        "title": "A Versatile Graph Learning Approach through LLM-based Agent",
        "authors": [
            "Lanning Wei",
            "Huan Zhao",
            "Xiaohan Zheng",
            "Zhiqiang He",
            "Quanming Yao"
        ],
        "date": "2023/09/08",
        "pdf": "http://arxiv.org/pdf/2309.04565",
        "abstract": "Designing versatile graph learning approaches is important, considering the diverse graphs and tasks existing in real-world applications. Existing methods have attempted to achieve this target through automated machine learning techniques, pre-training and fine-tuning strategies, and large language models. However, these methods are not versatile enough for graph learning, as they work on either limited types of graphs or a single task. In this paper, we propose to explore versatile graph learning approaches with LLM-based agents, and the key insight is customizing the graph learning procedures for diverse graphs and tasks. To achieve this, we develop several LLM-based agents, equipped with diverse profiles, tools, functions and human experience. They collaborate to configure each procedure with task and data-specific settings step by step towards versatile solutions, and the proposed method is dubbed GL-Agent. By evaluating on diverse tasks and graphs, the correct results of the agent and its comparable performance showcase the versatility of the proposed method, especially in complex scenarios.The low resource cost and the potential to use open-source LLMs highlight the efficiency of GL-Agent.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.04565"
    },
    "c80b679cf1da13cfa8781f22f67a9488": {
        "title": "RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models",
        "authors": [
            "Zekun Moore Wang",
            "Zhongyuan Peng",
            "Haoran Que",
            "Jiaheng Liu",
            "Wangchunshu Zhou",
            "Yuhan Wu",
            "Hongcheng Guo",
            "Ruitong Gan",
            "Zehao Ni",
            "Jian Yang",
            "Man Zhang",
            "Zhaoxiang Zhang",
            "Wanli Ouyang",
            "Ke Xu",
            "Stephen W. Huang",
            "Jie Fu",
            "Junran Peng"
        ],
        "date": "2023/10/01",
        "pdf": "http://arxiv.org/pdf/2310.00746",
        "abstract": "The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.00746"
    },
    "f46f37266200ef7081c636ebaf459a15": {
        "title": "Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations",
        "authors": [
            "Zengqing Wu",
            "Run Peng",
            "Xu Han",
            "Shuyuan Zheng",
            "Yixin Zhang",
            "Chuan Xiao"
        ],
        "date": "2023/11/10",
        "pdf": "http://arxiv.org/pdf/2311.06330",
        "abstract": "Computer simulations offer a robust toolset for exploring complex systems across various disciplines. A particularly impactful approach within this realm is Agent-Based Modeling (ABM), which harnesses the interactions of individual agents to emulate intricate system dynamics. ABM&#39;s strength lies in its bottom-up methodology, illuminating emergent phenomena by modeling the behaviors of individual components of a system. Yet, ABM has its own set of challenges, notably its struggle with modeling natural language instructions and common sense in mathematical equations or rules. This paper seeks to transcend these boundaries by integrating Large Language Models (LLMs) like GPT into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based Modeling (SABM). Building upon the concept of smart agents -- entities characterized by their intelligence, adaptability, and computation ability -- we explore in the direction of utilizing LLM-powered agents to simulate real-world scenarios with increased nuance and realism. In this comprehensive exploration, we elucidate the state of the art of ABM, introduce SABM&#39;s potential and methodology, and present three case studies (source codes available at https://github.com/Roihn/SABM), demonstrating the SABM methodology and validating its effectiveness in modeling real-world systems. Furthermore, we cast a vision towards several aspects of the future of SABM, anticipating a broader horizon for its applications. Through this endeavor, we aspire to redefine the boundaries of computer simulations, enabling a more profound understanding of complex systems.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.06330"
    },
    "de513f1e7dfb0963d0a6da0cda7e8108": {
        "title": "ChatGPT as a commenter to the news: can LLMs generate human-like opinions?",
        "authors": [
            "Rayden Tseng",
            "Suzan Verberne",
            "Peter van der Putten"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13961",
        "abstract": "ChatGPT, GPT-3.5, and other large language models (LLMs) have drawn significant attention since their release, and the abilities of these models have been investigated for a wide variety of tasks. In this research we investigate to what extent GPT-3.5 can generate human-like comments on Dutch news articles. We define human likeness as `not distinguishable from human comments&#39;, approximated by the difficulty of automatic classification between human and GPT comments. We analyze human likeness across multiple prompting techniques. In particular, we utilize zero-shot, few-shot and context prompts, for two generated personas. We found that our fine-tuned BERT models can easily distinguish human-written comments from GPT-3.5 generated comments, with none of the used prompting methods performing noticeably better. We further analyzed that human comments consistently showed higher lexical diversity than GPT-generated comments. This indicates that although generative LLMs can generate fluent text, their capability to create human-like opinionated comments is still limited.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.13961"
    },
    "41a3472c1867c5546c3cac80da7266ef": {
        "title": "Can ChatGPT be Your Personal Medical Assistant?",
        "authors": [
            "Md. Rafiul Biswas",
            "Ashhadul Islam",
            "Zubair Shah",
            "Wajdi Zaghouani",
            "Samir Brahim Belhaouari"
        ],
        "date": "2023/12/19",
        "pdf": "http://arxiv.org/pdf/2312.12006",
        "abstract": "The advanced large language model (LLM) ChatGPT has shown its potential in different domains and remains unbeaten due to its characteristics compared to other LLMs. This study aims to evaluate the potential of using a fine-tuned ChatGPT model as a personal medical assistant in the Arabic language. To do so, this study uses publicly available online questions and answering datasets in Arabic language. There are almost 430K questions and answers for 20 disease-specific categories. GPT-3.5-turbo model was fine-tuned with a portion of this dataset. The performance of this fine-tuned model was evaluated through automated and human evaluation. The automated evaluations include perplexity, coherence, similarity, and token count. Native Arabic speakers with medical knowledge evaluated the generated text by calculating relevance, accuracy, precision, logic, and originality. The overall result shows that ChatGPT has a bright future in medical assistance.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.12006"
    },
    "9a8576963b068bd4380c72b187026243": {
        "title": "Reasoning with Language Model is Planning with World Model",
        "authors": [
            "Shibo Hao",
            "Yi Gu",
            "Haodi Ma",
            "Joshua Jiahua Hong",
            "Zhen Wang",
            "Daisy Zhe Wang",
            "Zhiting Hu"
        ],
        "date": "2023/05/24",
        "pdf": "http://arxiv.org/pdf/2305.14992",
        "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.14992"
    },
    "05010b701c190badb6151c0c424f1d62": {
        "title": "War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars",
        "authors": [
            "Wenyue Hua",
            "Lizhou Fan",
            "Lingyao Li",
            "Kai Mei",
            "Jianchao Ji",
            "Yingqiang Ge",
            "Libby Hemphill",
            "Yongfeng Zhang"
        ],
        "date": "2023/11/28",
        "pdf": "http://arxiv.org/pdf/2311.17227",
        "abstract": "Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \\textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems&#39; abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findings offer data-driven and AI-augmented insights that can redefine how we approach conflict resolution and peacekeeping strategies. The implications stretch beyond historical analysis, offering a blueprint for using AI to understand human history and possibly prevent future international conflicts. Code and data are available at \\url{https://github.com/agiresearch/WarAgent}.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.17227"
    },
    "e1722153565c5e01fff23a692a6cfb06": {
        "title": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem",
        "authors": [
            "Yingqiang Ge",
            "Yujie Ren",
            "Wenyue Hua",
            "Shuyuan Xu",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "date": "2023/12/06",
        "pdf": "http://arxiv.org/pdf/2312.03815",
        "abstract": "This paper envisions a revolutionary AIOS-Agent ecosystem, where Large Language Model (LLM) serves as the (Artificial) Intelligent Operating System (IOS, or AIOS)--an operating system &#34;with soul&#34;. Upon this foundation, a diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift from the traditional OS-APP ecosystem. We envision that LLM&#39;s impact will not be limited to the AI application level, instead, it will in turn revolutionize the design and implementation of computer system, architecture, software, and programming language, featured by several main concepts: LLM as OS (system-level), Agents as Applications (application-level), Natural Language as Programming Interface (user-level), and Tools as Devices/Libraries (hardware/middleware-level). We begin by introducing the architecture of traditional OS. Then we formalize a conceptual framework for AIOS through &#34;LLM as OS (LLMOS)&#34;, drawing analogies between AIOS and traditional OS: LLM is likened to OS kernel, context window to memory, external storage to file system, hardware tools to peripheral devices, software tools to programming libraries, and user prompts to user commands. Subsequently, we introduce the new AIOS-Agent Ecosystem, where users can easily program Agent Applications (AAPs) using natural language, democratizing the development of software, which is different from the traditional OS-APP ecosystem. Following this, we explore the diverse scope of Agent Applications. We delve into both single-agent and multi-agent systems, as well as human-agent interaction. Lastly, drawing on the insights from traditional OS-APP ecosystem, we propose a roadmap for the evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the future research and development, suggesting systematic progresses of AIOS and its Agent applications.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.03815"
    },
    "527cb71d35909520605f0d39509bfac8": {
        "title": "Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using LLMs",
        "authors": [
            "Sanjay Oruganti",
            "Sergei Nirenburg",
            "Jesse English",
            "Marjorie McShane"
        ],
        "date": "2023/12/27",
        "pdf": "http://arxiv.org/pdf/2312.16378",
        "abstract": "The paper describes a system that uses large language model (LLM) technology to support the automatic learning of new entries in an intelligent agent&#39;s semantic lexicon. The process is bootstrapped by an existing non-toy lexicon and a natural language generator that converts formal, ontologically-grounded representations of meaning into natural language sentences. The learning method involves a sequence of LLM requests and includes an automatic quality control step. To date, this learning method has been applied to learning multiword expressions whose meanings are equivalent to those of transitive verbs in the agent&#39;s lexicon. The experiment demonstrates the benefits of a hybrid learning architecture that integrates knowledge-based methods and resources with both traditional data analytics and LLMs.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.16378"
    },
    "3c5cb3f3c825afd95557058b3c655d7e": {
        "title": "Experiential Co-Learning of Software-Developing Agents",
        "authors": [
            "Chen Qian",
            "Yufan Dang",
            "Jiahao Li",
            "Wei Liu",
            "Zihao Xie",
            "Yifei Wang",
            "Weize Chen",
            "Cheng Yang",
            "Xin Cong",
            "Xiaoyin Che",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2023/12/28",
        "pdf": "http://arxiv.org/pdf/2312.17025",
        "abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.17025"
    },
    "1a7cb27152159d355d47bfa90274d0ab": {
        "title": "Towards an On-device Agent for Text Rewriting",
        "authors": [
            "Yun Zhu",
            "Yinxiao Liu",
            "Felix Stahlberg",
            "Shankar Kumar",
            "Yu-hui Chen",
            "Liangchen Luo",
            "Lei Shu",
            "Renjie Liu",
            "Jindong Chen",
            "Lei Meng"
        ],
        "date": "2023/08/22",
        "pdf": "http://arxiv.org/pdf/2308.11807",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities for text rewriting. Nonetheless, the large sizes of these models make them impractical for on-device inference, which would otherwise allow for enhanced privacy and economical inference. Creating a smaller yet potent language model for text rewriting presents a formidable challenge because it requires balancing the need for a small size with the need to retain the emergent capabilities of the LLM, that requires costly data collection. To address the above challenge, we introduce a new instruction tuning approach for building a mobile-centric text rewriting model. Our strategies enable the generation of high quality training data without any human labeling. In addition, we propose a heuristic reinforcement learning framework which substantially enhances performance without requiring preference data. To further bridge the performance gap with the larger server-side model, we propose an effective approach that combines the mobile rewrite agent with the server model using a cascade. To tailor the text rewriting tasks to mobile scenarios, we introduce MessageRewriteEval, a benchmark that focuses on text rewriting for messages through natural language instructions. Through empirical experiments, we demonstrate that our on-device model surpasses the current state-of-the-art LLMs in text rewriting while maintaining a significantly reduced model size. Notably, we show that our proposed cascading approach improves model performance.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2308.11807"
    },
    "2243b2331fd2574d72aca0cb65e10376": {
        "title": "Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach",
        "authors": [
            "Bin Zhang",
            "Hangyu Mao",
            "Jingqing Ruan",
            "Ying Wen",
            "Yang Li",
            "Shao Zhang",
            "Zhiwei Xu",
            "Dapeng Li",
            "Ziyue Li",
            "Rui Zhao",
            "Lijuan Li",
            "Guoliang Fan"
        ],
        "date": "2023/11/23",
        "pdf": "http://arxiv.org/pdf/2311.13884",
        "abstract": "The remarkable progress in Large Language Models (LLMs) opens up new avenues for addressing planning and decision-making problems in Multi-Agent Systems (MAS). However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent. Additionally, the efficient utilization of tokens emerges as a critical consideration when employing LLMs to facilitate the interactions among a substantial number of agents. In this paper, we develop a modular framework called LLaMAC to mitigate these challenges. LLaMAC implements a value distribution encoding similar to that found in the human brain, utilizing internal and external feedback mechanisms to facilitate collaboration and iterative reasoning among its modules. Through evaluations involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.13884"
    },
    "28a8719ce1adca46dae998bec22518f1": {
        "title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
        "authors": [
            "Chen Feng Tsai",
            "Xiaochen Zhou",
            "Sierra S. Liu",
            "Jing Li",
            "Mo Yu",
            "Hongyuan Mei"
        ],
        "date": "2023/04/06",
        "pdf": "http://arxiv.org/pdf/2304.02868",
        "abstract": "Large language models (LLMs) such as ChatGPT and GPT-4 have recently demonstrated their remarkable abilities of communicating with human users. In this technical report, we take an initiative to investigate their capacities of playing text games, in which a player has to understand the environment and respond to situations by having dialogues with the game world. Our experiments show that ChatGPT performs competitively compared to all the existing systems but still exhibits a low level of intelligence. Precisely, ChatGPT can not construct the world model by playing the game or even reading the game manual; it may fail to leverage the world knowledge that it already has; it cannot infer the goal of each step as the game progresses. Our results open up new research questions at the intersection of artificial intelligence, machine learning, and natural language processing.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2304.02868"
    },
    "26a0f2b5b4d2ff9c7cc8fad72d1a94ff": {
        "title": "Next Steps for Human-Centered Generative AI: A Technical Perspective",
        "authors": [
            "Xiang &#39;Anthony&#39; Chen",
            "Jeff Burke",
            "Ruofei Du",
            "Matthew K. Hong",
            "Jennifer Jacobs",
            "Philippe Laban",
            "Dingzeyu Li",
            "Nanyun Peng",
            "Karl D. D. Willis",
            "Chien-Sheng Wu",
            "Bolei Zhou"
        ],
        "date": "2023/06/27",
        "pdf": "http://arxiv.org/pdf/2306.15774",
        "abstract": "Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI). We contribute a comprehensive research agenda that lays out future directions of Generative AI spanning three levels: aligning with human values; assimilating human intents; and augmenting human abilities. By identifying these next-steps, we intend to draw interdisciplinary research teams to pursue a coherent set of emergent ideas in HGAI, focusing on their interested topics while maintaining a coherent big picture of the future work landscape.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2306.15774"
    },
    "75288ef81bd454aa2c0c646d5fdc4101": {
        "title": "A Survey on Large Language Model based Autonomous Agents",
        "authors": [
            "Lei Wang",
            "Chen Ma",
            "Xueyang Feng",
            "Zeyu Zhang",
            "Hao Yang",
            "Jingsen Zhang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Xu Chen",
            "Yankai Lin",
            "Wayne Xin Zhao",
            "Zhewei Wei",
            "Ji-Rong Wen"
        ],
        "date": "2023/08/22",
        "pdf": "http://arxiv.org/pdf/2308.11432",
        "abstract": "Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2308.11432"
    },
    "bb0bbf6c3da5efdcd8f7acdce436c1f1": {
        "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
        "authors": [
            "Zhiheng Xi",
            "Wenxiang Chen",
            "Xin Guo",
            "Wei He",
            "Yiwen Ding",
            "Boyang Hong",
            "Ming Zhang",
            "Junzhe Wang",
            "Senjie Jin",
            "Enyu Zhou",
            "Rui Zheng",
            "Xiaoran Fan",
            "Xiao Wang",
            "Limao Xiong",
            "Yuhao Zhou",
            "Weiran Wang",
            "Changhao Jiang",
            "Yicheng Zou",
            "Xiangyang Liu",
            "Zhangyue Yin",
            "Shihan Dou",
            "Rongxiang Weng",
            "Wensen Cheng",
            "Qi Zhang",
            "Wenjuan Qin",
            "Yongyan Zheng",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "date": "2023/09/14",
        "pdf": "http://arxiv.org/pdf/2309.07864",
        "abstract": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2309.07864"
    },
    "fa928d3d54acadb993f4b8fc3cb3acee": {
        "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
        "authors": [
            "Chen Gao",
            "Xiaochong Lan",
            "Nian Li",
            "Yuan Yuan",
            "Jingtao Ding",
            "Zhilun Zhou",
            "Fengli Xu",
            "Yong Li"
        ],
        "date": "2023/12/19",
        "pdf": "http://arxiv.org/pdf/2312.11970",
        "abstract": "Agent-based modeling and simulation has evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, examining their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2312.11970"
    },
    "2c30a38090198bd31c3053691e42f5f1": {
        "title": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots",
        "authors": [
            "Richard Sutcliffe"
        ],
        "date": "2023/12/31",
        "pdf": "http://arxiv.org/pdf/2401.00609",
        "abstract": "We present a review of personality in neural conversational agents (CAs), also called chatbots. First, we define Personality, Persona, and Profile. We explain all personality schemes which have been used in CAs, and list models under the scheme(s) which they use. Second we describe 21 datasets which have been developed in recent CA personality research. Third, we define the methods used to embody personality in a CA, and review recent models using them. Fourth, we survey some relevant reviews on CAs, personality, and related topics. Finally, we draw conclusions and identify some research challenges for this important emerging field.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2401.00609"
    },
    "a89d7d064ee50b5f5d994406024db2df": {
        "title": "AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems",
        "authors": [
            "Junjie Zhang",
            "Yupeng Hou",
            "Ruobing Xie",
            "Wenqi Sun",
            "Julian McAuley",
            "Wayne Xin Zhao",
            "Leyu Lin",
            "Ji-Rong Wen"
        ],
        "date": "2023/10/13",
        "pdf": "http://arxiv.org/pdf/2310.09233",
        "abstract": "Recently, there has been an emergence of employing LLM-powered agents as believable human proxies, based on their remarkable decision-making capability. However, existing studies mainly focus on simulating human dialogue. Human non-verbal behaviors, such as item clicking in recommender systems, although implicitly exhibiting user preferences and could enhance the modeling of users, have not been deeply explored. The main reasons lie in the gap between language modeling and behavior modeling, as well as the incomprehension of LLMs about user-item relations. To address this issue, we propose AgentCF for simulating user-item interactions in recommender systems through agent-based collaborative filtering. We creatively consider not only users but also items as agents, and develop a collaborative learning approach that optimizes both kinds of agents together. Specifically, at each time step, we first prompt the user and item agents to interact autonomously. Then, based on the disparities between the agents&#39; decisions and real-world interaction records, user and item agents are prompted to reflect on and adjust the misleading simulations collaboratively, thereby modeling their two-sided relations. The optimized agents can also propagate their preferences to other agents in subsequent interactions, implicitly capturing the collaborative filtering idea. Overall, the optimized agents exhibit diverse interaction behaviors within our framework, including user-item, user-user, item-item, and collective interactions. The results show that these agents can demonstrate personalized behaviors akin to those of real-world individuals, sparking the development of next-generation user behavior simulation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.09233"
    },
    "6bf24ac7d0056e0bc0240714642d96f8": {
        "title": "User Behavior Simulation with Large Language Model based Agents",
        "authors": [
            "Lei Wang",
            "Jingsen Zhang",
            "Hao Yang",
            "Zhiyuan Chen",
            "Jiakai Tang",
            "Zeyu Zhang",
            "Xu Chen",
            "Yankai Lin",
            "Ruihua Song",
            "Wayne Xin Zhao",
            "Jun Xu",
            "Zhicheng Dou",
            "Jun Wang",
            "Ji-Rong Wen"
        ],
        "date": "2023/06/05",
        "pdf": "http://arxiv.org/pdf/2306.02552",
        "abstract": "Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2306.02552"
    },
    "79e1c9fc13e42067576b1a2670c7134b": {
        "title": "MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models",
        "authors": [
            "Dingyao Yu",
            "Kaitao Song",
            "Peiling Lu",
            "Tianyu He",
            "Xu Tan",
            "Wei Ye",
            "Shikun Zhang",
            "Jiang Bian"
        ],
        "date": "2023/10/18",
        "pdf": "http://arxiv.org/pdf/2310.11954",
        "abstract": "AI-empowered music processing is a diverse field that encompasses dozens of tasks, ranging from generation tasks (e.g., timbre synthesis) to comprehension tasks (e.g., music classification). For developers and amateurs, it is very difficult to grasp all of these task to satisfy their requirements in music processing, especially considering the huge differences in the representations of music data and the model applicability across platforms among various tasks. Consequently, it is necessary to build a system to organize and integrate these tasks, and thus help practitioners to automatically analyze their demand and call suitable tools as solutions to fulfill their requirements. Inspired by the recent success of large language models (LLMs) in task automation, we develop a system, named MusicAgent, which integrates numerous music-related tools and an autonomous workflow to address user requirements. More specifically, we build 1) toolset that collects tools from diverse sources, including Hugging Face, GitHub, and Web API, etc. 2) an autonomous workflow empowered by LLMs (e.g., ChatGPT) to organize these tools and automatically decompose user requests into multiple sub-tasks and invoke corresponding music tools. The primary goal of this system is to free users from the intricacies of AI-music tools, enabling them to concentrate on the creative aspect. By granting users the freedom to effortlessly combine tools, the system offers a seamless and enriching music experience.",
        "code": "",
        "category": [
            [
                "Application",
                "Art"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.11954"
    },
    "73579681b0fcf709b1b58ba2083317fa": {
        "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
        "authors": [
            "Chenliang Li",
            "Hehong Chen",
            "Ming Yan",
            "Weizhou Shen",
            "Haiyang Xu",
            "Zhikai Wu",
            "Zhicheng Zhang",
            "Wenmeng Zhou",
            "Yingda Chen",
            "Chen Cheng",
            "Hongzhu Shi",
            "Ji Zhang",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "date": "2023/09/02",
        "pdf": "http://arxiv.org/pdf/2309.00986",
        "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.00986"
    },
    "a555d236f43f11398588858b6003f747": {
        "title": "A Zero-Shot Language Agent for Computer Control with Structured Reflection",
        "authors": [
            "Tao Li",
            "Gang Li",
            "Zhiwei Deng",
            "Bryan Wang",
            "Yang Li"
        ],
        "date": "2023/10/12",
        "pdf": "http://arxiv.org/pdf/2310.08740",
        "abstract": "Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior best models, even though previous works had the advantages of accessing expert traces or additional screen information.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.08740"
    },
    "2f63080abf12a0214d3f2cbe658b934c": {
        "title": "TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems",
        "authors": [
            "Yilun Kong",
            "Jingqing Ruan",
            "Yihong Chen",
            "Bin Zhang",
            "Tianpeng Bao",
            "Shiwei Shi",
            "Guoqing Du",
            "Xiaoru Hu",
            "Hangyu Mao",
            "Ziyue Li",
            "Xingyu Zeng",
            "Rui Zhao"
        ],
        "date": "2023/11/19",
        "pdf": "http://arxiv.org/pdf/2311.11315",
        "abstract": "Large Language Models (LLMs) have demonstrated proficiency in addressing tasks that necessitate a combination of task planning and the usage of external tools that require a blend of task planning and the utilization of external tools, such as APIs. However, real-world complex systems present three prevalent challenges concerning task planning and tool usage: (1) The real system usually has a vast array of APIs, so it is impossible to feed the descriptions of all APIs to the prompt of LLMs as the token length is limited; (2) the real system is designed for handling complex tasks, and the base LLMs can hardly plan a correct sub-task order and API-calling order for such tasks; (3) Similar semantics and functionalities among APIs in real systems create challenges for both LLMs and even humans in distinguishing between them. In response, this paper introduces a comprehensive framework aimed at enhancing the Task Planning and Tool Usage (TPTU) abilities of LLM-based agents operating within real-world systems. Our framework comprises three key components designed to address these challenges: (1) the API Retriever selects the most pertinent APIs for the user task among the extensive array available; (2) LLM Finetuner tunes a base LLM so that the finetuned LLM can be more capable for task planning and API calling; (3) the Demo Selector adaptively retrieves different demonstrations related to hard-to-distinguish APIs, which is further used for in-context learning to boost the final performance. We validate our methods using a real-world commercial system as well as an open-sourced academic dataset, and the outcomes clearly showcase the efficacy of each individual component as well as the integrated framework.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.11315"
    },
    "32379c096b4ff85ab67a07fc51cbd301": {
        "title": "CogAgent: A Visual Language Model for GUI Agents",
        "authors": [
            "Wenyi Hong",
            "Weihan Wang",
            "Qingsong Lv",
            "Jiazheng Xu",
            "Wenmeng Yu",
            "Junhui Ji",
            "Yan Wang",
            "Zihan Wang",
            "Yuxuan Zhang",
            "Juanzi Li",
            "Bin Xu",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "date": "2023/12/14",
        "pdf": "http://arxiv.org/pdf/2312.08914",
        "abstract": "People are spending an enormous amount of time on digital devices through graphical user interfaces (GUIs), e.g., computer or smartphone screens. Large language models (LLMs) such as ChatGPT can assist people in tasks like writing emails, but struggle to understand and interact with GUIs, thus limiting their potential to increase automation levels. In this paper, we introduce CogAgent, an 18-billion-parameter visual language model (VLM) specializing in GUI understanding and navigation. By utilizing both low-resolution and high-resolution image encoders, CogAgent supports input at a resolution of 1120*1120, enabling it to recognize tiny page elements and text. As a generalist visual language model, CogAgent achieves the state of the art on five text-rich and four general VQA benchmarks, including VQAv2, OK-VQA, Text-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using only screenshots as input, outperforms LLM-based methods that consume extracted HTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW, advancing the state of the art. The model and codes are available at https://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220 available at https://github.com/THUDM/CogAgent.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.08914"
    },
    "07b43c95e66ae36855f382f95cea8e41": {
        "title": "Team Flow at DRC2023: Building Common Ground and Text-based Turn-taking in a Travel Agent Spoken Dialogue System",
        "authors": [
            "Ryu Hirai",
            "Shinya Iizuka",
            "Haruhisa Iseno",
            "Ao Guo",
            "Jingjing Jiang",
            "Atsumoto Ohashi",
            "Ryuichiro Higashinaka"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13816",
        "abstract": "At the Dialogue Robot Competition 2023 (DRC2023), which was held to improve the capability of dialogue robots, our team developed a system that could build common ground and take more natural turns based on user utterance texts. Our system generated queries for sightseeing spot searches using the common ground and engaged in dialogue while waiting for user comprehension.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.13816"
    },
    "a268d9a93ef4c6dfe9e15a988ec88129": {
        "title": "AppAgent: Multimodal Agents as Smartphone Users",
        "authors": [
            "Chi Zhang",
            "Zhao Yang",
            "Jiaxuan Liu",
            "Yucheng Han",
            "Xin Chen",
            "Zebiao Huang",
            "Bin Fu",
            "Gang Yu"
        ],
        "date": "2023/12/21",
        "pdf": "http://arxiv.org/pdf/2312.13771",
        "abstract": "Recent advancements in large language models (LLMs) have led to the creation of intelligent agents capable of performing complex tasks. This paper introduces a novel LLM-based multimodal agent framework designed to operate smartphone applications. Our framework enables the agent to operate smartphone applications through a simplified action space, mimicking human-like interactions such as tapping and swiping. This novel approach bypasses the need for system back-end access, thereby broadening its applicability across diverse apps. Central to our agent&#39;s functionality is its innovative learning method. The agent learns to navigate and use new apps either through autonomous exploration or by observing human demonstrations. This process generates a knowledge base that the agent refers to for executing complex tasks across different applications. To demonstrate the practicality of our agent, we conducted extensive testing over 50 tasks in 10 different applications, including social media, email, maps, shopping, and sophisticated image editing tools. The results affirm our agent&#39;s proficiency in handling a diverse array of high-level tasks.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.13771"
    },
    "870815e42db4a9b074915ce5a5cd06d8": {
        "title": "MindAgent: Emergent Gaming Interaction",
        "authors": [
            "Ran Gong",
            "Qiuyuan Huang",
            "Xiaojian Ma",
            "Hoi Vo",
            "Zane Durante",
            "Yusuke Noda",
            "Zilong Zheng",
            "Song-Chun Zhu",
            "Demetri Terzopoulos",
            "Li Fei-Fei",
            "Jianfeng Gao"
        ],
        "date": "2023/09/18",
        "pdf": "http://arxiv.org/pdf/2309.09971",
        "abstract": "Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.",
        "code": "https://mindagent.github.io/",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2309.09971"
    },
    "e8069ea17b87bc2381127ebd0b0e49cc": {
        "title": "CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update",
        "authors": [
            "Zhi Gao",
            "Yuntao Du",
            "Xintong Zhang",
            "Xiaojian Ma",
            "Wenjuan Han",
            "Song-Chun Zhu",
            "Qing Li"
        ],
        "date": "2023/12/18",
        "pdf": "http://arxiv.org/pdf/2312.10908",
        "abstract": "Utilizing large language models (LLMs) to compose off-the-shelf visual tools represents a promising avenue of research for developing robust visual assistants capable of addressing diverse visual tasks. However, these methods often overlook the potential for continual learning, typically by freezing the utilized tools, thus limiting their adaptation to environments requiring new knowledge. To tackle this challenge, we propose CLOVA, a Closed-Loop Visual Assistant, which operates within a framework encompassing inference, reflection, and learning phases. During the inference phase, LLMs generate programs and execute corresponding tools to complete assigned tasks. In the reflection phase, a multimodal global-local reflection scheme analyzes human feedback to determine which tools require updating. Lastly, the learning phase employs three flexible approaches to automatically gather training data and introduces a novel prompt tuning scheme to update the tools, allowing CLOVA to efficiently acquire new knowledge. Experimental findings demonstrate that CLOVA surpasses existing tool-usage methods by 5% in visual question answering and multiple-image reasoning, by 10% in knowledge tagging, and by 20% in image editing. These results underscore the significance of the continual learning capability in general visual assistants.",
        "code": "https://clova-tool.github.io/",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.10908"
    },
    "38db43d88f116a51413cbb3dbf6dde12": {
        "title": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models",
        "authors": [
            "Zihao Wang",
            "Shaofei Cai",
            "Anji Liu",
            "Yonggang Jin",
            "Jinbing Hou",
            "Bowei Zhang",
            "Haowei Lin",
            "Zhaofeng He",
            "Zilong Zheng",
            "Yaodong Yang",
            "Xiaojian Ma",
            "Yitao Liang"
        ],
        "date": "2023/11/10",
        "pdf": "http://arxiv.org/pdf/2311.05997",
        "abstract": "Achieving human-like planning and control with multimodal observations in an open world is a key milestone for more functional generalist agents. Existing approaches can handle certain long-horizon tasks in an open world. However, they still struggle when the number of open-world tasks could potentially be infinite and lack the capability to progressively enhance task completion as game time progresses. We introduce JARVIS-1, an open-world agent that can perceive multimodal input (visual observations and human instructions), generate sophisticated plans, and perform embodied control, all within the popular yet challenging open-world Minecraft universe. Specifically, we develop JARVIS-1 on top of pre-trained multimodal language models, which map visual observations and textual instructions to plans. The plans will be ultimately dispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a multimodal memory, which facilitates planning using both pre-trained knowledge and its actual game survival experiences. JARVIS-1 is the existing most general agent in Minecraft, capable of completing over 200 different tasks using control and observation space similar to humans. These tasks range from short-horizon tasks, e.g., &#34;chopping trees&#34; to long-horizon tasks, e.g., &#34;obtaining a diamond pickaxe&#34;. JARVIS-1 performs exceptionally well in short-horizon tasks, achieving nearly perfect performance. In the classic long-term task of $\\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the reliability of current state-of-the-art agents by 5 times and can successfully complete longer-horizon and more challenging tasks. The project page is available at https://craftjarvis.org/JARVIS-1",
        "code": "https://github.com/CraftJarvis/JARVIS-1",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.05997"
    },
    "b1ce54ca409d3fedd20b7d6b04b4aaa8": {
        "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
        "authors": [
            "Zihao Wang",
            "Shaofei Cai",
            "Guanzhou Chen",
            "Anji Liu",
            "Xiaojian Ma",
            "Yitao Liang"
        ],
        "date": "2023/02/03",
        "pdf": "http://arxiv.org/pdf/2302.01560",
        "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose &#34;$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect&#34; ($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method&#39;s general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2302.01560"
    },
    "eef6165c96965f04155ff5cc3d1b393e": {
        "title": "Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning",
        "authors": [
            "Lin Guan",
            "Karthik Valmeekam",
            "Sarath Sreedharan",
            "Subbarao Kambhampati"
        ],
        "date": "2023/05/24",
        "pdf": "http://arxiv.org/pdf/2305.14909",
        "abstract": "There is a growing interest in applying pre-trained large language models (LLMs) to planning problems. However, methods that use LLMs directly as planners are currently impractical due to several factors, including limited correctness of plans, strong reliance on feedback from interactions with simulators or even the actual environment, and the inefficiency in utilizing human feedback. In this work, we introduce a novel alternative paradigm that constructs an explicit world (domain) model in planning domain definition language (PDDL) and then uses it to plan with sound domain-independent planners. To address the fact that LLMs may not generate a fully functional PDDL model initially, we employ LLMs as an interface between PDDL and sources of corrective feedback, such as PDDL validators and humans. For users who lack a background in PDDL, we show that LLMs can translate PDDL into natural language and effectively encode corrective feedback back to the underlying domain model. Our framework not only enjoys the correctness guarantee offered by the external planners but also reduces human involvement by allowing users to correct domain models at the beginning, rather than inspecting and correcting (through interactive prompting) every generated plan as in previous work. On two IPC domains and a Household domain that is more complicated than commonly used benchmarks such as ALFWorld, we demonstrate that GPT-4 can be leveraged to produce high-quality PDDL models for over 40 actions, and the corrected PDDL models are then used to successfully solve 48 challenging planning tasks. Resources, including the source code, are released at: https://guansuns.github.io/pages/llm-dm.",
        "code": "https://github.com/GuanSuns/LLMs-World-Models-for-Planning",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2305.14909"
    },
    "37c843ffe31783b0915c083bee24ed9d": {
        "title": "Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning",
        "authors": [
            "Filippos Christianos",
            "Georgios Papoudakis",
            "Matthieu Zimmer",
            "Thomas Coste",
            "Zhihao Wu",
            "Jingxuan Chen",
            "Khyati Khandelwal",
            "James Doran",
            "Xidong Feng",
            "Jiacheng Liu",
            "Zheng Xiong",
            "Yicheng Luo",
            "Jianye Hao",
            "Kun Shao",
            "Haitham Bou-Ammar",
            "Jun Wang"
        ],
        "date": "2023/12/22",
        "pdf": "http://arxiv.org/pdf/2312.14878",
        "abstract": "A key method for creating Artificial Intelligence (AI) agents is Reinforcement Learning (RL). However, constructing a standalone RL policy that maps perception to action directly encounters severe problems, chief among them being its lack of generality across multiple tasks and the need for a large amount of training data. The leading cause is that it cannot effectively integrate prior information into the perception-action cycle when devising the policy. Large language models (LLMs) emerged as a fundamental way to incorporate cross-domain knowledge into AI agents but lack crucial learning and adaptation toward specific decision problems. This paper presents a general framework model for integrating and learning structured reasoning into AI agents&#39; policies. Our methodology is motivated by the modularity found in the human brain. The framework utilises the construction of intrinsic and extrinsic functions to add previous understandings of reasoning structures. It also provides the adaptive ability to learn models inside every module or function, consistent with the modular structure of cognitive processes. We describe the framework in-depth and compare it with other AI pipelines and existing frameworks. The paper explores practical applications, covering experiments that show the effectiveness of our method. Our results indicate that AI agents perform and adapt far better when organised reasoning and prior knowledge are embedded. This opens the door to more resilient and general AI agent systems.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2312.14878"
    },
    "a078befa2b9350c5c5120719572a4f7c": {
        "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
        "authors": [
            "Yijun Yang",
            "Tianyi Zhou",
            "Kanxue Li",
            "Dapeng Tao",
            "Lusong Li",
            "Li Shen",
            "Xiaodong He",
            "Jing Jiang",
            "Yuhui Shi"
        ],
        "date": "2023/11/28",
        "pdf": "http://arxiv.org/pdf/2311.16714",
        "abstract": "While large language models (LLMs) excel in a simulated world of texts, they struggle to interact with the more realistic world without perceptions of other modalities such as visual or audio signals. Although vision-language models (VLMs) integrate LLM modules (1) aligned with static image features, and (2) may possess prior knowledge of world dynamics (as demonstrated in the text world), they have not been trained in an embodied visual world and thus cannot align with its dynamics. On the other hand, training an embodied agent in a noisy visual world without expert guidance is often challenging and inefficient. In this paper, we train a VLM agent living in a visual world using an LLM agent excelling in a parallel text world. Specifically, we distill LLM&#39;s reflection outcomes (improved actions by analyzing mistakes) in a text world&#39;s tasks to finetune the VLM on the same tasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA) quickly adapting to the visual world dynamics. Such cross-modality imitation learning between the two parallel worlds is achieved by a novel DAgger-DPO algorithm, enabling EMMA to generalize to a broad scope of new tasks without any further guidance from the LLM expert. Extensive evaluations on the ALFWorld benchmark&#39;s diverse tasks highlight EMMA&#39;s superior performance to SOTA VLM-based agents, e.g., 20%-70% improvement in the success rate.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2311.16714"
    },
    "5ebef6611789db70622d69a8435e3f04": {
        "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models",
        "authors": [
            "Mengkang Hu",
            "Yao Mu",
            "Xinmiao Yu",
            "Mingyu Ding",
            "Shiguang Wu",
            "Wenqi Shao",
            "Qiguang Chen",
            "Bin Wang",
            "Yu Qiao",
            "Ping Luo"
        ],
        "date": "2023/10/12",
        "pdf": "http://arxiv.org/pdf/2310.08582",
        "abstract": "This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2310.08582"
    }
}