{
    "61b5038fc9df8e1d37fef5c460876350": {
        "title": "On Memory Construction and Retrieval for Personalized Conversational Agents",
        "authors": [
            "Zhuoshi Pan",
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Xufang Luo",
            "Hao Cheng",
            "Dongsheng Li",
            "Yuqing Yang",
            "Chin-Yew Lin",
            "H. Vicky Zhao",
            "Lili Qiu",
            "Jianfeng Gao"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05589",
        "abstract": "To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \\textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SeCom, a method that constructs a memory bank with topical segments by introducing a conversation Segmentation model, while performing memory retrieval based on Compressed memory units. Experimental results show that SeCom outperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such as LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05589"
    },
    "281691883e4589edb720c4d85eed1799": {
        "title": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
        "authors": [
            "Pranav Bhandari",
            "Usman Naseem",
            "Amitava Datta",
            "Nicolas Fay",
            "Mehwish Nasim"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.05248",
        "abstract": "Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05248"
    },
    "552a2c4e2a51224ee6298d6c34bf9aac": {
        "title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging",
        "authors": [
            "Md. Ashraful Islam",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05664",
        "abstract": "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim&#39;s remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/).",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05664"
    },
    "c13ba9544a2a06f9994ce2dc692c444a": {
        "title": "MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational Agents",
        "authors": [
            "Wanqi Yang",
            "Yanda Li",
            "Meng Fang",
            "Ling Chen"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05887",
        "abstract": "Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model&#39;s ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05887"
    },
    "19879c35d36631c355da5a719ebceb80": {
        "title": "HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents",
        "authors": [
            "Mohammad Amin Abbasi",
            "Farnaz Sadat Mirnezami",
            "Hassan Naderi"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05982",
        "abstract": "This paper presents HamRaz, a novel Persian-language mental health dataset designed for Person-Centered Therapy (PCT) using Large Language Models (LLMs). Despite the growing application of LLMs in AI-driven psychological counseling, existing datasets predominantly focus on Western and East Asian contexts, overlooking cultural and linguistic nuances essential for effective Persian-language therapy. To address this gap, HamRaz combines script-based dialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy interactions. We also introduce HamRazEval, a dual evaluation framework that measures conversational quality and therapeutic effectiveness using General Dialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI). Experimental results show HamRaz outperforms conventional Script Mode and Two-Agent Mode, producing more empathetic, context-aware, and realistic therapy sessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven resource to advance AI-powered psychotherapy research in diverse communities.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05982"
    },
    "3c329c9902e3a5342ab10c8f05ce2a56": {
        "title": "Preventing Rogue Agents Improves Multi-Agent Collaboration",
        "authors": [
            "Ohav Barbi",
            "Ori Yoran",
            "Mor Geva"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05986",
        "abstract": "Multi-agent systems, where specialized agents collaborate to solve a shared task hold great potential, from increased modularity to simulating complex environments. However, they also have a major caveat -- a single agent can cause the entire system to fail. Consider a simple game where the knowledge to solve the task is distributed between agents, which share information in a communication channel. At each round, any of the agents can terminate the game and make the final prediction, even if they are uncertain about the outcome of their action. Detection of such rogue agents $\\textit{before they act}$ may prevent the system&#39;s failure. In this work, we propose to $\\textit{monitor}$ agents during action prediction and $\\textit{intervene}$ when a future error is likely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent collaboration environment that allows modular control over task complexity and communication structure. Experiments on two variants of WhoDunitEnv and the GovSim environment for resource sustainability show that our approach leads to substantial performance gains up to 17.4% and 20%, respectively. Moreover, a thorough analysis shows that our monitors successfully identify critical points of agent confusion and our interventions effectively stop agent errors from propagating.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05986"
    },
    "49063743e04babb9acce0f2448fbf35c": {
        "title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?",
        "authors": [
            "Wenzhe Li",
            "Yong Lin",
            "Mengzhou Xia",
            "Chi Jin"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.00674",
        "abstract": "Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00674"
    },
    "bf4e41ed6254617cf3cafcde348322e0": {
        "title": "Efficient Multi-Agent System Training with Data Influence-Oriented Tree Search",
        "authors": [
            "Wentao Shi",
            "Zichun Yu",
            "Fuli Feng",
            "Xiangnan He",
            "Chenyan Xiong"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.00955",
        "abstract": "Monte Carlo Tree Search (MCTS) based methods provide promising approaches for generating synthetic data to enhance the self-training of Large Language Model (LLM) based multi-agent systems (MAS). These methods leverage Q-values to estimate individual agent contributions. However, relying solely on Q-values to identify informative data may misalign with the data synthesis objective, as the focus should be on selecting data that best enhances model training. To address this discrepancy, we propose Data Influence-oriented Tree Search (DITS), a novel framework that incorporates influence scores to guide both tree search and data selection. By leveraging influence scores, we effectively identify the most impactful data for system improvement, thereby enhancing model performance. Furthermore, we derive influence score estimation methods tailored for non-differentiable metrics, significantly reducing computational overhead by utilizing inference computations. Extensive experiments on eight multi-agent datasets demonstrate the robustness and effectiveness of the proposed methods. Notably, our findings reveal that allocating more inference resources to estimate influence scores, rather than Q-values, during data synthesis can more effectively and efficiently enhance model training.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00955"
    },
    "d6e36d1157819c499d4276bf95a792d0": {
        "title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.00988",
        "abstract": "Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00988"
    },
    "d51ef18416d3cba8e2e4d488e3f63a01": {
        "title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.00989",
        "abstract": "Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00989"
    },
    "f59baeb0b866580954a53deab6e95535": {
        "title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models",
        "authors": [
            "Diyana Muhammed",
            "Gollam Rabby",
            "Sören Auer"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01812",
        "abstract": "Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce SelfCheckAgent, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89% but reveals trade-offs in Factual with 30.58% and Ranking with 30.68%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent&#39;s applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01812"
    },
    "ac59740d2e4f33bd00b9ea23dc8b1137": {
        "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development",
        "authors": [
            "Genghan Zhang",
            "Weixin Liang",
            "Olivia Hsu",
            "Kunle Olukotun"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.02534",
        "abstract": "ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02534"
    },
    "62c9e6521d85e0e094da3c4da01a0bfe": {
        "title": "CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration",
        "authors": [
            "Yizhe Yang",
            "Palakorn Achananuparp",
            "Heyan Huang",
            "Jing Jiang",
            "Kit Phey Leng",
            "Nicholas Gabriel Lim",
            "Cameron Tan Shi Ern",
            "Ee-peng Lim"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.02807",
        "abstract": "Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client&#39;s state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI&#39;s performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client&#39;s state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02807"
    },
    "979770a3febd45fbb5fa84d479b32601": {
        "title": "ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation",
        "authors": [
            "Qinzhuo Wu",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.02955",
        "abstract": "Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks. Furthermore, we propose ReachAgent, a two-stage framework that focuses on improving its task-completion abilities. It utilizes the page reaching and page operation subtasks, along with reward-based preference GUI flows, to further enhance the agent. Experimental results show that ReachAgent significantly improves the IoU Acc and Text Acc by 7.12% and 7.69% on the step-level and 4.72% and 4.63% on the task-level compared to the SOTA agent. Our data and code will be released upon acceptance.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02955"
    },
    "3d449aa86235533946b11e039c77566c": {
        "title": "Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model",
        "authors": [
            "Hadas Ben-Atya",
            "Naama Gavrielov",
            "Zvi Badash",
            "Gili Focht",
            "Ruth Cytter-Kuint",
            "Talar Hagopian",
            "Dan Turner",
            "Moti Freiman"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.01691",
        "abstract": "Reliable extraction of structured data from radiology reports using Large Language Models (LLMs) remains challenging, especially for complex, non-English texts like Hebrew. This study introduces an agent-based uncertainty-aware approach to improve the trustworthiness of LLM predictions in medical applications. We analyzed 9,683 Hebrew radiology reports from Crohn&#39;s disease patients (from 2010 to 2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining reports were automatically annotated using HSMP-BERT. Structured data extraction was performed using Llama 3.1 (Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty. An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty and was compared against three entropy-based models. Performance was evaluated using accuracy, F1 score, precision, recall, and Cohen&#39;s Kappa before and after filtering high-uncertainty cases. The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen&#39;s Kappa of 0.3006. After filtering high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to 0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates. By incorporating uncertainty-aware prompt ensembles and an agent-based decision model, this approach enhances the performance and reliability of LLMs in structured data extraction from radiology reports, offering a more interpretable and trustworthy solution for high-stakes medical applications.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01691"
    },
    "3c3db16cc3a88ba4b8542eff8dc4ea14": {
        "title": "PsyPlay: Personality-Infused Role-Playing Conversational Agents",
        "authors": [
            "Tao Yang",
            "Yuhua Zhu",
            "Xiaojun Quan",
            "Cong Liu",
            "Qifan Wang"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.03821",
        "abstract": "The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits.~In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31% on GPT-3.5. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue corpus for personality-infused role-playing, called PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03821"
    },
    "f4ea715917df0171b1e32754f198332c": {
        "title": "Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents",
        "authors": [
            "Yuchen Lian",
            "Arianna Bisazza",
            "Tessa Verhoef"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04038",
        "abstract": "Differential Case Marking (DCM) refers to the phenomenon where grammatical case marking is applied selectively based on semantic, pragmatic, or other factors. The emergence of DCM has been studied in artificial language learning experiments with human participants, which were specifically aimed at disentangling the effects of learning from those of communication (Smith &amp; Culbertson, 2020). Multi-agent reinforcement learning frameworks based on neural networks have gained significant interest to simulate the emergence of human-like linguistic phenomena. In this study, we employ such a framework in which agents first acquire an artificial language before engaging in communicative interactions, enabling direct comparisons to human result. Using a very generic communication optimization algorithm and neural-network learners that have no prior experience with language or semantic preferences, our results demonstrate that learning alone does not lead to DCM, but when agents communicate, differential use of markers arises. This supports Smith and Culbertson (2020)&#39;s findings that highlight the critical role of communication in shaping DCM and showcases the potential of neural-agent models to complement experimental research on language evolution.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04038"
    },
    "7882b7249c571f63c48cc7312a64c042": {
        "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization",
        "authors": [
            "Yinjie Wang",
            "Ling Yang",
            "Guohao Li",
            "Mengdi Wang",
            "Bryon Aragam"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04306",
        "abstract": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04306"
    },
    "0effcb668d9d61287d8d4a3d308cea17": {
        "title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives",
        "authors": [
            "Elliot Meyerson",
            "Xin Qiu"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.04358",
        "abstract": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04358"
    },
    "cc8782f2d8d6834d10194088c6413f05": {
        "title": "Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents",
        "authors": [
            "Chenyang Shao",
            "Xinyuan Hu",
            "Yutang Lin",
            "Fengli Xu"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04392",
        "abstract": "The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models offer a promising path for next-generation on-device AI agents. However, deploying full-scale Large Language Models (LLMs) on resource-limited local devices is challenging. In this paper, we propose Division-of-Thoughts (DoT), a collaborative reasoning framework leveraging the synergy between locally deployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT leverages a Task Decomposer to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an additional task head attached to the SLM that does not alter the SLM&#39;s parameters. To boost adapter&#39;s task allocation capability, we propose a self-reinforced training method that relies solely on task execution feedback. Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12% and 83.57%, while achieving comparable reasoning accuracy with the best baseline methods.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04392"
    },
    "c2895095f35c0f46f199a820e04634db": {
        "title": "Multi-Agent Reinforcement Learning with Focal Diversity Optimization",
        "authors": [
            "Selim Furkan Tekin",
            "Fatih Ilhan",
            "Tiansheng Huang",
            "Sihao Hu",
            "Zachary Yahn",
            "Ling Liu"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04492",
        "abstract": "The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at https://github.com/sftekin/rl-focal",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04492"
    },
    "d130907042a9d1c207814e4a43bf6fbc": {
        "title": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency",
        "authors": [
            "Yuting Zeng",
            "Weizhe Huang",
            "Lei Jiang",
            "Tongxuan Liu",
            "Xitai Jin",
            "Chen Tianying Tiana",
            "Jing Li",
            "Xiaohua Xu"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.04790",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5\\% in token costs while maintaining performance degradation below 2.0\\%.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04790"
    },
    "9d031511e3eba748343236b6affe8743": {
        "title": "nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow",
        "authors": [
            "Geliang Ouyang",
            "Jingyao Chen",
            "Zhihe Nie",
            "Yi Gui",
            "Yao Wan",
            "Hongyu Zhang",
            "Dongping Chen"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.05036",
        "abstract": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05036"
    },
    "f526e3a89c0265bf86d6eac5672e6b5f": {
        "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment",
        "authors": [
            "Yuxing Lu",
            "Jinzhuo Wang"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06472",
        "abstract": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06472"
    },
    "57a611637f85a3f9d2d547f5f0f012f7": {
        "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
        "authors": [
            "Yuchen Zhuang",
            "Jingfeng Yang",
            "Haoming Jiang",
            "Xin Liu",
            "Kewei Cheng",
            "Sanket Lokegaonkar",
            "Yifan Gao",
            "Qing Ping",
            "Tianyi Liu",
            "Binxuan Huang",
            "Zheng Li",
            "Zhengyang Wang",
            "Pei Chen",
            "Ruijie Wang",
            "Rongzhi Zhang",
            "Nasser Zalmout",
            "Priyanka Nigam",
            "Bing Yin",
            "Chao Zhang"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06589",
        "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06589"
    },
    "98494fac2a6a4669636e9b32fe0936f9": {
        "title": "Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction",
        "authors": [
            "Shengbin Yue",
            "Ting Huang",
            "Zheng Jia",
            "Siyuan Wang",
            "Shujun Liu",
            "Yun Song",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.06882",
        "abstract": "Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants&#39; characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs&#39; performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06882"
    },
    "d5c21311f0587298d8b6e1d42dabab8e": {
        "title": "Don&#39;t Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification",
        "authors": [
            "Peipei Wei",
            "Dimitris Dimitriadis",
            "Yan Xu",
            "Mingwei Shen"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07165",
        "abstract": "We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07165"
    },
    "230d0a37dd7aec9d7bf30af185d6d90a": {
        "title": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning",
        "authors": [
            "Jian Yang",
            "Wei Zhang",
            "Jiaxi Yang",
            "Yibo Miao",
            "Shanghaoran Quan",
            "Zhenhe Wu",
            "Qiyao Peng",
            "Liqun Yang",
            "Tianyu Liu",
            "Zeyu Cui",
            "Binyuan Hui",
            "Junyang Lin"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07487",
        "abstract": "Recent advancement in code understanding and generation demonstrates that code LLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities to address wide-ranging code-related tasks. However, most previous existing methods mainly view each programming language in isolation and ignore the knowledge transfer among different programming languages. To bridge the gap among different programming languages, we introduce a novel multi-agent collaboration framework to enhance multilingual instruction tuning for code LLMs, where multiple language-specific intelligent agent components with generation memory work together to transfer knowledge from one language to another efficiently and effectively. Specifically, we first generate the language-specific instruction data from the code snippets and then provide the generated data as the seed data for language-specific agents. Multiple language-specific agents discuss and collaborate to formulate a new instruction and its corresponding solution (A new programming language or existing programming language), To further encourage the cross-lingual transfer, each agent stores its generation history as memory and then summarizes its merits and faults. Finally, the high-quality multilingual instruction data is used to encourage knowledge transfer among different programming languages to train Qwen2.5-xCoder. Experimental results on multilingual programming benchmarks demonstrate the superior performance of Qwen2.5-xCoder in sharing common knowledge, highlighting its potential to reduce the cross-lingual gap.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07487"
    },
    "a25ee4a3ab77c6faf8c4a3819ed5fc28": {
        "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation",
        "authors": [
            "Mahnaz Koupaee",
            "Jake W. Vincent",
            "Saab Mansour",
            "Igor Shalyminov",
            "Han He",
            "Hwanjun Song",
            "Raphael Shu",
            "Jianfeng He",
            "Yi Nian",
            "Amy Wing-mei Wong",
            "Kyu J. Han",
            "Hang Su"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08514",
        "abstract": "Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08514"
    },
    "474c1ff5a3034e7a01037edcf62d0eb5": {
        "title": "SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent",
        "authors": [
            "Keyeun Lee",
            "Seo Hyeong Kim",
            "Seolhee Lee",
            "Jinsu Eun",
            "Yena Ko",
            "Hayeon Jeon",
            "Esther Hehsun Kim",
            "Seonghye Cho",
            "Soeun Yang",
            "Eun-mee Kim",
            "Hajin Lim"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08599",
        "abstract": "Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual&#39;s multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum&#39;s effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)-derived from short essays on preferences and daily routines-modeled characters&#39; identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08599"
    },
    "777d7f5ea00e3b823ca61eb6914014a7": {
        "title": "If Multi-Agent Debate is the Answer, What is the Question?",
        "authors": [
            "Hangfan Zhang",
            "Zhiyao Cui",
            "Xinrun Wang",
            "Qiaosheng Zhang",
            "Zhen Wang",
            "Dinghao Wu",
            "Shuyue Hu"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08788",
        "abstract": "Multi-agent debate (MAD) has emerged as a promising approach to enhance the factual accuracy and reasoning quality of large language models (LLMs) by engaging multiple agents in iterative discussions during inference. Despite its potential, we argue that current MAD research suffers from critical shortcomings in evaluation practices, including limited dataset overlap and inconsistent baselines, raising significant concerns about generalizability. Correspondingly, this paper presents a systematic evaluation of five representative MAD methods across nine benchmarks using four foundational models. Surprisingly, our findings reveal that MAD methods fail to reliably outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming additional inference-time computation. From our analysis, we found that model heterogeneity can significantly improve MAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the output from heterogeneous foundation models, which boosts the performance of current MAD frameworks. Finally, we outline potential directions for advancing MAD, aiming to spark a broader conversation and inspire future work in this area.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08788"
    },
    "a6e83c9ede12dbf90c97e86fa5d68e1c": {
        "title": "Agentic Verification for Ambiguous Query Disambiguation",
        "authors": [
            "Youngwon Lee",
            "Seung-won Hwang",
            "Ruofan Wu",
            "Feng Yan",
            "Danmei Xu",
            "Moutasem Akkad",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.10352",
        "abstract": "In this work, we tackle the challenge of disambiguating queries in retrieval-augmented generation (RAG) to diverse yet answerable interpretations. State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse interpretations are generated by an LLM, later used as search queries to retrieve supporting passages. Such a process may introduce noise in either interpretations or retrieval, particularly in enterprise settings, where LLMs -- trained on static data -- may struggle with domain-specific disambiguations. Thus, a post-hoc verification phase is introduced to prune noises. Our distinction is to unify diversification with verification by incorporating feedback from retriever and generator early on. This joint approach improves both efficiency and robustness by reducing reliance on multiple retrieval and inference steps, which are susceptible to cascading errors. We validate the efficiency and effectiveness of our method, Verified-Diversification with Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve diverse yet verifiable interpretations. Empirical results show that VERDICT improves grounding-aware F1 score by an average of 23% over the strongest baseline across different backbone LLMs.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10352"
    },
    "d800ee6acc686566a88098700f570602": {
        "title": "Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation",
        "authors": [
            "Haoyuan Wu",
            "Haisheng Zheng",
            "Zhuolun He",
            "Bei Yu"
        ],
        "date": "2025/02/15",
        "pdf": "http://arxiv.org/pdf/2502.10857",
        "abstract": "Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts. However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms. Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps. Any errors will lead to the instability and failure of EDA flow automation. To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation. Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10857"
    },
    "051910306e3634c99f24831585d5497b": {
        "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?",
        "authors": [
            "Wenxuan Wang",
            "Zizhan Ma",
            "Zheng Wang",
            "Chenghan Wu",
            "Wenting Chen",
            "Xiang Li",
            "Yixuan Yuan"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.11211",
        "abstract": "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents&#39; performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11211"
    },
    "3e7779e7fe5866d84eb6ed7de18b487b": {
        "title": "&#34;Nuclear Deployed!&#34;: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents",
        "authors": [
            "Rongwu Xu",
            "Xiaojian Li",
            "Shuo Chen",
            "Wei Xu"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11355",
        "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent&#39;s Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11355"
    },
    "8fee1a3c3a526591ec860fe41e53fe01": {
        "title": "LLM Agents Making Agent Tools",
        "authors": [
            "Georg Wölflein",
            "Dyke Ferber",
            "Daniel Truhn",
            "Ognjen Arandjelović",
            "Jakob Nikolas Kather"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11705",
        "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11705"
    },
    "4126585e4c5333fe61494856b3e6e47f": {
        "title": "Can LLM Agents Maintain a Persona in Discourse?",
        "authors": [
            "Pranav Bhandari",
            "Nicolas Fay",
            "Michael Wise",
            "Amitava Datta",
            "Stephanie Meek",
            "Usman Naseem",
            "Mehwish Nasim"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11843",
        "abstract": "Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11843"
    },
    "b217ee7110f1139e7d6d3e6f8f401f96": {
        "title": "A-MEM: Agentic Memory for LLM Agents",
        "authors": [
            "Wujiang Xu",
            "Zujie Liang",
            "Kai Mei",
            "Hang Gao",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12110",
        "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems&#39; fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at https://github.com/WujiangXu/AgenticMemory.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12110"
    },
    "4628e63125c3c42557cd811300aa9a85": {
        "title": "InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context",
        "authors": [
            "Bryan L. M. de Oliveira",
            "Luana G. B. Martins",
            "Bruno Brandão",
            "Luckeciano C. Melo"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12257",
        "abstract": "While large language models excel at following explicit instructions, they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses rather than seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. The benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue through clarifying questions before providing appropriate responses. Our evaluation of both open and closed-source models reveals that while proprietary models generally perform better, all current assistants struggle with effectively gathering critical information, often requiring multiple turns to infer user intent and frequently defaulting to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models&#39; information-seeking capabilities, offering insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12257"
    },
    "62a231ee39598886fa9227024afd2f0b": {
        "title": "LM Agents for Coordinating Multi-User Information Gathering",
        "authors": [
            "Harsh Jhamtani",
            "Jacob Andreas",
            "Benjamin Van Durme"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12328",
        "abstract": "This paper introduces PeopleJoin, a benchmark for evaluating LM-mediated collaborative problem solving. Given a user request, PeopleJoin agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer or summary for the original user. PeopleJoin comprises two evaluation domains: PeopleJoin-QA, focused on questions about tabular data, and PeopleJoin-DocCreation, focused on document creation tasks. The two domains are adapted from existing NLP benchmarks for database question answering and multi-document summarization; here, however, the information needed to complete these tasks is distributed across synthetic ``organizations&#39;&#39; of 2--20 users, simulating natural multi-user collaboration scenarios. We implemented several popular LM agent architectures, evaluating their accuracy and efficiency at completing tasks, and highlight new research questions that can be studied using PeopleJoin.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12328"
    },
    "5ebddc73abd7382da524ae84d744a569": {
        "title": "One Size doesn&#39;t Fit All: A Personalized Conversational Tutoring Agent for Mathematics Instruction",
        "authors": [
            "Ben Liu",
            "Jihan Zhang",
            "Fangquan Lin",
            "Xu Jia",
            "Min Peng"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12633",
        "abstract": "Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized \\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics instruction. PACE simulates students&#39; learning styles based on the Felder and Silverman learning style model, aligning with each student&#39;s persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance students&#39; comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12633"
    },
    "5dce6608d2cc0f603531908cd183e0b6": {
        "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs",
        "authors": [
            "Sumin Jo",
            "Junseong Choi",
            "Jiho Kim",
            "Edward Choi"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12767",
        "abstract": "Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning. To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across multiple KG-based reasoning tasks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12767"
    },
    "aeb3f654e791e27e76d124ba503913cd": {
        "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation",
        "authors": [
            "Mohammad Feli",
            "Iman Azimi",
            "Pasi Liljeberg",
            "Amir M. Rahmani"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12836",
        "abstract": "Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs&#39; limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent&#39;s performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12836"
    },
    "8009cf67dd8c4184850e002591f18b70": {
        "title": "SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems",
        "authors": [
            "Mike Zhang",
            "Amalie Pernille Dilling",
            "Léon Gondelman",
            "Niels Erik Ruan Lyngdorf",
            "Euan D. Lindsay",
            "Johannes Bjerva"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12927",
        "abstract": "Providing high-quality feedback is crucial for student success but is constrained by time, cost, and limited data availability. We introduce Synthetic Educational Feedback Loops (SEFL), a novel framework designed to deliver immediate, on-demand feedback at scale without relying on extensive, real-world student data. In SEFL, two large language models (LLMs) operate in teacher--student roles to simulate assignment completion and formative feedback, generating abundant synthetic pairs of student work and corresponding critiques. We then fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher--&gt;student feedback loop for diverse assignments. Through both LLM-as-a-judge and human evaluations, we demonstrate that SEFL-tuned models outperform their non-tuned counterparts in feedback quality, clarity, and timeliness. These findings reveal SEFL&#39;s potential to transform feedback processes for higher education and beyond, offering an ethical and scalable alternative to conventional manual feedback cycles.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12927"
    },
    "04b64a4d266e9bdf14c25aee46d25a8e": {
        "title": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile Agents Against Active Environmental Injection Attacks",
        "authors": [
            "Yurun Chen",
            "Xueyu Hu",
            "Keting Yin",
            "Juncheng Li",
            "Shengyu Zhang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13053",
        "abstract": "As researchers continuously optimize AI agents to perform tasks more effectively within operating systems, they often neglect to address the critical need for enabling these agents to identify &#34;impostors&#34; within the system. Through an analysis of the agents&#39; operating environment, we identified a potential threat: attackers can disguise their attack methods as environmental elements, injecting active disturbances into the agents&#39; execution process, thereby disrupting their decision-making. We define this type of attack as Active Environment Injection Attack (AEIA). Based on this, we propose AEIA-MN, an active environment injection attack scheme that exploits interaction vulnerabilities in the mobile operating system to evaluate the robustness of MLLM-based agents against such threats. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% in the AndroidWorld benchmark.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13053"
    },
    "da2942b9b05b8b3b487fb9190fb379a6": {
        "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors",
        "authors": [
            "Jian Wang",
            "Yinpei Dai",
            "Yichi Zhang",
            "Ziqiao Ma",
            "Wenjie Li",
            "Joyce Chai"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13311",
        "abstract": "Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student&#39;s knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13311"
    },
    "67b2128ecda1f7a56e0289cbd4509b50": {
        "title": "DataSciBench: An LLM Agent Benchmark for Data Science",
        "authors": [
            "Dan Zhang",
            "Sining Zhoubian",
            "Min Cai",
            "Fengzu Li",
            "Lekang Yang",
            "Wei Wang",
            "Tianjiao Dong",
            "Ziniu Hu",
            "Jie Tang",
            "Yisong Yue"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13897",
        "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13897"
    },
    "8ff3660d187b851f21a4873ff43ab836": {
        "title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Xiao Wang",
            "Yin Fang",
            "Haolin Liu",
            "Yifan Yang",
            "Fangyuan Chen",
            "Zhixing Song",
            "Dengyu Wang",
            "Minjia Zhang",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13957",
        "abstract": "Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6\\% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at https://rag-gym.github.io/.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13957"
    },
    "a7e1c6ae709b2d76c85e0c521a2b1fbc": {
        "title": "LIDDIA: Language-based Intelligent Drug Discovery Agent",
        "authors": [
            "Reza Averly",
            "Frazier N. Baker",
            "Xia Ning"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13959",
        "abstract": "Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it can identify promising novel drug candidates on EGFR, a critical target for cancers.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13959"
    },
    "386e8830f028c8f68c61db0e4c811df3": {
        "title": "UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text",
        "authors": [
            "Primoz Kocbek",
            "Leon Kopitar",
            "Zhihong Zhang",
            "Emirhan Aydin",
            "Maxim Topaz",
            "Gregor Stiglic"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.14144",
        "abstract": "This paper describes our submissions to the TREC 2024 PLABA track with the aim to simplify biomedical abstracts for a K8-level audience (13-14 years old students). We tested three approaches using OpenAI&#39;s gpt-4o and gpt-4o-mini models: baseline prompt engineering, a two-AI agent approach, and fine-tuning. Adaptations were evaluated using qualitative metrics (5-point Likert scales for simplicity, accuracy, completeness, and brevity) and quantitative readability scores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the two-agent approach and baseline prompt engineering with gpt-4o-mini models show superior qualitative performance, while fine-tuned models excelled in accuracy and completeness but were less simple. The evaluation results demonstrated that prompt engineering with gpt-4o-mini outperforms iterative improvement strategies via two-agent approach as well as fine-tuning with gpt-4o. We intend to expand our investigation of the results and explore advanced evaluations.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14144"
    },
    "1d7f1c1a670d5ef09e524fa614cd496f": {
        "title": "Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction",
        "authors": [
            "Mohammadmahdi Jafari",
            "Devin Yuncheng Hua",
            "Hao Xue",
            "Flora Salim"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14171",
        "abstract": "Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14171"
    },
    "f3bf6c6ce4a547bfaec4ce33b4d88476": {
        "title": "Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization",
        "authors": [
            "Zhitao He",
            "Zijun Liu",
            "Peng Li",
            "May Fung",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14496",
        "abstract": "LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents&#39; policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14496"
    },
    "52cdf9bddd3a749978a92ea7a6021c18": {
        "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
        "authors": [
            "Deepak Nathani",
            "Lovish Madaan",
            "Nicholas Roberts",
            "Nikolay Bashlykov",
            "Ajay Menon",
            "Vincent Moens",
            "Amar Budhiraja",
            "Despoina Magka",
            "Vladislav Vorotilov",
            "Gaurav Chaurasia",
            "Dieuwke Hupkes",
            "Ricardo Silveira Cabral",
            "Tatiana Shavrina",
            "Jakob Foerster",
            "Yoram Bachrach",
            "William Yang Wang",
            "Roberta Raileanu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14499",
        "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14499"
    },
    "51c734eb714ade9075f7a1d98fea4b33": {
        "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
        "authors": [
            "Zhenhong Zhou",
            "Zherui Li",
            "Jie Zhang",
            "Yuanhe Zhang",
            "Kun Wang",
            "Yang Liu",
            "Qing Guo"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14529",
        "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14529"
    },
    "d716b75b322213af61a1b675038385ab": {
        "title": "InstructAgent: Building User Controllable Recommender via LLM Agent",
        "authors": [
            "Wujiang Xu",
            "Yunxiao Shi",
            "Zujie Liang",
            "Xuying Ning",
            "Kai Mei",
            "Kun Wang",
            "Xi Zhu",
            "Min Xu",
            "Yongfeng Zhang"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14662",
        "abstract": "Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform&#39;s recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform&#39;s benefits, which may hinder their ability to protect and capture users&#39; true interests. Second, these models are typically optimized using data from all users, which may overlook individual user&#39;s preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as $\\dataset$, along with user instructions for each record.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14662"
    },
    "bdba08a4f13bc86187c524d06a4de449": {
        "title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search",
        "authors": [
            "Zujie Liang",
            "Feng Wei",
            "Wujiang Xu",
            "Lin Chen",
            "Yuxi Qian",
            "Xinhui Wu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14693",
        "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process.Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node&#39;s solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier.Applied to the various ML tasks, our approach demonstrates a6\\% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14693"
    },
    "5c7caf97e434d7b064871a5b80ed88a0": {
        "title": "ALU: Agentic LLM Unlearning",
        "authors": [
            "Debdeep Sanyal",
            "Murari Mandal"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00406",
        "abstract": "Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. We present the first agentic LLM unlearning (ALU) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our ALU framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and ALU seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that ALU consistently stands out as the most robust LLM unlearning framework among current state-of-the-art methods while incurring a low constant-time cost. We further highlight ALU&#39;s superior performance compared to existing methods when evaluated at scale. Specifically, ALU is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00406"
    },
    "1d0189ce757508161729909a2c110e35": {
        "title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents",
        "authors": [
            "George Fatouros",
            "Kostas Metaxas",
            "John Soldatos",
            "Manos Karathanassis"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00415",
        "abstract": "MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\\&amp;P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\\&amp;P 500 stocks during 2024 demonstrates the framework&#39;s scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00415"
    },
    "1d1ac354ff3472f17033899e0a1ca010": {
        "title": "Who&#39;s the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents",
        "authors": [
            "Yingxuan Yang",
            "Bo Huang",
            "Siyuan Qi",
            "Chao Feng",
            "Haoyi Hu",
            "Yuxuan Zhu",
            "Jinbo Hu",
            "Haoran Zhao",
            "Ziyi He",
            "Xiao Liu",
            "Zongyu Wang",
            "Lin Qiu",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00510",
        "abstract": "Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce CapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in cooperative game theory&#39;s Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent&#39;s architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. Key contributions include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,500 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00510"
    },
    "423438288d35797d3481f1c75c359e77": {
        "title": "Eliciting Language Model Behaviors with Investigator Agents",
        "authors": [
            "Xiang Lisa Li",
            "Neil Chowdhury",
            "Daniel D. Johnson",
            "Tatsunori Hashimoto",
            "Percy Liang",
            "Sarah Schwettmann",
            "Jacob Steinhardt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01236",
        "abstract": "Language models exhibit complex, diverse behaviors when prompted with free-form text, making it difficult to characterize the space of possible outputs. We study the problem of behavior elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations or harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train investigator models to map randomly-chosen target behaviors to a diverse distribution of outputs that elicit them, similar to amortized Bayesian inference. We do this through supervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe training objective to iteratively discover diverse prompting strategies. Our investigator models surface a variety of effective and human-interpretable prompts leading to jailbreaks, hallucinations, and open-ended aberrant behaviors, obtaining a 100% attack success rate on a subset of AdvBench (Harmful Behaviors) and an 85% hallucination rate.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01236"
    },
    "ca182eb5c94457e3d3864caf8ce295e7": {
        "title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant",
        "authors": [
            "Gaole He",
            "Gianluca Demartini",
            "Ujwal Gadiraju"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01390",
        "abstract": "Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of &#39;LLM-modulo&#39; setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01390"
    },
    "c2e5f782a453b942ea940d54760e2a21": {
        "title": "Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies",
        "authors": [
            "Han Zhou",
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hamid Palangi",
            "Shariq Iqbal",
            "Ivan Vulić",
            "Anna Korhonen",
            "Sercan Ö. Arık"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.02533",
        "abstract": "Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design. Based on the insights, we propose Multi-Agent System Search (MASS), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that MASS-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the MASS-found systems, we finally propose design principles behind building effective multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02533"
    },
    "3d2598771a524886d8dc2e962511838c": {
        "title": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs",
        "authors": [
            "Ben Liu",
            "Jihai Zhang",
            "Fangquan Lin",
            "Cheng Yang",
            "Min Peng",
            "Wotao Yin"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.03283",
        "abstract": "Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM&#39;s inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03283"
    },
    "fdd09538eac934057c78a0fd7a49744e": {
        "title": "Enhancing Online Learning Efficiency Through Heterogeneous Resource Integration with a Multi-Agent RAG System",
        "authors": [
            "Devansh Srivastav",
            "Hasan Md Tusfiqur Alam",
            "Afsaneh Asaei",
            "Mahmoud Fazeli",
            "Tanisha Sharma",
            "Daniel Sonntag"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.03948",
        "abstract": "Efficient online learning requires seamless access to diverse resources such as videos, code repositories, documentation, and general web content. This poster paper introduces early-stage work on a Multi-Agent Retrieval-Augmented Generation (RAG) System designed to enhance learning efficiency by integrating these heterogeneous resources. Using specialized agents tailored for specific resource types (e.g., YouTube tutorials, GitHub repositories, documentation websites, and search engines), the system automates the retrieval and synthesis of relevant information. By streamlining the process of finding and combining knowledge, this approach reduces manual effort and enhances the learning experience. A preliminary user study confirmed the system&#39;s strong usability and moderate-high utility, demonstrating its potential to improve the efficiency of knowledge acquisition.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03948"
    },
    "abe134e8f38cffcbf25cd687a6a0ff5c": {
        "title": "Multi-agent Architecture Search via Agentic Supernet",
        "authors": [
            "Guibin Zhang",
            "Luyang Niu",
            "Junfeng Fang",
            "Kun Wang",
            "Lei Bai",
            "Xiang Wang"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04180",
        "abstract": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \\textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \\textbf{(I)} requires only $6\\sim45\\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and \\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04180"
    },
    "7b5586a1f6eadb8c20bc07706cd4df4a": {
        "title": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research",
        "authors": [
            "Junde Wu",
            "Jiayuan Zhu",
            "Yuyuan Liu"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.04644",
        "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Unlike conventional LLM-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation (RAG) systems and closed-source LLMs. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving. The code is at: https://github.com/theworldofagents/Agentic-Reasoning.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04644"
    },
    "6da1ea26c443026424681e04a6652c0f": {
        "title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents",
        "authors": [
            "Gonzalo Gonzalez-Pumariega",
            "Leong Su Yean",
            "Neha Sunkara",
            "Sanjiban Choudhury"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.05227",
        "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents&#39; ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at https://github.com/portal-cornell/robotouille.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05227"
    },
    "6705ef475ab018d54e7b0a56a51daf0a": {
        "title": "Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews",
        "authors": [
            "Izunna Okpala",
            "Ashkan Golgoon",
            "Arjun Ravi Kannan"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05439",
        "abstract": "The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a manager and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection, hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a manager along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05439"
    },
    "71f0307a5b3c4c2ec46c52e57025f0c3": {
        "title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents",
        "authors": [
            "Jiabin Tang",
            "Tianyu Fan",
            "Chao Huang"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05957",
        "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent&#39;s effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent&#39;s Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05957"
    },
    "58977408842677b889fa9d2e02d0b7b9": {
        "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
        "authors": [
            "Bidipta Sarkar",
            "Warren Xia",
            "C. Karen Liu",
            "Dorsa Sadigh"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.06060",
        "abstract": "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent&#39;s goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model&#39;s listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model&#39;s speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06060"
    },
    "315df09f94bc364e0f21a8c65af0071b": {
        "title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering",
        "authors": [
            "Xuehang Guo",
            "Xingyao Wang",
            "Yangyi Chen",
            "Sha Li",
            "Chi Han",
            "Manling Li",
            "Heng Ji"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06994",
        "abstract": "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator&#39;s understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator&#39;s actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents&#39; capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent &lt;= 3.33% to Claude-3.5-Sonnet &gt;= 28.18%), their consistently low collaboration willingness (&lt;= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents&#39; resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: https://xhguo7.github.io/SyncMind/.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06994"
    },
    "9aad7e351bc7be72d4e16483798e2e0f": {
        "title": "EvoFlow: Evolving Diverse Agentic Workflows On The Fly",
        "authors": [
            "Guibin Zhang",
            "Kaijie Chen",
            "Guancheng Wan",
            "Heng Chang",
            "Hong Cheng",
            "Kun Wang",
            "Shuyue Hu",
            "Lei Bai"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07373",
        "abstract": "The past two years have witnessed the evolution of large language model (LLM)-based multi-agent systems from labor-intensive manual design to partial automation (\\textit{e.g.}, prompt engineering, communication topology) and eventually to fully automated design. However, existing agentic automation pipelines often lack LLM heterogeneity and focus on single-objective performance optimization, limiting their potential to combine weaker models for more customized and cost-effective solutions. To address this challenge, we propose EvoFlow, a niching evolutionary algorithm-based framework to automatically search a population of heterogeneous and complexity-adaptive agentic workflows, rather than a single homogeneous, complex workflow. Technically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract parent workflows from an agentic population, evolves new workflows through \\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4) niching-based selection} to maintain population diversity and quality. Extensive evaluations across seven benchmarks demonstrate that EvoFlow is: \\textbf{(I) diverse}, evolving a population of workflows ranging from simple I/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing}, outperforming previous handcrafted and automated workflows by $1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful \\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source models.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07373"
    },
    "f3089465cbbb57b6f59153af036fecb1": {
        "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model",
        "authors": [
            "Emre Can Acikgoz",
            "Jeremiah Greer",
            "Akul Datta",
            "Ze Yang",
            "William Zeng",
            "Oussama Elachqar",
            "Emmanouil Koukoumidis",
            "Dilek Hakkani-Tür",
            "Gokhan Tur"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08820",
        "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08820"
    },
    "1b26f138e797bc52615ba6d476544dfb": {
        "title": "PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology",
        "authors": [
            "Fatemeh Ghezloo",
            "Mehmet Saygin Seyfioglu",
            "Rustin Soraki",
            "Wisdom O. Ikezogwo",
            "Beibin Li",
            "Tejoram Vivekanandan",
            "Joann G. Elmore",
            "Ranjay Krishna",
            "Linda Shapiro"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.08916",
        "abstract": "Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents, the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient&#39;s diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent&#39;s outputs are of high quality and comparable to GPT-4o. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Data, code and models available at https://pathfinder-dx.github.io/",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08916"
    },
    "c498e01526b6a1bd27683f246148d1cf": {
        "title": "Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for Autonomous Vehicles",
        "authors": [
            "Galileo Sartor",
            "Adam Wyner",
            "Giuseppe Contissa"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09216",
        "abstract": "In this paper, we present a modular system for representing and reasoning with legal aspects of traffic rules for autonomous vehicles. We focus on a subset of the United Kingdom&#39;s Highway Code (HC) related to junctions. As human drivers and automated vehicles (AVs) will interact on the roads, especially in urban environments, we claim that an accessible, unitary, high-level computational model should exist and be applicable to both users. Autonomous vehicles introduce a shift in liability that should not bring disadvantages or increased burden on human drivers. We develop a system &#34;in silico&#34; of the model. The proposed system is built of three main components: a natural language interface, using Logical English, which encodes the rules; an internal representation of the rules in Prolog; and an multi-agent-based simulation environment, built in NetLogo. The three components interact: Logical English is translated into and out of Prolog (along with some support code); Prolog and NetLogo interface via predicates. Such a modular approach enables the different components to carry different &#34;burdens&#34; in the overall system; it also allows swapping of modules. Given NetLogo, we can visualize the effect of the modeled rules as well as validate the system with a simple dynamic running scenario. Designated agents monitor the behaviour of the vehicles for compliance and record potential violations where they occur. The information on potential violations is then utilized by Validators, to determine whether the violation is punishable, differentiating between exceptions and cases.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09216"
    },
    "6a7e5e4132fc422294461d5575b527c7": {
        "title": "Reliable Conversational Agents under ASP Control that Understand Natural Language",
        "authors": [
            "Yankai Zeng"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09237",
        "abstract": "Efforts have been made to make machines converse like humans in the past few decades. The recent techniques of Large Language Models (LLMs) make it possible to have human-like conversations with machines, but LLM&#39;s flaws of lacking understanding and reliability are well documented. We believe that the best way to eliminate this problem is to use LLMs only as parsers to translate text to knowledge and vice versa and carry out the conversation by reasoning over this knowledge using the answer set programming. I have been developing a framework based on LLMs and ASP to realize reliable chatbots that &#34;understand&#34; human conversation. This framework has been used to develop task-specific chatbots as well as socialbots. My future research is focused on making these chatbots scalable and trainable.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09237"
    },
    "fc84925a30798aa4c28a8e1c8de68052": {
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "authors": [
            "Daniel Jarrett",
            "Miruna Pîslar",
            "Michiel A. Bakker",
            "Michael Henry Tessler",
            "Raphael Köster",
            "Jan Balaguer",
            "Romuald Elie",
            "Christopher Summerfield",
            "Andrea Tacchetti"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09369",
        "abstract": "Consider the process of collective decision-making, in which a group of individuals interactively select a preferred outcome from among a universe of alternatives. In this context, &#34;representation&#34; is the activity of making an individual&#39;s preferences present in the process via participation by a proxy agent -- i.e. their &#34;representative&#34;. To this end, learned models of human behavior have the potential to fill this role, with practical implications for multi-agent scenario studies and mechanism design. In this work, we investigate the possibility of training \\textit{language agents} to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. First, we formalize the setting of \\textit{collective decision-making} -- as the episodic process of interaction between a group of agents and a decision mechanism. On this basis, we then formalize the problem of \\textit{digital representation} -- as the simulation of an agent&#39;s behavior to yield equivalent outcomes from the mechanism. Finally, we conduct an empirical case study in the setting of \\textit{consensus-finding} among diverse humans, and demonstrate the feasibility of fine-tuning large language models to act as digital representatives.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09369"
    },
    "dba7339fc3fd44954118e551b06df63e": {
        "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
        "authors": [
            "Rui Yang",
            "Hanyang Chen",
            "Junyu Zhang",
            "Mark Zhao",
            "Cheng Qian",
            "Kangrui Wang",
            "Qineng Wang",
            "Teja Venkat Koripella",
            "Marziyeh Movahedi",
            "Manling Li",
            "Heng Ji",
            "Huan Zhang",
            "Tong Zhang"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09560",
        "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09560"
    },
    "67dff052fc4f00bb9be082ebd2f58b3f": {
        "title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
        "authors": [
            "Kexin Huang",
            "Ying Jin",
            "Ryan Li",
            "Michael Y. Li",
            "Emmanuel Candès",
            "Jure Leskovec"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.09858",
        "abstract": "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper&#39;s principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ],
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09858"
    },
    "f47e1ae0afc0e200a076140e674e8b3b": {
        "title": "Position: Stop Acting Like Language Model Agents Are Normal Agents",
        "authors": [
            "Elija Perrier",
            "Michael Timothy Bennett"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.10420",
        "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10420"
    },
    "30f0457199ead9ca5893677c4b505f2a": {
        "title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention",
        "authors": [
            "Chengshuai Zhao",
            "Zhen Tan",
            "Chau-Wai Wong",
            "Xinyan Zhao",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.10937",
        "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10937"
    },
    "844230bbee12afb73e6bb9f3b8904a23": {
        "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
        "authors": [
            "Pan Lu",
            "Bowen Chen",
            "Sheng Liu",
            "Rahul Thapa",
            "Joseph Boen",
            "James Zou"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.11271",
        "abstract": "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools&#39; generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11271"
    },
    "82ef266d2dae8ac235159ac655f5984e": {
        "title": "SMART: Self-Aware Agent for Tool Overuse Mitigation",
        "authors": [
            "Cheng Qian",
            "Emre Can Acikgoz",
            "Hongru Wang",
            "Xiusi Chen",
            "Avirup Sil",
            "Dilek Hakkani-Tür",
            "Gokhan Tur",
            "Heng Ji"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11435",
        "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent&#39;s self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11435"
    },
    "11e616912a082b8226ecb99dad9fee51": {
        "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning",
        "authors": [
            "Peiying Yu",
            "Guoxin Chen",
            "Jingjing Wang"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11799",
        "abstract": "Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11799"
    },
    "709740a97b1400008d356a32d40fe33b": {
        "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
        "authors": [
            "Shao Zhang",
            "Xihuai Wang",
            "Wenhao Zhang",
            "Chaoran Li",
            "Junru Song",
            "Tingyu Li",
            "Lin Qiu",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Wen Yao",
            "Weinan Zhang",
            "Xinbing Wang",
            "Ying Wen"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11882",
        "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent&#39;s System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent&#39;s System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11882"
    },
    "974a9b5b1ac39b4038a36b846be9ba99": {
        "title": "A Study on Leveraging Search and Self-Feedback for Agent Reasoning",
        "authors": [
            "Karthikeyan K",
            "Michelle Yuan",
            "Elman Mansimov",
            "Katerina Margatina",
            "Anurag Pratik",
            "Daniele Bonadiman",
            "Monica Sunkara",
            "Yi Zhang",
            "Yassine Benajiba"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12094",
        "abstract": "Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model&#39;s own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model&#39;s self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12094"
    },
    "d3a80535890b30f6cd75f002dc747f2e": {
        "title": "HARBOR: Exploring Persona Dynamics in Multi-Agent Competition",
        "authors": [
            "Kenan Jiang",
            "Li Xiong",
            "Fei Liu"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12149",
        "abstract": "We investigate factors contributing to LLM agents&#39; success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent&#39;s behavior in a competitive setting? (b) Can an agent effectively profile its competitors&#39; behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12149"
    },
    "2b96f0e93540445ce1ad323c790e645a": {
        "title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design",
        "authors": [
            "Yuxuan Lu",
            "Bingsheng Yao",
            "Hansu Gu",
            "Jing Huang",
            "Jessie Wang",
            "Laurence Li",
            "Jiri Gesi",
            "Qi He",
            "Toby Jia-Jun Li",
            "Dakuo Wang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12561",
        "abstract": "Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12561"
    },
    "4858bc6a428a55b9018b0d9d36998f50": {
        "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations",
        "authors": [
            "Frederic Kirstein",
            "Muneeb Khan",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13001",
        "abstract": "Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13001"
    },
    "0257d95b6b6e7237f9c4efdbca38552b": {
        "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents",
        "authors": [
            "Chaoran Chen",
            "Bingsheng Yao",
            "Ruishi Zou",
            "Wenyue Hua",
            "Weimin Lyu",
            "Toby Jia-Jun Li",
            "Dakuo Wang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13012",
        "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13012"
    },
    "b457fdcfd3dc55e76ef3b3ba43a83bc3": {
        "title": "Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks",
        "authors": [
            "Markus J. Buehler"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13025",
        "abstract": "We present an agentic, autonomous graph expansion framework that iteratively structures and refines knowledge in situ. Unlike conventional knowledge graph construction methods relying on static extraction or single-pass learning, our approach couples a reasoning-native large language model with a continually updated graph representation. At each step, the system actively generates new concepts and relationships, merges them into a global graph, and formulates subsequent prompts based on its evolving structure. Through this feedback-driven loop, the model organizes information into a scale-free network characterized by hub formation, stable modularity, and bridging nodes that link disparate knowledge clusters. Over hundreds of iterations, new nodes and edges continue to appear without saturating, while centrality measures and shortest path distributions evolve to yield increasingly distributed connectivity. Our analysis reveals emergent patterns, such as the rise of highly connected &#39;hub&#39; concepts and the shifting influence of &#39;bridge&#39; nodes, indicating that agentic, self-reinforcing graph construction can yield open-ended, coherent knowledge structures. Applied to materials design problems, we present compositional reasoning experiments by extracting node-specific and synergy-level principles to foster genuinely novel knowledge synthesis, yielding cross-domain ideas that transcend rote summarization and strengthen the framework&#39;s potential for open-ended scientific discovery. We discuss other applications in scientific discovery and outline future directions for enhancing scalability and interpretability.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13025"
    },
    "0d91bbdcb48cfdec3f34a78e2d5b1ab4": {
        "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions",
        "authors": [
            "Taedong Yun",
            "Eric Yang",
            "Mustafa Safdari",
            "Jong Ha Lee",
            "Vaishnavi Vinod Kumar",
            "S. Sara Mahdavi",
            "Jonathan Amar",
            "Derek Peyton",
            "Reut Aharony",
            "Andreas Michaelides",
            "Logan Schneider",
            "Isaac Galatzer-Levy",
            "Yugang Jia",
            "John Canny",
            "Arthur Gretton",
            "Maja Matarić"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13135",
        "abstract": "We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent&#39;s understanding of the synthetic users&#39; needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13135"
    },
    "f9514f2310d03555870205d78411a23a": {
        "title": "An LLM-based Agent for Reliable Docker Environment Configuration",
        "authors": [
            "Ruida Hu",
            "Chao Peng",
            "Xinchen Wang",
            "Cuiyun Gao"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13681",
        "abstract": "Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment &#34;pollution&#34; from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13681"
    },
    "3563a557a7d757a94cdabed8117ca752": {
        "title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning",
        "authors": [
            "Hanlin Wang",
            "Jian Wang",
            "Chak Tou Leong",
            "Wenjie Li"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14276",
        "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14276"
    },
    "a13576fb7b6c8f02b32a8aa7a0ea6c60": {
        "title": "Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems",
        "authors": [
            "Bingyu Yan",
            "Xiaoming Zhang",
            "Litian Zhang",
            "Lian Zhang",
            "Ziyi Zhou",
            "Dezhuang Miao",
            "Chaozhuo Li"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14321",
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in reasoning, planning, and decision-making. Building upon these strengths, researchers have begun incorporating LLMs into multi-agent systems (MAS), where agents collaborate or compete through natural language interactions to tackle tasks beyond the scope of single-agent setups. In this survey, we present a communication-centric perspective on LLM-based multi-agent systems, examining key system-level features such as architecture design and communication goals, as well as internal mechanisms like communication strategies, paradigms, objects and content. We illustrate how these communication elements interplay to enable collective intelligence and flexible collaboration. Furthermore, we discuss prominent challenges, including scalability, security, and multimodal integration, and propose directions for future work to advance research in this emerging domain. Ultimately, this survey serves as a catalyst for further innovation, fostering more robust, scalable, and intelligent multi-agent systems across diverse application domains.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14321"
    },
    "65c80808ab2692e24531b9ec34ebe56d": {
        "title": "Optimizing Model Selection for Compound AI Systems",
        "authors": [
            "Lingjiao Chen",
            "Jared Quincy Davis",
            "Boris Hanin",
            "Peter Bailis",
            "Matei Zaharia",
            "James Zou",
            "Ion Stoica"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14815",
        "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14815"
    },
    "a5653a5ea070143a12a1f50be4a977b7": {
        "title": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents",
        "authors": [
            "Jingoo Lee",
            "Kyungho Lim",
            "Young-Chul Jung",
            "Byung-Hoon Kim"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01594",
        "abstract": "Recent advances in large language models (LLMs) have accelerated the development of conversational agents capable of generating human-like responses. Since psychiatric assessments typically involve complex conversational interactions between psychiatrists and patients, there is growing interest in developing LLM-based psychiatric assessment conversational agents (PACAs) that aim to simulate the role of psychiatrists in clinical evaluations. However, standardized methods for benchmarking the clinical appropriateness of PACAs&#39; interaction with patients still remain underexplored. Here, we propose PSYCHE, a novel framework designed to enable the 1) clinically relevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation of PACAs. This is achieved by simulating psychiatric patients based on a multi-faceted psychiatric construct that defines the simulated patients&#39; profiles, histories, and behaviors, which PACAs are expected to assess. We validate the effectiveness of PSYCHE through a study with 10 board-certified psychiatrists, supported by an in-depth analysis of the simulated patient utterances.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01594"
    },
    "e15c9128ad8d94e1ec27e0a2a5bd996b": {
        "title": "LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models",
        "authors": [
            "Pouria Rouzrokh",
            "Moein Shariatnia"
        ],
        "date": "2025/01/05",
        "pdf": "http://arxiv.org/pdf/2501.05468",
        "abstract": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process. Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction. These agents operate within orchestrated workflows, supporting sequential and parallel review rounds, dynamic decision-making, and iterative refinement based on user feedback. LatteReview&#39;s architecture integrates LLM providers, enabling compatibility with both cloud-based and locally hosted models. The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets. The framework is available on the GitHub repository, with detailed documentation and an installable package.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05468"
    },
    "2ebcc67f683e9d099bb4494cd6479ade": {
        "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
        "authors": [
            "Vighnesh Subramaniam",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Shuang Li",
            "Igor Mordatch"
        ],
        "date": "2025/01/10",
        "pdf": "http://arxiv.org/pdf/2501.05707",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05707"
    },
    "63fe8ee82c8b5f95a514e76fb4e4a33b": {
        "title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
        "authors": [
            "Xiangru Tang",
            "Tianyu Hu",
            "Muyang Ye",
            "Yanjun Shao",
            "Xunjian Yin",
            "Siru Ouyang",
            "Wangchunshu Zhou",
            "Pan Lu",
            "Zhuosheng Zhang",
            "Yilun Zhao",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "date": "2025/01/11",
        "pdf": "http://arxiv.org/pdf/2501.06590",
        "abstract": "Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found at https://github.com/gersteinlab/chemagent",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.06590"
    },
    "6963aadc233c22e271b20c0392235c22": {
        "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation",
        "authors": [
            "Jiaxin Guo",
            "Yuanchang Luo",
            "Daimeng Wei",
            "Ling Zhang",
            "Zongyao Li",
            "Hengchao Shang",
            "Zhiqiang Rao",
            "Shaojun Li",
            "Jinlong Yang",
            "Zhanglin Wu",
            "Hao Yang"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.08523",
        "abstract": "The field of artificial intelligence has witnessed significant advancements in natural language processing, largely attributed to the capabilities of Large Language Models (LLMs). These models form the backbone of Agents designed to address long-context dependencies, particularly in Document-level Machine Translation (DocMT). DocMT presents unique challenges, with quality, consistency, and fluency being the key metrics for evaluation. Existing approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an incremental sentence-level forced decoding strategy \\textbf{to ensure every sentence is translated while enhancing the fluency of adjacent sentences.} Our Agent leverages a Doc-Guided Memory, focusing solely on the summary and its translation, which we find to be an efficient approach to maintaining consistency. Through extensive testing across multiple languages and domains, we demonstrate that Sent2Sent++ outperforms other methods in terms of quality, consistency, and fluency. The results indicate that, our approach has achieved significant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and document-level perplexity (d-ppl). The contributions of this paper include a detailed analysis of current DocMT research, the introduction of the Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of its effectiveness across languages and domains.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.08523"
    },
    "6980029f3a23fdd6bfca03f5bca21072": {
        "title": "Personality Modeling for Persuasion of Misinformation using AI Agent",
        "authors": [
            "Qianmin Lou",
            "Wentao Xu"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.08985",
        "abstract": "The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.08985"
    },
    "65f6d238b1eb6890b606f14a2881ac8d": {
        "title": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral Therapy in Psychological Counseling",
        "authors": [
            "Ancheng Xu",
            "Di Yang",
            "Renhao Li",
            "Jingwei Zhu",
            "Minghuan Tan",
            "Min Yang",
            "Wanxin Qiu",
            "Mingchen Ma",
            "Haihong Wu",
            "Bingyu Li",
            "Feng Sha",
            "Chengming Li",
            "Xiping Hu",
            "Qiang Qu",
            "Derek F. Wong",
            "Ruifeng Xu"
        ],
        "date": "2025/01/16",
        "pdf": "http://arxiv.org/pdf/2501.09426",
        "abstract": "Traditional in-person psychological counseling remains primarily niche, often chosen by individuals with psychological issues, while online automated counseling offers a potential solution for those hesitant to seek help due to feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and widely used approach in psychological counseling. The advent of large language models (LLMs) and agent technology enables automatic CBT diagnosis and treatment. However, current LLM-based CBT systems use agents with a fixed structure, limiting their self-optimization capabilities, or providing hollow, unhelpful suggestions due to redundant response patterns. In this work, we utilize Quora-like and YiXinLi single-round consultation models to build a general agent framework that generates high-quality responses for single-turn psychological consultation scenarios. We use a bilingual dataset to evaluate the quality of single-response consultations generated by each framework. Then, we incorporate dynamic routing and supervisory mechanisms inspired by real psychological counseling to construct a CBT-oriented autonomous multi-agent framework, demonstrating its general applicability. Experimental results indicate that AutoCBT can provide higher-quality automated psychological counseling services.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09426"
    },
    "3eb3d74a0ca144a0709f7ea8645be09d": {
        "title": "Agent-as-Judge for Factual Summarization of Long Narratives",
        "authors": [
            "Yeonseok Jeong",
            "Minsoo Kim",
            "Seung-won Hwang",
            "Byung-Hak Kim"
        ],
        "date": "2025/01/17",
        "pdf": "http://arxiv.org/pdf/2501.09993",
        "abstract": "Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (&gt;100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel &#34;Agent-as-a-Judge&#34; framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09993"
    },
    "49f35b96e2d2abc2615029be99395cd7": {
        "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
        "authors": [
            "Elad Levi",
            "Ilan Kadar"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.11067",
        "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11067"
    },
    "08b501dfcdada77f8db89771fbd74895": {
        "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
        "authors": [
            "Zhenhailong Wang",
            "Haiyang Xu",
            "Junyang Wang",
            "Xi Zhang",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Heng Ji"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.11733",
        "abstract": "Smartphones have become indispensable in modern life, yet navigating complex tasks on mobile devices often remains frustrating. Recent advancements in large multimodal model (LMM)-based mobile agents have demonstrated the ability to perceive and act in mobile environments. However, current approaches face significant limitations: they fall short in addressing real-world human needs, struggle with reasoning-intensive and long-horizon tasks, and lack mechanisms to learn and improve from prior experiences. To overcome these challenges, we introduce Mobile-Agent-E, a hierarchical multi-agent framework capable of self-evolution through past experience. By hierarchical, we mean an explicit separation of high-level planning and low-level action execution. The framework comprises a Manager, responsible for devising overall plans by breaking down complex tasks into subgoals, and four subordinate agents--Perceptor, Operator, Action Reflector, and Notetaker--which handle fine-grained visual perception, immediate action execution, error verification, and information aggregation, respectively. Mobile-Agent-E also features a novel self-evolution module which maintains a persistent long-term memory comprising Tips and Shortcuts. Tips are general guidance and lessons learned from prior tasks on how to effectively interact with the environment. Shortcuts are reusable, executable sequences of atomic operations tailored for specific subroutines. The inclusion of Tips and Shortcuts facilitates continuous refinement in performance and efficiency. Alongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring complex mobile tasks requiring long-horizon, multi-app interactions. Empirical results show that Mobile-Agent-E achieves a 22% absolute improvement over previous state-of-the-art approaches across three foundation model backbones. Project page: https://x-plug.github.io/MobileAgent.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11733"
    },
    "6dbbf1cb2add3b300212f60478a59d68": {
        "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
        "authors": [
            "Zhenran Xu",
            "Longyue Wang",
            "Jifang Wang",
            "Zhouyi Li",
            "Senbao Shi",
            "Xue Yang",
            "Yiyu Wang",
            "Baotian Hu",
            "Jun Yu",
            "Min Zhang"
        ],
        "date": "2025/01/22",
        "pdf": "http://arxiv.org/pdf/2501.12909",
        "abstract": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LLM-based multi-agent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FilmAgent simulates various crew roles, including directors, screenwriters, actors, and cinematographers, and covers key stages of a film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. A team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FilmAgent outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FilmAgent, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of a well-coordinated multi-agent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAI&#39;s text-to-video model Sora and our FilmAgent in filmmaking.",
        "code": "",
        "category": [
            [
                "Application",
                "Art"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12909"
    },
    "6ac0e6879afe3f2a6a53d4d966db8877": {
        "title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
        "authors": [
            "Shrinidhi Kumbhar",
            "Venkatesh Mishra",
            "Kevin Coutinho",
            "Divij Handa",
            "Ashif Iquebal",
            "Chitta Baral"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.13299",
        "abstract": "Materials discovery and design are essential for advancing technology across various industries by enabling the development of application-specific materials. Recent research has leveraged Large Language Models (LLMs) to accelerate this process. We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery. Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing real-world applications. Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints. To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically. Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs.",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ],
            [
                "Application",
                "Physics"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13299"
    },
    "388d238c303993b6eb0a201766c76829": {
        "title": "Self-Explanation in Social AI Agents",
        "authors": [
            "Rhea Basappa",
            "Mustafa Tekman",
            "Hong Lu",
            "Benjamin Faught",
            "Sandeep Kakar",
            "Ashok K. Goel"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.13945",
        "abstract": "Social AI agents interact with members of a community, thereby changing the behavior of the community. For example, in online learning, an AI social assistant may connect learners and thereby enhance social interaction. These social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners. We present a method of self-explanation that uses introspection over a self-model of an AI social assistant. The self-model is captured as a functional model that specifies how the methods of the agent use knowledge to achieve its tasks. The process of generating self-explanations uses Chain of Thought to reflect on the self-model and ChatGPT to provide explanations about its functioning. We evaluate the self-explanation of the AI social assistant for completeness and correctness. We also report on its deployment in a live class.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13945"
    },
    "c0ad4257855e22352665edac9454a908": {
        "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
        "authors": [
            "Diego Gosmar",
            "Deborah A. Dahl"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.13946",
        "abstract": "Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI systems and their reliability. This study investigates how orchestrating multiple specialized Artificial Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations, into a front-end agent. The outputs are then systematically reviewed and refined by second- and third-level agents, each employing distinct large language models and tailored strategies to detect unverified claims, incorporate explicit disclaimers, and clarify speculative content. Additionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed to evaluate hallucination score levels. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed assessments and ensuring accurate quantification of shifts in hallucination-related behaviors. A core component of this investigation is the use of the OVON (Open Voice Network) framework, which relies on universal NLP-based interfaces to transfer contextual information among agents. Through structured JSON messages, each agent communicates its assessment of the hallucination likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to refine the text without losing context. The results demonstrate that employing multiple specialized agents capable of interoperating with each other through NLP-based agentic frameworks can yield promising outcomes in hallucination mitigation, ultimately bolstering trust within the AI community.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13946"
    },
    "5f82858e162b2c670172c9a8e2d121a2": {
        "title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
        "authors": [
            "Preston Rasmussen",
            "Pavlo Paliychuk",
            "Travis Beauvais",
            "Jack Ryan",
            "Daniel Chalef"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.13956",
        "abstract": "We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti -- a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance (94.8% vs 93.4%). Beyond DMR, Zep&#39;s capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to 18.5% while simultaneously reducing response latency by 90% compared to baseline implementations. These results are particularly pronounced in enterprise-critical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep&#39;s effectiveness for deployment in real-world applications.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13956"
    },
    "884bfd6c802d7881215907e5e22a1a50": {
        "title": "Communicating Activations Between Language Model Agents",
        "authors": [
            "Vignav Ramesh",
            "Kenneth Li"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.14082",
        "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\\textit{B}$&#39;s computation at an intermediate layer, combine its current activation with another LM $\\textit{A}$&#39;s intermediate activation via some function $\\textit{f}$, then pass $\\textit{f}$&#39;s output into the next layer of $\\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\\%$ improvement over natural language communication across datasets with $&lt;$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative &#34;language&#34; for communication between LMs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14082"
    },
    "16b1588c25fd0f3fdd2663b75443cc23": {
        "title": "Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game",
        "authors": [
            "Rong Ye",
            "Yongxin Zhang",
            "Yikai Zhang",
            "Haoyu Kuang",
            "Zhongyu Wei",
            "Peng Sun"
        ],
        "date": "2025/01/24",
        "pdf": "http://arxiv.org/pdf/2501.14225",
        "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can not only make stratigic decisions but also engage in flexible and meaningful communication. Inspired by Wittgenstein&#39;s language game theory in Philosophical Investigations, we propose that language agents can learn through in-context interaction rather than traditional multi-stage frameworks that separate decision-making from language expression. Using Werewolf, a social deduction game that tests language understanding, strategic interaction, and adaptability, we develop the Multi-agent Kahneman &amp; Tversky&#39;s Optimization (MaKTO). MaKTO engages diverse models in extensive gameplay to generate unpaired desirable and unacceptable responses, then employs KTO to refine the model&#39;s decision-making process. In 9-player Werewolf games, MaKTO achieves a 61% average win rate across various models, outperforming GPT-4o and two-stage RL agents by relative improvements of 23.0% and 10.9%, respectively. Notably, MaKTO also demonstrates human-like performance, winning 60% against expert players and showing only 49% detectability in Turing-style blind tests. These results showcase MaKTO&#39;s superior decision-making, strategic adaptation, and natural language generation in complex social deduction games.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14225"
    },
    "669c9a4b755d8188c634d5905b8db9c1": {
        "title": "Unmasking Conversational Bias in AI Multiagent Systems",
        "authors": [
            "Erica Coppolillo",
            "Giuseppe Manco",
            "Luca Maria Aiello"
        ],
        "date": "2025/01/24",
        "pdf": "http://arxiv.org/pdf/2501.14844",
        "abstract": "Detecting biases in the outputs produced by generative models is essential to reduce the potential risks associated with their application in critical settings. However, the majority of existing methodologies for identifying biases in generated text consider the models in isolation and neglect their contextual applications. Specifically, the biases that may arise in multi-agent systems involving generative models remain under-researched. To address this gap, we present a framework designed to quantify biases within multi-agent systems of conversational Large Language Models (LLMs). Our approach involves simulating small echo chambers, where pairs of LLMs, initialized with aligned perspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe significant shifts in the stance expressed in the generated messages, particularly within echo chambers where all agents initially express conservative viewpoints, in line with the well-documented political bias of many LLMs toward liberal positions. Crucially, the bias observed in the echo-chamber experiment remains undetected by current state-of-the-art bias detection methods that rely on questionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias detection and mitigation for AI multi-agent systems. The code to perform the experiments is publicly available at https://anonymous.4open.science/r/LLMsConversationalBias-7725.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14844"
    },
    "595bb55d6de8444d8ea62370b7df247e": {
        "title": "Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning",
        "authors": [
            "Yiqun Chen",
            "Lingyong Yan",
            "Weiwei Sun",
            "Xinyu Ma",
            "Yi Zhang",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Yiming Yang",
            "Jiaxin Mao"
        ],
        "date": "2025/01/25",
        "pdf": "http://arxiv.org/pdf/2501.15228",
        "abstract": "Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents&#39; goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is on https://github.com/chenyiqun/MMOA-RAG.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15228"
    },
    "953ba69bdd56a78f559bae8a78959bd4": {
        "title": "Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions",
        "authors": [
            "Naihao Deng",
            "Rada Mihalcea"
        ],
        "date": "2025/01/25",
        "pdf": "http://arxiv.org/pdf/2501.15283",
        "abstract": "As Large Language Models (LLMs) advance in their capabilities, researchers have increasingly employed them for social simulation. In this paper, we investigate whether interactions among LLM agents resemble those of humans. Specifically, we focus on the pronoun usage difference between leaders and non-leaders, examining whether the simulation would lead to human-like pronoun usage patterns during the LLMs&#39; interactions. Our evaluation reveals the significant discrepancies between LLM-based simulations and human pronoun usage, with prompt-based or specialized agents failing to demonstrate human-like pronoun usage patterns. In addition, we reveal that even if LLMs understand the human pronoun usage patterns, they fail to demonstrate them in the actual interaction process. Our study highlights the limitations of social simulations based on LLM agents, urging caution in using such social simulation in practitioners&#39; decision-making process.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15283"
    },
    "46fe4f04f470a7053cc835b5c47b45e6": {
        "title": "Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection",
        "authors": [
            "Bo Yang",
            "Jiaxian Guo",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "date": "2025/01/26",
        "pdf": "http://arxiv.org/pdf/2501.15355",
        "abstract": "Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent&#39;s perception of its counterpart&#39;s mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts&#39; inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the \\textit{second-order} ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart&#39;s behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15355"
    },
    "fe12b9738b6d527d4e4380a84b6e1b96": {
        "title": "MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral Mental Health Question Answer",
        "authors": [
            "Qi Chen",
            "Dexi Liu"
        ],
        "date": "2025/01/27",
        "pdf": "http://arxiv.org/pdf/2501.15826",
        "abstract": "The Mental Health Question Answer (MHQA) task requires the seeker and supporter to complete the support process in one-turn dialogue. Given the richness of help-seeker posts, supporters must thoroughly understand the content and provide logical, comprehensive, and well-structured responses. Previous works in MHQA mostly focus on single-agent approaches based on the cognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the interactions among various CBT elements, such as emotion and cognition. This limitation hinders the models&#39; ability to thoroughly understand the distress of help-seekers. To address this, we propose a framework named Multi-Agent Deductive Planning (MADP), which is based on the interactions between the various psychological elements of CBT. This method guides Large Language Models (LLMs) to achieve a deeper understanding of the seeker&#39;s context and provide more personalized assistance based on individual circumstances. Furthermore, we construct a new dataset based on the MADP framework and use it to fine-tune LLMs, resulting in a specialized model named MADP-LLM. We conduct extensive experiments, including comparisons with multiple LLMs, human evaluations, and automatic evaluations, to validate the effectiveness of the MADP framework and MADP-LLM.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15826"
    },
    "89f171e17eb49ccf57191b08a3a19db2": {
        "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models",
        "authors": [
            "Yuxuan Li",
            "Hirokazu Shirado",
            "Sauvik Das"
        ],
        "date": "2025/01/29",
        "pdf": "http://arxiv.org/pdf/2501.17420",
        "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.17420"
    },
    "37aaa3a77aacdb7837741416f3de3591": {
        "title": "Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models",
        "authors": [
            "Manish Sanwal"
        ],
        "date": "2025/01/29",
        "pdf": "http://arxiv.org/pdf/2501.18645",
        "abstract": "Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to provide step-by-step rationales, improving performance on complex tasks. Despite its benefits, vanilla CoT often fails to fully verify intermediate inferences and can produce misleading explanations. In this work, we propose Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that systematically segments the reasoning process into multiple layers, each subjected to external checks and optional user feedback. We expand on the key concepts, present three scenarios -- medical triage, financial risk assessment, and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT in terms of transparency, correctness, and user engagement. By integrating references from recent arXiv papers on interactive explainability, multi-agent frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves the way for more reliable and grounded explanations in high-stakes domains.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.18645"
    },
    "5e77a1660e986fc9c52ea6f640afef7c": {
        "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search",
        "authors": [
            "Haoran Luo",
            "Haihong E",
            "Yikai Guo",
            "Qika Lin",
            "Xiaobao Wu",
            "Xinyu Mu",
            "Wenhao Liu",
            "Meina Song",
            "Yifan Zhu",
            "Luu Anh Tuan"
        ],
        "date": "2025/01/31",
        "pdf": "http://arxiv.org/pdf/2501.18922",
        "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration&#39;s performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model&#39;s GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.18922"
    },
    "83b6080f352a88b9b83b2f83a876dcc9": {
        "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects",
        "authors": [
            "Abdullah Mushtaq",
            "Muhammad Rafay Naeem",
            "Ibrahim Ghaznavi",
            "Muhammad Imran Taj",
            "Imran Hashmi",
            "Junaid Qadir"
        ],
        "date": "2025/01/02",
        "pdf": "http://arxiv.org/pdf/2501.01205",
        "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01205"
    },
    "0becf291bbb8de204c69d5f70cbe9794": {
        "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
        "authors": [
            "Dayuan Fu",
            "Keqing He",
            "Yejie Wang",
            "Wentao Hong",
            "Zhuoma Gongque",
            "Weihao Zeng",
            "Wei Wang",
            "Jingang Wang",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01702",
        "abstract": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01702"
    },
    "319d39b7410b1699ca230cfc0076a29f": {
        "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
        "authors": [
            "Aobo Kong",
            "Wentao Ma",
            "Shiwan Zhao",
            "Yongbin Li",
            "Yuchuan Wu",
            "Ke Wang",
            "Xiaoqian Liu",
            "Qicheng Li",
            "Yong Qin",
            "Fei Huang"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01821",
        "abstract": "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO&#39;s potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
        "code": "",
        "category": [
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01821"
    },
    "578096cec3465cb082c34dec69c9d8f9": {
        "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
        "authors": [
            "Hao Zheng",
            "Xinyan Guan",
            "Hao Kong",
            "Jia Zheng",
            "Weixiang Zhou",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "date": "2025/01/07",
        "pdf": "http://arxiv.org/pdf/2501.03936",
        "abstract": "Automatically generating presentations from documents is a challenging task that requires accommodating content quality, visual appeal, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, overlooking visual appeal and structural coherence, which limits their practical applicability. To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows. PPTAgent first analyzes reference presentations to extract slide-level functional types and content schemas, then drafts an outline and iteratively generates editing actions based on selected reference slides to create new slides. To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Results demonstrate that PPTAgent significantly outperforms existing automatic presentation generation methods across all three dimensions.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.03936"
    },
    "b8adfebfe98f0ced2e2d099d89c81412": {
        "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
        "authors": [
            "Samuel Schmidgall",
            "Yusheng Su",
            "Ze Wang",
            "Ximeng Sun",
            "Jialian Wu",
            "Xiaodong Yu",
            "Jiang Liu",
            "Zicheng Liu",
            "Emad Barsoum"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.04227",
        "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.04227"
    },
    "2d7c164824c91765d8b336c520d7f4d4": {
        "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
        "authors": [
            "Yuhang Liu",
            "Pengxiang Li",
            "Zishu Wei",
            "Congkai Xie",
            "Xueyu Hu",
            "Xinchen Xu",
            "Shengyu Zhang",
            "Xiaotian Han",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.04575",
        "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at \\url{https://github.com/Reallm-Labs/InfiGUIAgent}.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.04575"
    },
    "ae6eb6ca01d94c79165bd75a33a7b227": {
        "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models",
        "authors": [
            "Xiaoxi Li",
            "Guanting Dong",
            "Jiajie Jin",
            "Yuyao Zhang",
            "Yujia Zhou",
            "Yutao Zhu",
            "Peitian Zhang",
            "Zhicheng Dou"
        ],
        "date": "2025/01/09",
        "pdf": "http://arxiv.org/pdf/2501.05366",
        "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \\textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \\url{https://github.com/sunnynexus/Search-o1}.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05366"
    },
    "1f869839ffaecbb783aa788d0ed5ebdc": {
        "title": "LLMs Model Non-WEIRD Populations: Experiments with Synthetic Cultural Agents",
        "authors": [
            "Augusto Gonzalez-Bonorino",
            "Monica Capra",
            "Emilio Pantoja"
        ],
        "date": "2025/01/12",
        "pdf": "http://arxiv.org/pdf/2501.06834",
        "abstract": "Despite its importance, studying economic behavior across diverse, non-WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations presents significant challenges. We address this issue by introducing a novel methodology that uses Large Language Models (LLMs) to create synthetic cultural agents (SCAs) representing these populations. We subject these SCAs to classic behavioral experiments, including the dictator and ultimatum games. Our results demonstrate substantial cross-cultural variability in experimental behavior. Notably, for populations with available data, SCAs&#39; behaviors qualitatively resemble those of real human subjects. For unstudied populations, our method can generate novel, testable hypotheses about economic behavior. By integrating AI into experimental economics, this approach offers an effective and ethical method to pilot experiments and refine protocols for hard-to-reach populations. Our study provides a new tool for cross-cultural economic studies and demonstrates how LLMs can help experimental behavioral research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.06834"
    },
    "7b9dd6fc37e1a20f64f4c5f22ae6aa6a": {
        "title": "Talk to Right Specialists: Routing and Planning in Multi-agent System for Question Answering",
        "authors": [
            "Feijie Wu",
            "Zitao Li",
            "Fei Wei",
            "Yaliang Li",
            "Bolin Ding",
            "Jing Gao"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.07813",
        "abstract": "Leveraging large language models (LLMs), an agent can utilize retrieval-augmented generation (RAG) techniques to integrate external knowledge and increase the reliability of its responses. Current RAG-based agents integrate single, domain-specific knowledge sources, limiting their ability and leading to hallucinated or inaccurate responses when addressing cross-domain queries. Integrating multiple knowledge bases into a unified RAG-based agent raises significant challenges, including increased retrieval overhead and data sovereignty when sensitive data is involved. In this work, we propose RopMura, a novel multi-agent system that addresses these limitations by incorporating highly efficient routing and planning mechanisms. RopMura features two key components: a router that intelligently selects the most relevant agents based on knowledge boundaries and a planner that decomposes complex multi-hop queries into manageable steps, allowing for coordinating cross-domain responses. Experimental results demonstrate that RopMura effectively handles both single-hop and multi-hop queries, with the routing mechanism enabling precise answers for single-hop queries and the combined routing and planning mechanisms achieving accurate, multi-step resolutions for complex queries.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.07813"
    },
    "063b44796c8c8be3d6c9b02aee88adf6": {
        "title": "Agent-Centric Projection of Prompting Techniques and Implications for Synthetic Training Data for Large Language Models",
        "authors": [
            "Dhruv Dhamani",
            "Mary Lou Maher"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.07815",
        "abstract": "Recent advances in prompting techniques and multi-agent systems for Large Language Models (LLMs) have produced increasingly complex approaches. However, we lack a framework for characterizing and comparing prompting techniques or understanding their relationship to multi-agent LLM systems. This position paper introduces and explains the concepts of linear contexts (a single, continuous sequence of interactions) and non-linear contexts (branching or multi-path) in LLM systems. These concepts enable the development of an agent-centric projection of prompting techniques, a framework that can reveal deep connections between prompting strategies and multi-agent systems. We propose three conjectures based on this framework: (1) results from non-linear prompting techniques can predict outcomes in equivalent multi-agent systems, (2) multi-agent system architectures can be replicated through single-LLM prompting techniques that simulate equivalent interaction patterns, and (3) these equivalences suggest novel approaches for generating synthetic training data. We argue that this perspective enables systematic cross-pollination of research findings between prompting and multi-agent domains, while providing new directions for improving both the design and training of future LLM systems.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.07815"
    },
    "5d735467a3ca05b8eaad8dd80a19c261": {
        "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
        "authors": [
            "Aditi Singh",
            "Abul Ehtesham",
            "Saket Kumar",
            "Tala Talaei Khoei"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.09136",
        "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09136"
    },
    "5613ddd27fb336ab0265da940cb4c896": {
        "title": "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.11233",
        "abstract": "Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11233"
    },
    "2d5682f900e4853ed0ca1e1328887a98": {
        "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
        "authors": [
            "Zhili Cheng",
            "Yuge Tu",
            "Ran Li",
            "Shiqi Dai",
            "Jinyi Hu",
            "Shengding Hu",
            "Jiahao Li",
            "Yang Shi",
            "Tianyu Yu",
            "Weize Chen",
            "Lei Shi",
            "Maosong Sun"
        ],
        "date": "2025/01/21",
        "pdf": "http://arxiv.org/pdf/2501.11858",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11858"
    },
    "09312c290520cd1bcdfd197df5ef21be": {
        "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
        "authors": [
            "Yujia Qin",
            "Yining Ye",
            "Junjie Fang",
            "Haoming Wang",
            "Shihao Liang",
            "Shizuo Tian",
            "Junda Zhang",
            "Jiahao Li",
            "Yunxin Li",
            "Shijue Huang",
            "Wanjun Zhong",
            "Kuanye Li",
            "Jiale Yang",
            "Yu Miao",
            "Woyu Lin",
            "Longxiang Liu",
            "Xu Jiang",
            "Qianli Ma",
            "Jingyu Li",
            "Xiaojun Xiao",
            "Kai Cai",
            "Chuang Li",
            "Yaowei Zheng",
            "Chaolin Jin",
            "Chen Li",
            "Xiao Zhou",
            "Minchao Wang",
            "Haoli Chen",
            "Zhaojian Li",
            "Haihua Yang",
            "Haifeng Liu",
            "Feng Lin",
            "Tao Peng",
            "Xin Liu",
            "Guang Shi"
        ],
        "date": "2025/01/21",
        "pdf": "http://arxiv.org/pdf/2501.12326",
        "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12326"
    },
    "16bf32d84a47590d45c85592a76488fb": {
        "title": "FinSphere: A Conversational Stock Analysis Agent Equipped with Quantitative Tools based on Real-Time Database",
        "authors": [
            "Shijie Han",
            "Changhai Zhou",
            "Yiqing Shen",
            "Tianning Sun",
            "Yuhua Zhou",
            "Xiaoxia Wang",
            "Zhixiao Yang",
            "Jingshu Zhang",
            "Hongguang Li"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.12399",
        "abstract": "Current financial Large Language Models (LLMs) struggle with two critical limitations: a lack of depth in stock analysis, which impedes their ability to generate professional-grade insights, and the absence of objective evaluation metrics to assess the quality of stock analysis reports. To address these challenges, this paper introduces FinSphere, a conversational stock analysis agent, along with three major contributions: (1) Stocksis, a dataset curated by industry experts to enhance LLMs&#39; stock analysis capabilities, (2) AnalyScore, a systematic evaluation framework for assessing stock analysis quality, and (3) FinSphere, an AI agent that can generate high-quality stock analysis reports in response to user queries. Experiments demonstrate that FinSphere achieves superior performance compared to both general and domain-specific LLMs, as well as existing agent-based systems, even when they are enhanced with real-time data access and few-shot guidance. The integrated framework, which combines real-time data feeds, quantitative tools, and an instruction-tuned LLM, yields substantial improvements in both analytical quality and practical applicability for real-world stock analysis.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12399"
    },
    "47477179b28782d60edc21bdd95ea6ea": {
        "title": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback",
        "authors": [
            "Joshua Park",
            "Yongfeng Zhang"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.13333",
        "abstract": "Multi-agent systems must decide which agent is the most appropriate for a given task. We propose a novel architecture for recommending which LLM agent out of many should perform a task given a natural language prompt by extending the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a top-1 accuracy of 92.2% with each classification taking less than 300 milliseconds. In contrast to traditional classification methods, our architecture is computationally cheap, adaptive to new classes, interpretable, and controllable with arbitrary metrics through reinforcement learning. By encoding natural language prompts into sentence embeddings, our model captures the semantic content relevant to recommending an agent. The distance between sentence embeddings that belong to the same agent is then minimized through fine-tuning and aligned to human values through reinforcement learning from human feedback. This allows the classification of natural language prompts based on their nearest neighbors by measuring the cosine similarity between embeddings. This work is made possible through the generation of a synthetic dataset for agent recommendation, which we have open-sourced to the public along with the code for AgentRec recommendation system at https://github.com/joshprk/agentrec.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13333"
    },
    "63c1f5c83cd2e780c8ee5eca3cc188be": {
        "title": "Developing Enhanced Conversational Agents for Social Virtual Worlds",
        "authors": [
            "D. Griol",
            "A. Sanchis",
            "J. M. Molina",
            "Z. Callejas"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.16341",
        "abstract": "In this paper, we present a methodology for the development of embodied conversational agents for social virtual worlds. The agents provide multimodal communication with their users in which speech interaction is included. Our proposal combines different techniques related to Artificial Intelligence, Natural Language Processing, Affective Computing, and User Modeling. Firstly, the developed conversational agents. A statistical methodology has been developed to model the system conversational behavior, which is learned from an initial corpus and improved with the knowledge acquired from the successive interactions. In addition, the selection of the next system response is adapted considering information stored into users profiles and also the emotional contents detected in the users utterances. Our proposal has been evaluated with the successful development of an embodied conversational agent which has been placed in the Second Life social virtual world. The avatar includes the different models and interacts with the users who inhabit the virtual world in order to provide academic information. The experimental results show that the agents conversational behavior adapts successfully to the specific characteristics of users interacting in such environments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.16341"
    },
    "45a66881a718ec4b28663d50c69f62df": {
        "title": "CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation",
        "authors": [
            "Faria Huq",
            "Zora Zhiruo Wang",
            "Frank F. Xu",
            "Tianyue Ou",
            "Shuyan Zhou",
            "Jeffrey P. Bigham",
            "Graham Neubig"
        ],
        "date": "2025/01/28",
        "pdf": "http://arxiv.org/pdf/2501.16609",
        "abstract": "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent&#39;s capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.16609"
    },
    "8b5a90b00bf3332dc22a161e86b46376": {
        "title": "Context is Key for Agent Security",
        "authors": [
            "Lillian Tsai",
            "Eugene Bagdasarian"
        ],
        "date": "2025/01/28",
        "pdf": "http://arxiv.org/pdf/2501.17070",
        "abstract": "Judging the safety of an action, whether taken by a human or a system, must take into account the context in which the action takes place. For example, deleting an email from a user&#39;s mailbox may or may not be appropriate depending on the email&#39;s content, the user&#39;s goals, or even available space. Systems today that make these judgements -- providing security against harmful or inappropriate actions -- rely on manually-crafted policies or user confirmation for each relevant context. With the upcoming deployment of systems like generalist agents, we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual security for agents (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.17070"
    },
    "7c0c41b3a914bc731bc0f01e3bd65515": {
        "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
        "authors": [
            "Fenglin Yu",
            "Fangkai Yang",
            "Xiaoting Qin",
            "Zhiyang Zhang",
            "Jue Zhang",
            "Qingwei Lin",
            "Hongyu Zhang",
            "Yingnong Dang",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "date": "2025/01/31",
        "pdf": "http://arxiv.org/pdf/2501.19056",
        "abstract": "The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.19056"
    },
    "177773166ac1ce4eec643e253a85c0cc": {
        "title": "The Ann Arbor Architecture for Agent-Oriented Programming",
        "authors": [
            "Wei Dong"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.09903",
        "abstract": "In this paper, we reexamine prompt engineering for large language models through the lens of automata theory. We argue that language models function as automata and, like all automata, should be programmed in the languages they accept, a unified collection of all natural and formal languages. Therefore, traditional software engineering practices--conditioned on the clear separation of programming languages and natural languages--must be rethought. We introduce the Ann Arbor Architecture, a conceptual framework for agent-oriented programming of language models, as a higher-level abstraction over raw token generation, and provide a new perspective on in-context learning. Based on this framework, we present the design of our agent platform Postline, and report on our initial experiments in agent training.",
        "code": "https://github.com/aaalgo/postline_0.1",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09903"
    },
    "6f9c1ac089493ef3ed381bcddd216a44": {
        "title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems",
        "authors": [
            "Zengqing Wu",
            "Takayuki Ito"
        ],
        "date": "2025/02/23",
        "pdf": "http://arxiv.org/pdf/2502.16565",
        "abstract": "Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios -- Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision -- confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.",
        "code": "https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.16565"
    },
    "99419075ed21de8e2af08ee4afa13062": {
        "title": "MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution to Automate Cross-App Instructions",
        "authors": [
            "Yuxuan Liu",
            "Hongda Sun",
            "Wei Liu",
            "Jian Luan",
            "Bo Du",
            "Rui Yan"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.16796",
        "abstract": "Mobile phone agents can assist people in automating daily tasks on their phones, which have emerged as a pivotal research spotlight. However, existing procedure-oriented agents struggle with cross-app instructions, due to the following challenges: (1) complex task relationships, (2) diverse app environment, and (3) error propagation and information loss in multi-step execution. Drawing inspiration from object-oriented programming principles, we recognize that object-oriented solutions is more suitable for cross-app instruction. To address these challenges, we propose a self-evolving multi-agent framework named MobileSteward, which integrates multiple app-oriented StaffAgents coordinated by a centralized StewardAgent. We design three specialized modules in MobileSteward: (1) Dynamic Recruitment generates a scheduling graph guided by information flow to explicitly associate tasks among apps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each equipped with app-specialized expertise to address the diversity between apps. (3) Adjusted Evaluation conducts evaluation to provide reflection tips or deliver key information, which alleviates error propagation and information loss during multi-step execution. To continuously improve the performance of MobileSteward, we develop a Memory-based Self-evolution mechanism, which summarizes the experience from successful execution, to improve the performance of MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench) in the real-world environment to evaluate the agents&#39; capabilities of solving complex cross-app instructions. Experimental results demonstrate that MobileSteward achieves the best performance compared to both single-agent and multi-agent frameworks, highlighting the superiority of MobileSteward in better handling user instructions with diverse complexity.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.16796"
    },
    "9c6712e928529179d418e2f944f501a6": {
        "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Xi Zhang",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17110",
        "abstract": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks. The code will be open-sourced at https://github.com/X-PLUG/MobileAgent.",
        "code": "https://github.com/X-PLUG/MobileAgent",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17110"
    },
    "3331d194b7f2bf339cf564c4b2e98a9e": {
        "title": "Stay Focused: Problem Drift in Multi-Agent Debate",
        "authors": [
            "Jonas Becker",
            "Lars Benedikt Kaesberg",
            "Andreas Stephan",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19559",
        "abstract": "Multi-agent debate - multiple instances of large language models discussing problems in turn-based interaction - has shown promise for solving knowledge and reasoning tasks. However, these methods show limitations, particularly when scaling them to longer reasoning chains. In this study, we unveil a new issue of multi-agent debate: discussions drift away from the initial problem over multiple turns. We define this phenomenon as problem drift and quantify its presence across ten tasks (i.e., three generative, three knowledge, three reasoning, and one instruction-following task). To identify the reasons for this issue, we perform a human study with eight experts on discussions suffering from problem drift, who find the most common issues are a lack of progress (35% of cases), low-quality feedback (26% of cases), and a lack of clarity (25% of cases). To systematically address the issue of problem drift, we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of problem drift cases. Our study can be seen as a first step to understanding a key limitation of multi-agent debate, highlighting pathways for improving their effectiveness in the future.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19559"
    },
    "cf78b1c292d49a864a8fed69270d38b7": {
        "title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents",
        "authors": [
            "Ashley Lewis",
            "Michael White",
            "Jing Liu",
            "Toshiaki Koike-Akino",
            "Kieran Parsons",
            "Ye Wang"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19545",
        "abstract": "The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination-generating false information-and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models&#39; outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized &#34;I don&#39;t know&#34; responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19545"
    },
    "d1c5e4d780996dc488d652dbb43e3a14": {
        "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
        "authors": [
            "Jeffrey Yang Fan Chiang",
            "Seungjae Lee",
            "Jia-Bin Huang",
            "Furong Huang",
            "Yizheng Chen"
        ],
        "date": "2025/02/27",
        "pdf": "http://arxiv.org/pdf/2502.20383",
        "abstract": "Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.20383"
    },
    "653218e7695b65021161f0517044ac67": {
        "title": "M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging",
        "authors": [
            "Jinghao Feng",
            "Qiaoyu Zheng",
            "Chaoyi Wu",
            "Ziheng Zhao",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "date": "2025/02/27",
        "pdf": "http://arxiv.org/pdf/2502.20301",
        "abstract": "Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.20301"
    },
    "33eceb2cc2ba664c62485d4f0ca022af": {
        "title": "Voting or Consensus? Decision-Making in Multi-Agent Debate",
        "authors": [
            "Lars Benedikt Kaesberg",
            "Jonas Becker",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19130",
        "abstract": "Much of the success of multi-agent debates depends on carefully choosing the right parameters. Among them, the decision-making protocol stands out. Systematic comparison of decision protocols is difficult because studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making addresses the challenges of different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time (i.e., decision protocol) to analyze how different methods affect the collaboration between agents and test different protocols on knowledge (MMLU, MMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks over the other decision protocol. Increasing the number of agents improves performance, while more discussion rounds before voting reduces it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19130"
    },
    "f5de8450ff665a0dd1c99cb1221a1488": {
        "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems",
        "authors": [
            "Hao Peng",
            "Yunjia Qi",
            "Xiaozhi Wang",
            "Zijun Yao",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19328",
        "abstract": "Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
        "code": "https://github.com/THU-KEG/Agentic-Reward-Modeling",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19328"
    },
    "3fe124c386a094c51407860228416e77": {
        "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis",
        "authors": [
            "Daniel Rose",
            "Chia-Chien Hung",
            "Marco Lepri",
            "Israa Alqassem",
            "Kiril Gashteovski",
            "Carolin Lawrence"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19175",
        "abstract": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19175"
    },
    "bad7c9860f3358a986e817507d081e4b": {
        "title": "Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT",
        "authors": [
            "Hediyeh Baban",
            "Sai A Pidapar",
            "Aashutosh Nema",
            "Sichen Lu"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18653",
        "abstract": "We introduce a novel multi-agent collaboration framework designed to enhance the accuracy and robustness of text classification models. Leveraging BERT as the primary classifier, our framework dynamically escalates low-confidence predictions to a specialized multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative approach allows for comprehensive analysis and consensus-driven decision-making, significantly improving classification performance across diverse text classification tasks. Empirical evaluations on benchmark datasets demonstrate that our framework achieves a 5.5% increase in accuracy compared to standard BERT-based classifiers, underscoring its effectiveness and academic novelty in advancing multi-agent systems within natural language processing.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18653"
    },
    "6c11b5a1ee6175776c6a68083bedff85": {
        "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
        "authors": [
            "Max Ku",
            "Thomas Chong",
            "Jonathan Leung",
            "Krish Shah",
            "Alvin Yu",
            "Wenhu Chen"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19400",
        "abstract": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19400"
    },
    "929d8ef67b10589b81a331f50b04b5b8": {
        "title": "A Cooperative Multi-Agent Framework for Zero-Shot Named Entity Recognition",
        "authors": [
            "Zihan Wang",
            "Ziqi Zhao",
            "Yougang Lyu",
            "Zhumin Chen",
            "Maarten de Rijke",
            "Zhaochun Ren"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18702",
        "abstract": "Zero-shot named entity recognition (NER) aims to develop entity recognition systems from unannotated text corpora. This task presents substantial challenges due to minimal human intervention. Recent work has adapted large language models (LLMs) for zero-shot NER by crafting specialized prompt templates. It advances model self-learning abilities by incorporating self-annotated demonstrations. However, two important challenges persist: (i) Correlations between contexts surrounding entities are overlooked, leading to wrong type predictions or entity omissions. (ii) The indiscriminate use of task demonstrations, retrieved through shallow similarity-based strategies, severely misleads LLMs during inference. In this paper, we introduce the cooperative multi-agent system (CMAS), a novel framework for zero-shot NER that uses the collective intelligence of multiple agents to address the challenges outlined above. CMAS has four main agents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor, (iii) a demonstration discriminator, and (iv) an overall predictor. To explicitly capture correlations between contexts surrounding entities, CMAS reformulates NER into two subtasks: recognizing named entities and identifying entity type-related features within the target sentence. To enable controllable utilization of demonstrations, a demonstration discriminator is established to incorporate the self-reflection mechanism, automatically evaluating helpfulness scores for the target sentence. Experimental results show that CMAS significantly improves zero-shot NER performance across six benchmarks, including both domain-specific and general-domain scenarios. Furthermore, CMAS demonstrates its effectiveness in few-shot settings and with various LLM backbones.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18702"
    },
    "b7d328538c325c921ce66febff2a3c75": {
        "title": "Scaffolding Empathy: Training Counselors with Simulated Patients and Utterance-level Performance Visualizations",
        "authors": [
            "Ian Steenstra",
            "Farnaz Nouraei",
            "Timothy W. Bickmore"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18673",
        "abstract": "Learning therapeutic counseling involves significant role-play experience with mock patients, with current manual training methods providing only intermittent granular feedback. We seek to accelerate and optimize counselor training by providing frequent, detailed feedback to trainees as they interact with a simulated patient. Our first application domain involves training motivational interviewing skills for counselors. Motivational interviewing is a collaborative counseling style in which patients are guided to talk about changing their behavior, with empathetic counseling an essential ingredient. We developed and evaluated an LLM-powered training system that features a simulated patient and visualizations of turn-by-turn performance feedback tailored to the needs of counselors learning motivational interviewing. We conducted an evaluation study with professional and student counselors, demonstrating high usability and satisfaction with the system. We present design implications for the development of automated systems that train users in counseling skills and their generalizability to other types of social skills training.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18673"
    },
    "69e70ffee7c91c8f86a98df96a2933e8": {
        "title": "AgentRM: Enhancing Agent Generalization with Reward Modeling",
        "authors": [
            "Yu Xia",
            "Jingru Fan",
            "Weize Chen",
            "Siyu Yan",
            "Xin Cong",
            "Zhong Zhang",
            "Yaxi Lu",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18407",
        "abstract": "Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by $8.8$ points on average, surpassing the top general agent by $4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by $11.4$ on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18407"
    },
    "40b0a9827149a058e2153598bbe02542": {
        "title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction",
        "authors": [
            "Jianhao Yan",
            "Yun Luo",
            "Yue Zhang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18308",
        "abstract": "In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses. However, evaluating an LLM&#39;s ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment. We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues. https://github.com/ElliottYan/RefuteBench-2.0",
        "code": "https://github.com/ElliottYan/RefuteBench-2.0",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18308"
    },
    "31071f7714f91d731ef370175c9eec55": {
        "title": "Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent",
        "authors": [
            "Xiaofeng Wang",
            "Zhixin Zhang",
            "Jinguang Zheng",
            "Yiming Ai",
            "Rui Wang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18228",
        "abstract": "Debt collection negotiations (DCN) are vital for managing non-performing loans (NPLs) and reducing creditor losses. Traditional methods are labor-intensive, while large language models (LLMs) offer promising automation potential. However, prior systems lacked dynamic negotiation and real-time decision-making capabilities. This paper explores LLMs in automating DCN and proposes a novel evaluation framework with 13 metrics across 4 aspects. Our experiments reveal that LLMs tend to over-concede compared to human negotiators. To address this, we propose the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules to improve decision rationality. We also apply post-training techniques, including DPO with rejection sampling, to optimize performance. Our studies provide valuable insights for practitioners and researchers seeking to enhance efficiency and outcomes in this domain.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18228"
    },
    "08a4ce85e4e77cacad09ecb623d02ad6": {
        "title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding",
        "authors": [
            "Jian Wu",
            "Jiayu Zhang",
            "Dongyuan Li",
            "Linyi Yang",
            "Aoxiao Zhong",
            "Renhe Jiang",
            "Qingsong Wen",
            "Yue Zhang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18209",
        "abstract": "This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper&#39;s proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18209"
    },
    "f4db64903e9b8877fbfba9827856f04b": {
        "title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models",
        "authors": [
            "Hongzhan Lin",
            "Yang Deng",
            "Yuxuan Gu",
            "Wenxuan Zhang",
            "Jing Ma",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.17924",
        "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs&#39; fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs&#39; factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17924"
    },
    "710a7abc2158db6d47ede7575e9a90b1": {
        "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
        "authors": [
            "Qiuchen Wang",
            "Ruixue Ding",
            "Zehui Chen",
            "Weiqi Wu",
            "Shihang Wang",
            "Pengjun Xie",
            "Feng Zhao"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18017",
        "abstract": "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model&#39;s reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18017"
    },
    "aeee067bd0d4eb3c4a4a866683b07066": {
        "title": "LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena",
        "authors": [
            "Tianmi Ma",
            "Jiawei Du",
            "Wenxin Huang",
            "Wenjie Wang",
            "Liang Xie",
            "Xian Zhong",
            "Joey Tianyi Zhou"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.17967",
        "abstract": "Recent advancements in large language models (LLMs) have significantly improved performance in natural language processing tasks. However, their ability to generalize to dynamic, unseen tasks, particularly in numerical reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on problems with predefined optimal solutions, which may not align with real-world scenarios where clear answers are absent. To bridge this gap, we design the Agent Trading Arena, a virtual numerical game simulating complex economic systems through zero-sum games, where agents invest in stock portfolios. Our experiments reveal that LLMs, including GPT-4o, struggle with algebraic reasoning when dealing with plain-text stock data, often focusing on local details rather than global trends. In contrast, LLMs perform significantly better with geometric reasoning when presented with visual data, such as scatter plots or K-line charts, suggesting that visual representations enhance numerical reasoning. This capability is further improved by incorporating the reflection module, which aids in the analysis and interpretation of complex data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate stronger reasoning with visual data compared to text. Our code and data are publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17967"
    },
    "b72e503d6aad3525a17bee4add471676": {
        "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
        "authors": [
            "Bingxuan Li",
            "Yiwei Wang",
            "Jiuxiang Gu",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17651",
        "abstract": "Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17651"
    },
    "2be39e1583cea697b224e3752c123671": {
        "title": "Training a Generally Curious Agent",
        "authors": [
            "Fahim Tajwar",
            "Yiding Jiang",
            "Abitha Thankaraj",
            "Sumaita Sadia Rahman",
            "J Zico Kolter",
            "Jeff Schneider",
            "Ruslan Salakhutdinov"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17543",
        "abstract": "Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present PAPRIKA, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, PAPRIKA teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with PAPRIKA can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach&#39;s primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17543"
    },
    "8d04925aa85c0d1934ad4dbbb502c5ec": {
        "title": "Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents",
        "authors": [
            "Prafulla Kumar Choubey",
            "Xiangyu Peng",
            "Shilpa Bhagavath",
            "Caiming Xiong",
            "Shiva Kumar Pentyala",
            "Chien-Sheng Wu"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17321",
        "abstract": "Automated service agents require well-structured workflows to provide consistent and accurate responses to customer queries. However, these workflows are often undocumented, and their automatic extraction from conversations remains unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process consists of two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation process using a question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively assess the quality of extracted workflows, we introduce an automated agent and customer bots simulation framework that measures their effectiveness in resolving customer issues. Extensive experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT technique improves workflow extraction by 12.16\\% in average macro accuracy over the baseline. Moreover, our evaluation method closely aligns with human assessments, providing a reliable and scalable framework for future research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17321"
    },
    "bca4b1f6624b83e6b83c47eebd6f5952": {
        "title": "Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data",
        "authors": [
            "Tianyi Huang",
            "Elsa Fan"
        ],
        "date": "2025/03/01",
        "pdf": "http://arxiv.org/pdf/2503.00355",
        "abstract": "From disinformation spread by AI chatbots to AI recommendations that inadvertently reinforce stereotypes, textual bias poses a significant challenge to the trustworthiness of large language models (LLMs). In this paper, we propose a multi-agent framework that systematically identifies biases by disentangling each statement as fact or opinion, assigning a bias intensity score, and providing concise, factual justifications. Evaluated on 1,500 samples from the WikiNPOV dataset, the framework achieves 84.9% accuracy$\\unicode{x2014}$an improvement of 13.0% over the zero-shot baseline$\\unicode{x2014}$demonstrating the efficacy of explicitly modeling fact versus opinion prior to quantifying bias intensity. By combining enhanced detection accuracy with interpretable explanations, this approach sets a foundation for promoting fairness and accountability in modern language models.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.00355"
    },
    "2433ebab4f4ee9f7d1255f99a80ace07": {
        "title": "Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with Query-Oriented Pivot Tasks",
        "authors": [
            "Zongru Wu",
            "Pengzhou Cheng",
            "Zheng Wu",
            "Tianjie Ju",
            "Zhuosheng Zhang",
            "Gongshen Liu"
        ],
        "date": "2025/03/01",
        "pdf": "http://arxiv.org/pdf/2503.00401",
        "abstract": "Perception-enhanced pre-training, particularly through grounding techniques, is widely adopted to enhance the performance of graphical user interface (GUI) agents. However, in resource-constrained scenarios, the format discrepancy between coordinate-oriented grounding and action-oriented reasoning limits the effectiveness of grounding for reasoning tasks. To address this challenge, we propose a query-oriented pivot approach called query inference, which serves as a bridge between GUI grounding and reasoning. By inferring potential user queries from a screenshot and its associated element coordinates, query inference improves the understanding of coordinates while aligning more closely with reasoning tasks. Experimental results show that query inference outperforms previous grounding techniques under the same training data scale. Notably, query inference achieves comparable or even better performance to large-scale grounding-enhanced OS-Atlas with less than 0.1% of training data. Furthermore, we explore the impact of reasoning formats and demonstrate that integrating additional semantic information into the input further boosts reasoning performance. The code is publicly available at https://github.com/ZrW00/GUIPivot.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.00401"
    },
    "a86efc9b62297533bb868f58f75d4758": {
        "title": "Improving Retrospective Language Agents via Joint Policy Gradient Optimization",
        "authors": [
            "Xueyang Feng",
            "Bo Lan",
            "Quanyu Dai",
            "Lei Wang",
            "Jiakai Tang",
            "Xu Chen",
            "Zhenhua Dong",
            "Ji-Rong Wen"
        ],
        "date": "2025/03/03",
        "pdf": "http://arxiv.org/pdf/2503.01490",
        "abstract": "In recent research advancements within the community, large language models (LLMs) have sparked great interest in creating autonomous agents. However, current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile, although fine-tuning methods significantly enhance the capabilities of smaller LLMs, the fine-tuned agents often lack the potential for self-reflection and self-improvement. To address these challenges, we introduce a novel agent framework named RetroAct, which is a framework that jointly optimizes both task-planning and self-reflective evolution capabilities in language agents. Specifically, we develop a two-stage joint optimization process that integrates imitation learning and reinforcement learning, and design an off-policy joint policy gradient optimization algorithm with imitation learning regularization to enhance the data efficiency and training stability in agent tasks. RetroAct significantly improves the performance of open-source models, reduces dependency on closed-source LLMs, and enables fine-tuned agents to learn and evolve continuously. We conduct extensive experiments across various testing environments, demonstrating RetroAct has substantial improvements in task performance and decision-making processes.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.01490"
    },
    "a69dad462d5c2ed209611c5855132b06": {
        "title": "ATLaS: Agent Tuning via Learning Critical Steps",
        "authors": [
            "Zhixun Chen",
            "Ming Li",
            "Yuxuan Huang",
            "Yali Du",
            "Meng Fang",
            "Tianyi Zhou"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02197",
        "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training&#39;s focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02197"
    },
    "bf02b3247bcd402f5ac26cca0992eaa1": {
        "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent",
        "authors": [
            "Xingzuo Li",
            "Kehai Chen",
            "Yunfei Long",
            "Xuefeng Bai",
            "Yong Xu",
            "Min Zhang"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02519",
        "abstract": "Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02519"
    },
    "6c0c2a1aa25b10122be50884b469956a": {
        "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
        "authors": [
            "Weimin Xiong",
            "Yifan Song",
            "Qingxiu Dong",
            "Bingchan Zhao",
            "Feifan Song",
            "Xun Wang",
            "Sujian Li"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02682",
        "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent&#39;s task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02682"
    },
    "2a0ccb233db8bb358cf267a1d83129bb": {
        "title": "MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving",
        "authors": [
            "Ruida Wang",
            "Rui Pan",
            "Yuxin Li",
            "Jipeng Zhang",
            "Yizhen Jia",
            "Shizhe Diao",
            "Renjie Pi",
            "Junjie Hu",
            "Tong Zhang"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03205",
        "abstract": "Solving mathematical problems using computer-verifiable languages like Lean has significantly impacted mathematical and computer science communities. State-of-the-art methods utilize single Large Language Models (LLMs) as agents or provers to either generate complete proof or perform tree searches. However, single-agent methods inherently lack a structured way to combine high-level reasoning in Natural Language (NL) with Formal Language (FL) verification feedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought framework, (to the best of our knowledge), the first multi-agent framework for Lean4 theorem proving that balance high-level NL reasoning and FL verification in Long CoT. Using this structured interaction, our approach enables deeper insights and long-term coherence in proof generation, with which past methods struggle. We do this by leveraging emergent formal reasoning ability in Long CoT using our novel LoT-Transfer Learning training-inference pipeline. Extensive experiments show that our framework achieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset, largely outperforming GPT-4 (22.95%), single-agent tree search (InternLM-Step-Prover, 50.70%), and whole-proof generation (DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight the potential of combining Long CoT with formal verification for a more insightful generation in a broader perspective.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Math"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03205"
    },
    "91d1c46c1506ae6e222d3f66227a1cb5": {
        "title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems",
        "authors": [
            "Rui Ye",
            "Shuo Tang",
            "Rui Ge",
            "Yaxin Du",
            "Zhenfei Yin",
            "Siheng Chen",
            "Jing Shao"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03686",
        "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT&#39;s high effectiveness, efficiency and strong generalization ability. Code will be available at https://github.com/rui-ye/MAS-GPT.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03686"
    },
    "4fc071aaaffd1b4a23a5fdd07b601e42": {
        "title": "Measuring temporal effects of agent knowledge by date-controlled tool use",
        "authors": [
            "R. Patrick Xian",
            "Qiming Cui",
            "Stefan Bauer",
            "Reza Abbasi-Asl"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.04188",
        "abstract": "Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet its inappropriate configuration affects the quality of agent responses. Here, we construct a tool-based out-of-sample testing framework to measure the knowledge variability of large language model (LLM) agents from distinct date-controlled tools (DCTs). We demonstrate the temporal effects of an LLM agent as a writing assistant, which can use web search to help complete scientific publication abstracts. We show that temporal effects of the search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent evaluation should take a dynamical view and account for the temporal influence of tools and the updates of external resources.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04188"
    },
    "8eee22be7cee0f74a475f80af5cb6428": {
        "title": "PreMind: Multi-Agent Video Understanding for Advanced Indexing of Presentation-style Videos",
        "authors": [
            "Kangda Wei",
            "Zhengyu Zhou",
            "Bingqing Wang",
            "Jun Araki",
            "Lukas Lange",
            "Ruihong Huang",
            "Zhe Feng"
        ],
        "date": "2025/02/28",
        "pdf": "http://arxiv.org/pdf/2503.00162",
        "abstract": "In recent years, online lecture videos have become an increasingly popular resource for acquiring new knowledge. Systems capable of effectively understanding/indexing lecture videos are thus highly desirable, enabling downstream tasks like question answering to help users efficiently locate specific information within videos. This work proposes PreMind, a novel multi-agent multimodal framework that leverages various large models for advanced understanding/indexing of presentation-style videos. PreMind first segments videos into slide-presentation segments using a Vision-Language Model (VLM) to enhance modern shot-detection techniques. Each segment is then analyzed to generate multimodal indexes through three key steps: (1) extracting slide visual content, (2) transcribing speech narratives, and (3) consolidating these visual and speech contents into an integrated understanding. Three innovative mechanisms are also proposed to improve performance: leveraging prior lecture knowledge to refine visual understanding, detecting/correcting speech transcription errors using a VLM, and utilizing a critic agent for dynamic iterative self-reflection in vision analysis. Compared to traditional video indexing methods, PreMind captures rich, reliable multimodal information, allowing users to search for details like abbreviations shown only on slides. Systematic evaluations on the public LPM dataset and an internal enterprise dataset are conducted to validate PreMind&#39;s effectiveness, supported by detailed analyses.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.00162"
    },
    "2659d842cae295662ec22d68402d3e6b": {
        "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
        "authors": [
            "Kunlun Zhu",
            "Hongyi Du",
            "Zhaochen Hong",
            "Xiaocheng Yang",
            "Shuyi Guo",
            "Zhe Wang",
            "Zhenhailong Wang",
            "Cheng Qian",
            "Xiangru Tang",
            "Heng Ji",
            "Jiaxuan You"
        ],
        "date": "2025/03/03",
        "pdf": "http://arxiv.org/pdf/2503.01935",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.01935"
    },
    "65c60ff242d27a0ebd6709bae5e75ef2": {
        "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling",
        "authors": [
            "Hao Li",
            "Yu-Hao Huang",
            "Chang Xu",
            "Viktor Schlegel",
            "Ren-He Jiang",
            "Riza Batista-Navarro",
            "Goran Nenadic",
            "Jiang Bian"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02445",
        "abstract": "Time-series Generation (TSG) is a prominent research area with broad applications in simulations, data augmentation, and counterfactual analysis. While existing methods have shown promise in unconditional single-domain TSG, real-world applications demand for cross-domain approaches capable of controlled generation tailored to domain-specific constraints and instance-level requirements. In this paper, we argue that text can provide semantic insights, domain information and instance-specific temporal patterns, to guide and improve TSG. We introduce ``Text-Controlled TSG&#39;&#39;, a task focused on generating realistic time series by incorporating textual descriptions. To address data scarcity in this setting, we propose a novel LLM-based Multi-Agent framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore, we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates semantic prototypes with text description for supporting domain-level guidance. This approach achieves state-of-the-art generation fidelity on 11 of 12 datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared to no text input generation, highlighting its potential for generating tailored time-series data.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02445"
    },
    "8848ad517b3ee7778c1809830b35142f": {
        "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
        "authors": [
            "Danqing Zhang",
            "Balaji Rama",
            "Jingyi Ni",
            "Shiying He",
            "Fu Zhao",
            "Kunyu Chen",
            "Arnold Chen",
            "Junyu Cao"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02950",
        "abstract": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent applications. Our framework addresses a critical gap in the web agent ecosystem with a production-ready solution that combines minimal serverless backend configuration, intuitive user and browser interfaces, and extensible research capabilities in agent planning, memory, and tree search. For the core LiteWebAgent agent framework, we implemented a simple yet effective baseline using recursive function calling, providing with decoupled action generation and action grounding. In addition, we integrate advanced research components such as agent planning, agent workflow memory, and tree search in a modular and extensible manner. We then integrate the LiteWebAgent agent framework with frontend and backend as deployed systems in two formats: (1) a production Vercel-based web application, which provides users with an agent-controlled remote browser, (2) a Chrome extension leveraging LiteWebAgent&#39;s API to control an existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent framework is available at https://github.com/PathOnAI/LiteWebAgent, with deployed frontend at https://lite-web-agent.vercel.app/.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02950"
    },
    "d7323d56eb84e078d18f7f314a606b04": {
        "title": "Unified Mind Model: Reimagining Autonomous Agents in the LLM Era",
        "authors": [
            "Pengbo Hu",
            "Xiang Ying"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03459",
        "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4), reviving the research of general autonomous agents with human-like cognitive abilities. Such human-level agents require semantic comprehension and instruction-following capabilities, which exactly fall into the strengths of LLMs. Although there have been several initial attempts to build human-level agents based on LLMs, the theoretical foundation remains a challenging open problem. In this paper, we propose a novel theoretical cognitive architecture, the Unified Mind Model (UMM), which offers guidance to facilitate the rapid creation of autonomous agents with human-level cognitive abilities. Specifically, our UMM starts with the global workspace theory and further leverage LLMs to enable the agent with various cognitive abilities, such as multi-modal perception, planning, reasoning, tool use, learning, memory, reflection and motivation. Building upon UMM, we then develop an agent-building engine, MindOS, which allows users to quickly create domain-/task-specific autonomous agents without any programming effort.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03459"
    },
    "b615ad93e66685fd45e5227addf662a6": {
        "title": "Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence",
        "authors": [
            "Cristian Jimenez-Romero",
            "Alper Yegenoglu",
            "Christian Blum"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03800",
        "abstract": "This work examines the integration of large language models (LLMs) into multi-agent simulations by replacing the hard-coded programs of agents with LLM-driven prompts. The proposed approach is showcased in the context of two examples of complex systems from the field of swarm intelligence: ant colony foraging and bird flocking. Central to this study is a toolchain that integrates LLMs with the NetLogo simulation platform, leveraging its Python extension to enable communication with GPT-4o via the OpenAI API. This toolchain facilitates prompt-driven behavior generation, allowing agents to respond adaptively to environmental data. For both example applications mentioned above, we employ both structured, rule-based prompts and autonomous, knowledge-driven prompts. Our work demonstrates how this toolchain enables LLMs to study self-organizing processes and induce emergent behaviors within multi-agent environments, paving the way for new approaches to exploring intelligent systems and modeling swarm intelligence inspired by natural phenomena. We provide the code, including simulation files and data at https://github.com/crjimene/swarm_gpt.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03800"
    },
    "3a433f91fa4ce9d60c858dbe2393bf8e": {
        "title": "Multi-Agent System for AI-Assisted Extraction of Narrative Arcs in TV Series",
        "authors": [
            "Roberto Balestri",
            "Guglielmo Pescatore"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.04817",
        "abstract": "Serialized TV shows are built on complex storylines that can be hard to track and evolve in ways that defy straightforward analysis. This paper introduces a multi-agent system designed to extract and analyze these narrative arcs. Tested on the first season of Grey&#39;s Anatomy (ABC 2005-), the system identifies three types of arcs: Anthology (self-contained), Soap (relationship-focused), and Genre-Specific (strictly related to the series&#39; genre). Episodic progressions of these arcs are stored in both relational and semantic (vectorial) databases, enabling structured analysis and comparison. To bridge the gap between automation and critical interpretation, the system is paired with a graphical interface that allows for human refinement using tools to enhance and visualize the data. The system performed strongly in identifying Anthology Arcs and character entities, but its reliance on textual paratexts (such as episode summaries) revealed limitations in recognizing overlapping arcs and subtler dynamics. This approach highlights the potential of combining computational and human expertise in narrative analysis. Beyond television, it offers promise for serialized written formats, where the narrative resides entirely in the text. Future work will explore the integration of multimodal inputs, such as dialogue and visuals, and expand testing across a wider range of genres to refine the system further.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04817"
    },
    "93b0123769b5d1ad82af92e8ba53fbcc": {
        "title": "Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems",
        "authors": [
            "Mahfuz Ahmed Anik",
            "Abdur Rahman",
            "Azmine Toushik Wasi",
            "Md Manjurul Ahsan"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.04827",
        "abstract": "Language is a cornerstone of cultural identity, yet globalization and the dominance of major languages have placed nearly 3,000 languages at risk of extinction. Existing AI-driven translation models prioritize efficiency but often fail to capture cultural nuances, idiomatic expressions, and historical significance, leading to translations that marginalize linguistic diversity. To address these challenges, we propose a multi-agent AI framework designed for culturally adaptive translation in underserved language communities. Our approach leverages specialized agents for translation, interpretation, content synthesis, and bias evaluation, ensuring that linguistic accuracy and cultural relevance are preserved. Using CrewAI and LangChain, our system enhances contextual fidelity while mitigating biases through external validation. Comparative analysis shows that our framework outperforms GPT-4o, producing contextually rich and culturally embedded translations, a critical advancement for Indigenous, regional, and low-resource languages. This research underscores the potential of multi-agent AI in fostering equitable, sustainable, and culturally sensitive NLP technologies, aligning with the AI Governance, Cultural NLP, and Sustainable NLP pillars of Language Models for Underserved Communities. Our full experimental codebase is publicly available at: https://github.com/ciol-researchlab/Context-Aware_Translation_MAS",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04827"
    },
    "d10d79664866bf586b54857d871cdae1": {
        "title": "Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents",
        "authors": [
            "Jingying Zeng",
            "Hui Liu",
            "Zhenwei Dai",
            "Xianfeng Tang",
            "Chen Luo",
            "Samarth Varshney",
            "Zhen Li",
            "Qi He"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.04830",
        "abstract": "With the advancement of conversational large language models (LLMs), several LLM-based Conversational Shopping Agents (CSA) have been developed to help customers answer questions and smooth their shopping journey in e-commerce domain. The primary objective in building a trustworthy CSA is to ensure the agent&#39;s responses are accurate and factually grounded, which is essential for building customer trust and encouraging continuous engagement. However, two challenges remain. First, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk spreading misinformation and diminishing customer trust. Second, without providing knowledge source attribution in CSA response, customers struggle to verify LLM-generated information. To address these challenges, we present an easily productionized solution that enables a &#34;citation experience&#34; utilizing In-context Learning (ICL) and Multi-UX-Inference (MUI) to generate responses with citations to attribute its original sources without interfering other existing UX features. With proper UX design, these citation marks can be linked to the related product information and display the source to our customers. In this work, we also build auto-metrics and scalable benchmarks to holistically evaluate LLM&#39;s grounding and attribution capabilities. Our experiments demonstrate that incorporating this citation generation paradigm can substantially enhance the grounding of LLM responses by 13.83% on the real-world data. As such, our solution not only addresses the immediate challenges of LLM grounding issues but also adds transparency to conversational AI.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04830"
    },
    "2e343af2d215e8ed471330cb2f977c48": {
        "title": "Enhancing Collective Intelligence in Large Language Models Through Emotional Integration",
        "authors": [
            "Likith Kadiyala",
            "Ramteja Sajja",
            "Yusuf Sermet",
            "Ibrahim Demir"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.04849",
        "abstract": "This research investigates the integration of emotional diversity into Large Language Models (LLMs) to enhance collective intelligence. Inspired by the human wisdom of crowds phenomenon, where group decisions often outperform individual judgments, we fine-tuned the DarkIdol-Llama-3.1-8B model using Google&#39;s GoEmotions dataset and Low-Rank Adaptation (LoRA) to simulate emotionally diverse responses. Evaluating the model on a distance estimation task between Fargo, ND, and Seattle, WA, across 15,064 unique persona configurations, we analyzed how emotional states and social attributes influence decision-making. Our findings demonstrate that emotional integration shapes response patterns while maintaining acceptable prediction accuracy, revealing its potential to enhance artificial collective intelligence. This study provides valuable insights into the interplay of emotional diversity and decision-making in LLMs, suggesting pathways for creating emotionally aware AI systems that balance emotional depth with analytical precision.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04849"
    },
    "5f941837291f315e0572978ad8229411": {
        "title": "VQEL: Enabling Self-Developed Symbolic Language in Agents through Vector Quantization in Emergent Language Games",
        "authors": [
            "Mohammad Mahdi Samiei Paqaleh",
            "Mahdieh Soleymani Baghshah"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.04940",
        "abstract": "In the field of emergent language, efforts have traditionally focused on developing communication protocols through interactions between agents in referential games. However, the aspect of internal language learning, where language serves not only as a communicative tool with others but also as a means for individual thinking, self-reflection, and problem-solving remains underexplored. Developing a language through self-play, without another agent&#39;s involvement, poses a unique challenge. It requires an agent to craft symbolic representations and train them using direct gradient methods. The challenge here is that if an agent attempts to learn symbolic representations through self-play using conventional modeling and techniques such as REINFORCE, the solution will offer no advantage over previous multi-agent approaches. We introduce VQEL, a novel method that incorporates Vector Quantization into the agents&#39; architecture, enabling them to autonomously invent and develop discrete symbolic representations in a self-play referential game. Following the self-play phase, agents can enhance their language through reinforcement learning and interactions with other agents in the mutual-play phase. Our experiments across various datasets demonstrate that VQEL not only outperforms the traditional REINFORCE method but also benefits from improved control and reduced susceptibility to collapse, thanks to the incorporation of vector quantization.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04940"
    },
    "84b36d72f7cbe637c81e22714582d943": {
        "title": "MM-StoryAgent: Immersive Narrated Storybook Video Generation with a Multi-Agent Paradigm across Text, Image and Audio",
        "authors": [
            "Xuenan Xu",
            "Jiahao Mei",
            "Chenliang Li",
            "Yuning Wu",
            "Ming Yan",
            "Shaopeng Lai",
            "Ji Zhang",
            "Mengyue Wu"
        ],
        "date": "2025/03/07",
        "pdf": "http://arxiv.org/pdf/2503.05242",
        "abstract": "The rapid advancement of large language models (LLMs) and artificial intelligence-generated content (AIGC) has accelerated AI-native applications, such as AI-based storybooks that automate engaging story production for children. However, challenges remain in improving story attractiveness, enriching storytelling expressiveness, and developing open-source evaluation benchmarks and frameworks. Therefore, we propose and opensource MM-StoryAgent, which creates immersive narrated video storybooks with refined plots, role-consistent images, and multi-channel audio. MM-StoryAgent designs a multi-agent framework that employs LLMs and diverse expert tools (generative models and APIs) across several modalities to produce expressive storytelling videos. The framework enhances story attractiveness through a multi-stage writing pipeline. In addition, it improves the immersive storytelling experience by integrating sound effects with visual, music and narrative assets. MM-StoryAgent offers a flexible, open-source platform for further development, where generative modules can be substituted. Both objective and subjective evaluation regarding textual story quality and alignment between modalities validate the effectiveness of our proposed MM-StoryAgent system. The demo and source code are available.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.05242"
    },
    "4f1fa0b4324132fd8d732715647f5513": {
        "title": "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report Evaluation",
        "authors": [
            "Zhenxuan Zhang",
            "Kinhei Lee",
            "Weihang Deng",
            "Huichi Zhou",
            "Zihao Jin",
            "Jiahao Huang",
            "Zhifan Gao",
            "Dominic C Marshall",
            "Yingying Fang",
            "Guang Yang"
        ],
        "date": "2025/03/07",
        "pdf": "http://arxiv.org/pdf/2503.05347",
        "abstract": "Automatic medical report generation supports clinical diagnosis, reduces the workload of radiologists, and holds the promise of improving diagnosis consistency. However, existing evaluation metrics primarily assess the accuracy of key medical information coverage in generated reports compared to human-written reports, while overlooking crucial details such as the location and certainty of reported abnormalities. These limitations hinder the comprehensive assessment of the reliability of generated reports and pose risks in their selection for clinical use. Therefore, we propose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both objective quantification and subjective evaluation through a large language model-based multi-agent workflow. Our GEMA-Score parses structured reports and employs NER-F1 calculations through interactive exchanges of information among agents to assess disease diagnosis, location, severity, and uncertainty. Additionally, an LLM-based scoring agent evaluates completeness, readability, and clinical terminology while providing explanatory feedback. Extensive experiments validate that GEMA-Score achieves the highest correlation with human expert evaluations on a public dataset, demonstrating its effectiveness in clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall coefficient = 0.54 for RadEvalX dataset). The anonymous project demo is available at: https://github.com/Zhenxuan-Zhang/GEMA_score.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ],
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.05347"
    },
    "1b886b0c530807ee9ffd2e564d21571c": {
        "title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data Science",
        "authors": [
            "Ziming You",
            "Yumiao Zhang",
            "Dexuan Xu",
            "Yiwei Lou",
            "Yandong Yan",
            "Wei Wang",
            "Huaming Zhang",
            "Yu Huang"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07044",
        "abstract": "Data Science tasks are multifaceted, dynamic, and often domain-specific. Existing LLM-based approaches largely concentrate on isolated phases, neglecting the interdependent nature of many data science tasks and limiting their capacity for comprehensive end-to-end support. We propose DatawiseAgent, a notebook-centric LLM agent framework that unifies interactions among user, agent and the computational environment through markdown and executable code cells, supporting flexible and adaptive automated data science. Built on a Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including DSF-like planning, incremental execution, self-debugging, and post-filtering. Specifically, the DFS-like planning stage systematically explores the solution space, while incremental execution harnesses real-time feedback and accommodates LLM&#39;s limited capabilities to progressively complete tasks. The self-debugging and post-filtering modules further enhance reliability by diagnosing and correcting errors and pruning extraneous information. Extensive experiments on diverse tasks, including data analysis, visualization, and data modeling, show that DatawiseAgent consistently outperforms or matches state-of-the-art methods across multiple model settings. These results highlight its potential to generalize across data science scenarios and lay the groundwork for more efficient, fully automated workflows.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07044"
    },
    "dd09b677a3dcc943afe402930cfd15a3": {
        "title": "ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning through Action in Dynamic Offer Optimization",
        "authors": [
            "Deuksin Kwon",
            "Jiwon Hae",
            "Emma Clift",
            "Daniel Shamsoddini",
            "Jonathan Gratch",
            "Gale M. Lucas"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07129",
        "abstract": "Negotiation requires dynamically balancing self-interest and cooperation to maximize one&#39;s own utility. Yet, existing agents struggle due to bounded rationality in human data, low adaptability to counterpart behavior, and limited strategic reasoning. To address this, we introduce principle-driven negotiation agents, powered by ASTRA, a novel framework for turn-level offer optimization grounded in two core principles: opponent modeling and Tit-for-Tat reciprocity. ASTRA operates in three stages: (1) interpreting counterpart behavior, (2) optimizing counteroffers via a linear programming (LP) solver, and (3) selecting offers based on negotiation tactics and the partner&#39;s acceptance probability. Through simulations and human evaluations, our agent effectively adapts to an opponent&#39;s shifting stance and achieves favorable outcomes through enhanced adaptability and strategic reasoning. Beyond improving negotiation performance, it also serves as a powerful coaching tool, offering interpretable strategic feedback and optimal offer recommendations.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07129"
    },
    "d45b96c8298e8580d4a8a23840b54b75": {
        "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning",
        "authors": [
            "Xiangru Tang",
            "Daniel Shao",
            "Jiwoong Sohn",
            "Jiapeng Chen",
            "Jiayi Zhang",
            "Jinyu Xiang",
            "Fang Wu",
            "Yilun Zhao",
            "Chenglin Wu",
            "Wenqi Shi",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07459",
        "abstract": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07459"
    },
    "bdc771b5efeac8623a2dd2e1d6c7bdb9": {
        "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents",
        "authors": [
            "Zhen Tan",
            "Jun Yan",
            "I-Hung Hsu",
            "Rujun Han",
            "Zifeng Wang",
            "Long T. Le",
            "Yiwen Song",
            "Yanfei Chen",
            "Hamid Palangi",
            "George Lee",
            "Anand Iyer",
            "Tianlong Chen",
            "Huan Liu",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "date": "2025/03/11",
        "pdf": "http://arxiv.org/pdf/2503.08026",
        "abstract": "Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities-utterances, turns, and sessions-into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs&#39; cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08026"
    },
    "7589557b1ebf73ece2a977f592b10c6a": {
        "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews",
        "authors": [
            "Xian Gao",
            "Jiacheng Ruan",
            "Jingsheng Gao",
            "Ting Liu",
            "Yuzhuo Fu"
        ],
        "date": "2025/03/11",
        "pdf": "http://arxiv.org/pdf/2503.08506",
        "abstract": "Academic paper review is a critical yet time-consuming task within the research community. With the increasing volume of academic publications, automating the review process has become a significant challenge. The primary issue lies in generating comprehensive, accurate, and reasoning-consistent review comments that align with human reviewers&#39; judgments. In this paper, we address this challenge by proposing ReviewAgents, a framework that leverages large language models (LLMs) to generate academic paper reviews. We first introduce a novel dataset, Review-CoT, consisting of 142k review comments, designed for training LLM agents. This dataset emulates the structured reasoning process of human reviewers-summarizing the paper, referencing relevant works, identifying strengths and weaknesses, and generating a review conclusion. Building upon this, we train LLM reviewer agents capable of structured reasoning using a relevant-paper-aware training method. Furthermore, we construct ReviewAgents, a multi-role, multi-LLM agent review framework, to enhance the review comment generation process. Additionally, we propose ReviewBench, a benchmark for evaluating the review comments generated by LLMs. Our experimental results on ReviewBench demonstrate that while existing LLMs exhibit a certain degree of potential for automating the review process, there remains a gap when compared to human-generated reviews. Moreover, our ReviewAgents framework further narrows this gap, outperforming advanced LLMs in generating review comments.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08506"
    },
    "5eea5e3f0e1815a1338d740b2ae9ad71": {
        "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
        "authors": [
            "Zekun Li",
            "Shinda Huang",
            "Jiangtian Wang",
            "Nathan Zhang",
            "Antonis Antoniades",
            "Wenyue Hua",
            "Kaijie Zhu",
            "Sirui Zeng",
            "William Yang Wang",
            "Xifeng Yan"
        ],
        "date": "2025/03/11",
        "pdf": "http://arxiv.org/pdf/2503.08669",
        "abstract": "As language agents progressively automate critical tasks across domains, their ability to operate within operational constraints and safety protocols becomes essential. While extensive research has demonstrated these agents&#39; effectiveness in downstream task completion, their reliability in following operational procedures and constraints remains largely unexplored. To this end, we present AgentOrca, a dual-system framework for evaluating language agents&#39; compliance with operational constraints and routines. Our framework encodes action constraints and routines through both natural language prompts for agents and corresponding executable code serving as ground truth for automated verification. Through an automated pipeline of test case generation and evaluation across five real-world domains, we quantitatively assess current language agents&#39; adherence to operational constraints. Our findings reveal notable performance gaps among state-of-the-art models, with large reasoning models like o1 demonstrating superior compliance while others show significantly lower performance, particularly when encountering complex constraints or user persuasion attempts.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08669"
    },
    "71e6a4bac3df7dda75ae7d06c814f628": {
        "title": "Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions",
        "authors": [
            "Mourad Gridach",
            "Jay Nanavati",
            "Khaldoun Zine El Abidine",
            "Lenon Mendes",
            "Christina Mack"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.08979",
        "abstract": "The integration of Agentic AI into scientific discovery marks a new frontier in research automation. These AI systems, capable of reasoning, planning, and autonomous decision-making, are transforming how scientists perform literature review, generate hypotheses, conduct experiments, and analyze results. This survey provides a comprehensive overview of Agentic AI for scientific discovery, categorizing existing systems and tools, and highlighting recent progress across fields such as chemistry, biology, and materials science. We discuss key evaluation metrics, implementation frameworks, and commonly used datasets to offer a detailed understanding of the current state of the field. Finally, we address critical challenges, such as literature review automation, system reliability, and ethical concerns, while outlining future research directions that emphasize human-AI collaboration and enhanced system calibration.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08979"
    },
    "82a101ffb49d4dcadd90c77b90c3de64": {
        "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
        "authors": [
            "Lutfi Eren Erdogan",
            "Nicholas Lee",
            "Sehoon Kim",
            "Suhong Moon",
            "Hiroki Furuta",
            "Gopala Anumanchipalli",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09572",
        "abstract": "Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of the-art 54% success rate on the WebArena-Lite benchmark.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09572"
    },
    "3a03c1c0ddb611408734756cc711da9b": {
        "title": "Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents",
        "authors": [
            "Dongjun Lee",
            "Juyong Lee",
            "Kyuyoung Kim",
            "Jihoon Tack",
            "Jinwoo Shin",
            "Yee Whye Teh",
            "Kimin Lee"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.10689",
        "abstract": "Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 15.6%, and demonstrates a 23.7% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark. Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts. The relevant code materials are available at our project page: https://lcowiclr2025.github.io.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.10689"
    },
    "e22446cf9c1268138eb33d5b0fcae7c2": {
        "title": "DeskVision: Large Scale Desktop Region Captioning for Advanced GUI Agents",
        "authors": [
            "Yibin Xu",
            "Liang Yang",
            "Hao Chen",
            "Hua Wang",
            "Zhi Chen",
            "Yaohua Tang"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11170",
        "abstract": "The limitation of graphical user interface (GUI) data has been a significant barrier to the development of GUI agents today, especially for the desktop / computer use scenarios. To address this, we propose an automated GUI data generation pipeline, AutoCaptioner, which generates data with rich descriptions while minimizing human effort. Using AutoCaptioner, we created a novel large-scale desktop GUI dataset, DeskVision, along with the largest desktop test benchmark, DeskVision-Eval, which reflects daily usage and covers diverse systems and UI elements, each with rich descriptions. With DeskVision, we train a new GUI understanding model, GUIExplorer. Results show that GUIExplorer achieves state-of-the-art (SOTA) performance in understanding/grounding visual elements without the need for complex architectural designs. We further validated the effectiveness of the DeskVision dataset through ablation studies on various large visual language models (LVLMs). We believe that AutoCaptioner and DeskVision will significantly advance the development of GUI agents, and will open-source them for the community.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11170"
    },
    "f34ab1d71cd38e25cc1f194db27e012d": {
        "title": "GNNs as Predictors of Agentic Workflow Performances",
        "authors": [
            "Yuanshuo Zhang",
            "Yuchen Hou",
            "Bohan Tang",
            "Shuo Chen",
            "Muhan Zhang",
            "Xiaowen Dong",
            "Siheng Chen"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11301",
        "abstract": "Agentic workflows invoked by Large Language Models (LLMs) have achieved remarkable success in handling complex tasks. However, optimizing such workflows is costly and inefficient in real-world applications due to extensive invocations of LLMs. To fill this gap, this position paper formulates agentic workflows as computational graphs and advocates Graph Neural Networks (GNNs) as efficient predictors of agentic workflow performances, avoiding repeated LLM invocations for evaluation. To empirically ground this position, we construct FLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic workflow performances. With extensive experiments, we arrive at the following conclusion: GNNs are simple yet effective predictors. This conclusion supports new applications of GNNs and a novel direction towards automating agentic workflow optimization. All codes, models, and data are available at https://github.com/youngsoul0731/Flora-Bench.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11301"
    },
    "f37f12bbfd5a31aee1107ecdcc90226b": {
        "title": "AIstorian lets AI be a historian: A KG-powered multi-agent system for accurate biography generation",
        "authors": [
            "Fengyu Li",
            "Yilin Li",
            "Junhao Zhu",
            "Lu Chen",
            "Yanfei Zhang",
            "Jia Zhou",
            "Hui Zu",
            "Jingwen Zhao",
            "Yunjun Gao"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11346",
        "abstract": "Huawei has always been committed to exploring the AI application in historical research. Biography generation, as a specialized form of abstractive summarization, plays a crucial role in historical research but faces unique challenges that existing large language models (LLMs) struggle to address. These challenges include maintaining stylistic adherence to historical writing conventions, ensuring factual fidelity, and handling fragmented information across multiple documents. We present AIstorian, a novel end-to-end agentic system featured with a knowledge graph (KG)-powered retrieval-augmented generation (RAG) and anti-hallucination multi-agents. Specifically, AIstorian introduces an in-context learning based chunking strategy and a KG-based index for accurate and efficient reference retrieval. Meanwhile, AIstorian orchestrates multi-agents to conduct on-the-fly hallucination detection and error-type-aware correction. Additionally, to teach LLMs a certain language style, we finetune LLMs based on a two-step training approach combining data augmentation-enhanced supervised fine-tuning with stylistic preference optimization. Extensive experiments on a real-life historical Jinshi dataset demonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and a 47.6% reduction in hallucination rate compared to existing baselines. The data and code are available at: https://github.com/ZJU-DAILY/AIstorian.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11346"
    },
    "3b472d8f1d74627b8a4b76905acf9de6": {
        "title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of Reasoning Capabilities",
        "authors": [
            "Xueyang Zhou",
            "Guiyao Tie",
            "Guowen Zhang",
            "Weidong Wang",
            "Zhigang Zuo",
            "Di Wu",
            "Duanfeng Chu",
            "Pan Zhou",
            "Lichao Sun",
            "Neil Zhenqiang Gong"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11074",
        "abstract": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward advanced computational reasoning. Yet, this progress disrupts traditional agent frameworks, traditionally anchored by execution-oriented Large Language Models (LLMs). To explore this transformation, we propose the LaRMA framework, encompassing nine tasks across Tool Usage, Plan Design, and Problem Solving, assessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs (e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass LLMs in reasoning-intensive tasks like Plan Design, leveraging iterative reflection for superior outcomes; LLMs excel in execution-driven tasks such as Tool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing LLMs as actors with LRMs as reflectors, optimize agent performance by blending execution speed with reasoning depth; and LRMs&#39; enhanced reasoning incurs higher computational costs, prolonged processing, and behavioral challenges, including overthinking and fact-ignoring tendencies. This study fosters deeper inquiry into LRMs&#39; balance of deep thinking and overthinking, laying a critical foundation for future agent design advancements.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11074"
    },
    "68afc1df36640456afee26fa41632e72": {
        "title": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment, Distribution, and Discovery",
        "authors": [
            "Balaji Rama",
            "Kai Mei",
            "Yongfeng Zhang"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11444",
        "abstract": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex task execution, yet the field lacks standardized tools for development, deployment, distribution and discovery of agents. We present Cerebrum, an Agent SDK for AIOS that addresses this gap through three key components: (1) a comprehensive SDK featuring a modular four-layer architecture for agent development, encompassing LLM, memory, storage, and tool management; (2) a community-driven Agent Hub for sharing and discovering agents, complete with version control and dependency management; (3) an interactive web interface for testing and evaluating agents. The platform&#39;s effectiveness is demonstrated through implementations of various agent architectures, including Chain of Thought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by providing a unified framework that standardizes agent development while maintaining flexibility for researchers and developers to innovate and distribute their agents. The live website is at https://app.aios.foundation, the code is at https://github.com/agiresearch/Cerebrum, and video is at https://app.aios.foundation/video-demo.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11444"
    },
    "e4a9de64131ed45863b1bbf22aff2382": {
        "title": "Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks",
        "authors": [
            "Diego Gosmar",
            "Deborah A. Dahl",
            "Dario Gosmar"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11517",
        "abstract": "Prompt injection constitutes a significant challenge for generative AI systems by inducing unintended outputs. We introduce a multi-agent NLP framework specifically designed to address prompt injection vulnerabilities through layered detection and enforcement mechanisms. The framework orchestrates specialized agents for generating responses, sanitizing outputs, and enforcing policy compliance. Evaluation on 500 engineered injection prompts demonstrates a marked reduction in injection success and policy breaches. Novel metrics, including Injection Success Rate (ISR), Policy Override Frequency (POF), Prompt Sanitization Rate (PSR), and Compliance Consistency Score (CCS), are proposed to derive a composite Total Injection Vulnerability Score (TIVS). The system utilizes the OVON (Open Voice Network) framework for inter-agent communication via structured JSON messages, extending a previously established multi-agent architecture from hallucination mitigation to address the unique challenges of prompt injection.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11517"
    },
    "7f91926edeb6ef52ee6dae176a2bd7fa": {
        "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation",
        "authors": [
            "Kaiyuan Liu",
            "Youcheng Pan",
            "Jing Li",
            "Daojing He",
            "Yang Xiang",
            "Yexing Du",
            "Tianrun Gao"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07010",
        "abstract": "Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users&#39; perspective, and also lack the explainability of the results of LLM agents&#39; code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation&#39;s automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07010"
    },
    "0b0222a69bb4d3e9734f347c4889a86d": {
        "title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code",
        "authors": [
            "Dhruv Gautam",
            "Spandan Garg",
            "Jinu Jang",
            "Neel Sundaresan",
            "Roshanak Zilouchian Moghaddam"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07832",
        "abstract": "Recent advances in language model (LM) agents and function calling have enabled autonomous, feedback-driven systems to solve problems across various digital domains. To better understand the unique limitations of LM agents, we introduce RefactorBench, a benchmark consisting of 100 large handcrafted multi-file refactoring tasks in popular open-source repositories. Solving tasks within RefactorBench requires thorough exploration of dependencies across multiple files and strong adherence to relevant instructions. Every task is defined by 3 natural language instructions of varying specificity and is mutually exclusive, allowing for the creation of longer combined tasks on the same repository. Baselines on RefactorBench reveal that current LM agents struggle with simple compositional tasks, solving only 22% of tasks with base instructions, in contrast to a human developer with short time constraints solving 87%. Through trajectory analysis, we identify various unique failure modes of LM agents, and further explore the failure mode of tracking past actions. By adapting a baseline agent to condition on representations of state, we achieve a 43.9% improvement in solving RefactorBench tasks. We further extend our state-aware approach to encompass entire digital environments and outline potential directions for future research. RefactorBench aims to support the study of LM agents by providing a set of real-world, multi-hop tasks within the realm of code.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07832"
    },
    "fe786f6903fcdf3bb37e717b5d479c9f": {
        "title": "BEARCUBS: A benchmark for computer-using web agents",
        "authors": [
            "Yixiao Song",
            "Katherine Thai",
            "Chau Minh Pham",
            "Yapei Chang",
            "Mazin Nadaf",
            "Mohit Iyyer"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07919",
        "abstract": "Modern web agents possess computer use abilities that allow them to interact with webpages by sending commands to a virtual keyboard and mouse. While such agents have considerable potential to assist human users with complex tasks, evaluating their capabilities in real-world settings poses a major challenge. To this end, we introduce BEARCUBS, a &#34;small but mighty&#34; benchmark of 111 information-seeking questions designed to evaluate a web agent&#39;s ability to search, browse, and identify factual information from the web. Unlike prior web agent benchmarks, solving BEARCUBS requires (1) accessing live web content rather than synthetic or simulated pages, which captures the unpredictability of real-world web interactions; and (2) performing a broad range of multimodal interactions (e.g., video understanding, 3D navigation) that cannot be bypassed via text-based workarounds. Each question in BEARCUBS has a corresponding short, unambiguous answer and a human-validated browsing trajectory, allowing for transparent evaluation of agent performance and strategies. A human study confirms that BEARCUBS questions are solvable but non-trivial (84.7% human accuracy), revealing search inefficiencies and domain knowledge gaps as common failure points. By contrast, state-of-the-art computer-using agents underperform, with the best-scoring system (OpenAI&#39;s Operator) reaching only 24.3% accuracy. These results highlight critical areas for improvement, including reliable source selection and more powerful multimodal capabilities. To facilitate future research, BEARCUBS will be updated periodically to replace invalid or contaminated questions, keeping the benchmark fresh for future generations of web agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07919"
    },
    "2808666d6676ab9e856d077010922df3": {
        "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
        "authors": [
            "Zhaoling Chen",
            "Xiangru Tang",
            "Gangda Deng",
            "Fang Wu",
            "Jialong Wu",
            "Zhiwei Jiang",
            "Viktor Prasanna",
            "Arman Cohan",
            "Xingyao Wang"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09089",
        "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09089"
    },
    "c7938c2a855107580bff21151d018254": {
        "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning",
        "authors": [
            "Ziyu Wan",
            "Yunxiang Li",
            "Yan Song",
            "Hanjing Wang",
            "Linyi Yang",
            "Mark Schmidt",
            "Jun Wang",
            "Weinan Zhang",
            "Shuyue Hu",
            "Ying Wen"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09501",
        "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking -- enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09501"
    },
    "bac6b47f3aaeaab4718b9606271c4ce4": {
        "title": "Factorio Learning Environment",
        "authors": [
            "Jack Hopkins",
            "Mart Bakler",
            "Akbir Khan"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.09617",
        "abstract": "Large Language Models (LLMs) are rapidly saturating existing benchmarks, necessitating new open-ended evaluations. We introduce the Factorio Learning Environment (FLE), based on the game of Factorio, that tests agents in long-term planning, program synthesis, and resource optimization. FLE provides exponentially scaling challenges -- from basic automation to complex factories processing millions of resource units per second. We provide two settings: (1) lab-play consisting of eight structured tasks with fixed resources, and (2) open-play with the unbounded task of building the largest factory on an procedurally generated map. We demonstrate across both settings that models still lack strong spatial reasoning. In lab-play, we find that LLMs exhibit promising short-horizon skills, yet are unable to operate effectively in constrained environments, reflecting limitations in error analysis. In open-play, while LLMs discover automation strategies that improve growth (e.g electric-powered drilling), they fail to achieve complex automation (e.g electronic-circuit manufacturing).",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09617"
    },
    "56ff3940cfa84ad8b291b9e7c49f0625": {
        "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
        "authors": [
            "Abe Bohan Hou",
            "Hongru Du",
            "Yichen Wang",
            "Jingyu Zhang",
            "Zixiao Wang",
            "Paul Pu Liang",
            "Daniel Khashabi",
            "Lauren Gardner",
            "Tianxing He"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09639",
        "abstract": "Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents&#39; attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09639"
    },
    "18eafd063c50a85cd349cdabfd081a6b": {
        "title": "CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control",
        "authors": [
            "Zirui Yuan",
            "Siqi Lai",
            "Hao Liu"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11739",
        "abstract": "Traffic Signal Control (TSC) plays a critical role in urban traffic management by optimizing traffic flow and mitigating congestion. While Large Language Models (LLMs) have recently emerged as promising tools for TSC due to their exceptional problem-solving and generalization capabilities, existing approaches fail to address the essential need for inter-agent coordination, limiting their effectiveness in achieving network-wide optimization. To bridge this gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC. Specifically, we first construct a structured spatiotemporal graph to capture real-time traffic dynamics and spatial relationships among neighboring intersections, enabling the LLM to reason about complex traffic interactions. Moreover, we introduce a complexity-aware reasoning mechanism that dynamically adapts reasoning depth based on real-time traffic conditions, ensuring optimal computational efficiency without sacrificing decision quality. Besides, we propose a fine-tuning strategy that leverages iterative simulation-driven data collection and environmental feedback to build a lightweight LLM tailored for cooperative TSC. Extensive experiments on both synthetic and real-world datasets demonstrate that CoLLMLight outperforms state-of-the-art methods in diverse traffic scenarios, showcasing its effectiveness, scalability, and robustness.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11739"
    },
    "c6dfccc19688759367c8142f5669e60d": {
        "title": "VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures",
        "authors": [
            "Yoo Yeon Sung",
            "Hannah Kim",
            "Dan Zhang"
        ],
        "date": "2025/03/16",
        "pdf": "http://arxiv.org/pdf/2503.12651",
        "abstract": "AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system&#39;s overall performance. Addressing these failures through human intervention is challenging due to the agents&#39; opaque reasoning processes, misalignment with human expectations, the complexity of agent dependencies, and the high cost of manual inspection. This paper thus introduces a human-centered evaluation framework for Verifying LLM Agent failures (VeriLA), which systematically assesses agent failures to reduce human effort and make these agent failures interpretable to humans. The framework first defines clear expectations of each agent by curating human-designed agent criteria. Then, it develops a human-aligned agent verifier module, trained with human gold standards, to assess each agent&#39;s execution output. This approach enables granular evaluation of each agent&#39;s performance by revealing failures from a human standard, offering clear guidelines for revision, and reducing human cognitive load. Our case study results show that VeriLA is both interpretable and efficient in helping practitioners interact more effectively with the system. By upholding accountability in human-agent collaboration, VeriLA paves the way for more trustworthy and human-aligned compound AI systems.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.12651"
    },
    "861743010ad121aeb0f4b32f3acc28c3": {
        "title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
        "authors": [
            "Kenneth J. K. Ong",
            "Lye Jia Jun",
            "Hieu Minh &#34;Jord&#34; Nguyen",
            "Seong Hah Cho",
            "Natalia Pérez-Campanero Antolín"
        ],
        "date": "2025/03/17",
        "pdf": "http://arxiv.org/pdf/2503.12722",
        "abstract": "As Large Language Models (LLMs) gain autonomous capabilities, their coordination in multi-agent settings becomes increasingly important. However, they often struggle with cooperation, leading to suboptimal outcomes. Inspired by Axelrod&#39;s Iterated Prisoner&#39;s Dilemma (IPD) tournaments, we explore how personality traits influence LLM cooperation. Using representation engineering, we steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and analyze their impact on IPD decision-making. Our results show that higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation, highlighting both the potential and limitations of personality-based steering for aligning AI agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.12722"
    },
    "2fe5b0ecc3fdbed27db943dc10707d9f": {
        "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways",
        "authors": [
            "Zhen Chen",
            "Zhihao Peng",
            "Xusheng Liang",
            "Cheng Wang",
            "Peigan Liang",
            "Linsheng Zeng",
            "Minjie Ju",
            "Yixuan Yuan"
        ],
        "date": "2025/03/17",
        "pdf": "http://arxiv.org/pdf/2503.13205",
        "abstract": "Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited research focused on artificial intelligence (AI) inpatient pathways systems, due to the lack of large-scale inpatient datasets. Moreover, existing medical benchmarks typically concentrated on medical question-answering and examinations, ignoring the multifaceted nature of clinical decision-making in inpatient settings. To address these gaps, we first developed the Inpatient Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database, encompassing 51,274 cases across nine triage departments and 17 major disease categories alongside 16 standardized treatment options. Then, we proposed the Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways with three clinical agents, including a triage agent managing the patient admission, a diagnosis agent serving as the primary decision maker at the department, and a treatment agent providing treatment plans. Additionally, our MAP framework includes a chief agent overseeing the inpatient pathways to guide and promote these three clinician agents. Extensive experiments showed our MAP improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant clinical compliance, outperforming three board-certified clinicians by 10%-12%, establishing a foundation for inpatient pathways systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13205"
    },
    "126f4dfb493aec4dfd3e5e52a7f8f43c": {
        "title": "LLM-Mediated Guidance of MARL Systems",
        "authors": [
            "Philipp D. Siedler",
            "Ian Gemp"
        ],
        "date": "2025/03/16",
        "pdf": "http://arxiv.org/pdf/2503.13553",
        "abstract": "In complex multi-agent environments, achieving efficient learning and desirable behaviours is a significant challenge for Multi-Agent Reinforcement Learning (MARL) systems. This work explores the potential of combining MARL with Large Language Model (LLM)-mediated interventions to guide agents toward more desirable behaviours. Specifically, we investigate how LLMs can be used to interpret and facilitate interventions that shape the learning trajectories of multiple agents. We experimented with two types of interventions, referred to as controllers: a Natural Language (NL) Controller and a Rule-Based (RB) Controller. The NL Controller, which uses an LLM to simulate human-like interventions, showed a stronger impact than the RB Controller. Our findings indicate that agents particularly benefit from early interventions, leading to more efficient training and higher performance. Both intervention types outperform the baseline without interventions, highlighting the potential of LLM-mediated guidance to accelerate training and enhance MARL performance in challenging environments.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13553"
    },
    "54d02ff5b4ebf3895fdb5f95d0900bda": {
        "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection",
        "authors": [
            "Tittaya Mairittha",
            "Tanakon Sawanglok",
            "Panuwit Raden",
            "Sorrawit Treesuk"
        ],
        "date": "2025/03/19",
        "pdf": "http://arxiv.org/pdf/2503.15204",
        "abstract": "Swine disease surveillance is critical to the sustainability of global agriculture, yet its effectiveness is frequently undermined by limited veterinary resources, delayed identification of cases, and variability in diagnostic accuracy. To overcome these barriers, we introduce a novel AI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented Generation (RAG) to deliver timely, evidence-based disease detection and clinical guidance. By automatically classifying user inputs into either Knowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system ensures targeted information retrieval and facilitates precise diagnostic reasoning. An adaptive questioning protocol systematically collects relevant clinical signs, while a confidence-weighted decision fusion mechanism integrates multiple diagnostic hypotheses to generate robust disease predictions and treatment recommendations. Comprehensive evaluations encompassing query classification, disease diagnosis, and knowledge retrieval demonstrate that the system achieves high accuracy, rapid response times, and consistent reliability. By providing a scalable, AI-driven diagnostic framework, this approach enhances veterinary decision-making, advances sustainable livestock management practices, and contributes substantively to the realization of global food security.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.15204"
    },
    "caa5cbc1e61e7aa77360d88b8b61fb06": {
        "title": "DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments",
        "authors": [
            "Wenjie Tang",
            "Yuan Zhou",
            "Erqiang Xu",
            "Keyan Cheng",
            "Minne Li",
            "Liquan Xiao"
        ],
        "date": "2025/03/08",
        "pdf": "http://arxiv.org/pdf/2503.06047",
        "abstract": "Large Language Model~(LLM) based agents have been increasingly popular in solving complex and dynamic tasks, which requires proper evaluation systems to assess their capabilities. Nevertheless, existing benchmarks usually either focus on single-objective tasks or use overly broad assessing metrics, failing to provide a comprehensive inspection of the actual capabilities of LLM-based agents in complicated decision-making tasks. To address these issues, we introduce DSGBench, a more rigorous evaluation platform for strategic decision-making. Firstly, it incorporates six complex strategic games which serve as ideal testbeds due to their long-term and multi-dimensional decision-making demands and flexibility in customizing tasks of various difficulty levels or multiple targets. Secondly, DSGBench employs a fine-grained evaluation scoring system which examines the decision-making capabilities by looking into the performance in five specific dimensions and offering a comprehensive assessment in a well-designed way. Furthermore, DSGBench also incorporates an automated decision-tracking mechanism which enables in-depth analysis of agent behaviour patterns and the changes in their strategies. We demonstrate the advances of DSGBench by applying it to multiple popular LLM-based agents and our results suggest that DSGBench provides valuable insights in choosing LLM-based agents as well as improving their future development. DSGBench is available at https://github.com/DeciBrain-Group/DSGBench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.06047"
    },
    "66336244f893cb2a11dca17d2d0daed7": {
        "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
        "authors": [
            "Ada Defne Tur",
            "Nicholas Meade",
            "Xing Han Lù",
            "Alejandra Zambrano",
            "Arkil Patel",
            "Esin Durmus",
            "Spandana Gella",
            "Karolina Stańczak",
            "Siva Reddy"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.04957",
        "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04957"
    },
    "ac343ad69210d689e37d4d1455f87f5c": {
        "title": "Multi Agent based Medical Assistant for Edge Devices",
        "authors": [
            "Sakharam Gawade",
            "Shivam Akhouri",
            "Chinmay Kulkarni",
            "Jagdish Samant",
            "Pragya Sahu",
            "Aastik",
            "Jai Pahal",
            "Saswat Meher"
        ],
        "date": "2025/03/07",
        "pdf": "http://arxiv.org/pdf/2503.05397",
        "abstract": "Large Action Models (LAMs) have revolutionized intelligent automation, but their application in healthcare faces challenges due to privacy concerns, latency, and dependency on internet access. This report introduces an ondevice, multi-agent healthcare assistant that overcomes these limitations. The system utilizes smaller, task-specific agents to optimize resources, ensure scalability and high performance. Our proposed system acts as a one-stop solution for health care needs with features like appointment booking, health monitoring, medication reminders, and daily health reporting. Powered by the Qwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an average RougeL score of 85.5 for planning and 96.5 for calling for our tasks while being lightweight for on-device deployment. This innovative approach combines the benefits of ondevice systems with multi-agent architectures, paving the way for user-centric healthcare solutions.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.05397"
    },
    "0b1d0f2c8ea897885df0051048568fdf": {
        "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration",
        "authors": [
            "David Wan",
            "Justin Chih-Yao Chen",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "date": "2025/03/19",
        "pdf": "http://arxiv.org/pdf/2503.15272",
        "abstract": "Multi-agent collaboration among models has shown promise in reasoning tasks but is underexplored in long-form generation tasks like summarization and question-answering. We extend multi-agent multi-model reasoning to generation, specifically to improving faithfulness through refinement, i.e., revising model-generated outputs to remove factual inconsistencies. We investigate how iterative collaboration among multiple instances and types of large language models (LLMs) enhances subtasks in the refinement process, such as error detection, critiquing unfaithful sentences, and making corrections based on critiques. We design intrinsic evaluations for each subtask, with our findings indicating that both multi-agent (multiple instances) and multi-model (diverse LLM types) approaches benefit error detection and critiquing. Additionally, reframing critiquing and refinement as reranking rather than generation tasks improves multi-agent performance. We consolidate these insights into a final &#34;recipe&#34; called Multi-Agent Multi-Model Refinement (MAMM-Refine), where multi-agent and multi-model collaboration significantly boosts performance on three summarization datasets as well as on long-form question answering, demonstrating the effectiveness and generalizability of our recipe.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.15272"
    },
    "8b725539fe724dd9bee9773e1ff16fab": {
        "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement",
        "authors": [
            "Ruihan Yang",
            "Fanghua Ye",
            "Jian Li",
            "Siyu Yuan",
            "Yikai Zhang",
            "Zhaopeng Tu",
            "Xiaolong Li",
            "Deqing Yang"
        ],
        "date": "2025/03/20",
        "pdf": "http://arxiv.org/pdf/2503.16024",
        "abstract": "Large language models (LLMs) have recently transformed from text-based assistants to autonomous agents capable of planning, reasoning, and iteratively improving their actions. While numerical reward signals and verifiers can effectively rank candidate actions, they often provide limited contextual guidance. In contrast, natural language feedback better aligns with the generative capabilities of LLMs, providing richer and more actionable suggestions. However, parsing and implementing this feedback effectively can be challenging for LLM-based agents. In this work, we introduce Critique-Guided Improvement (CGI), a novel two-player framework, comprising an actor model that explores an environment and a critic model that generates detailed nature language feedback. By training the critic to produce fine-grained assessments and actionable revisions, and the actor to utilize these critiques, our approach promotes more robust exploration of alternative strategies while avoiding local optima. Experiments in three interactive environments show that CGI outperforms existing baselines by a substantial margin. Notably, even a small critic model surpasses GPT-4 in feedback quality. The resulting actor achieves state-of-the-art performance, demonstrating the power of explicit iterative guidance to enhance decision-making in LLM-based agents.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16024"
    },
    "c57e2e896e7e9441f17383a92b3e78b0": {
        "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration",
        "authors": [
            "Hong Qing Yu",
            "Frank McQuade"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.13514",
        "abstract": "This paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by integrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) with an Incremental Learning (IL) approach. Despite recent advancements, LLMs still face significant challenges in reasoning with structured data, handling dynamic knowledge evolution, and mitigating hallucinations, particularly in mission-critical domains. Our proposed RAG-KG-IL framework addresses these limitations by employing a multi-agent architecture that enables continuous knowledge updates, integrates structured knowledge, and incorporates autonomous agents for enhanced explainability and reasoning. The framework utilizes RAG to ensure the generated responses are grounded in verifiable information, while KGs provide structured domain knowledge for improved consistency and depth of understanding. The Incremental Learning approach allows for dynamic updates to the knowledge base without full retraining, significantly reducing computational overhead and improving the model&#39;s adaptability. We evaluate the framework using real-world case studies involving health-related queries, comparing it to state-of-the-art models like GPT-4o and a RAG-only baseline. Experimental results demonstrate that our approach significantly reduces hallucination rates and improves answer completeness and reasoning accuracy. The results underscore the potential of combining RAG, KGs, and multi-agent systems to create intelligent, adaptable systems capable of real-time knowledge integration and reasoning in complex domains.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13514"
    },
    "d36777a949aa2b6220dafae9287053c8": {
        "title": "Agent-Enhanced Large Language Models for Researching Political Institutions",
        "authors": [
            "Joseph R. Loffredo",
            "Suyeol Yun"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.13524",
        "abstract": "The applications of Large Language Models (LLMs) in political science are rapidly expanding. This paper demonstrates how LLMs, when augmented with predefined functions and specialized tools, can serve as dynamic agents capable of streamlining tasks such as data collection, preprocessing, and analysis. Central to this approach is agentic retrieval-augmented generation (Agentic RAG), which equips LLMs with action-calling capabilities for interaction with external knowledge bases. Beyond information retrieval, LLM agents may incorporate modular tools for tasks like document summarization, transcript coding, qualitative variable classification, and statistical modeling. To demonstrate the potential of this approach, we introduce CongressRA, an LLM agent designed to support scholars studying the U.S. Congress. Through this example, we highlight how LLM agents can reduce the costs of replicating, testing, and extending empirical research using the domain-specific data that drives the study of political institutions.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13524"
    },
    "74e8b2991fcb479af68b03a9470c9684": {
        "title": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by Adaptive Tree Traversal",
        "authors": [
            "Vaibhav Aggarwal",
            "Ojasv Kamal",
            "Abhinav Japesh",
            "Zhijing Jin",
            "Bernhard Schölkopf"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14269",
        "abstract": "Large Language Models (LLMs) have revolutionized various domains, including natural language processing, data analysis, and software development, by enabling automation. In software engineering, LLM-powered coding agents have garnered significant attention due to their potential to automate complex development tasks, assist in debugging, and enhance productivity. However, existing approaches often struggle with sub-optimal decision-making, requiring either extensive manual intervention or inefficient compute scaling strategies. To improve coding agent performance, we present Dynamic Action Re-Sampling (DARS), a novel inference time compute scaling approach for coding agents, that is faster and more effective at recovering from sub-optimal decisions compared to baselines. While traditional agents either follow linear trajectories or rely on random sampling for scaling compute, our approach DARS works by branching out a trajectory at certain key decision points by taking an alternative action given the history of the trajectory and execution feedback of the previous attempt from that point. We evaluate our approach on SWE-Bench Lite benchmark, demonstrating that this scaling strategy achieves a pass@k score of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of 47%, outperforming state-of-the-art (SOTA) open-source frameworks.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14269"
    },
    "f94d953e86fdfa5c1fe6fcc313debd56": {
        "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
        "authors": [
            "Wei Fang",
            "Yang Zhang",
            "Kaizhi Qian",
            "James Glass",
            "Yada Zhu"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14432",
        "abstract": "Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically &#34;plays&#34; with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14432"
    },
    "8ea63e6e401e0f9b6287664f0b7f2b73": {
        "title": "Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations",
        "authors": [
            "Hikaru Shimadzu",
            "Takehito Utsuro",
            "Daisuke Kitayama"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14620",
        "abstract": "In the 2023 edition of the White Paper on Information and Communications, it is estimated that the population of social networking services in Japan will exceed 100 million by 2022, and the influence of social networking services in Japan is growing significantly. In addition, marketing using SNS and research on the propagation of emotions and information on SNS are being actively conducted, creating the need for a system for predicting trends in SNS interactions. We have already created a system that simulates the behavior of various communities on SNS by building a virtual SNS environment in which agents post and reply to each other in a chat community created by agents using a LLMs. In this paper, we evaluate the impact of the search extension generation mechanism used to create posts and replies in a virtual SNS environment using a simulation system on the ability to generate posts and replies. As a result of the evaluation, we confirmed that the proposed search extension generation mechanism, which mimics human search behavior, generates the most natural exchange.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ],
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14620"
    },
    "26f89c6e2b8e1b813c667c6f30ec185b": {
        "title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs",
        "authors": [
            "Vincent Li",
            "Yule Fu",
            "Tim Knappe",
            "Kevin Han",
            "Kevin Zhu"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2503.11657",
        "abstract": "Large Language Models have demonstrated remarkable capabilities in natural language processing tasks, including mathematical problem-solving that requires multi-step logical reasoning. However, challenges persist in automating the identification of key mathematical concepts, understanding their interrelations, and formalizing proofs within a rigorous framework. We present a novel framework that leverages knowledge graphs to augment LLMs to construct and formalize mathematical proofs. Our results demonstrate significant performance improvements across multiple datasets, with using knowledge graphs, achieving up to a 34% success rate on the MUSTARDSAUCE dataset on o1-mini and consistently outperforming baseline approaches by 2-11% across different models. We show how this approach bridges the gap between natural language understanding and formal logic proof systems and achieve elevated results for foundation models over baseline.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11657"
    },
    "601ca0bd430e0e1367925b2b1bcbac03": {
        "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection",
        "authors": [
            "Tharindu Kumarage",
            "Cameron Johnson",
            "Jadie Adams",
            "Lin Ai",
            "Matthias Kirchner",
            "Anthony Hoogs",
            "Joshua Garland",
            "Julia Hirschberg",
            "Arslan Basharat",
            "Huan Liu"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.15552",
        "abstract": "The rapid advancement of conversational agents, particularly chatbots powered by Large Language Models (LLMs), poses a significant risk of social engineering (SE) attacks on social media platforms. SE detection in multi-turn, chat-based interactions is considerably more complex than single-instance detection due to the dynamic nature of these conversations. A critical factor in mitigating this threat is understanding the mechanisms through which SE attacks operate, specifically how attackers exploit vulnerabilities and how victims&#39; personality traits contribute to their susceptibility. In this work, we propose an LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating multi-turn conversations. We model victim agents with varying personality traits to assess how psychological profiles influence susceptibility to manipulation. Using a dataset of over 1000 simulated conversations, we examine attack scenarios in which adversaries, posing as recruiters, funding agencies, and journalists, attempt to extract sensitive information. Based on this analysis, we present a proof of concept, SE-OmniGuard, to offer personalized protection to users by leveraging prior knowledge of the victims personality, evaluating attack strategies, and monitoring information exchanges in conversations to identify potential SE attempts.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.15552"
    },
    "e92acde6c28267f954d98f55f31a8234": {
        "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
        "authors": [
            "Akhil Perincherry",
            "Jacob Krantz",
            "Stefan Lee"
        ],
        "date": "2025/03/20",
        "pdf": "http://arxiv.org/pdf/2503.16394",
        "abstract": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at https://www.akhilperincherry.com/VLN-Imagine-website/.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16394"
    },
    "8c0786d88200f9dfc117d7eef0b8a500": {
        "title": "Survey on Evaluation of LLM-based Agents",
        "authors": [
            "Asaf Yehudai",
            "Lilach Eden",
            "Alan Li",
            "Guy Uziel",
            "Yilun Zhao",
            "Roy Bar-Haim",
            "Arman Cohan",
            "Michal Shmueli-Scheuer"
        ],
        "date": "2025/03/20",
        "pdf": "http://arxiv.org/pdf/2503.16416",
        "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16416"
    },
    "84e653c9399f57eecb52bcf44a237716": {
        "title": "LLM Agents for Education: Advances and Applications",
        "authors": [
            "Zhendong Chu",
            "Shen Wang",
            "Jian Xie",
            "Tinghui Zhu",
            "Yibo Yan",
            "Jinheng Ye",
            "Aoxiao Zhong",
            "Xuming Hu",
            "Jing Liang",
            "Philip S. Yu",
            "Qingsong Wen"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11733",
        "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) \\emph{Domain-Specific Educational Agents}, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11733"
    },
    "8f6a31651270ccb2ce8a8ca378e3cb62": {
        "title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
        "authors": [
            "Junyu Luo",
            "Weizhi Zhang",
            "Ye Yuan",
            "Yusheng Zhao",
            "Junwei Yang",
            "Yiyang Gu",
            "Bohan Wu",
            "Binqi Chen",
            "Ziyue Qiao",
            "Qingqing Long",
            "Rongcheng Tu",
            "Xiao Luo",
            "Wei Ju",
            "Zhiping Xiao",
            "Yifan Wang",
            "Meng Xiao",
            "Chenwu Liu",
            "Jingyang Yuan",
            "Shichang Zhang",
            "Yiqiao Jin",
            "Fan Zhang",
            "Xian Wu",
            "Hanqing Zhao",
            "Dacheng Tao",
            "Philip S. Yu",
            "Ming Zhang"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21460",
        "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21460"
    },
    "f53122daba2b2f39df85deba36cf51b9": {
        "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
        "authors": [
            "Souradip Chakraborty",
            "Sujay Bhatt",
            "Udari Madhushani Sehwag",
            "Soumya Suvra Ghosal",
            "Jiahao Qiu",
            "Mengdi Wang",
            "Dinesh Manocha",
            "Furong Huang",
            "Alec Koppel",
            "Sumitra Ganesh"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21720",
        "abstract": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21720"
    },
    "09441fc23ddc1d49f1819056c7a47c08": {
        "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
        "authors": [
            "Rana Salama",
            "Jason Cai",
            "Michelle Yuan",
            "Anna Currey",
            "Monica Sunkara",
            "Yi Zhang",
            "Yassine Benajiba"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21760",
        "abstract": "Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21760"
    },
    "c4fdf661bf8928db15c3ac6bdad2b56d": {
        "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
        "authors": [
            "Shengyue Guan",
            "Haoyi Xiong",
            "Jindong Wang",
            "Jiang Bian",
            "Bin Zhu",
            "Jian-guang Lou"
        ],
        "date": "2025/03/28",
        "pdf": "http://arxiv.org/pdf/2503.22458",
        "abstract": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22458"
    },
    "7e76098cde9cedbbd7fd621f39a8c57f": {
        "title": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents",
        "authors": [
            "Hanchao Liu",
            "Rongjun Li",
            "Weimin Xiong",
            "Ziyu Zhou",
            "Wei Peng"
        ],
        "date": "2025/03/28",
        "pdf": "http://arxiv.org/pdf/2503.22473",
        "abstract": "Workflows play a crucial role in enhancing enterprise efficiency by orchestrating complex processes with multiple tools or components. However, hand-crafted workflow construction requires expert knowledge, presenting significant technical barriers. Recent advancements in Large Language Models (LLMs) have improved the generation of workflows from natural language instructions (aka NL2Workflow), yet existing single LLM agent-based methods face performance degradation on complex tasks due to the need for specialized knowledge and the strain of task-switching. To tackle these challenges, we propose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor, orchestrator, and filler agent, each with distinct roles that collaboratively enhance the conversion process. As there are currently no publicly available NL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which includes 3,695 real-world business samples for training and evaluation. Experimental results show that our approach significantly increases the success rate of workflow construction, providing a novel and effective solution for enterprise NL2Workflow services.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22473"
    },
    "7a72c0080495957f1480c962b3f24423": {
        "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions",
        "authors": [
            "Mohammad Almansoori",
            "Komal Kumar",
            "Hisham Cholakkal"
        ],
        "date": "2025/03/28",
        "pdf": "http://arxiv.org/pdf/2503.22678",
        "abstract": "In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-of-thought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLM&#39;s ability to engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our code, simulation tool, and benchmark are available at \\href{https://medagentsim.netlify.app/}.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22678"
    },
    "f67387b5779ea69017fdaef549bd7fe3": {
        "title": "A Survey of Large Language Model Agents for Question Answering",
        "authors": [
            "Murong Yue"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.19213",
        "abstract": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19213"
    },
    "3e64c47554e09749e9d5fa9be3c0fb66": {
        "title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
        "authors": [
            "Xuechen Liang",
            "Meiling Tao",
            "Yinghui Xia",
            "Jianhui Wang",
            "Kun Li",
            "Yijin Wang",
            "Jingsong Yang",
            "Tianyu Shi",
            "Yuantao Wang",
            "Miao Zhang",
            "Xueqian Wang"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19271",
        "abstract": "Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments. To address these issues, this paper proposes an innovative framework Memory-Enhanced Agents with Reflective Self-improvement. The MARS framework comprises three agents: the User, the Assistant, and the Checker. By integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents capabilities in handling multi-tasking and long-span information.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19271"
    },
    "7ff8f93830a7e8402a1d098309aed2bc": {
        "title": "CoMAC: Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions",
        "authors": [
            "Junfeng Liu",
            "Christopher T. Symons",
            "Ranga Raju Vatsavai"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19274",
        "abstract": "Recent advancements in AI-driven conversational agents have exhibited immense potential of AI applications. Effective response generation is crucial to the success of these agents. While extensive research has focused on leveraging multiple auxiliary data sources (e.g., knowledge bases and personas) to enhance response generation, existing methods often struggle to efficiently extract relevant information from these sources. There are still clear limitations in the ability to combine versatile conversational capabilities with adherence to known facts and adaptation to large variations in user preferences and belief systems, which continues to hinder the wide adoption of conversational AI tools. This paper introduces a novel method, Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions (CoMAC), for conversation generation, which employs specialized encoding streams and post-fusion grounding networks for multiple data sources to identify relevant persona and knowledge information for the conversation. CoMAC also leverages a novel text similarity metric that allows bi-directional information sharing among multiple sources and focuses on a selective subset of meaningful words. Our experiments show that CoMAC improves the relevant persona and knowledge prediction accuracies and response generation quality significantly over two state-of-the-art methods.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19274"
    },
    "617c1a6b61b00a6ac9ffd650e4533b87": {
        "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents",
        "authors": [
            "Vidya Srinivas",
            "Xuhai Xu",
            "Xin Liu",
            "Kumar Ayush",
            "Isaac Galatzer-Levy",
            "Shwetak Patel",
            "Daniel McDuff",
            "Tim Althoff"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19328",
        "abstract": "While NLP research has made strides in conversational tasks, many approaches focus on single-turn responses with well-defined objectives or evaluation criteria. In contrast, coaching presents unique challenges with initially undefined goals that evolve through multi-turn interactions, subjective evaluation criteria, mixed-initiative dialogue. In this work, we describe and implement five multi-turn coaching agents that exhibit distinct conversational styles, and evaluate them through a user study, collecting first-person feedback on 155 conversations. We find that users highly value core functionality, and that stylistic components in absence of core components are viewed negatively. By comparing user feedback with third-person evaluations from health experts and an LM, we reveal significant misalignment across evaluation approaches. Our findings provide insights into design and evaluation of conversational coaching agents and contribute toward improving human-centered NLP applications.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19328"
    },
    "13fc78ed52933fba3f818696e3ac75a1": {
        "title": "Writing as a testbed for open ended agents",
        "authors": [
            "Sian Gooding",
            "Lucia Lopez-Rivilla",
            "Edward Grefenstette"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19711",
        "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast solution space, demanding both expansive exploration and adaptable strategies, especially when success lacks a clear, objective definition. Writing, with its vast solution space and subjective evaluation criteria, provides a compelling testbed for studying such problems. In this paper, we investigate the potential of LLMs to act as collaborative co-writers, capable of suggesting and implementing text improvements autonomously. We analyse three prominent LLMs - Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action diversity, human alignment, and iterative improvement capabilities impact overall performance. This work establishes a framework for benchmarking autonomous writing agents and, more broadly, highlights fundamental challenges and potential solutions for building systems capable of excelling in diverse open-ended domains.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19711"
    },
    "dd94fe9dbeb1bbf821b3cd196aa31edf": {
        "title": "sudo rm -rf agentic_security",
        "authors": [
            "Sejin Lee",
            "Jian Kim",
            "Haon Park",
            "Ashkan Yousefpour",
            "Sangyoon Yu",
            "Min Song"
        ],
        "date": "2025/03/26",
        "pdf": "http://arxiv.org/pdf/2503.20279",
        "abstract": "Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.20279"
    },
    "ccbd8bb4a4dd80af4b255d06a33770f6": {
        "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues",
        "authors": [
            "Yuhan Liu",
            "Yunbo Long"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21080",
        "abstract": "While large language model (LLM)-based chatbots have been applied for effective engagement in credit dialogues, their capacity for dynamic emotional expression remains limited. Current agents primarily rely on passive empathy rather than affective reasoning. For instance, when faced with persistent client negativity, the agent should employ strategic emotional adaptation by expressing measured anger to discourage counterproductive behavior and guide the conversation toward resolution. This context-aware emotional modulation is essential for imitating the nuanced decision-making of human negotiators. This paper introduces an EQ-negotiator that combines emotion sensing from pre-trained language models (PLMs) with emotional reasoning based on Game Theory and Hidden Markov Models. It takes into account both the current and historical emotions of the client to better manage and address negative emotions during interactions. By fine-tuning pre-trained language models (PLMs) on public emotion datasets and validating them on the credit dialogue datasets, our approach enables LLM-based agents to effectively capture shifts in client emotions and dynamically adjust their response tone based on our emotion decision policies in real-world financial negotiations. This EQ-negotiator can also help credit agencies foster positive client relationships, enhancing satisfaction in credit services.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21080"
    },
    "a56dcf15ff339dc5ac74c58a256e9a7f": {
        "title": "ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach",
        "authors": [
            "Reem Gody",
            "Mahmoud Goudy",
            "Ahmed Y. Tawfik"
        ],
        "date": "2025/03/21",
        "pdf": "http://arxiv.org/pdf/2503.17460",
        "abstract": "In this paper, we present ConvoGen: an innovative framework for generating synthetic conversational data using multi-agent systems. Our method leverages few-shot learning and introduces iterative sampling from a dynamically updated few-shot hub to create diverse and realistic conversational scenarios. The generated data has numerous applications, including training and evaluating conversational AI models, and augmenting existing datasets for tasks like conversational intent classification or conversation summarization. Our experiments demonstrate the effectiveness of this method in producing high-quality diverse synthetic conversational data, highlighting its potential to enhance the development and evaluation of conversational AI systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.17460"
    },
    "31567ecef9d254394aadbdeb260767ac": {
        "title": "Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information",
        "authors": [
            "Hojun Cho",
            "Donghu Kim",
            "Soyoung Yang",
            "Chan Lee",
            "Hunjoo Lee",
            "Jaegul Choo"
        ],
        "date": "2025/03/22",
        "pdf": "http://arxiv.org/pdf/2503.17753",
        "abstract": "Language agents powered by large language models (LLMs) face significant deployment challenges in resource-constrained environments, particularly for specialized domains and less-common languages. This paper presents Tox-chat, a Korean chemical toxicity information agent devised within these limitations. We propose two key innovations: a context-efficient architecture that reduces token consumption through hierarchical section search, and a scenario-based dialogue generation methodology that effectively distills tool-using capabilities from larger models. Experimental evaluations demonstrate that our fine-tuned 8B parameter model substantially outperforms both untuned models and baseline approaches, in terms of DB faithfulness and preference. Our work offers valuable insights for researchers developing domain-specific language agents under practical constraints.",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.17753"
    },
    "d838fec710c0454fbba7c27a691344c5": {
        "title": "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection",
        "authors": [
            "Yibo Yan",
            "Shen Wang",
            "Jiahao Huo",
            "Philip S. Yu",
            "Xuming Hu",
            "Qingsong Wen"
        ],
        "date": "2025/03/23",
        "pdf": "http://arxiv.org/pdf/2503.18132",
        "abstract": "Mathematical error detection in educational settings presents a significant challenge for Multimodal Large Language Models (MLLMs), requiring a sophisticated understanding of both visual and textual mathematical content along with complex reasoning capabilities. Though effective in mathematical problem-solving, MLLMs often struggle with the nuanced task of identifying and categorizing student errors in multimodal mathematical contexts. Therefore, we introduce MathAgent, a novel Mixture-of-Math-Agent framework designed specifically to address these challenges. Our approach decomposes error detection into three phases, each handled by a specialized agent: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer. This architecture enables more accurate processing of mathematical content by explicitly modeling relationships between multimodal problems and student solution steps. We evaluate MathAgent on real-world educational data, demonstrating approximately 5% higher accuracy in error step identification and 3% improvement in error categorization compared to baseline models. Besides, MathAgent has been successfully deployed in an educational platform that has served over one million K-12 students, achieving nearly 90% student satisfaction while generating significant cost savings by reducing manual error detection.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18132"
    },
    "24a2a5e1718557cf809a8e87b23439ab": {
        "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration",
        "authors": [
            "Zhexuan Wang",
            "Yutong Wang",
            "Xuebo Liu",
            "Liang Ding",
            "Miao Zhang",
            "Jie Liu",
            "Min Zhang"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18891",
        "abstract": "Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents&#39; communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout, which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.com/wangzx1219/AgentDropout.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18891"
    },
    "cecfefa9723e716554aa5e3297d135df": {
        "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization",
        "authors": [
            "Jian Zhang",
            "Zhangqi Wang",
            "Haiping Zhu",
            "Jun Liu",
            "Qika Lin",
            "Erik Cambria"
        ],
        "date": "2025/03/21",
        "pdf": "http://arxiv.org/pdf/2503.16874",
        "abstract": "The basic question-answering format of large language models involves inputting a prompt and receiving a response, and the quality of the prompt directly impacts the effectiveness of the response. Automated Prompt Optimization (APO) aims to break free from the cognitive biases of manually designed prompts and explores a broader design space for prompts. However, existing APO methods suffer from limited flexibility of fixed templates and inefficient search in prompt spaces as key issues. To this end, we propose a Multi-Agent framework Incorporating Socratic guidance (MARS), which utilizes multi-agent fusion technology for automatic planning, with gradual continuous optimization and evaluation. Specifically, MARS comprises seven agents, each with distinct functionalities, which autonomously use the Planner to devise an optimization path that ensures flexibility. Additionally, it employs a Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the prompts while conducting effective search. We conduct extensive experiments on various datasets to validate the effectiveness of our method, and perform additional analytical experiments to assess the model&#39;s advancement as well as the interpretability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16874"
    },
    "a0a87a7ff5e8224f802c82812bdd2143": {
        "title": "Gricean Norms as a Basis for Effective Collaboration",
        "authors": [
            "Fardin Saad",
            "Pradeep K. Murukannaiah",
            "Munindar P. Singh"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14484",
        "abstract": "Effective human-AI collaboration hinges not only on the AI agent&#39;s ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks -- common ground, relevance theory, and theory of mind -- into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent&#39;s pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14484"
    },
    "9d6f8e56edbca6455c7f3eb4f3732bf2": {
        "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay",
        "authors": [
            "Young-Ho Bae",
            "Casey C. Bennett"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2503.16432",
        "abstract": "This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game &#34;Dont Starve Together&#34;, comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16432"
    },
    "14d37e57f142fb713f00584b2a688dda": {
        "title": "The Application of MATEC (Multi-AI Agent Team Care) Framework in Sepsis Care",
        "authors": [
            "Andrew Cho",
            "Jason M. Woo",
            "Brian Shi",
            "Aishwaryaa Udeshi",
            "Jonathan S. H. Woo"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2503.16433",
        "abstract": "Under-resourced or rural hospitals have limited access to medical specialists and healthcare professionals, which can negatively impact patient outcomes in sepsis. To address this gap, we developed the MATEC (Multi-AI Agent Team Care) framework, which integrates a team of specialized AI agents for sepsis care. The sepsis AI agent team includes five doctor agents, four health professional agents, and a risk prediction model agent, with an additional 33 doctor agents available for consultations. Ten attending physicians at a teaching hospital evaluated this framework, spending approximately 40 minutes on the web-based MATEC application and participating in the 5-point Likert scale survey (rated from 1-unfavorable to 5-favorable). The physicians found the MATEC framework very useful (Median=4, P=0.01), and very accurate (Median=4, P&lt;0.01). This pilot study demonstrates that a Multi-AI Agent Team Care framework (MATEC) can potentially be useful in assisting medical professionals, particularly in under-resourced hospital settings.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16433"
    },
    "ce7b16fc0bccbeecaadef583b9b67b86": {
        "title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning",
        "authors": [
            "Zhoujian Sun",
            "Ziyi Liu",
            "Cheng Luo",
            "Jiebin Chu",
            "Zhengxing Huang"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2503.16463",
        "abstract": "Recent advances in large language models (LLMs) have shown promising results in medical diagnosis, with some studies indicating superior performance compared to human physicians in specific scenarios. However, the diagnostic capabilities of LLMs are often overestimated, as their performance significantly deteriorates in interactive diagnostic settings that require active information gathering. This study investigates the underlying mechanisms behind the performance degradation phenomenon and proposes a solution. We identified that the primary deficiency of LLMs lies in the initial diagnosis phase, particularly in information-gathering efficiency and initial diagnosis formation, rather than in the subsequent differential diagnosis phase. To address this limitation, we developed a plug-and-play method enhanced (PPME) LLM agent, leveraging over 3.5 million electronic medical records from Chinese and American healthcare facilities. Our approach integrates specialized models for initial disease diagnosis and inquiry into the history of the present illness, trained through supervised and reinforcement learning techniques. The experimental results indicate that the PPME LLM achieved over 30% improvement compared to baselines. The final diagnostic accuracy of the PPME LLM in interactive diagnostic scenarios approached levels comparable to those achieved using complete clinical data. These findings suggest a promising potential for developing autonomous diagnostic systems, although further validation studies are needed.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16463"
    },
    "d770b8c68188c38012236a5219526f91": {
        "title": "EmpathyAgent: Can Embodied Agents Conduct Empathetic Actions?",
        "authors": [
            "Xinyan Chen",
            "Jiaxin Ge",
            "Hongming Dai",
            "Qiang Zhou",
            "Qiuxuan Feng",
            "Jingtong Hu",
            "Yizhou Wang",
            "Jiaming Liu",
            "Shanghang Zhang"
        ],
        "date": "2025/03/19",
        "pdf": "http://arxiv.org/pdf/2503.16545",
        "abstract": "Empathy is fundamental to human interactions, yet it remains unclear whether embodied agents can provide human-like empathetic support. Existing works have studied agents&#39; tasks solving and social interactions abilities, but whether agents can understand empathetic needs and conduct empathetic behaviors remains overlooked. To address this, we introduce EmpathyAgent, the first benchmark to evaluate and enhance agents&#39; empathetic actions across diverse scenarios. EmpathyAgent contains 10,000 multimodal samples with corresponding empathetic task plans and three different challenges. To systematically evaluate the agents&#39; empathetic actions, we propose an empathy-specific evaluation suite that evaluates the agents&#39; empathy process. We benchmark current models and found that exhibiting empathetic actions remains a significant challenge. Meanwhile, we train Llama3-8B using EmpathyAgent and find it can potentially enhance empathetic behavior. By establishing a standard benchmark for evaluating empathetic actions, we hope to advance research in empathetic embodied agents. Our code and data are publicly available at https://github.com/xinyan-cxy/EmpathyAgent.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16545"
    },
    "a9b278e1b45d901ea8a302a9f21e9354": {
        "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent",
        "authors": [
            "Humza Nusrat",
            "Bing Luo",
            "Ryan Hall",
            "Joshua Kim",
            "Hassan Bagher-Ebadian",
            "Anthony Doemer",
            "Benjamin Movsas",
            "Kundan Thind"
        ],
        "date": "2025/03/21",
        "pdf": "http://arxiv.org/pdf/2503.17553",
        "abstract": "Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making. To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy. DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL). Operating entirely within secure local infrastructure, this agent eliminates external data sharing. We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations. The 70B model demonstrated significantly improved performance, achieving approximately 16.4% higher final scores than the 8B model. The RAG approach outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated convergence, highlighting the synergy of retrieval-based memory and reinforcement learning. Optimal temperature hyperparameter analysis identified 0.4 as providing the best balance between exploration and exploitation. This proof of concept study represents the first successful deployment of locally hosted LLM agents for autonomous optimization of treatment plans within a commercial radiotherapy planning system. By extending human-machine interaction through interpretable natural language reasoning, DOLA offers a scalable and privacy-conscious framework, with significant potential for clinical implementation and workflow improvement.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.17553"
    },
    "7babf881d292a7545391dbdaaa6d7eba": {
        "title": "AgentRxiv: Towards Collaborative Autonomous Research",
        "authors": [
            "Samuel Schmidgall",
            "Michael Moor"
        ],
        "date": "2025/03/23",
        "pdf": "http://arxiv.org/pdf/2503.18102",
        "abstract": "Progress in scientific discovery is rarely the result of a single &#34;Eureka&#34; moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other&#39;s research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18102"
    },
    "487afe7e9a6604888b28f55e5e96f22d": {
        "title": "Safeguarding Mobile GUI Agent via Logic-based Action Verification",
        "authors": [
            "Jungjae Lee",
            "Dongjae Lee",
            "Chihun Choi",
            "Youngmin Im",
            "Jaeyoung Wi",
            "Kihong Heo",
            "Sangeun Oh",
            "Sunjae Lee",
            "Insik Shin"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18492",
        "abstract": "Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interpreting GUIs. These agents promise to revolutionize mobile computing by allowing users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA): a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA is designed to deterministically ensure that an agent&#39;s actions strictly align with user intent before conducting an action. At its core, VSA introduces a novel autoformalization technique that translates natural language user instructions into a formally verifiable specification, expressed in our domain-specific language (DSL). This enables runtime, rule-based verification, allowing VSA to detect and prevent erroneous actions executing an action, either by providing corrective feedback or halting unsafe behavior. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agent. effectively bridging the gap between LFM-driven automation and formal software verification. We implement VSA using off-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a significant 20.4%-25.6% improvement over existing LLM-based verification methods, and consequently increases the GUI agent&#39;s task completion rate by 90%-130%.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18492"
    },
    "d3941739f04e589fa8d9b0a6c0e8b673": {
        "title": "Verbal Process Supervision Elicits Better Coding Agents",
        "authors": [
            "Hao-Yuan Chen",
            "Cheng-Pong Huang",
            "Jui-Ming Yao"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18494",
        "abstract": "The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18494"
    },
    "b197193c35af6b85a40cc3c4c96ef159": {
        "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents",
        "authors": [
            "Haoyu Wang",
            "Christopher M. Poskitt",
            "Jun Sun"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18666",
        "abstract": "Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identifying 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18666"
    },
    "c480a8219a40bd84955386135c5ca67b": {
        "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments",
        "authors": [
            "Sara Fish",
            "Julia Shephard",
            "Minkai Li",
            "Ran I. Shorrer",
            "Yannai A. Gonczarowski"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18825",
        "abstract": "We develop benchmarks for LLM agents that act in, learn from, and strategize in unknown environments, the specifications of which the LLM agent must learn over time from deliberate exploration. Our benchmarks consist of decision-making tasks derived from key problems in economics. To forestall saturation, the benchmark tasks are synthetically generated with scalable difficulty levels. Additionally, we propose litmus tests, a new kind of quantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests quantify differences in character, values, and tendencies of LLMs and LLM agents, by considering their behavior when faced with tradeoffs (e.g., efficiency versus equality) where there is no objectively right or wrong behavior. Overall, our benchmarks and litmus tests assess the abilities and tendencies of LLM agents in tackling complex economic problems in diverse settings spanning procurement, scheduling, task allocation, and pricing -- applications that should grow in importance as such agents are further integrated into the economy.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18825"
    },
    "8d093e6b11f15e74cfcd9bbb2bac0a35": {
        "title": "Multi-agent Application System in Office Collaboration Scenarios",
        "authors": [
            "Songtao Sun",
            "Jingyi Li",
            "Yuanfei Dong",
            "Haoguang Liu",
            "Chenxin Xu",
            "Fuyang Li",
            "Qiang Liu"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19584",
        "abstract": "This paper introduces a multi-agent application system designed to enhance office collaboration efficiency and work quality. The system integrates artificial intelligence, machine learning, and natural language processing technologies, achieving functionalities such as task allocation, progress monitoring, and information sharing. The agents within the system are capable of providing personalized collaboration support based on team members&#39; needs and incorporate data analysis tools to improve decision-making quality. The paper also proposes an intelligent agent architecture that separates Plan and Solver, and through techniques such as multi-turn query rewriting and business tool retrieval, it enhances the agent&#39;s multi-intent and multi-turn dialogue capabilities. Furthermore, the paper details the design of tools and multi-turn dialogue in the context of office collaboration scenarios, and validates the system&#39;s effectiveness through experiments and evaluations. Ultimately, the system has demonstrated outstanding performance in real business applications, particularly in query understanding, task planning, and tool calling. Looking forward, the system is expected to play a more significant role in addressing complex interaction issues within dynamic environments and large-scale multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19584"
    },
    "041b86faedf7fe252d13a1480bae33b9": {
        "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
        "authors": [
            "Salaheddin Alzubi",
            "Creston Brooks",
            "Purva Chiniya",
            "Edoardo Contente",
            "Chiara von Gerlach",
            "Lucas Irwin",
            "Yihan Jiang",
            "Arda Kaz",
            "Windsor Nguyen",
            "Sewoong Oh",
            "Himanshu Tyagi",
            "Pramod Viswanath"
        ],
        "date": "2025/03/26",
        "pdf": "http://arxiv.org/pdf/2503.20201",
        "abstract": "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity&#39;s Sonar Reasoning Pro and OpenAI&#39;s GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.20201"
    },
    "31163d24a54eb32676b64f7e919b255f": {
        "title": "TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews",
        "authors": [
            "Huimin Xu",
            "Seungjun Yi",
            "Terence Lim",
            "Jiawei Xu",
            "Andrew Well",
            "Carlos Mery",
            "Aidong Zhang",
            "Yuji Zhang",
            "Heng Ji",
            "Keshav Pingali",
            "Yan Leng",
            "Ying Ding"
        ],
        "date": "2025/03/26",
        "pdf": "http://arxiv.org/pdf/2503.20666",
        "abstract": "Thematic analysis (TA) is a widely used qualitative approach for uncovering latent meanings in unstructured text data. TA provides valuable insights in healthcare but is resource-intensive. Large Language Models (LLMs) have been introduced to perform TA, yet their applications in healthcare remain unexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis framework using Multi-Agent LLMs for clinical interviews. We leverage the scalability and coherence of multi-agent systems through structured conversations between agents and coordinate the expertise of cardiac experts in TA. Using interview transcripts from parents of children with Anomalous Aortic Origin of a Coronary Artery (AAOCA), a rare congenital heart disease, we demonstrate that TAMA outperforms existing LLM-assisted TA approaches, achieving higher thematic hit rate, coverage, and distinctiveness. TAMA demonstrates strong potential for automated TA in clinical settings by leveraging multi-agent LLM systems with human-in-the-loop integration by enhancing quality while significantly reducing manual workload.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ],
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.20666"
    },
    "1784972f2e5bc05dd88a3a48000f1531": {
        "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
        "authors": [
            "Karanbir Singh",
            "William Ngu"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21237",
        "abstract": "Advancements in retrieving accessible information have evolved faster in the last few years compared to the decades since the internet&#39;s creation. Search engines, like Google, have been the number one way to find relevant data. They have always relied on the user&#39;s abilities to find the best information in its billions of links and sources at everybody&#39;s fingertips. The advent of large language models (LLMs) has completely transformed the field of information retrieval. The LLMs excel not only at retrieving relevant knowledge but also at summarizing it effectively, making information more accessible and consumable for users. On top of it, the rise of AI Agents has introduced another aspect to information retrieval i.e. dynamic information retrieval which enables the integration of real-time data such as weather forecasts, and financial data with the knowledge base to curate context-aware knowledge. However, despite these advancements the agents remain susceptible to issues of bias and fairness, challenges deeply rooted within the knowledge base and training of LLMs. This study introduces a novel approach to bias-aware knowledge retrieval by leveraging agentic framework and the innovative use of bias detectors as tools to identify and highlight inherent biases in the retrieved content. By empowering users with transparency and awareness, this approach aims to foster more equitable information systems and promote the development of responsible AI.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21237"
    },
    "d77e4c32a22fd63bd02e87399958d736": {
        "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics",
        "authors": [
            "Arsham Gholamzadeh Khoee",
            "Shuai Wang",
            "Yinan Yu",
            "Robert Feldt",
            "Dhasarathy Parthasarathy"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21735",
        "abstract": "Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21735"
    },
    "5d15d8ea90e2bd8389937e46fc7983a3": {
        "title": "Debate-Driven Multi-Agent LLMs for Phishing Email Detection",
        "authors": [
            "Ngoc Tuong Vy Nguyen",
            "Felix D Childress",
            "Yunting Yin"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.22038",
        "abstract": "Phishing attacks remain a critical cybersecurity threat. Attackers constantly refine their methods, making phishing emails harder to detect. Traditional detection methods, including rule-based systems and supervised machine learning models, either rely on predefined patterns like blacklists, which can be bypassed with slight modifications, or require large datasets for training and still can generate false positives and false negatives. In this work, we propose a multi-agent large language model (LLM) prompting technique that simulates debates among agents to detect whether the content presented on an email is phishing. Our approach uses two LLM agents to present arguments for or against the classification task, with a judge agent adjudicating the final verdict based on the quality of reasoning provided. This debate mechanism enables the models to critically analyze contextual cue and deceptive patterns in text, which leads to improved classification accuracy. The proposed framework is evaluated on multiple phishing email datasets and demonstrate that mixed-agent configurations consistently outperform homogeneous configurations. Results also show that the debate structure itself is sufficient to yield accurate decisions without extra prompting strategies.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22038"
    },
    "c3d5c0cddc1bcca3c3ff3a6628c40791": {
        "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
        "authors": [
            "Yanzheng Xiang",
            "Hanqi Yan",
            "Shuyin Ouyang",
            "Lin Gui",
            "Yulan He"
        ],
        "date": "2025/03/31",
        "pdf": "http://arxiv.org/pdf/2504.00255",
        "abstract": "This study evaluates large language models (LLMs) in generating code from algorithm descriptions from recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a multi-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implement solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful Non-Reasoning LLMs and Reasoning LLMs as foundational models. The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark&#39;s difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We will open-source our benchmark, and code at https://github.com/xyzCS/SciReplicate-Bench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00255"
    },
    "869bee540143ad8df01c0ae32f1dc629": {
        "title": "When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)",
        "authors": [
            "Mahak Agarwal",
            "Divyam Khanna"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00374",
        "abstract": "In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims-some accurate, others forcefully incorrect-and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B-14B parameters), where we systematically vary agent verbosity (30-300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers-often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00374"
    },
    "651adb5d58cb69adcddabac87519905a": {
        "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
        "authors": [
            "Jiuzhou Han",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00406",
        "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00406"
    },
    "e598e2e2537ec06e7c4cb01ab5b2b3cb": {
        "title": "On the Robustness of Agentic Function Calling",
        "authors": [
            "Ella Rabinovich",
            "Ateret Anaby-Tavor"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00914",
        "abstract": "Large Language Models (LLMs) are increasingly acting as autonomous agents, with function calling (FC) capabilities enabling them to invoke specific tools for tasks. While prior research has primarily focused on improving FC accuracy, little attention has been given to the robustness of these agents to perturbations in their input. We introduce a benchmark assessing FC robustness in two key areas: resilience to naturalistic query variations, and stability in function calling when the toolkit expands with semantically related tools. Evaluating best-performing FC models on a carefully expanded subset of the Berkeley function calling leaderboard (BFCL), we identify critical weaknesses in existing evaluation methodologies, and highlight areas for improvement in real-world agentic deployments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00914"
    },
    "8e058bc67633854b1e830ffd69c3e49f": {
        "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection",
        "authors": [
            "Souradip Chakraborty",
            "Mohammadreza Pourreza",
            "Ruoxi Sun",
            "Yiwen Song",
            "Nino Scherrer",
            "Jindong Gu",
            "Furong Huang",
            "Amrit Singh Bedi",
            "Ahmad Beirami",
            "Hamid Palangi",
            "Tomas Pfister"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.01931",
        "abstract": "While AI agents have shown remarkable performance at various tasks, they still struggle with complex multi-modal applications, structured generation and strategic planning. Improvements via standard fine-tuning is often impractical, as solving agentic tasks usually relies on black box API access without control over model parameters. Inference-time methods such as Best-of-N (BON) sampling offer a simple yet effective alternative to improve performance. However, BON lacks iterative feedback integration mechanism. Hence, we propose Iterative Agent Decoding (IAD) which combines iterative refinement with dynamic candidate evaluation and selection guided by a verifier. IAD differs in how feedback is designed and integrated, specifically optimized to extract maximal signal from reward scores. We conduct a detailed comparison of baselines across key metrics on Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms baselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and without LLM judges) and 8--10% gains on Webshop across multiple metrics. To better understand the source of IAD&#39;s gains, we perform controlled experiments to disentangle the effect of adaptive feedback from stochastic sampling, and find that IAD&#39;s improvements are primarily driven by verifier-guided refinement, not merely sampling diversity. We also show that both IAD and BON exhibit inference-time scaling with increased compute when guided by an optimal verifier. Our analysis highlights the critical role of verifier quality in effective inference-time optimization and examines the impact of noisy and sparse rewards on scaling behavior. Together, these findings offer key insights into the trade-offs and principles of effective inference-time optimization.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.01931"
    },
    "cb60bbcab95e7d11cc2b6339a2a8f004": {
        "title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks",
        "authors": [
            "Seunghyun Yoo"
        ],
        "date": "2025/04/03",
        "pdf": "http://arxiv.org/pdf/2504.02254",
        "abstract": "Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game &#34;Connections&#34;, we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02254"
    },
    "dd08795743487f7e05abb8dc97d2fb5d": {
        "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications",
        "authors": [
            "Hongliu Cao",
            "Ilias Driouich",
            "Robin Singh",
            "Eoin Thomas"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.02867",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across diverse domains, yet they still encounter challenges such as insufficient domain-specific knowledge, biases, and hallucinations. This underscores the need for robust evaluation methodologies to accurately assess LLM-based applications. Traditional evaluation methods, which rely on word overlap or text embeddings, are inadequate for capturing the nuanced semantic information necessary to evaluate dynamic, open-ended text generation. Recent research has explored leveraging LLMs to mimic human reasoning and decision-making processes for evaluation purposes known as LLM-as-a-judge framework. However, these existing frameworks have two significant limitations. First, they lack the flexibility to adapt to different text styles, including various answer and ground truth styles, thereby reducing their generalization performance. Second, the evaluation scores produced by these frameworks are often skewed and hard to interpret, showing a low correlation with human judgment. To address these challenges, we propose a novel dynamic multi-agent system that automatically designs personalized LLM judges for various natural language generation applications. This system iteratively refines evaluation prompts and balances the trade-off between the adaptive requirements of downstream tasks and the alignment with human perception. Our experimental results show that the proposed multi-agent LLM Judge framework not only enhances evaluation accuracy compared to existing methods but also produces evaluation scores that better align with human perception.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02867"
    },
    "5b4e54c0d1b19adb3af364d5dede720e": {
        "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening",
        "authors": [
            "Frank P. -W. Lo",
            "Jianing Qiu",
            "Zeyu Wang",
            "Haibao Yu",
            "Yeming Chen",
            "Gao Zhang",
            "Benny Lo"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.02870",
        "abstract": "Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02870"
    },
    "a70c1b702a56414e2cc1382419bb06eb": {
        "title": "Automated Survey Collection with LLM-based Conversational Agents",
        "authors": [
            "Kurmanbek Kaiyrbekov",
            "Nicholas J Dobbins",
            "Sean D Mooney"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.02891",
        "abstract": "Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs). Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience. Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction. Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02891"
    },
    "8e41e0ff92cd02812d3fcf7135b9e855": {
        "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
        "authors": [
            "Jaymari Chua",
            "Chen Wang",
            "Lina Yao"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03185",
        "abstract": "Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03185"
    },
    "f1e7f650d24b258f5ca46a80709373e3": {
        "title": "Agentic Knowledgeable Self-awareness",
        "authors": [
            "Shuofei Qiao",
            "Zhisong Qiu",
            "Baochang Ren",
            "Xiaobin Wang",
            "Xiangyuan Ru",
            "Ningyu Zhang",
            "Xiang Chen",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03553",
        "abstract": "Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a &#34;flood irrigation&#34; methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent&#39;s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03553"
    },
    "7f41b5e225a4622d908b1745fc77dee3": {
        "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
        "authors": [
            "Runnan Fang",
            "Xiaobin Wang",
            "Yuan Liang",
            "Shuofei Qiao",
            "Jialong Wu",
            "Zekun Xi",
            "Ningyu Zhang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03561",
        "abstract": "In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03561"
    },
    "a0851d5e71d927ddf0522d160534778d": {
        "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
        "authors": [
            "Akshara Prabhakar",
            "Zuxin Liu",
            "Weiran Yao",
            "Jianguo Zhang",
            "Ming Zhu",
            "Shiyu Wang",
            "Zhiwei Liu",
            "Tulika Awalgaonkar",
            "Haolin Chen",
            "Thai Hoang",
            "Juan Carlos Niebles",
            "Shelby Heinecke",
            "Huan Wang",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03601",
        "abstract": "Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03601"
    },
    "12764ab8f05768ddb96724bc25ecf7f4": {
        "title": "$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks",
        "authors": [
            "Rana Muhammad Shahroz Khan",
            "Zhen Tan",
            "Sukwon Yun",
            "Charles Flemming",
            "Tianlong Chen"
        ],
        "date": "2025/03/31",
        "pdf": "http://arxiv.org/pdf/2504.00218",
        "abstract": "Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the novel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\\texttt{Llama}$, $\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on various datasets like $\\texttt{JailBreakBench}$ and $\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00218"
    },
    "d431300dd5ec7f09b642bdc1a951c612": {
        "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems",
        "authors": [
            "Yingxuan Yang",
            "Huacan Chai",
            "Shuai Shao",
            "Yuanyi Song",
            "Siyuan Qi",
            "Renting Rui",
            "Weinan Zhang"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00587",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of multi-agent systems, where multiple LLM-based agents collaborate to solve complex tasks. However, existing systems predominantly rely on centralized coordination, which introduces scalability bottlenecks, limits adaptability, and creates single points of failure. Additionally, concerns over privacy and proprietary knowledge sharing hinder cross-organizational collaboration, leading to siloed expertise. To address these challenges, we propose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based framework that enables LLM-based agents to autonomously evolve their capabilities and collaborate efficiently in a Directed Acyclic Graph (DAG)-structured network. Unlike traditional multi-agent systems that depend on static role assignments or centralized control, AgentNet allows agents to specialize dynamically, adjust their connectivity, and route tasks without relying on predefined workflows. AgentNet&#39;s core design is built upon several key innovations: (1) Fully Decentralized Paradigm: Removing the central orchestrator, allowing agents to coordinate and specialize autonomously, fostering fault tolerance and emergent collective intelligence. (2) Dynamically Evolving Graph Topology: Real-time adaptation of agent connections based on task demands, ensuring scalability and resilience.(3) Adaptive Learning for Expertise Refinement: A retrieval-based memory system that enables agents to continuously update and refine their specialized skills. By eliminating centralized control, AgentNet enhances fault tolerance, promotes scalable specialization, and enables privacy-preserving collaboration across organizations. Through decentralized coordination and minimal data exchange, agents can leverage diverse knowledge sources while safeguarding sensitive information.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00587"
    },
    "96a937d20277044f3eac65e33ebe67a5": {
        "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents",
        "authors": [
            "Saaket Agashe",
            "Kyle Wong",
            "Vincent Tu",
            "Jiachen Yang",
            "Ang Li",
            "Xin Eric Wang"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00906",
        "abstract": "Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00906"
    },
    "5e77490cfe3998c72dcaacb6832c305d": {
        "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
        "authors": [
            "Tianci Xue",
            "Weijian Qi",
            "Tianneng Shi",
            "Chan Hee Song",
            "Boyu Gou",
            "Dawn Song",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.01382",
        "abstract": "As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.01382"
    },
    "c0935b16b561f9ad742d8149e1d32ef6": {
        "title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
        "authors": [
            "R. M. Aratchige",
            "W. M. K. S. Ilmini"
        ],
        "date": "2025/03/13",
        "pdf": "http://arxiv.org/pdf/2504.01963",
        "abstract": "This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.01963"
    },
    "d320b94a2f0a9406ba6bc990ca004ebd": {
        "title": "Self-Resource Allocation in Multi-Agent LLM Systems",
        "authors": [
            "Alfonso Amayuelas",
            "Jingbo Yang",
            "Saaket Agashe",
            "Ashwin Nagarajan",
            "Antonis Antoniades",
            "Xin Eric Wang",
            "William Wang"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.02051",
        "abstract": "With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02051"
    },
    "bb24a3a2b05728444d59fd3bca4010b1": {
        "title": "Achieving Unanimous Consensus in Decision Making Using Multi-Agents",
        "authors": [
            "Apurba Pokharel",
            "Ram Dantu",
            "Shakila Zaman",
            "Sirisha Talapuru",
            "Vinh Quach"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.02128",
        "abstract": "Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agreement based on honest majority or weighted consensus. This paper introduces a novel deliberation-based consensus mechanism where Large Language Models (LLMs) act as rational agents engaging in structured discussions to reach a unanimous consensus. By leveraging graded consensus and a multi-round deliberation process, our approach ensures both unanimous consensus for definitive problems and graded confidence for prioritized decisions and policies. We provide a formalization of our system and use it to show that the properties of blockchains: consistency, agreement, liveness, and determinism are maintained. Moreover, experimental results demonstrate our system&#39;s feasibility, showcasing how our deliberation method&#39;s convergence, block properties, and accuracy enable decision-making on blockchain networks. We also address key challenges with this novel approach such as degeneration of thoughts, hallucinations, malicious models and nodes, resource consumption, and scalability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02128"
    },
    "2e1105998bfe886b039781b55af1b916": {
        "title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective",
        "authors": [
            "Garry A. Gabison",
            "R. Patrick Xian"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03255",
        "abstract": "Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention over effective governance policies, monitoring and control protocols. Based on emerging landscapes of the agentic market, we analyze the potential liability issues stemming from delegated use of LLM agents and their extended systems from a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing and monitoring approaches to enhancing transparency and accountability.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03255"
    }
}