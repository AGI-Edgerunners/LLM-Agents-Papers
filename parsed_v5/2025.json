{
    "61b5038fc9df8e1d37fef5c460876350": {
        "title": "On Memory Construction and Retrieval for Personalized Conversational Agents",
        "authors": [
            "Zhuoshi Pan",
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Xufang Luo",
            "Hao Cheng",
            "Dongsheng Li",
            "Yuqing Yang",
            "Chin-Yew Lin",
            "H. Vicky Zhao",
            "Lili Qiu",
            "Jianfeng Gao"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05589",
        "abstract": "To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \\textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SeCom, a method that constructs a memory bank with topical segments by introducing a conversation Segmentation model, while performing memory retrieval based on Compressed memory units. Experimental results show that SeCom outperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such as LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05589"
    },
    "281691883e4589edb720c4d85eed1799": {
        "title": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
        "authors": [
            "Pranav Bhandari",
            "Usman Naseem",
            "Amitava Datta",
            "Nicolas Fay",
            "Mehwish Nasim"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.05248",
        "abstract": "Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05248"
    },
    "552a2c4e2a51224ee6298d6c34bf9aac": {
        "title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging",
        "authors": [
            "Md. Ashraful Islam",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05664",
        "abstract": "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim&#39;s remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/).",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05664"
    },
    "c13ba9544a2a06f9994ce2dc692c444a": {
        "title": "MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational Agents",
        "authors": [
            "Wanqi Yang",
            "Yanda Li",
            "Meng Fang",
            "Ling Chen"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05887",
        "abstract": "Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model&#39;s ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05887"
    },
    "19879c35d36631c355da5a719ebceb80": {
        "title": "HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents",
        "authors": [
            "Mohammad Amin Abbasi",
            "Farnaz Sadat Mirnezami",
            "Hassan Naderi"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05982",
        "abstract": "This paper presents HamRaz, a novel Persian-language mental health dataset designed for Person-Centered Therapy (PCT) using Large Language Models (LLMs). Despite the growing application of LLMs in AI-driven psychological counseling, existing datasets predominantly focus on Western and East Asian contexts, overlooking cultural and linguistic nuances essential for effective Persian-language therapy. To address this gap, HamRaz combines script-based dialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy interactions. We also introduce HamRazEval, a dual evaluation framework that measures conversational quality and therapeutic effectiveness using General Dialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI). Experimental results show HamRaz outperforms conventional Script Mode and Two-Agent Mode, producing more empathetic, context-aware, and realistic therapy sessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven resource to advance AI-powered psychotherapy research in diverse communities.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05982"
    },
    "3c329c9902e3a5342ab10c8f05ce2a56": {
        "title": "Preventing Rogue Agents Improves Multi-Agent Collaboration",
        "authors": [
            "Ohav Barbi",
            "Ori Yoran",
            "Mor Geva"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05986",
        "abstract": "Multi-agent systems, where specialized agents collaborate to solve a shared task hold great potential, from increased modularity to simulating complex environments. However, they also have a major caveat -- a single agent can cause the entire system to fail. Consider a simple game where the knowledge to solve the task is distributed between agents, which share information in a communication channel. At each round, any of the agents can terminate the game and make the final prediction, even if they are uncertain about the outcome of their action. Detection of such rogue agents $\\textit{before they act}$ may prevent the system&#39;s failure. In this work, we propose to $\\textit{monitor}$ agents during action prediction and $\\textit{intervene}$ when a future error is likely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent collaboration environment that allows modular control over task complexity and communication structure. Experiments on two variants of WhoDunitEnv and the GovSim environment for resource sustainability show that our approach leads to substantial performance gains up to 17.4% and 20%, respectively. Moreover, a thorough analysis shows that our monitors successfully identify critical points of agent confusion and our interventions effectively stop agent errors from propagating.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05986"
    },
    "49063743e04babb9acce0f2448fbf35c": {
        "title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?",
        "authors": [
            "Wenzhe Li",
            "Yong Lin",
            "Mengzhou Xia",
            "Chi Jin"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.00674",
        "abstract": "Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00674"
    },
    "bf4e41ed6254617cf3cafcde348322e0": {
        "title": "Efficient Multi-Agent System Training with Data Influence-Oriented Tree Search",
        "authors": [
            "Wentao Shi",
            "Zichun Yu",
            "Fuli Feng",
            "Xiangnan He",
            "Chenyan Xiong"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.00955",
        "abstract": "Monte Carlo Tree Search (MCTS) based methods provide promising approaches for generating synthetic data to enhance the self-training of Large Language Model (LLM) based multi-agent systems (MAS). These methods leverage Q-values to estimate individual agent contributions. However, relying solely on Q-values to identify informative data may misalign with the data synthesis objective, as the focus should be on selecting data that best enhances model training. To address this discrepancy, we propose Data Influence-oriented Tree Search (DITS), a novel framework that incorporates influence scores to guide both tree search and data selection. By leveraging influence scores, we effectively identify the most impactful data for system improvement, thereby enhancing model performance. Furthermore, we derive influence score estimation methods tailored for non-differentiable metrics, significantly reducing computational overhead by utilizing inference computations. Extensive experiments on eight multi-agent datasets demonstrate the robustness and effectiveness of the proposed methods. Notably, our findings reveal that allocating more inference resources to estimate influence scores, rather than Q-values, during data synthesis can more effectively and efficiently enhance model training.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00955"
    },
    "d6e36d1157819c499d4276bf95a792d0": {
        "title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.00988",
        "abstract": "Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00988"
    },
    "d51ef18416d3cba8e2e4d488e3f63a01": {
        "title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.00989",
        "abstract": "Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00989"
    },
    "f59baeb0b866580954a53deab6e95535": {
        "title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models",
        "authors": [
            "Diyana Muhammed",
            "Gollam Rabby",
            "Sören Auer"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01812",
        "abstract": "Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce SelfCheckAgent, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89% but reveals trade-offs in Factual with 30.58% and Ranking with 30.68%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent&#39;s applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01812"
    },
    "ac59740d2e4f33bd00b9ea23dc8b1137": {
        "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development",
        "authors": [
            "Genghan Zhang",
            "Weixin Liang",
            "Olivia Hsu",
            "Kunle Olukotun"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.02534",
        "abstract": "ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02534"
    },
    "62c9e6521d85e0e094da3c4da01a0bfe": {
        "title": "CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration",
        "authors": [
            "Yizhe Yang",
            "Palakorn Achananuparp",
            "Heyan Huang",
            "Jing Jiang",
            "Kit Phey Leng",
            "Nicholas Gabriel Lim",
            "Cameron Tan Shi Ern",
            "Ee-peng Lim"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.02807",
        "abstract": "Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client&#39;s state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI&#39;s performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client&#39;s state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02807"
    },
    "979770a3febd45fbb5fa84d479b32601": {
        "title": "ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation",
        "authors": [
            "Qinzhuo Wu",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.02955",
        "abstract": "Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks. Furthermore, we propose ReachAgent, a two-stage framework that focuses on improving its task-completion abilities. It utilizes the page reaching and page operation subtasks, along with reward-based preference GUI flows, to further enhance the agent. Experimental results show that ReachAgent significantly improves the IoU Acc and Text Acc by 7.12% and 7.69% on the step-level and 4.72% and 4.63% on the task-level compared to the SOTA agent. Our data and code will be released upon acceptance.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02955"
    },
    "3d449aa86235533946b11e039c77566c": {
        "title": "Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model",
        "authors": [
            "Hadas Ben-Atya",
            "Naama Gavrielov",
            "Zvi Badash",
            "Gili Focht",
            "Ruth Cytter-Kuint",
            "Talar Hagopian",
            "Dan Turner",
            "Moti Freiman"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.01691",
        "abstract": "Reliable extraction of structured data from radiology reports using Large Language Models (LLMs) remains challenging, especially for complex, non-English texts like Hebrew. This study introduces an agent-based uncertainty-aware approach to improve the trustworthiness of LLM predictions in medical applications. We analyzed 9,683 Hebrew radiology reports from Crohn&#39;s disease patients (from 2010 to 2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining reports were automatically annotated using HSMP-BERT. Structured data extraction was performed using Llama 3.1 (Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty. An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty and was compared against three entropy-based models. Performance was evaluated using accuracy, F1 score, precision, recall, and Cohen&#39;s Kappa before and after filtering high-uncertainty cases. The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen&#39;s Kappa of 0.3006. After filtering high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to 0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates. By incorporating uncertainty-aware prompt ensembles and an agent-based decision model, this approach enhances the performance and reliability of LLMs in structured data extraction from radiology reports, offering a more interpretable and trustworthy solution for high-stakes medical applications.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01691"
    },
    "3c3db16cc3a88ba4b8542eff8dc4ea14": {
        "title": "PsyPlay: Personality-Infused Role-Playing Conversational Agents",
        "authors": [
            "Tao Yang",
            "Yuhua Zhu",
            "Xiaojun Quan",
            "Cong Liu",
            "Qifan Wang"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.03821",
        "abstract": "The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits.~In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31% on GPT-3.5. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue corpus for personality-infused role-playing, called PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03821"
    },
    "f4ea715917df0171b1e32754f198332c": {
        "title": "Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents",
        "authors": [
            "Yuchen Lian",
            "Arianna Bisazza",
            "Tessa Verhoef"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04038",
        "abstract": "Differential Case Marking (DCM) refers to the phenomenon where grammatical case marking is applied selectively based on semantic, pragmatic, or other factors. The emergence of DCM has been studied in artificial language learning experiments with human participants, which were specifically aimed at disentangling the effects of learning from those of communication (Smith &amp; Culbertson, 2020). Multi-agent reinforcement learning frameworks based on neural networks have gained significant interest to simulate the emergence of human-like linguistic phenomena. In this study, we employ such a framework in which agents first acquire an artificial language before engaging in communicative interactions, enabling direct comparisons to human result. Using a very generic communication optimization algorithm and neural-network learners that have no prior experience with language or semantic preferences, our results demonstrate that learning alone does not lead to DCM, but when agents communicate, differential use of markers arises. This supports Smith and Culbertson (2020)&#39;s findings that highlight the critical role of communication in shaping DCM and showcases the potential of neural-agent models to complement experimental research on language evolution.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04038"
    },
    "7882b7249c571f63c48cc7312a64c042": {
        "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization",
        "authors": [
            "Yinjie Wang",
            "Ling Yang",
            "Guohao Li",
            "Mengdi Wang",
            "Bryon Aragam"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04306",
        "abstract": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04306"
    },
    "0effcb668d9d61287d8d4a3d308cea17": {
        "title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives",
        "authors": [
            "Elliot Meyerson",
            "Xin Qiu"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.04358",
        "abstract": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04358"
    },
    "cc8782f2d8d6834d10194088c6413f05": {
        "title": "Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents",
        "authors": [
            "Chenyang Shao",
            "Xinyuan Hu",
            "Yutang Lin",
            "Fengli Xu"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04392",
        "abstract": "The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models offer a promising path for next-generation on-device AI agents. However, deploying full-scale Large Language Models (LLMs) on resource-limited local devices is challenging. In this paper, we propose Division-of-Thoughts (DoT), a collaborative reasoning framework leveraging the synergy between locally deployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT leverages a Task Decomposer to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an additional task head attached to the SLM that does not alter the SLM&#39;s parameters. To boost adapter&#39;s task allocation capability, we propose a self-reinforced training method that relies solely on task execution feedback. Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12% and 83.57%, while achieving comparable reasoning accuracy with the best baseline methods.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04392"
    },
    "c2895095f35c0f46f199a820e04634db": {
        "title": "Multi-Agent Reinforcement Learning with Focal Diversity Optimization",
        "authors": [
            "Selim Furkan Tekin",
            "Fatih Ilhan",
            "Tiansheng Huang",
            "Sihao Hu",
            "Zachary Yahn",
            "Ling Liu"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04492",
        "abstract": "The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at https://github.com/sftekin/rl-focal",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04492"
    },
    "d130907042a9d1c207814e4a43bf6fbc": {
        "title": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency",
        "authors": [
            "Yuting Zeng",
            "Weizhe Huang",
            "Lei Jiang",
            "Tongxuan Liu",
            "Xitai Jin",
            "Chen Tianying Tiana",
            "Jing Li",
            "Xiaohua Xu"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.04790",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5\\% in token costs while maintaining performance degradation below 2.0\\%.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04790"
    },
    "9d031511e3eba748343236b6affe8743": {
        "title": "nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow",
        "authors": [
            "Geliang Ouyang",
            "Jingyao Chen",
            "Zhihe Nie",
            "Yi Gui",
            "Yao Wan",
            "Hongyu Zhang",
            "Dongping Chen"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.05036",
        "abstract": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05036"
    },
    "f526e3a89c0265bf86d6eac5672e6b5f": {
        "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment",
        "authors": [
            "Yuxing Lu",
            "Jinzhuo Wang"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06472",
        "abstract": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06472"
    },
    "57a611637f85a3f9d2d547f5f0f012f7": {
        "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
        "authors": [
            "Yuchen Zhuang",
            "Jingfeng Yang",
            "Haoming Jiang",
            "Xin Liu",
            "Kewei Cheng",
            "Sanket Lokegaonkar",
            "Yifan Gao",
            "Qing Ping",
            "Tianyi Liu",
            "Binxuan Huang",
            "Zheng Li",
            "Zhengyang Wang",
            "Pei Chen",
            "Ruijie Wang",
            "Rongzhi Zhang",
            "Nasser Zalmout",
            "Priyanka Nigam",
            "Bing Yin",
            "Chao Zhang"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06589",
        "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06589"
    },
    "98494fac2a6a4669636e9b32fe0936f9": {
        "title": "Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction",
        "authors": [
            "Shengbin Yue",
            "Ting Huang",
            "Zheng Jia",
            "Siyuan Wang",
            "Shujun Liu",
            "Yun Song",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.06882",
        "abstract": "Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants&#39; characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs&#39; performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06882"
    },
    "d5c21311f0587298d8b6e1d42dabab8e": {
        "title": "Don&#39;t Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification",
        "authors": [
            "Peipei Wei",
            "Dimitris Dimitriadis",
            "Yan Xu",
            "Mingwei Shen"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07165",
        "abstract": "We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07165"
    },
    "230d0a37dd7aec9d7bf30af185d6d90a": {
        "title": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning",
        "authors": [
            "Jian Yang",
            "Wei Zhang",
            "Jiaxi Yang",
            "Yibo Miao",
            "Shanghaoran Quan",
            "Zhenhe Wu",
            "Qiyao Peng",
            "Liqun Yang",
            "Tianyu Liu",
            "Zeyu Cui",
            "Binyuan Hui",
            "Junyang Lin"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07487",
        "abstract": "Recent advancement in code understanding and generation demonstrates that code LLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities to address wide-ranging code-related tasks. However, most previous existing methods mainly view each programming language in isolation and ignore the knowledge transfer among different programming languages. To bridge the gap among different programming languages, we introduce a novel multi-agent collaboration framework to enhance multilingual instruction tuning for code LLMs, where multiple language-specific intelligent agent components with generation memory work together to transfer knowledge from one language to another efficiently and effectively. Specifically, we first generate the language-specific instruction data from the code snippets and then provide the generated data as the seed data for language-specific agents. Multiple language-specific agents discuss and collaborate to formulate a new instruction and its corresponding solution (A new programming language or existing programming language), To further encourage the cross-lingual transfer, each agent stores its generation history as memory and then summarizes its merits and faults. Finally, the high-quality multilingual instruction data is used to encourage knowledge transfer among different programming languages to train Qwen2.5-xCoder. Experimental results on multilingual programming benchmarks demonstrate the superior performance of Qwen2.5-xCoder in sharing common knowledge, highlighting its potential to reduce the cross-lingual gap.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07487"
    },
    "a25ee4a3ab77c6faf8c4a3819ed5fc28": {
        "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation",
        "authors": [
            "Mahnaz Koupaee",
            "Jake W. Vincent",
            "Saab Mansour",
            "Igor Shalyminov",
            "Han He",
            "Hwanjun Song",
            "Raphael Shu",
            "Jianfeng He",
            "Yi Nian",
            "Amy Wing-mei Wong",
            "Kyu J. Han",
            "Hang Su"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08514",
        "abstract": "Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08514"
    },
    "474c1ff5a3034e7a01037edcf62d0eb5": {
        "title": "SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent",
        "authors": [
            "Keyeun Lee",
            "Seo Hyeong Kim",
            "Seolhee Lee",
            "Jinsu Eun",
            "Yena Ko",
            "Hayeon Jeon",
            "Esther Hehsun Kim",
            "Seonghye Cho",
            "Soeun Yang",
            "Eun-mee Kim",
            "Hajin Lim"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08599",
        "abstract": "Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual&#39;s multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum&#39;s effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)-derived from short essays on preferences and daily routines-modeled characters&#39; identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08599"
    },
    "777d7f5ea00e3b823ca61eb6914014a7": {
        "title": "If Multi-Agent Debate is the Answer, What is the Question?",
        "authors": [
            "Hangfan Zhang",
            "Zhiyao Cui",
            "Xinrun Wang",
            "Qiaosheng Zhang",
            "Zhen Wang",
            "Dinghao Wu",
            "Shuyue Hu"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08788",
        "abstract": "Multi-agent debate (MAD) has emerged as a promising approach to enhance the factual accuracy and reasoning quality of large language models (LLMs) by engaging multiple agents in iterative discussions during inference. Despite its potential, we argue that current MAD research suffers from critical shortcomings in evaluation practices, including limited dataset overlap and inconsistent baselines, raising significant concerns about generalizability. Correspondingly, this paper presents a systematic evaluation of five representative MAD methods across nine benchmarks using four foundational models. Surprisingly, our findings reveal that MAD methods fail to reliably outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming additional inference-time computation. From our analysis, we found that model heterogeneity can significantly improve MAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the output from heterogeneous foundation models, which boosts the performance of current MAD frameworks. Finally, we outline potential directions for advancing MAD, aiming to spark a broader conversation and inspire future work in this area.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08788"
    },
    "a6e83c9ede12dbf90c97e86fa5d68e1c": {
        "title": "Agentic Verification for Ambiguous Query Disambiguation",
        "authors": [
            "Youngwon Lee",
            "Seung-won Hwang",
            "Ruofan Wu",
            "Feng Yan",
            "Danmei Xu",
            "Moutasem Akkad",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.10352",
        "abstract": "In this work, we tackle the challenge of disambiguating queries in retrieval-augmented generation (RAG) to diverse yet answerable interpretations. State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse interpretations are generated by an LLM, later used as search queries to retrieve supporting passages. Such a process may introduce noise in either interpretations or retrieval, particularly in enterprise settings, where LLMs -- trained on static data -- may struggle with domain-specific disambiguations. Thus, a post-hoc verification phase is introduced to prune noises. Our distinction is to unify diversification with verification by incorporating feedback from retriever and generator early on. This joint approach improves both efficiency and robustness by reducing reliance on multiple retrieval and inference steps, which are susceptible to cascading errors. We validate the efficiency and effectiveness of our method, Verified-Diversification with Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve diverse yet verifiable interpretations. Empirical results show that VERDICT improves grounding-aware F1 score by an average of 23% over the strongest baseline across different backbone LLMs.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10352"
    },
    "d800ee6acc686566a88098700f570602": {
        "title": "Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation",
        "authors": [
            "Haoyuan Wu",
            "Haisheng Zheng",
            "Zhuolun He",
            "Bei Yu"
        ],
        "date": "2025/02/15",
        "pdf": "http://arxiv.org/pdf/2502.10857",
        "abstract": "Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts. However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms. Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps. Any errors will lead to the instability and failure of EDA flow automation. To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation. Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10857"
    },
    "051910306e3634c99f24831585d5497b": {
        "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?",
        "authors": [
            "Wenxuan Wang",
            "Zizhan Ma",
            "Zheng Wang",
            "Chenghan Wu",
            "Wenting Chen",
            "Xiang Li",
            "Yixuan Yuan"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.11211",
        "abstract": "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents&#39; performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11211"
    },
    "3e7779e7fe5866d84eb6ed7de18b487b": {
        "title": "&#34;Nuclear Deployed!&#34;: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents",
        "authors": [
            "Rongwu Xu",
            "Xiaojian Li",
            "Shuo Chen",
            "Wei Xu"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11355",
        "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent&#39;s Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11355"
    },
    "8fee1a3c3a526591ec860fe41e53fe01": {
        "title": "LLM Agents Making Agent Tools",
        "authors": [
            "Georg Wölflein",
            "Dyke Ferber",
            "Daniel Truhn",
            "Ognjen Arandjelović",
            "Jakob Nikolas Kather"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11705",
        "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11705"
    },
    "4126585e4c5333fe61494856b3e6e47f": {
        "title": "Can LLM Agents Maintain a Persona in Discourse?",
        "authors": [
            "Pranav Bhandari",
            "Nicolas Fay",
            "Michael Wise",
            "Amitava Datta",
            "Stephanie Meek",
            "Usman Naseem",
            "Mehwish Nasim"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11843",
        "abstract": "Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11843"
    },
    "b217ee7110f1139e7d6d3e6f8f401f96": {
        "title": "A-MEM: Agentic Memory for LLM Agents",
        "authors": [
            "Wujiang Xu",
            "Zujie Liang",
            "Kai Mei",
            "Hang Gao",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12110",
        "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems&#39; fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at https://github.com/WujiangXu/AgenticMemory.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12110"
    },
    "4628e63125c3c42557cd811300aa9a85": {
        "title": "InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context",
        "authors": [
            "Bryan L. M. de Oliveira",
            "Luana G. B. Martins",
            "Bruno Brandão",
            "Luckeciano C. Melo"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12257",
        "abstract": "While large language models excel at following explicit instructions, they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses rather than seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. The benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue through clarifying questions before providing appropriate responses. Our evaluation of both open and closed-source models reveals that while proprietary models generally perform better, all current assistants struggle with effectively gathering critical information, often requiring multiple turns to infer user intent and frequently defaulting to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models&#39; information-seeking capabilities, offering insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12257"
    },
    "62a231ee39598886fa9227024afd2f0b": {
        "title": "LM Agents for Coordinating Multi-User Information Gathering",
        "authors": [
            "Harsh Jhamtani",
            "Jacob Andreas",
            "Benjamin Van Durme"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12328",
        "abstract": "This paper introduces PeopleJoin, a benchmark for evaluating LM-mediated collaborative problem solving. Given a user request, PeopleJoin agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer or summary for the original user. PeopleJoin comprises two evaluation domains: PeopleJoin-QA, focused on questions about tabular data, and PeopleJoin-DocCreation, focused on document creation tasks. The two domains are adapted from existing NLP benchmarks for database question answering and multi-document summarization; here, however, the information needed to complete these tasks is distributed across synthetic ``organizations&#39;&#39; of 2--20 users, simulating natural multi-user collaboration scenarios. We implemented several popular LM agent architectures, evaluating their accuracy and efficiency at completing tasks, and highlight new research questions that can be studied using PeopleJoin.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12328"
    },
    "5ebddc73abd7382da524ae84d744a569": {
        "title": "One Size doesn&#39;t Fit All: A Personalized Conversational Tutoring Agent for Mathematics Instruction",
        "authors": [
            "Ben Liu",
            "Jihan Zhang",
            "Fangquan Lin",
            "Xu Jia",
            "Min Peng"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12633",
        "abstract": "Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized \\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics instruction. PACE simulates students&#39; learning styles based on the Felder and Silverman learning style model, aligning with each student&#39;s persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance students&#39; comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12633"
    },
    "5dce6608d2cc0f603531908cd183e0b6": {
        "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs",
        "authors": [
            "Sumin Jo",
            "Junseong Choi",
            "Jiho Kim",
            "Edward Choi"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12767",
        "abstract": "Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning. To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across multiple KG-based reasoning tasks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12767"
    },
    "aeb3f654e791e27e76d124ba503913cd": {
        "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation",
        "authors": [
            "Mohammad Feli",
            "Iman Azimi",
            "Pasi Liljeberg",
            "Amir M. Rahmani"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12836",
        "abstract": "Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs&#39; limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent&#39;s performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12836"
    },
    "8009cf67dd8c4184850e002591f18b70": {
        "title": "SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems",
        "authors": [
            "Mike Zhang",
            "Amalie Pernille Dilling",
            "Léon Gondelman",
            "Niels Erik Ruan Lyngdorf",
            "Euan D. Lindsay",
            "Johannes Bjerva"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12927",
        "abstract": "Providing high-quality feedback is crucial for student success but is constrained by time, cost, and limited data availability. We introduce Synthetic Educational Feedback Loops (SEFL), a novel framework designed to deliver immediate, on-demand feedback at scale without relying on extensive, real-world student data. In SEFL, two large language models (LLMs) operate in teacher--student roles to simulate assignment completion and formative feedback, generating abundant synthetic pairs of student work and corresponding critiques. We then fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher--&gt;student feedback loop for diverse assignments. Through both LLM-as-a-judge and human evaluations, we demonstrate that SEFL-tuned models outperform their non-tuned counterparts in feedback quality, clarity, and timeliness. These findings reveal SEFL&#39;s potential to transform feedback processes for higher education and beyond, offering an ethical and scalable alternative to conventional manual feedback cycles.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12927"
    },
    "04b64a4d266e9bdf14c25aee46d25a8e": {
        "title": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile Agents Against Active Environmental Injection Attacks",
        "authors": [
            "Yurun Chen",
            "Xueyu Hu",
            "Keting Yin",
            "Juncheng Li",
            "Shengyu Zhang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13053",
        "abstract": "As researchers continuously optimize AI agents to perform tasks more effectively within operating systems, they often neglect to address the critical need for enabling these agents to identify &#34;impostors&#34; within the system. Through an analysis of the agents&#39; operating environment, we identified a potential threat: attackers can disguise their attack methods as environmental elements, injecting active disturbances into the agents&#39; execution process, thereby disrupting their decision-making. We define this type of attack as Active Environment Injection Attack (AEIA). Based on this, we propose AEIA-MN, an active environment injection attack scheme that exploits interaction vulnerabilities in the mobile operating system to evaluate the robustness of MLLM-based agents against such threats. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% in the AndroidWorld benchmark.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13053"
    },
    "da2942b9b05b8b3b487fb9190fb379a6": {
        "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors",
        "authors": [
            "Jian Wang",
            "Yinpei Dai",
            "Yichi Zhang",
            "Ziqiao Ma",
            "Wenjie Li",
            "Joyce Chai"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13311",
        "abstract": "Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student&#39;s knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13311"
    },
    "67b2128ecda1f7a56e0289cbd4509b50": {
        "title": "DataSciBench: An LLM Agent Benchmark for Data Science",
        "authors": [
            "Dan Zhang",
            "Sining Zhoubian",
            "Min Cai",
            "Fengzu Li",
            "Lekang Yang",
            "Wei Wang",
            "Tianjiao Dong",
            "Ziniu Hu",
            "Jie Tang",
            "Yisong Yue"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13897",
        "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13897"
    },
    "8ff3660d187b851f21a4873ff43ab836": {
        "title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Xiao Wang",
            "Yin Fang",
            "Haolin Liu",
            "Yifan Yang",
            "Fangyuan Chen",
            "Zhixing Song",
            "Dengyu Wang",
            "Minjia Zhang",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13957",
        "abstract": "Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6\\% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at https://rag-gym.github.io/.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13957"
    },
    "a7e1c6ae709b2d76c85e0c521a2b1fbc": {
        "title": "LIDDIA: Language-based Intelligent Drug Discovery Agent",
        "authors": [
            "Reza Averly",
            "Frazier N. Baker",
            "Xia Ning"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13959",
        "abstract": "Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it can identify promising novel drug candidates on EGFR, a critical target for cancers.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13959"
    },
    "386e8830f028c8f68c61db0e4c811df3": {
        "title": "UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text",
        "authors": [
            "Primoz Kocbek",
            "Leon Kopitar",
            "Zhihong Zhang",
            "Emirhan Aydin",
            "Maxim Topaz",
            "Gregor Stiglic"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.14144",
        "abstract": "This paper describes our submissions to the TREC 2024 PLABA track with the aim to simplify biomedical abstracts for a K8-level audience (13-14 years old students). We tested three approaches using OpenAI&#39;s gpt-4o and gpt-4o-mini models: baseline prompt engineering, a two-AI agent approach, and fine-tuning. Adaptations were evaluated using qualitative metrics (5-point Likert scales for simplicity, accuracy, completeness, and brevity) and quantitative readability scores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the two-agent approach and baseline prompt engineering with gpt-4o-mini models show superior qualitative performance, while fine-tuned models excelled in accuracy and completeness but were less simple. The evaluation results demonstrated that prompt engineering with gpt-4o-mini outperforms iterative improvement strategies via two-agent approach as well as fine-tuning with gpt-4o. We intend to expand our investigation of the results and explore advanced evaluations.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14144"
    },
    "1d7f1c1a670d5ef09e524fa614cd496f": {
        "title": "Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction",
        "authors": [
            "Mohammadmahdi Jafari",
            "Devin Yuncheng Hua",
            "Hao Xue",
            "Flora Salim"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14171",
        "abstract": "Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14171"
    },
    "f3bf6c6ce4a547bfaec4ce33b4d88476": {
        "title": "Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization",
        "authors": [
            "Zhitao He",
            "Zijun Liu",
            "Peng Li",
            "May Fung",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14496",
        "abstract": "LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents&#39; policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14496"
    },
    "52cdf9bddd3a749978a92ea7a6021c18": {
        "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
        "authors": [
            "Deepak Nathani",
            "Lovish Madaan",
            "Nicholas Roberts",
            "Nikolay Bashlykov",
            "Ajay Menon",
            "Vincent Moens",
            "Amar Budhiraja",
            "Despoina Magka",
            "Vladislav Vorotilov",
            "Gaurav Chaurasia",
            "Dieuwke Hupkes",
            "Ricardo Silveira Cabral",
            "Tatiana Shavrina",
            "Jakob Foerster",
            "Yoram Bachrach",
            "William Yang Wang",
            "Roberta Raileanu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14499",
        "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14499"
    },
    "51c734eb714ade9075f7a1d98fea4b33": {
        "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
        "authors": [
            "Zhenhong Zhou",
            "Zherui Li",
            "Jie Zhang",
            "Yuanhe Zhang",
            "Kun Wang",
            "Yang Liu",
            "Qing Guo"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14529",
        "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14529"
    },
    "d716b75b322213af61a1b675038385ab": {
        "title": "InstructAgent: Building User Controllable Recommender via LLM Agent",
        "authors": [
            "Wujiang Xu",
            "Yunxiao Shi",
            "Zujie Liang",
            "Xuying Ning",
            "Kai Mei",
            "Kun Wang",
            "Xi Zhu",
            "Min Xu",
            "Yongfeng Zhang"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14662",
        "abstract": "Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform&#39;s recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform&#39;s benefits, which may hinder their ability to protect and capture users&#39; true interests. Second, these models are typically optimized using data from all users, which may overlook individual user&#39;s preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as $\\dataset$, along with user instructions for each record.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14662"
    },
    "bdba08a4f13bc86187c524d06a4de449": {
        "title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search",
        "authors": [
            "Zujie Liang",
            "Feng Wei",
            "Wujiang Xu",
            "Lin Chen",
            "Yuxi Qian",
            "Xinhui Wu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14693",
        "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process.Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node&#39;s solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier.Applied to the various ML tasks, our approach demonstrates a6\\% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14693"
    },
    "5c7caf97e434d7b064871a5b80ed88a0": {
        "title": "ALU: Agentic LLM Unlearning",
        "authors": [
            "Debdeep Sanyal",
            "Murari Mandal"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00406",
        "abstract": "Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. We present the first agentic LLM unlearning (ALU) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our ALU framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and ALU seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that ALU consistently stands out as the most robust LLM unlearning framework among current state-of-the-art methods while incurring a low constant-time cost. We further highlight ALU&#39;s superior performance compared to existing methods when evaluated at scale. Specifically, ALU is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00406"
    },
    "1d0189ce757508161729909a2c110e35": {
        "title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents",
        "authors": [
            "George Fatouros",
            "Kostas Metaxas",
            "John Soldatos",
            "Manos Karathanassis"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00415",
        "abstract": "MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\\&amp;P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\\&amp;P 500 stocks during 2024 demonstrates the framework&#39;s scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00415"
    },
    "1d1ac354ff3472f17033899e0a1ca010": {
        "title": "Who&#39;s the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents",
        "authors": [
            "Yingxuan Yang",
            "Bo Huang",
            "Siyuan Qi",
            "Chao Feng",
            "Haoyi Hu",
            "Yuxuan Zhu",
            "Jinbo Hu",
            "Haoran Zhao",
            "Ziyi He",
            "Xiao Liu",
            "Zongyu Wang",
            "Lin Qiu",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00510",
        "abstract": "Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce CapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in cooperative game theory&#39;s Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent&#39;s architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. Key contributions include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,500 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00510"
    },
    "423438288d35797d3481f1c75c359e77": {
        "title": "Eliciting Language Model Behaviors with Investigator Agents",
        "authors": [
            "Xiang Lisa Li",
            "Neil Chowdhury",
            "Daniel D. Johnson",
            "Tatsunori Hashimoto",
            "Percy Liang",
            "Sarah Schwettmann",
            "Jacob Steinhardt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01236",
        "abstract": "Language models exhibit complex, diverse behaviors when prompted with free-form text, making it difficult to characterize the space of possible outputs. We study the problem of behavior elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations or harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train investigator models to map randomly-chosen target behaviors to a diverse distribution of outputs that elicit them, similar to amortized Bayesian inference. We do this through supervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe training objective to iteratively discover diverse prompting strategies. Our investigator models surface a variety of effective and human-interpretable prompts leading to jailbreaks, hallucinations, and open-ended aberrant behaviors, obtaining a 100% attack success rate on a subset of AdvBench (Harmful Behaviors) and an 85% hallucination rate.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01236"
    },
    "ca182eb5c94457e3d3864caf8ce295e7": {
        "title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant",
        "authors": [
            "Gaole He",
            "Gianluca Demartini",
            "Ujwal Gadiraju"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01390",
        "abstract": "Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of &#39;LLM-modulo&#39; setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01390"
    },
    "c2e5f782a453b942ea940d54760e2a21": {
        "title": "Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies",
        "authors": [
            "Han Zhou",
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hamid Palangi",
            "Shariq Iqbal",
            "Ivan Vulić",
            "Anna Korhonen",
            "Sercan Ö. Arık"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.02533",
        "abstract": "Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design. Based on the insights, we propose Multi-Agent System Search (MASS), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that MASS-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the MASS-found systems, we finally propose design principles behind building effective multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02533"
    },
    "3d2598771a524886d8dc2e962511838c": {
        "title": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs",
        "authors": [
            "Ben Liu",
            "Jihai Zhang",
            "Fangquan Lin",
            "Cheng Yang",
            "Min Peng",
            "Wotao Yin"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.03283",
        "abstract": "Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM&#39;s inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03283"
    },
    "fdd09538eac934057c78a0fd7a49744e": {
        "title": "Enhancing Online Learning Efficiency Through Heterogeneous Resource Integration with a Multi-Agent RAG System",
        "authors": [
            "Devansh Srivastav",
            "Hasan Md Tusfiqur Alam",
            "Afsaneh Asaei",
            "Mahmoud Fazeli",
            "Tanisha Sharma",
            "Daniel Sonntag"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.03948",
        "abstract": "Efficient online learning requires seamless access to diverse resources such as videos, code repositories, documentation, and general web content. This poster paper introduces early-stage work on a Multi-Agent Retrieval-Augmented Generation (RAG) System designed to enhance learning efficiency by integrating these heterogeneous resources. Using specialized agents tailored for specific resource types (e.g., YouTube tutorials, GitHub repositories, documentation websites, and search engines), the system automates the retrieval and synthesis of relevant information. By streamlining the process of finding and combining knowledge, this approach reduces manual effort and enhances the learning experience. A preliminary user study confirmed the system&#39;s strong usability and moderate-high utility, demonstrating its potential to improve the efficiency of knowledge acquisition.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03948"
    },
    "abe134e8f38cffcbf25cd687a6a0ff5c": {
        "title": "Multi-agent Architecture Search via Agentic Supernet",
        "authors": [
            "Guibin Zhang",
            "Luyang Niu",
            "Junfeng Fang",
            "Kun Wang",
            "Lei Bai",
            "Xiang Wang"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04180",
        "abstract": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \\textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \\textbf{(I)} requires only $6\\sim45\\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and \\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04180"
    },
    "7b5586a1f6eadb8c20bc07706cd4df4a": {
        "title": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research",
        "authors": [
            "Junde Wu",
            "Jiayuan Zhu",
            "Yuyuan Liu"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.04644",
        "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Unlike conventional LLM-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation (RAG) systems and closed-source LLMs. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving. The code is at: https://github.com/theworldofagents/Agentic-Reasoning.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04644"
    },
    "6da1ea26c443026424681e04a6652c0f": {
        "title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents",
        "authors": [
            "Gonzalo Gonzalez-Pumariega",
            "Leong Su Yean",
            "Neha Sunkara",
            "Sanjiban Choudhury"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.05227",
        "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents&#39; ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at https://github.com/portal-cornell/robotouille.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05227"
    },
    "6705ef475ab018d54e7b0a56a51daf0a": {
        "title": "Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews",
        "authors": [
            "Izunna Okpala",
            "Ashkan Golgoon",
            "Arjun Ravi Kannan"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05439",
        "abstract": "The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a manager and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection, hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a manager along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05439"
    },
    "71f0307a5b3c4c2ec46c52e57025f0c3": {
        "title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents",
        "authors": [
            "Jiabin Tang",
            "Tianyu Fan",
            "Chao Huang"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05957",
        "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent&#39;s effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent&#39;s Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05957"
    },
    "58977408842677b889fa9d2e02d0b7b9": {
        "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
        "authors": [
            "Bidipta Sarkar",
            "Warren Xia",
            "C. Karen Liu",
            "Dorsa Sadigh"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.06060",
        "abstract": "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent&#39;s goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model&#39;s listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model&#39;s speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06060"
    },
    "315df09f94bc364e0f21a8c65af0071b": {
        "title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering",
        "authors": [
            "Xuehang Guo",
            "Xingyao Wang",
            "Yangyi Chen",
            "Sha Li",
            "Chi Han",
            "Manling Li",
            "Heng Ji"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06994",
        "abstract": "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator&#39;s understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator&#39;s actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents&#39; capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent &lt;= 3.33% to Claude-3.5-Sonnet &gt;= 28.18%), their consistently low collaboration willingness (&lt;= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents&#39; resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: https://xhguo7.github.io/SyncMind/.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06994"
    },
    "9aad7e351bc7be72d4e16483798e2e0f": {
        "title": "EvoFlow: Evolving Diverse Agentic Workflows On The Fly",
        "authors": [
            "Guibin Zhang",
            "Kaijie Chen",
            "Guancheng Wan",
            "Heng Chang",
            "Hong Cheng",
            "Kun Wang",
            "Shuyue Hu",
            "Lei Bai"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07373",
        "abstract": "The past two years have witnessed the evolution of large language model (LLM)-based multi-agent systems from labor-intensive manual design to partial automation (\\textit{e.g.}, prompt engineering, communication topology) and eventually to fully automated design. However, existing agentic automation pipelines often lack LLM heterogeneity and focus on single-objective performance optimization, limiting their potential to combine weaker models for more customized and cost-effective solutions. To address this challenge, we propose EvoFlow, a niching evolutionary algorithm-based framework to automatically search a population of heterogeneous and complexity-adaptive agentic workflows, rather than a single homogeneous, complex workflow. Technically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract parent workflows from an agentic population, evolves new workflows through \\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4) niching-based selection} to maintain population diversity and quality. Extensive evaluations across seven benchmarks demonstrate that EvoFlow is: \\textbf{(I) diverse}, evolving a population of workflows ranging from simple I/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing}, outperforming previous handcrafted and automated workflows by $1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful \\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source models.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07373"
    },
    "f3089465cbbb57b6f59153af036fecb1": {
        "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model",
        "authors": [
            "Emre Can Acikgoz",
            "Jeremiah Greer",
            "Akul Datta",
            "Ze Yang",
            "William Zeng",
            "Oussama Elachqar",
            "Emmanouil Koukoumidis",
            "Dilek Hakkani-Tür",
            "Gokhan Tur"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08820",
        "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08820"
    },
    "1b26f138e797bc52615ba6d476544dfb": {
        "title": "PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology",
        "authors": [
            "Fatemeh Ghezloo",
            "Mehmet Saygin Seyfioglu",
            "Rustin Soraki",
            "Wisdom O. Ikezogwo",
            "Beibin Li",
            "Tejoram Vivekanandan",
            "Joann G. Elmore",
            "Ranjay Krishna",
            "Linda Shapiro"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.08916",
        "abstract": "Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents, the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient&#39;s diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent&#39;s outputs are of high quality and comparable to GPT-4o. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Data, code and models available at https://pathfinder-dx.github.io/",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08916"
    },
    "c498e01526b6a1bd27683f246148d1cf": {
        "title": "Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for Autonomous Vehicles",
        "authors": [
            "Galileo Sartor",
            "Adam Wyner",
            "Giuseppe Contissa"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09216",
        "abstract": "In this paper, we present a modular system for representing and reasoning with legal aspects of traffic rules for autonomous vehicles. We focus on a subset of the United Kingdom&#39;s Highway Code (HC) related to junctions. As human drivers and automated vehicles (AVs) will interact on the roads, especially in urban environments, we claim that an accessible, unitary, high-level computational model should exist and be applicable to both users. Autonomous vehicles introduce a shift in liability that should not bring disadvantages or increased burden on human drivers. We develop a system &#34;in silico&#34; of the model. The proposed system is built of three main components: a natural language interface, using Logical English, which encodes the rules; an internal representation of the rules in Prolog; and an multi-agent-based simulation environment, built in NetLogo. The three components interact: Logical English is translated into and out of Prolog (along with some support code); Prolog and NetLogo interface via predicates. Such a modular approach enables the different components to carry different &#34;burdens&#34; in the overall system; it also allows swapping of modules. Given NetLogo, we can visualize the effect of the modeled rules as well as validate the system with a simple dynamic running scenario. Designated agents monitor the behaviour of the vehicles for compliance and record potential violations where they occur. The information on potential violations is then utilized by Validators, to determine whether the violation is punishable, differentiating between exceptions and cases.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09216"
    },
    "6a7e5e4132fc422294461d5575b527c7": {
        "title": "Reliable Conversational Agents under ASP Control that Understand Natural Language",
        "authors": [
            "Yankai Zeng"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09237",
        "abstract": "Efforts have been made to make machines converse like humans in the past few decades. The recent techniques of Large Language Models (LLMs) make it possible to have human-like conversations with machines, but LLM&#39;s flaws of lacking understanding and reliability are well documented. We believe that the best way to eliminate this problem is to use LLMs only as parsers to translate text to knowledge and vice versa and carry out the conversation by reasoning over this knowledge using the answer set programming. I have been developing a framework based on LLMs and ASP to realize reliable chatbots that &#34;understand&#34; human conversation. This framework has been used to develop task-specific chatbots as well as socialbots. My future research is focused on making these chatbots scalable and trainable.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09237"
    },
    "fc84925a30798aa4c28a8e1c8de68052": {
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "authors": [
            "Daniel Jarrett",
            "Miruna Pîslar",
            "Michiel A. Bakker",
            "Michael Henry Tessler",
            "Raphael Köster",
            "Jan Balaguer",
            "Romuald Elie",
            "Christopher Summerfield",
            "Andrea Tacchetti"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09369",
        "abstract": "Consider the process of collective decision-making, in which a group of individuals interactively select a preferred outcome from among a universe of alternatives. In this context, &#34;representation&#34; is the activity of making an individual&#39;s preferences present in the process via participation by a proxy agent -- i.e. their &#34;representative&#34;. To this end, learned models of human behavior have the potential to fill this role, with practical implications for multi-agent scenario studies and mechanism design. In this work, we investigate the possibility of training \\textit{language agents} to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. First, we formalize the setting of \\textit{collective decision-making} -- as the episodic process of interaction between a group of agents and a decision mechanism. On this basis, we then formalize the problem of \\textit{digital representation} -- as the simulation of an agent&#39;s behavior to yield equivalent outcomes from the mechanism. Finally, we conduct an empirical case study in the setting of \\textit{consensus-finding} among diverse humans, and demonstrate the feasibility of fine-tuning large language models to act as digital representatives.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09369"
    },
    "dba7339fc3fd44954118e551b06df63e": {
        "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
        "authors": [
            "Rui Yang",
            "Hanyang Chen",
            "Junyu Zhang",
            "Mark Zhao",
            "Cheng Qian",
            "Kangrui Wang",
            "Qineng Wang",
            "Teja Venkat Koripella",
            "Marziyeh Movahedi",
            "Manling Li",
            "Heng Ji",
            "Huan Zhang",
            "Tong Zhang"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09560",
        "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09560"
    },
    "67dff052fc4f00bb9be082ebd2f58b3f": {
        "title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
        "authors": [
            "Kexin Huang",
            "Ying Jin",
            "Ryan Li",
            "Michael Y. Li",
            "Emmanuel Candès",
            "Jure Leskovec"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.09858",
        "abstract": "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper&#39;s principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ],
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09858"
    },
    "f47e1ae0afc0e200a076140e674e8b3b": {
        "title": "Position: Stop Acting Like Language Model Agents Are Normal Agents",
        "authors": [
            "Elija Perrier",
            "Michael Timothy Bennett"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.10420",
        "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10420"
    },
    "30f0457199ead9ca5893677c4b505f2a": {
        "title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention",
        "authors": [
            "Chengshuai Zhao",
            "Zhen Tan",
            "Chau-Wai Wong",
            "Xinyan Zhao",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.10937",
        "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10937"
    },
    "844230bbee12afb73e6bb9f3b8904a23": {
        "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
        "authors": [
            "Pan Lu",
            "Bowen Chen",
            "Sheng Liu",
            "Rahul Thapa",
            "Joseph Boen",
            "James Zou"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.11271",
        "abstract": "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools&#39; generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11271"
    },
    "82ef266d2dae8ac235159ac655f5984e": {
        "title": "SMART: Self-Aware Agent for Tool Overuse Mitigation",
        "authors": [
            "Cheng Qian",
            "Emre Can Acikgoz",
            "Hongru Wang",
            "Xiusi Chen",
            "Avirup Sil",
            "Dilek Hakkani-Tür",
            "Gokhan Tur",
            "Heng Ji"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11435",
        "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent&#39;s self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11435"
    },
    "11e616912a082b8226ecb99dad9fee51": {
        "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning",
        "authors": [
            "Peiying Yu",
            "Guoxin Chen",
            "Jingjing Wang"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11799",
        "abstract": "Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11799"
    },
    "709740a97b1400008d356a32d40fe33b": {
        "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
        "authors": [
            "Shao Zhang",
            "Xihuai Wang",
            "Wenhao Zhang",
            "Chaoran Li",
            "Junru Song",
            "Tingyu Li",
            "Lin Qiu",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Wen Yao",
            "Weinan Zhang",
            "Xinbing Wang",
            "Ying Wen"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11882",
        "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent&#39;s System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent&#39;s System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11882"
    },
    "974a9b5b1ac39b4038a36b846be9ba99": {
        "title": "A Study on Leveraging Search and Self-Feedback for Agent Reasoning",
        "authors": [
            "Karthikeyan K",
            "Michelle Yuan",
            "Elman Mansimov",
            "Katerina Margatina",
            "Anurag Pratik",
            "Daniele Bonadiman",
            "Monica Sunkara",
            "Yi Zhang",
            "Yassine Benajiba"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12094",
        "abstract": "Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model&#39;s own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model&#39;s self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12094"
    },
    "d3a80535890b30f6cd75f002dc747f2e": {
        "title": "HARBOR: Exploring Persona Dynamics in Multi-Agent Competition",
        "authors": [
            "Kenan Jiang",
            "Li Xiong",
            "Fei Liu"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12149",
        "abstract": "We investigate factors contributing to LLM agents&#39; success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent&#39;s behavior in a competitive setting? (b) Can an agent effectively profile its competitors&#39; behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12149"
    },
    "2b96f0e93540445ce1ad323c790e645a": {
        "title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design",
        "authors": [
            "Yuxuan Lu",
            "Bingsheng Yao",
            "Hansu Gu",
            "Jing Huang",
            "Jessie Wang",
            "Laurence Li",
            "Jiri Gesi",
            "Qi He",
            "Toby Jia-Jun Li",
            "Dakuo Wang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12561",
        "abstract": "Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12561"
    },
    "4858bc6a428a55b9018b0d9d36998f50": {
        "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations",
        "authors": [
            "Frederic Kirstein",
            "Muneeb Khan",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13001",
        "abstract": "Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13001"
    },
    "0257d95b6b6e7237f9c4efdbca38552b": {
        "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents",
        "authors": [
            "Chaoran Chen",
            "Bingsheng Yao",
            "Ruishi Zou",
            "Wenyue Hua",
            "Weimin Lyu",
            "Toby Jia-Jun Li",
            "Dakuo Wang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13012",
        "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13012"
    },
    "b457fdcfd3dc55e76ef3b3ba43a83bc3": {
        "title": "Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks",
        "authors": [
            "Markus J. Buehler"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13025",
        "abstract": "We present an agentic, autonomous graph expansion framework that iteratively structures and refines knowledge in situ. Unlike conventional knowledge graph construction methods relying on static extraction or single-pass learning, our approach couples a reasoning-native large language model with a continually updated graph representation. At each step, the system actively generates new concepts and relationships, merges them into a global graph, and formulates subsequent prompts based on its evolving structure. Through this feedback-driven loop, the model organizes information into a scale-free network characterized by hub formation, stable modularity, and bridging nodes that link disparate knowledge clusters. Over hundreds of iterations, new nodes and edges continue to appear without saturating, while centrality measures and shortest path distributions evolve to yield increasingly distributed connectivity. Our analysis reveals emergent patterns, such as the rise of highly connected &#39;hub&#39; concepts and the shifting influence of &#39;bridge&#39; nodes, indicating that agentic, self-reinforcing graph construction can yield open-ended, coherent knowledge structures. Applied to materials design problems, we present compositional reasoning experiments by extracting node-specific and synergy-level principles to foster genuinely novel knowledge synthesis, yielding cross-domain ideas that transcend rote summarization and strengthen the framework&#39;s potential for open-ended scientific discovery. We discuss other applications in scientific discovery and outline future directions for enhancing scalability and interpretability.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13025"
    },
    "0d91bbdcb48cfdec3f34a78e2d5b1ab4": {
        "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions",
        "authors": [
            "Taedong Yun",
            "Eric Yang",
            "Mustafa Safdari",
            "Jong Ha Lee",
            "Vaishnavi Vinod Kumar",
            "S. Sara Mahdavi",
            "Jonathan Amar",
            "Derek Peyton",
            "Reut Aharony",
            "Andreas Michaelides",
            "Logan Schneider",
            "Isaac Galatzer-Levy",
            "Yugang Jia",
            "John Canny",
            "Arthur Gretton",
            "Maja Matarić"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13135",
        "abstract": "We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent&#39;s understanding of the synthetic users&#39; needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13135"
    },
    "f9514f2310d03555870205d78411a23a": {
        "title": "An LLM-based Agent for Reliable Docker Environment Configuration",
        "authors": [
            "Ruida Hu",
            "Chao Peng",
            "Xinchen Wang",
            "Cuiyun Gao"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13681",
        "abstract": "Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment &#34;pollution&#34; from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13681"
    },
    "3563a557a7d757a94cdabed8117ca752": {
        "title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning",
        "authors": [
            "Hanlin Wang",
            "Jian Wang",
            "Chak Tou Leong",
            "Wenjie Li"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14276",
        "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.",
        "code": "https://github.com/WangHanLinHenry/STeCa",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14276"
    },
    "a13576fb7b6c8f02b32a8aa7a0ea6c60": {
        "title": "Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems",
        "authors": [
            "Bingyu Yan",
            "Xiaoming Zhang",
            "Litian Zhang",
            "Lian Zhang",
            "Ziyi Zhou",
            "Dezhuang Miao",
            "Chaozhuo Li"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14321",
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in reasoning, planning, and decision-making. Building upon these strengths, researchers have begun incorporating LLMs into multi-agent systems (MAS), where agents collaborate or compete through natural language interactions to tackle tasks beyond the scope of single-agent setups. In this survey, we present a communication-centric perspective on LLM-based multi-agent systems, examining key system-level features such as architecture design and communication goals, as well as internal mechanisms like communication strategies, paradigms, objects and content. We illustrate how these communication elements interplay to enable collective intelligence and flexible collaboration. Furthermore, we discuss prominent challenges, including scalability, security, and multimodal integration, and propose directions for future work to advance research in this emerging domain. Ultimately, this survey serves as a catalyst for further innovation, fostering more robust, scalable, and intelligent multi-agent systems across diverse application domains.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14321"
    },
    "65c80808ab2692e24531b9ec34ebe56d": {
        "title": "Optimizing Model Selection for Compound AI Systems",
        "authors": [
            "Lingjiao Chen",
            "Jared Quincy Davis",
            "Boris Hanin",
            "Peter Bailis",
            "Matei Zaharia",
            "James Zou",
            "Ion Stoica"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14815",
        "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14815"
    },
    "a5653a5ea070143a12a1f50be4a977b7": {
        "title": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents",
        "authors": [
            "Jingoo Lee",
            "Kyungho Lim",
            "Young-Chul Jung",
            "Byung-Hoon Kim"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01594",
        "abstract": "Recent advances in large language models (LLMs) have accelerated the development of conversational agents capable of generating human-like responses. Since psychiatric assessments typically involve complex conversational interactions between psychiatrists and patients, there is growing interest in developing LLM-based psychiatric assessment conversational agents (PACAs) that aim to simulate the role of psychiatrists in clinical evaluations. However, standardized methods for benchmarking the clinical appropriateness of PACAs&#39; interaction with patients still remain underexplored. Here, we propose PSYCHE, a novel framework designed to enable the 1) clinically relevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation of PACAs. This is achieved by simulating psychiatric patients based on a multi-faceted psychiatric construct that defines the simulated patients&#39; profiles, histories, and behaviors, which PACAs are expected to assess. We validate the effectiveness of PSYCHE through a study with 10 board-certified psychiatrists, supported by an in-depth analysis of the simulated patient utterances.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01594"
    },
    "e15c9128ad8d94e1ec27e0a2a5bd996b": {
        "title": "LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models",
        "authors": [
            "Pouria Rouzrokh",
            "Moein Shariatnia"
        ],
        "date": "2025/01/05",
        "pdf": "http://arxiv.org/pdf/2501.05468",
        "abstract": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process. Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction. These agents operate within orchestrated workflows, supporting sequential and parallel review rounds, dynamic decision-making, and iterative refinement based on user feedback. LatteReview&#39;s architecture integrates LLM providers, enabling compatibility with both cloud-based and locally hosted models. The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets. The framework is available on the GitHub repository, with detailed documentation and an installable package.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05468"
    },
    "2ebcc67f683e9d099bb4494cd6479ade": {
        "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
        "authors": [
            "Vighnesh Subramaniam",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Shuang Li",
            "Igor Mordatch"
        ],
        "date": "2025/01/10",
        "pdf": "http://arxiv.org/pdf/2501.05707",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05707"
    },
    "63fe8ee82c8b5f95a514e76fb4e4a33b": {
        "title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
        "authors": [
            "Xiangru Tang",
            "Tianyu Hu",
            "Muyang Ye",
            "Yanjun Shao",
            "Xunjian Yin",
            "Siru Ouyang",
            "Wangchunshu Zhou",
            "Pan Lu",
            "Zhuosheng Zhang",
            "Yilun Zhao",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "date": "2025/01/11",
        "pdf": "http://arxiv.org/pdf/2501.06590",
        "abstract": "Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found at https://github.com/gersteinlab/chemagent",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.06590"
    },
    "6963aadc233c22e271b20c0392235c22": {
        "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation",
        "authors": [
            "Jiaxin Guo",
            "Yuanchang Luo",
            "Daimeng Wei",
            "Ling Zhang",
            "Zongyao Li",
            "Hengchao Shang",
            "Zhiqiang Rao",
            "Shaojun Li",
            "Jinlong Yang",
            "Zhanglin Wu",
            "Hao Yang"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.08523",
        "abstract": "The field of artificial intelligence has witnessed significant advancements in natural language processing, largely attributed to the capabilities of Large Language Models (LLMs). These models form the backbone of Agents designed to address long-context dependencies, particularly in Document-level Machine Translation (DocMT). DocMT presents unique challenges, with quality, consistency, and fluency being the key metrics for evaluation. Existing approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an incremental sentence-level forced decoding strategy \\textbf{to ensure every sentence is translated while enhancing the fluency of adjacent sentences.} Our Agent leverages a Doc-Guided Memory, focusing solely on the summary and its translation, which we find to be an efficient approach to maintaining consistency. Through extensive testing across multiple languages and domains, we demonstrate that Sent2Sent++ outperforms other methods in terms of quality, consistency, and fluency. The results indicate that, our approach has achieved significant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and document-level perplexity (d-ppl). The contributions of this paper include a detailed analysis of current DocMT research, the introduction of the Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of its effectiveness across languages and domains.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.08523"
    },
    "6980029f3a23fdd6bfca03f5bca21072": {
        "title": "Personality Modeling for Persuasion of Misinformation using AI Agent",
        "authors": [
            "Qianmin Lou",
            "Wentao Xu"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.08985",
        "abstract": "The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.08985"
    },
    "65f6d238b1eb6890b606f14a2881ac8d": {
        "title": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral Therapy in Psychological Counseling",
        "authors": [
            "Ancheng Xu",
            "Di Yang",
            "Renhao Li",
            "Jingwei Zhu",
            "Minghuan Tan",
            "Min Yang",
            "Wanxin Qiu",
            "Mingchen Ma",
            "Haihong Wu",
            "Bingyu Li",
            "Feng Sha",
            "Chengming Li",
            "Xiping Hu",
            "Qiang Qu",
            "Derek F. Wong",
            "Ruifeng Xu"
        ],
        "date": "2025/01/16",
        "pdf": "http://arxiv.org/pdf/2501.09426",
        "abstract": "Traditional in-person psychological counseling remains primarily niche, often chosen by individuals with psychological issues, while online automated counseling offers a potential solution for those hesitant to seek help due to feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and widely used approach in psychological counseling. The advent of large language models (LLMs) and agent technology enables automatic CBT diagnosis and treatment. However, current LLM-based CBT systems use agents with a fixed structure, limiting their self-optimization capabilities, or providing hollow, unhelpful suggestions due to redundant response patterns. In this work, we utilize Quora-like and YiXinLi single-round consultation models to build a general agent framework that generates high-quality responses for single-turn psychological consultation scenarios. We use a bilingual dataset to evaluate the quality of single-response consultations generated by each framework. Then, we incorporate dynamic routing and supervisory mechanisms inspired by real psychological counseling to construct a CBT-oriented autonomous multi-agent framework, demonstrating its general applicability. Experimental results indicate that AutoCBT can provide higher-quality automated psychological counseling services.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09426"
    },
    "3eb3d74a0ca144a0709f7ea8645be09d": {
        "title": "Agent-as-Judge for Factual Summarization of Long Narratives",
        "authors": [
            "Yeonseok Jeong",
            "Minsoo Kim",
            "Seung-won Hwang",
            "Byung-Hak Kim"
        ],
        "date": "2025/01/17",
        "pdf": "http://arxiv.org/pdf/2501.09993",
        "abstract": "Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (&gt;100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel &#34;Agent-as-a-Judge&#34; framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09993"
    },
    "49f35b96e2d2abc2615029be99395cd7": {
        "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
        "authors": [
            "Elad Levi",
            "Ilan Kadar"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.11067",
        "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11067"
    },
    "08b501dfcdada77f8db89771fbd74895": {
        "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
        "authors": [
            "Zhenhailong Wang",
            "Haiyang Xu",
            "Junyang Wang",
            "Xi Zhang",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Heng Ji"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.11733",
        "abstract": "Smartphones have become indispensable in modern life, yet navigating complex tasks on mobile devices often remains frustrating. Recent advancements in large multimodal model (LMM)-based mobile agents have demonstrated the ability to perceive and act in mobile environments. However, current approaches face significant limitations: they fall short in addressing real-world human needs, struggle with reasoning-intensive and long-horizon tasks, and lack mechanisms to learn and improve from prior experiences. To overcome these challenges, we introduce Mobile-Agent-E, a hierarchical multi-agent framework capable of self-evolution through past experience. By hierarchical, we mean an explicit separation of high-level planning and low-level action execution. The framework comprises a Manager, responsible for devising overall plans by breaking down complex tasks into subgoals, and four subordinate agents--Perceptor, Operator, Action Reflector, and Notetaker--which handle fine-grained visual perception, immediate action execution, error verification, and information aggregation, respectively. Mobile-Agent-E also features a novel self-evolution module which maintains a persistent long-term memory comprising Tips and Shortcuts. Tips are general guidance and lessons learned from prior tasks on how to effectively interact with the environment. Shortcuts are reusable, executable sequences of atomic operations tailored for specific subroutines. The inclusion of Tips and Shortcuts facilitates continuous refinement in performance and efficiency. Alongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring complex mobile tasks requiring long-horizon, multi-app interactions. Empirical results show that Mobile-Agent-E achieves a 22% absolute improvement over previous state-of-the-art approaches across three foundation model backbones. Project page: https://x-plug.github.io/MobileAgent.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11733"
    },
    "6dbbf1cb2add3b300212f60478a59d68": {
        "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
        "authors": [
            "Zhenran Xu",
            "Longyue Wang",
            "Jifang Wang",
            "Zhouyi Li",
            "Senbao Shi",
            "Xue Yang",
            "Yiyu Wang",
            "Baotian Hu",
            "Jun Yu",
            "Min Zhang"
        ],
        "date": "2025/01/22",
        "pdf": "http://arxiv.org/pdf/2501.12909",
        "abstract": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LLM-based multi-agent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FilmAgent simulates various crew roles, including directors, screenwriters, actors, and cinematographers, and covers key stages of a film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. A team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FilmAgent outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FilmAgent, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of a well-coordinated multi-agent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAI&#39;s text-to-video model Sora and our FilmAgent in filmmaking.",
        "code": "",
        "category": [
            [
                "Application",
                "Art"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12909"
    },
    "6ac0e6879afe3f2a6a53d4d966db8877": {
        "title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
        "authors": [
            "Shrinidhi Kumbhar",
            "Venkatesh Mishra",
            "Kevin Coutinho",
            "Divij Handa",
            "Ashif Iquebal",
            "Chitta Baral"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.13299",
        "abstract": "Materials discovery and design are essential for advancing technology across various industries by enabling the development of application-specific materials. Recent research has leveraged Large Language Models (LLMs) to accelerate this process. We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery. Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing real-world applications. Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints. To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically. Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs.",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ],
            [
                "Application",
                "Physics"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13299"
    },
    "388d238c303993b6eb0a201766c76829": {
        "title": "Self-Explanation in Social AI Agents",
        "authors": [
            "Rhea Basappa",
            "Mustafa Tekman",
            "Hong Lu",
            "Benjamin Faught",
            "Sandeep Kakar",
            "Ashok K. Goel"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.13945",
        "abstract": "Social AI agents interact with members of a community, thereby changing the behavior of the community. For example, in online learning, an AI social assistant may connect learners and thereby enhance social interaction. These social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners. We present a method of self-explanation that uses introspection over a self-model of an AI social assistant. The self-model is captured as a functional model that specifies how the methods of the agent use knowledge to achieve its tasks. The process of generating self-explanations uses Chain of Thought to reflect on the self-model and ChatGPT to provide explanations about its functioning. We evaluate the self-explanation of the AI social assistant for completeness and correctness. We also report on its deployment in a live class.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13945"
    },
    "c0ad4257855e22352665edac9454a908": {
        "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
        "authors": [
            "Diego Gosmar",
            "Deborah A. Dahl"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.13946",
        "abstract": "Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI systems and their reliability. This study investigates how orchestrating multiple specialized Artificial Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations, into a front-end agent. The outputs are then systematically reviewed and refined by second- and third-level agents, each employing distinct large language models and tailored strategies to detect unverified claims, incorporate explicit disclaimers, and clarify speculative content. Additionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed to evaluate hallucination score levels. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed assessments and ensuring accurate quantification of shifts in hallucination-related behaviors. A core component of this investigation is the use of the OVON (Open Voice Network) framework, which relies on universal NLP-based interfaces to transfer contextual information among agents. Through structured JSON messages, each agent communicates its assessment of the hallucination likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to refine the text without losing context. The results demonstrate that employing multiple specialized agents capable of interoperating with each other through NLP-based agentic frameworks can yield promising outcomes in hallucination mitigation, ultimately bolstering trust within the AI community.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13946"
    },
    "5f82858e162b2c670172c9a8e2d121a2": {
        "title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
        "authors": [
            "Preston Rasmussen",
            "Pavlo Paliychuk",
            "Travis Beauvais",
            "Jack Ryan",
            "Daniel Chalef"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.13956",
        "abstract": "We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti -- a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance (94.8% vs 93.4%). Beyond DMR, Zep&#39;s capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to 18.5% while simultaneously reducing response latency by 90% compared to baseline implementations. These results are particularly pronounced in enterprise-critical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep&#39;s effectiveness for deployment in real-world applications.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13956"
    },
    "884bfd6c802d7881215907e5e22a1a50": {
        "title": "Communicating Activations Between Language Model Agents",
        "authors": [
            "Vignav Ramesh",
            "Kenneth Li"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.14082",
        "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\\textit{B}$&#39;s computation at an intermediate layer, combine its current activation with another LM $\\textit{A}$&#39;s intermediate activation via some function $\\textit{f}$, then pass $\\textit{f}$&#39;s output into the next layer of $\\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\\%$ improvement over natural language communication across datasets with $&lt;$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative &#34;language&#34; for communication between LMs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14082"
    },
    "16b1588c25fd0f3fdd2663b75443cc23": {
        "title": "Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game",
        "authors": [
            "Rong Ye",
            "Yongxin Zhang",
            "Yikai Zhang",
            "Haoyu Kuang",
            "Zhongyu Wei",
            "Peng Sun"
        ],
        "date": "2025/01/24",
        "pdf": "http://arxiv.org/pdf/2501.14225",
        "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can not only make stratigic decisions but also engage in flexible and meaningful communication. Inspired by Wittgenstein&#39;s language game theory in Philosophical Investigations, we propose that language agents can learn through in-context interaction rather than traditional multi-stage frameworks that separate decision-making from language expression. Using Werewolf, a social deduction game that tests language understanding, strategic interaction, and adaptability, we develop the Multi-agent Kahneman &amp; Tversky&#39;s Optimization (MaKTO). MaKTO engages diverse models in extensive gameplay to generate unpaired desirable and unacceptable responses, then employs KTO to refine the model&#39;s decision-making process. In 9-player Werewolf games, MaKTO achieves a 61% average win rate across various models, outperforming GPT-4o and two-stage RL agents by relative improvements of 23.0% and 10.9%, respectively. Notably, MaKTO also demonstrates human-like performance, winning 60% against expert players and showing only 49% detectability in Turing-style blind tests. These results showcase MaKTO&#39;s superior decision-making, strategic adaptation, and natural language generation in complex social deduction games.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14225"
    },
    "669c9a4b755d8188c634d5905b8db9c1": {
        "title": "Unmasking Conversational Bias in AI Multiagent Systems",
        "authors": [
            "Erica Coppolillo",
            "Giuseppe Manco",
            "Luca Maria Aiello"
        ],
        "date": "2025/01/24",
        "pdf": "http://arxiv.org/pdf/2501.14844",
        "abstract": "Detecting biases in the outputs produced by generative models is essential to reduce the potential risks associated with their application in critical settings. However, the majority of existing methodologies for identifying biases in generated text consider the models in isolation and neglect their contextual applications. Specifically, the biases that may arise in multi-agent systems involving generative models remain under-researched. To address this gap, we present a framework designed to quantify biases within multi-agent systems of conversational Large Language Models (LLMs). Our approach involves simulating small echo chambers, where pairs of LLMs, initialized with aligned perspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe significant shifts in the stance expressed in the generated messages, particularly within echo chambers where all agents initially express conservative viewpoints, in line with the well-documented political bias of many LLMs toward liberal positions. Crucially, the bias observed in the echo-chamber experiment remains undetected by current state-of-the-art bias detection methods that rely on questionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias detection and mitigation for AI multi-agent systems. The code to perform the experiments is publicly available at https://anonymous.4open.science/r/LLMsConversationalBias-7725.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14844"
    },
    "595bb55d6de8444d8ea62370b7df247e": {
        "title": "Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning",
        "authors": [
            "Yiqun Chen",
            "Lingyong Yan",
            "Weiwei Sun",
            "Xinyu Ma",
            "Yi Zhang",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Yiming Yang",
            "Jiaxin Mao"
        ],
        "date": "2025/01/25",
        "pdf": "http://arxiv.org/pdf/2501.15228",
        "abstract": "Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents&#39; goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is on https://github.com/chenyiqun/MMOA-RAG.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15228"
    },
    "953ba69bdd56a78f559bae8a78959bd4": {
        "title": "Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions",
        "authors": [
            "Naihao Deng",
            "Rada Mihalcea"
        ],
        "date": "2025/01/25",
        "pdf": "http://arxiv.org/pdf/2501.15283",
        "abstract": "As Large Language Models (LLMs) advance in their capabilities, researchers have increasingly employed them for social simulation. In this paper, we investigate whether interactions among LLM agents resemble those of humans. Specifically, we focus on the pronoun usage difference between leaders and non-leaders, examining whether the simulation would lead to human-like pronoun usage patterns during the LLMs&#39; interactions. Our evaluation reveals the significant discrepancies between LLM-based simulations and human pronoun usage, with prompt-based or specialized agents failing to demonstrate human-like pronoun usage patterns. In addition, we reveal that even if LLMs understand the human pronoun usage patterns, they fail to demonstrate them in the actual interaction process. Our study highlights the limitations of social simulations based on LLM agents, urging caution in using such social simulation in practitioners&#39; decision-making process.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15283"
    },
    "46fe4f04f470a7053cc835b5c47b45e6": {
        "title": "Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection",
        "authors": [
            "Bo Yang",
            "Jiaxian Guo",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "date": "2025/01/26",
        "pdf": "http://arxiv.org/pdf/2501.15355",
        "abstract": "Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent&#39;s perception of its counterpart&#39;s mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts&#39; inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the \\textit{second-order} ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart&#39;s behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15355"
    },
    "fe12b9738b6d527d4e4380a84b6e1b96": {
        "title": "MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral Mental Health Question Answer",
        "authors": [
            "Qi Chen",
            "Dexi Liu"
        ],
        "date": "2025/01/27",
        "pdf": "http://arxiv.org/pdf/2501.15826",
        "abstract": "The Mental Health Question Answer (MHQA) task requires the seeker and supporter to complete the support process in one-turn dialogue. Given the richness of help-seeker posts, supporters must thoroughly understand the content and provide logical, comprehensive, and well-structured responses. Previous works in MHQA mostly focus on single-agent approaches based on the cognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the interactions among various CBT elements, such as emotion and cognition. This limitation hinders the models&#39; ability to thoroughly understand the distress of help-seekers. To address this, we propose a framework named Multi-Agent Deductive Planning (MADP), which is based on the interactions between the various psychological elements of CBT. This method guides Large Language Models (LLMs) to achieve a deeper understanding of the seeker&#39;s context and provide more personalized assistance based on individual circumstances. Furthermore, we construct a new dataset based on the MADP framework and use it to fine-tune LLMs, resulting in a specialized model named MADP-LLM. We conduct extensive experiments, including comparisons with multiple LLMs, human evaluations, and automatic evaluations, to validate the effectiveness of the MADP framework and MADP-LLM.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15826"
    },
    "89f171e17eb49ccf57191b08a3a19db2": {
        "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models",
        "authors": [
            "Yuxuan Li",
            "Hirokazu Shirado",
            "Sauvik Das"
        ],
        "date": "2025/01/29",
        "pdf": "http://arxiv.org/pdf/2501.17420",
        "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.17420"
    },
    "37aaa3a77aacdb7837741416f3de3591": {
        "title": "Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models",
        "authors": [
            "Manish Sanwal"
        ],
        "date": "2025/01/29",
        "pdf": "http://arxiv.org/pdf/2501.18645",
        "abstract": "Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to provide step-by-step rationales, improving performance on complex tasks. Despite its benefits, vanilla CoT often fails to fully verify intermediate inferences and can produce misleading explanations. In this work, we propose Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that systematically segments the reasoning process into multiple layers, each subjected to external checks and optional user feedback. We expand on the key concepts, present three scenarios -- medical triage, financial risk assessment, and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT in terms of transparency, correctness, and user engagement. By integrating references from recent arXiv papers on interactive explainability, multi-agent frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves the way for more reliable and grounded explanations in high-stakes domains.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.18645"
    },
    "5e77a1660e986fc9c52ea6f640afef7c": {
        "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search",
        "authors": [
            "Haoran Luo",
            "Haihong E",
            "Yikai Guo",
            "Qika Lin",
            "Xiaobao Wu",
            "Xinyu Mu",
            "Wenhao Liu",
            "Meina Song",
            "Yifan Zhu",
            "Luu Anh Tuan"
        ],
        "date": "2025/01/31",
        "pdf": "http://arxiv.org/pdf/2501.18922",
        "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration&#39;s performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model&#39;s GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.18922"
    },
    "83b6080f352a88b9b83b2f83a876dcc9": {
        "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects",
        "authors": [
            "Abdullah Mushtaq",
            "Muhammad Rafay Naeem",
            "Ibrahim Ghaznavi",
            "Muhammad Imran Taj",
            "Imran Hashmi",
            "Junaid Qadir"
        ],
        "date": "2025/01/02",
        "pdf": "http://arxiv.org/pdf/2501.01205",
        "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01205"
    },
    "0becf291bbb8de204c69d5f70cbe9794": {
        "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
        "authors": [
            "Dayuan Fu",
            "Keqing He",
            "Yejie Wang",
            "Wentao Hong",
            "Zhuoma Gongque",
            "Weihao Zeng",
            "Wei Wang",
            "Jingang Wang",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01702",
        "abstract": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01702"
    },
    "319d39b7410b1699ca230cfc0076a29f": {
        "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
        "authors": [
            "Aobo Kong",
            "Wentao Ma",
            "Shiwan Zhao",
            "Yongbin Li",
            "Yuchuan Wu",
            "Ke Wang",
            "Xiaoqian Liu",
            "Qicheng Li",
            "Yong Qin",
            "Fei Huang"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01821",
        "abstract": "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO&#39;s potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
        "code": "",
        "category": [
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01821"
    },
    "578096cec3465cb082c34dec69c9d8f9": {
        "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
        "authors": [
            "Hao Zheng",
            "Xinyan Guan",
            "Hao Kong",
            "Jia Zheng",
            "Weixiang Zhou",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "date": "2025/01/07",
        "pdf": "http://arxiv.org/pdf/2501.03936",
        "abstract": "Automatically generating presentations from documents is a challenging task that requires accommodating content quality, visual appeal, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, overlooking visual appeal and structural coherence, which limits their practical applicability. To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows. PPTAgent first analyzes reference presentations to extract slide-level functional types and content schemas, then drafts an outline and iteratively generates editing actions based on selected reference slides to create new slides. To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Results demonstrate that PPTAgent significantly outperforms existing automatic presentation generation methods across all three dimensions.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.03936"
    },
    "b8adfebfe98f0ced2e2d099d89c81412": {
        "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
        "authors": [
            "Samuel Schmidgall",
            "Yusheng Su",
            "Ze Wang",
            "Ximeng Sun",
            "Jialian Wu",
            "Xiaodong Yu",
            "Jiang Liu",
            "Zicheng Liu",
            "Emad Barsoum"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.04227",
        "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.04227"
    },
    "2d7c164824c91765d8b336c520d7f4d4": {
        "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
        "authors": [
            "Yuhang Liu",
            "Pengxiang Li",
            "Zishu Wei",
            "Congkai Xie",
            "Xueyu Hu",
            "Xinchen Xu",
            "Shengyu Zhang",
            "Xiaotian Han",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.04575",
        "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at \\url{https://github.com/Reallm-Labs/InfiGUIAgent}.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.04575"
    },
    "ae6eb6ca01d94c79165bd75a33a7b227": {
        "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models",
        "authors": [
            "Xiaoxi Li",
            "Guanting Dong",
            "Jiajie Jin",
            "Yuyao Zhang",
            "Yujia Zhou",
            "Yutao Zhu",
            "Peitian Zhang",
            "Zhicheng Dou"
        ],
        "date": "2025/01/09",
        "pdf": "http://arxiv.org/pdf/2501.05366",
        "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \\textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \\url{https://github.com/sunnynexus/Search-o1}.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05366"
    },
    "1f869839ffaecbb783aa788d0ed5ebdc": {
        "title": "LLMs Model Non-WEIRD Populations: Experiments with Synthetic Cultural Agents",
        "authors": [
            "Augusto Gonzalez-Bonorino",
            "Monica Capra",
            "Emilio Pantoja"
        ],
        "date": "2025/01/12",
        "pdf": "http://arxiv.org/pdf/2501.06834",
        "abstract": "Despite its importance, studying economic behavior across diverse, non-WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations presents significant challenges. We address this issue by introducing a novel methodology that uses Large Language Models (LLMs) to create synthetic cultural agents (SCAs) representing these populations. We subject these SCAs to classic behavioral experiments, including the dictator and ultimatum games. Our results demonstrate substantial cross-cultural variability in experimental behavior. Notably, for populations with available data, SCAs&#39; behaviors qualitatively resemble those of real human subjects. For unstudied populations, our method can generate novel, testable hypotheses about economic behavior. By integrating AI into experimental economics, this approach offers an effective and ethical method to pilot experiments and refine protocols for hard-to-reach populations. Our study provides a new tool for cross-cultural economic studies and demonstrates how LLMs can help experimental behavioral research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.06834"
    },
    "7b9dd6fc37e1a20f64f4c5f22ae6aa6a": {
        "title": "Talk to Right Specialists: Routing and Planning in Multi-agent System for Question Answering",
        "authors": [
            "Feijie Wu",
            "Zitao Li",
            "Fei Wei",
            "Yaliang Li",
            "Bolin Ding",
            "Jing Gao"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.07813",
        "abstract": "Leveraging large language models (LLMs), an agent can utilize retrieval-augmented generation (RAG) techniques to integrate external knowledge and increase the reliability of its responses. Current RAG-based agents integrate single, domain-specific knowledge sources, limiting their ability and leading to hallucinated or inaccurate responses when addressing cross-domain queries. Integrating multiple knowledge bases into a unified RAG-based agent raises significant challenges, including increased retrieval overhead and data sovereignty when sensitive data is involved. In this work, we propose RopMura, a novel multi-agent system that addresses these limitations by incorporating highly efficient routing and planning mechanisms. RopMura features two key components: a router that intelligently selects the most relevant agents based on knowledge boundaries and a planner that decomposes complex multi-hop queries into manageable steps, allowing for coordinating cross-domain responses. Experimental results demonstrate that RopMura effectively handles both single-hop and multi-hop queries, with the routing mechanism enabling precise answers for single-hop queries and the combined routing and planning mechanisms achieving accurate, multi-step resolutions for complex queries.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.07813"
    },
    "063b44796c8c8be3d6c9b02aee88adf6": {
        "title": "Agent-Centric Projection of Prompting Techniques and Implications for Synthetic Training Data for Large Language Models",
        "authors": [
            "Dhruv Dhamani",
            "Mary Lou Maher"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.07815",
        "abstract": "Recent advances in prompting techniques and multi-agent systems for Large Language Models (LLMs) have produced increasingly complex approaches. However, we lack a framework for characterizing and comparing prompting techniques or understanding their relationship to multi-agent LLM systems. This position paper introduces and explains the concepts of linear contexts (a single, continuous sequence of interactions) and non-linear contexts (branching or multi-path) in LLM systems. These concepts enable the development of an agent-centric projection of prompting techniques, a framework that can reveal deep connections between prompting strategies and multi-agent systems. We propose three conjectures based on this framework: (1) results from non-linear prompting techniques can predict outcomes in equivalent multi-agent systems, (2) multi-agent system architectures can be replicated through single-LLM prompting techniques that simulate equivalent interaction patterns, and (3) these equivalences suggest novel approaches for generating synthetic training data. We argue that this perspective enables systematic cross-pollination of research findings between prompting and multi-agent domains, while providing new directions for improving both the design and training of future LLM systems.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.07815"
    },
    "5d735467a3ca05b8eaad8dd80a19c261": {
        "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
        "authors": [
            "Aditi Singh",
            "Abul Ehtesham",
            "Saket Kumar",
            "Tala Talaei Khoei"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.09136",
        "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09136"
    },
    "5613ddd27fb336ab0265da940cb4c896": {
        "title": "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.11233",
        "abstract": "Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11233"
    },
    "2d5682f900e4853ed0ca1e1328887a98": {
        "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
        "authors": [
            "Zhili Cheng",
            "Yuge Tu",
            "Ran Li",
            "Shiqi Dai",
            "Jinyi Hu",
            "Shengding Hu",
            "Jiahao Li",
            "Yang Shi",
            "Tianyu Yu",
            "Weize Chen",
            "Lei Shi",
            "Maosong Sun"
        ],
        "date": "2025/01/21",
        "pdf": "http://arxiv.org/pdf/2501.11858",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11858"
    },
    "09312c290520cd1bcdfd197df5ef21be": {
        "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
        "authors": [
            "Yujia Qin",
            "Yining Ye",
            "Junjie Fang",
            "Haoming Wang",
            "Shihao Liang",
            "Shizuo Tian",
            "Junda Zhang",
            "Jiahao Li",
            "Yunxin Li",
            "Shijue Huang",
            "Wanjun Zhong",
            "Kuanye Li",
            "Jiale Yang",
            "Yu Miao",
            "Woyu Lin",
            "Longxiang Liu",
            "Xu Jiang",
            "Qianli Ma",
            "Jingyu Li",
            "Xiaojun Xiao",
            "Kai Cai",
            "Chuang Li",
            "Yaowei Zheng",
            "Chaolin Jin",
            "Chen Li",
            "Xiao Zhou",
            "Minchao Wang",
            "Haoli Chen",
            "Zhaojian Li",
            "Haihua Yang",
            "Haifeng Liu",
            "Feng Lin",
            "Tao Peng",
            "Xin Liu",
            "Guang Shi"
        ],
        "date": "2025/01/21",
        "pdf": "http://arxiv.org/pdf/2501.12326",
        "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12326"
    },
    "16bf32d84a47590d45c85592a76488fb": {
        "title": "FinSphere: A Conversational Stock Analysis Agent Equipped with Quantitative Tools based on Real-Time Database",
        "authors": [
            "Shijie Han",
            "Changhai Zhou",
            "Yiqing Shen",
            "Tianning Sun",
            "Yuhua Zhou",
            "Xiaoxia Wang",
            "Zhixiao Yang",
            "Jingshu Zhang",
            "Hongguang Li"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.12399",
        "abstract": "Current financial Large Language Models (LLMs) struggle with two critical limitations: a lack of depth in stock analysis, which impedes their ability to generate professional-grade insights, and the absence of objective evaluation metrics to assess the quality of stock analysis reports. To address these challenges, this paper introduces FinSphere, a conversational stock analysis agent, along with three major contributions: (1) Stocksis, a dataset curated by industry experts to enhance LLMs&#39; stock analysis capabilities, (2) AnalyScore, a systematic evaluation framework for assessing stock analysis quality, and (3) FinSphere, an AI agent that can generate high-quality stock analysis reports in response to user queries. Experiments demonstrate that FinSphere achieves superior performance compared to both general and domain-specific LLMs, as well as existing agent-based systems, even when they are enhanced with real-time data access and few-shot guidance. The integrated framework, which combines real-time data feeds, quantitative tools, and an instruction-tuned LLM, yields substantial improvements in both analytical quality and practical applicability for real-world stock analysis.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12399"
    },
    "47477179b28782d60edc21bdd95ea6ea": {
        "title": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback",
        "authors": [
            "Joshua Park",
            "Yongfeng Zhang"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.13333",
        "abstract": "Multi-agent systems must decide which agent is the most appropriate for a given task. We propose a novel architecture for recommending which LLM agent out of many should perform a task given a natural language prompt by extending the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a top-1 accuracy of 92.2% with each classification taking less than 300 milliseconds. In contrast to traditional classification methods, our architecture is computationally cheap, adaptive to new classes, interpretable, and controllable with arbitrary metrics through reinforcement learning. By encoding natural language prompts into sentence embeddings, our model captures the semantic content relevant to recommending an agent. The distance between sentence embeddings that belong to the same agent is then minimized through fine-tuning and aligned to human values through reinforcement learning from human feedback. This allows the classification of natural language prompts based on their nearest neighbors by measuring the cosine similarity between embeddings. This work is made possible through the generation of a synthetic dataset for agent recommendation, which we have open-sourced to the public along with the code for AgentRec recommendation system at https://github.com/joshprk/agentrec.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13333"
    },
    "63c1f5c83cd2e780c8ee5eca3cc188be": {
        "title": "Developing Enhanced Conversational Agents for Social Virtual Worlds",
        "authors": [
            "D. Griol",
            "A. Sanchis",
            "J. M. Molina",
            "Z. Callejas"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.16341",
        "abstract": "In this paper, we present a methodology for the development of embodied conversational agents for social virtual worlds. The agents provide multimodal communication with their users in which speech interaction is included. Our proposal combines different techniques related to Artificial Intelligence, Natural Language Processing, Affective Computing, and User Modeling. Firstly, the developed conversational agents. A statistical methodology has been developed to model the system conversational behavior, which is learned from an initial corpus and improved with the knowledge acquired from the successive interactions. In addition, the selection of the next system response is adapted considering information stored into users profiles and also the emotional contents detected in the users utterances. Our proposal has been evaluated with the successful development of an embodied conversational agent which has been placed in the Second Life social virtual world. The avatar includes the different models and interacts with the users who inhabit the virtual world in order to provide academic information. The experimental results show that the agents conversational behavior adapts successfully to the specific characteristics of users interacting in such environments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.16341"
    },
    "45a66881a718ec4b28663d50c69f62df": {
        "title": "CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation",
        "authors": [
            "Faria Huq",
            "Zora Zhiruo Wang",
            "Frank F. Xu",
            "Tianyue Ou",
            "Shuyan Zhou",
            "Jeffrey P. Bigham",
            "Graham Neubig"
        ],
        "date": "2025/01/28",
        "pdf": "http://arxiv.org/pdf/2501.16609",
        "abstract": "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent&#39;s capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.16609"
    },
    "8b5a90b00bf3332dc22a161e86b46376": {
        "title": "Context is Key for Agent Security",
        "authors": [
            "Lillian Tsai",
            "Eugene Bagdasarian"
        ],
        "date": "2025/01/28",
        "pdf": "http://arxiv.org/pdf/2501.17070",
        "abstract": "Judging the safety of an action, whether taken by a human or a system, must take into account the context in which the action takes place. For example, deleting an email from a user&#39;s mailbox may or may not be appropriate depending on the email&#39;s content, the user&#39;s goals, or even available space. Systems today that make these judgements -- providing security against harmful or inappropriate actions -- rely on manually-crafted policies or user confirmation for each relevant context. With the upcoming deployment of systems like generalist agents, we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual security for agents (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.17070"
    },
    "7c0c41b3a914bc731bc0f01e3bd65515": {
        "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
        "authors": [
            "Fenglin Yu",
            "Fangkai Yang",
            "Xiaoting Qin",
            "Zhiyang Zhang",
            "Jue Zhang",
            "Qingwei Lin",
            "Hongyu Zhang",
            "Yingnong Dang",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "date": "2025/01/31",
        "pdf": "http://arxiv.org/pdf/2501.19056",
        "abstract": "The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.19056"
    },
    "177773166ac1ce4eec643e253a85c0cc": {
        "title": "The Ann Arbor Architecture for Agent-Oriented Programming",
        "authors": [
            "Wei Dong"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.09903",
        "abstract": "In this paper, we reexamine prompt engineering for large language models through the lens of automata theory. We argue that language models function as automata and, like all automata, should be programmed in the languages they accept, a unified collection of all natural and formal languages. Therefore, traditional software engineering practices--conditioned on the clear separation of programming languages and natural languages--must be rethought. We introduce the Ann Arbor Architecture, a conceptual framework for agent-oriented programming of language models, as a higher-level abstraction over raw token generation, and provide a new perspective on in-context learning. Based on this framework, we present the design of our agent platform Postline, and report on our initial experiments in agent training.",
        "code": "https://github.com/aaalgo/postline_0.1",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09903"
    },
    "6f9c1ac089493ef3ed381bcddd216a44": {
        "title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems",
        "authors": [
            "Zengqing Wu",
            "Takayuki Ito"
        ],
        "date": "2025/02/23",
        "pdf": "http://arxiv.org/pdf/2502.16565",
        "abstract": "Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios -- Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision -- confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.",
        "code": "https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.16565"
    },
    "99419075ed21de8e2af08ee4afa13062": {
        "title": "MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution to Automate Cross-App Instructions",
        "authors": [
            "Yuxuan Liu",
            "Hongda Sun",
            "Wei Liu",
            "Jian Luan",
            "Bo Du",
            "Rui Yan"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.16796",
        "abstract": "Mobile phone agents can assist people in automating daily tasks on their phones, which have emerged as a pivotal research spotlight. However, existing procedure-oriented agents struggle with cross-app instructions, due to the following challenges: (1) complex task relationships, (2) diverse app environment, and (3) error propagation and information loss in multi-step execution. Drawing inspiration from object-oriented programming principles, we recognize that object-oriented solutions is more suitable for cross-app instruction. To address these challenges, we propose a self-evolving multi-agent framework named MobileSteward, which integrates multiple app-oriented StaffAgents coordinated by a centralized StewardAgent. We design three specialized modules in MobileSteward: (1) Dynamic Recruitment generates a scheduling graph guided by information flow to explicitly associate tasks among apps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each equipped with app-specialized expertise to address the diversity between apps. (3) Adjusted Evaluation conducts evaluation to provide reflection tips or deliver key information, which alleviates error propagation and information loss during multi-step execution. To continuously improve the performance of MobileSteward, we develop a Memory-based Self-evolution mechanism, which summarizes the experience from successful execution, to improve the performance of MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench) in the real-world environment to evaluate the agents&#39; capabilities of solving complex cross-app instructions. Experimental results demonstrate that MobileSteward achieves the best performance compared to both single-agent and multi-agent frameworks, highlighting the superiority of MobileSteward in better handling user instructions with diverse complexity.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.16796"
    },
    "9c6712e928529179d418e2f944f501a6": {
        "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Xi Zhang",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17110",
        "abstract": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks. The code will be open-sourced at https://github.com/X-PLUG/MobileAgent.",
        "code": "https://github.com/X-PLUG/MobileAgent",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17110"
    },
    "3331d194b7f2bf339cf564c4b2e98a9e": {
        "title": "Stay Focused: Problem Drift in Multi-Agent Debate",
        "authors": [
            "Jonas Becker",
            "Lars Benedikt Kaesberg",
            "Andreas Stephan",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19559",
        "abstract": "Multi-agent debate - multiple instances of large language models discussing problems in turn-based interaction - has shown promise for solving knowledge and reasoning tasks. However, these methods show limitations, particularly when scaling them to longer reasoning chains. In this study, we unveil a new issue of multi-agent debate: discussions drift away from the initial problem over multiple turns. We define this phenomenon as problem drift and quantify its presence across ten tasks (i.e., three generative, three knowledge, three reasoning, and one instruction-following task). To identify the reasons for this issue, we perform a human study with eight experts on discussions suffering from problem drift, who find the most common issues are a lack of progress (35% of cases), low-quality feedback (26% of cases), and a lack of clarity (25% of cases). To systematically address the issue of problem drift, we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of problem drift cases. Our study can be seen as a first step to understanding a key limitation of multi-agent debate, highlighting pathways for improving their effectiveness in the future.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19559"
    },
    "cf78b1c292d49a864a8fed69270d38b7": {
        "title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents",
        "authors": [
            "Ashley Lewis",
            "Michael White",
            "Jing Liu",
            "Toshiaki Koike-Akino",
            "Kieran Parsons",
            "Ye Wang"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19545",
        "abstract": "The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination-generating false information-and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models&#39; outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized &#34;I don&#39;t know&#34; responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19545"
    },
    "d1c5e4d780996dc488d652dbb43e3a14": {
        "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
        "authors": [
            "Jeffrey Yang Fan Chiang",
            "Seungjae Lee",
            "Jia-Bin Huang",
            "Furong Huang",
            "Yizheng Chen"
        ],
        "date": "2025/02/27",
        "pdf": "http://arxiv.org/pdf/2502.20383",
        "abstract": "Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.20383"
    },
    "653218e7695b65021161f0517044ac67": {
        "title": "M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging",
        "authors": [
            "Jinghao Feng",
            "Qiaoyu Zheng",
            "Chaoyi Wu",
            "Ziheng Zhao",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "date": "2025/02/27",
        "pdf": "http://arxiv.org/pdf/2502.20301",
        "abstract": "Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.20301"
    },
    "33eceb2cc2ba664c62485d4f0ca022af": {
        "title": "Voting or Consensus? Decision-Making in Multi-Agent Debate",
        "authors": [
            "Lars Benedikt Kaesberg",
            "Jonas Becker",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19130",
        "abstract": "Much of the success of multi-agent debates depends on carefully choosing the right parameters. Among them, the decision-making protocol stands out. Systematic comparison of decision protocols is difficult because studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making addresses the challenges of different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time (i.e., decision protocol) to analyze how different methods affect the collaboration between agents and test different protocols on knowledge (MMLU, MMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks over the other decision protocol. Increasing the number of agents improves performance, while more discussion rounds before voting reduces it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19130"
    },
    "f5de8450ff665a0dd1c99cb1221a1488": {
        "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems",
        "authors": [
            "Hao Peng",
            "Yunjia Qi",
            "Xiaozhi Wang",
            "Zijun Yao",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19328",
        "abstract": "Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
        "code": "https://github.com/THU-KEG/Agentic-Reward-Modeling",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19328"
    },
    "3fe124c386a094c51407860228416e77": {
        "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis",
        "authors": [
            "Daniel Rose",
            "Chia-Chien Hung",
            "Marco Lepri",
            "Israa Alqassem",
            "Kiril Gashteovski",
            "Carolin Lawrence"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19175",
        "abstract": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19175"
    },
    "bad7c9860f3358a986e817507d081e4b": {
        "title": "Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT",
        "authors": [
            "Hediyeh Baban",
            "Sai A Pidapar",
            "Aashutosh Nema",
            "Sichen Lu"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18653",
        "abstract": "We introduce a novel multi-agent collaboration framework designed to enhance the accuracy and robustness of text classification models. Leveraging BERT as the primary classifier, our framework dynamically escalates low-confidence predictions to a specialized multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative approach allows for comprehensive analysis and consensus-driven decision-making, significantly improving classification performance across diverse text classification tasks. Empirical evaluations on benchmark datasets demonstrate that our framework achieves a 5.5% increase in accuracy compared to standard BERT-based classifiers, underscoring its effectiveness and academic novelty in advancing multi-agent systems within natural language processing.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18653"
    },
    "6c11b5a1ee6175776c6a68083bedff85": {
        "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
        "authors": [
            "Max Ku",
            "Thomas Chong",
            "Jonathan Leung",
            "Krish Shah",
            "Alvin Yu",
            "Wenhu Chen"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19400",
        "abstract": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19400"
    },
    "929d8ef67b10589b81a331f50b04b5b8": {
        "title": "A Cooperative Multi-Agent Framework for Zero-Shot Named Entity Recognition",
        "authors": [
            "Zihan Wang",
            "Ziqi Zhao",
            "Yougang Lyu",
            "Zhumin Chen",
            "Maarten de Rijke",
            "Zhaochun Ren"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18702",
        "abstract": "Zero-shot named entity recognition (NER) aims to develop entity recognition systems from unannotated text corpora. This task presents substantial challenges due to minimal human intervention. Recent work has adapted large language models (LLMs) for zero-shot NER by crafting specialized prompt templates. It advances model self-learning abilities by incorporating self-annotated demonstrations. However, two important challenges persist: (i) Correlations between contexts surrounding entities are overlooked, leading to wrong type predictions or entity omissions. (ii) The indiscriminate use of task demonstrations, retrieved through shallow similarity-based strategies, severely misleads LLMs during inference. In this paper, we introduce the cooperative multi-agent system (CMAS), a novel framework for zero-shot NER that uses the collective intelligence of multiple agents to address the challenges outlined above. CMAS has four main agents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor, (iii) a demonstration discriminator, and (iv) an overall predictor. To explicitly capture correlations between contexts surrounding entities, CMAS reformulates NER into two subtasks: recognizing named entities and identifying entity type-related features within the target sentence. To enable controllable utilization of demonstrations, a demonstration discriminator is established to incorporate the self-reflection mechanism, automatically evaluating helpfulness scores for the target sentence. Experimental results show that CMAS significantly improves zero-shot NER performance across six benchmarks, including both domain-specific and general-domain scenarios. Furthermore, CMAS demonstrates its effectiveness in few-shot settings and with various LLM backbones.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18702"
    },
    "b7d328538c325c921ce66febff2a3c75": {
        "title": "Scaffolding Empathy: Training Counselors with Simulated Patients and Utterance-level Performance Visualizations",
        "authors": [
            "Ian Steenstra",
            "Farnaz Nouraei",
            "Timothy W. Bickmore"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18673",
        "abstract": "Learning therapeutic counseling involves significant role-play experience with mock patients, with current manual training methods providing only intermittent granular feedback. We seek to accelerate and optimize counselor training by providing frequent, detailed feedback to trainees as they interact with a simulated patient. Our first application domain involves training motivational interviewing skills for counselors. Motivational interviewing is a collaborative counseling style in which patients are guided to talk about changing their behavior, with empathetic counseling an essential ingredient. We developed and evaluated an LLM-powered training system that features a simulated patient and visualizations of turn-by-turn performance feedback tailored to the needs of counselors learning motivational interviewing. We conducted an evaluation study with professional and student counselors, demonstrating high usability and satisfaction with the system. We present design implications for the development of automated systems that train users in counseling skills and their generalizability to other types of social skills training.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18673"
    },
    "69e70ffee7c91c8f86a98df96a2933e8": {
        "title": "AgentRM: Enhancing Agent Generalization with Reward Modeling",
        "authors": [
            "Yu Xia",
            "Jingru Fan",
            "Weize Chen",
            "Siyu Yan",
            "Xin Cong",
            "Zhong Zhang",
            "Yaxi Lu",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18407",
        "abstract": "Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by $8.8$ points on average, surpassing the top general agent by $4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by $11.4$ on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18407"
    },
    "40b0a9827149a058e2153598bbe02542": {
        "title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction",
        "authors": [
            "Jianhao Yan",
            "Yun Luo",
            "Yue Zhang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18308",
        "abstract": "In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses. However, evaluating an LLM&#39;s ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment. We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues. https://github.com/ElliottYan/RefuteBench-2.0",
        "code": "https://github.com/ElliottYan/RefuteBench-2.0",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18308"
    },
    "31071f7714f91d731ef370175c9eec55": {
        "title": "Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent",
        "authors": [
            "Xiaofeng Wang",
            "Zhixin Zhang",
            "Jinguang Zheng",
            "Yiming Ai",
            "Rui Wang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18228",
        "abstract": "Debt collection negotiations (DCN) are vital for managing non-performing loans (NPLs) and reducing creditor losses. Traditional methods are labor-intensive, while large language models (LLMs) offer promising automation potential. However, prior systems lacked dynamic negotiation and real-time decision-making capabilities. This paper explores LLMs in automating DCN and proposes a novel evaluation framework with 13 metrics across 4 aspects. Our experiments reveal that LLMs tend to over-concede compared to human negotiators. To address this, we propose the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules to improve decision rationality. We also apply post-training techniques, including DPO with rejection sampling, to optimize performance. Our studies provide valuable insights for practitioners and researchers seeking to enhance efficiency and outcomes in this domain.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18228"
    },
    "08a4ce85e4e77cacad09ecb623d02ad6": {
        "title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding",
        "authors": [
            "Jian Wu",
            "Jiayu Zhang",
            "Dongyuan Li",
            "Linyi Yang",
            "Aoxiao Zhong",
            "Renhe Jiang",
            "Qingsong Wen",
            "Yue Zhang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18209",
        "abstract": "This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper&#39;s proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18209"
    },
    "f4db64903e9b8877fbfba9827856f04b": {
        "title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models",
        "authors": [
            "Hongzhan Lin",
            "Yang Deng",
            "Yuxuan Gu",
            "Wenxuan Zhang",
            "Jing Ma",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.17924",
        "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs&#39; fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs&#39; factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17924"
    },
    "710a7abc2158db6d47ede7575e9a90b1": {
        "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
        "authors": [
            "Qiuchen Wang",
            "Ruixue Ding",
            "Zehui Chen",
            "Weiqi Wu",
            "Shihang Wang",
            "Pengjun Xie",
            "Feng Zhao"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18017",
        "abstract": "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model&#39;s reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18017"
    },
    "aeee067bd0d4eb3c4a4a866683b07066": {
        "title": "LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena",
        "authors": [
            "Tianmi Ma",
            "Jiawei Du",
            "Wenxin Huang",
            "Wenjie Wang",
            "Liang Xie",
            "Xian Zhong",
            "Joey Tianyi Zhou"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.17967",
        "abstract": "Recent advancements in large language models (LLMs) have significantly improved performance in natural language processing tasks. However, their ability to generalize to dynamic, unseen tasks, particularly in numerical reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on problems with predefined optimal solutions, which may not align with real-world scenarios where clear answers are absent. To bridge this gap, we design the Agent Trading Arena, a virtual numerical game simulating complex economic systems through zero-sum games, where agents invest in stock portfolios. Our experiments reveal that LLMs, including GPT-4o, struggle with algebraic reasoning when dealing with plain-text stock data, often focusing on local details rather than global trends. In contrast, LLMs perform significantly better with geometric reasoning when presented with visual data, such as scatter plots or K-line charts, suggesting that visual representations enhance numerical reasoning. This capability is further improved by incorporating the reflection module, which aids in the analysis and interpretation of complex data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate stronger reasoning with visual data compared to text. Our code and data are publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17967"
    },
    "b72e503d6aad3525a17bee4add471676": {
        "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
        "authors": [
            "Bingxuan Li",
            "Yiwei Wang",
            "Jiuxiang Gu",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17651",
        "abstract": "Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17651"
    },
    "2be39e1583cea697b224e3752c123671": {
        "title": "Training a Generally Curious Agent",
        "authors": [
            "Fahim Tajwar",
            "Yiding Jiang",
            "Abitha Thankaraj",
            "Sumaita Sadia Rahman",
            "J Zico Kolter",
            "Jeff Schneider",
            "Ruslan Salakhutdinov"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17543",
        "abstract": "Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present PAPRIKA, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, PAPRIKA teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with PAPRIKA can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach&#39;s primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17543"
    },
    "8d04925aa85c0d1934ad4dbbb502c5ec": {
        "title": "Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents",
        "authors": [
            "Prafulla Kumar Choubey",
            "Xiangyu Peng",
            "Shilpa Bhagavath",
            "Caiming Xiong",
            "Shiva Kumar Pentyala",
            "Chien-Sheng Wu"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17321",
        "abstract": "Automated service agents require well-structured workflows to provide consistent and accurate responses to customer queries. However, these workflows are often undocumented, and their automatic extraction from conversations remains unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process consists of two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation process using a question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively assess the quality of extracted workflows, we introduce an automated agent and customer bots simulation framework that measures their effectiveness in resolving customer issues. Extensive experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT technique improves workflow extraction by 12.16\\% in average macro accuracy over the baseline. Moreover, our evaluation method closely aligns with human assessments, providing a reliable and scalable framework for future research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17321"
    },
    "bca4b1f6624b83e6b83c47eebd6f5952": {
        "title": "Structured Reasoning for Fairness: A Multi-Agent Approach to Bias Detection in Textual Data",
        "authors": [
            "Tianyi Huang",
            "Elsa Fan"
        ],
        "date": "2025/03/01",
        "pdf": "http://arxiv.org/pdf/2503.00355",
        "abstract": "From disinformation spread by AI chatbots to AI recommendations that inadvertently reinforce stereotypes, textual bias poses a significant challenge to the trustworthiness of large language models (LLMs). In this paper, we propose a multi-agent framework that systematically identifies biases by disentangling each statement as fact or opinion, assigning a bias intensity score, and providing concise, factual justifications. Evaluated on 1,500 samples from the WikiNPOV dataset, the framework achieves 84.9% accuracy$\\unicode{x2014}$an improvement of 13.0% over the zero-shot baseline$\\unicode{x2014}$demonstrating the efficacy of explicitly modeling fact versus opinion prior to quantifying bias intensity. By combining enhanced detection accuracy with interpretable explanations, this approach sets a foundation for promoting fairness and accountability in modern language models.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.00355"
    },
    "2433ebab4f4ee9f7d1255f99a80ace07": {
        "title": "Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with Query-Oriented Pivot Tasks",
        "authors": [
            "Zongru Wu",
            "Pengzhou Cheng",
            "Zheng Wu",
            "Tianjie Ju",
            "Zhuosheng Zhang",
            "Gongshen Liu"
        ],
        "date": "2025/03/01",
        "pdf": "http://arxiv.org/pdf/2503.00401",
        "abstract": "Perception-enhanced pre-training, particularly through grounding techniques, is widely adopted to enhance the performance of graphical user interface (GUI) agents. However, in resource-constrained scenarios, the format discrepancy between coordinate-oriented grounding and action-oriented reasoning limits the effectiveness of grounding for reasoning tasks. To address this challenge, we propose a query-oriented pivot approach called query inference, which serves as a bridge between GUI grounding and reasoning. By inferring potential user queries from a screenshot and its associated element coordinates, query inference improves the understanding of coordinates while aligning more closely with reasoning tasks. Experimental results show that query inference outperforms previous grounding techniques under the same training data scale. Notably, query inference achieves comparable or even better performance to large-scale grounding-enhanced OS-Atlas with less than 0.1% of training data. Furthermore, we explore the impact of reasoning formats and demonstrate that integrating additional semantic information into the input further boosts reasoning performance. The code is publicly available at https://github.com/ZrW00/GUIPivot.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.00401"
    },
    "a86efc9b62297533bb868f58f75d4758": {
        "title": "Improving Retrospective Language Agents via Joint Policy Gradient Optimization",
        "authors": [
            "Xueyang Feng",
            "Bo Lan",
            "Quanyu Dai",
            "Lei Wang",
            "Jiakai Tang",
            "Xu Chen",
            "Zhenhua Dong",
            "Ji-Rong Wen"
        ],
        "date": "2025/03/03",
        "pdf": "http://arxiv.org/pdf/2503.01490",
        "abstract": "In recent research advancements within the community, large language models (LLMs) have sparked great interest in creating autonomous agents. However, current prompt-based agents often heavily rely on large-scale LLMs. Meanwhile, although fine-tuning methods significantly enhance the capabilities of smaller LLMs, the fine-tuned agents often lack the potential for self-reflection and self-improvement. To address these challenges, we introduce a novel agent framework named RetroAct, which is a framework that jointly optimizes both task-planning and self-reflective evolution capabilities in language agents. Specifically, we develop a two-stage joint optimization process that integrates imitation learning and reinforcement learning, and design an off-policy joint policy gradient optimization algorithm with imitation learning regularization to enhance the data efficiency and training stability in agent tasks. RetroAct significantly improves the performance of open-source models, reduces dependency on closed-source LLMs, and enables fine-tuned agents to learn and evolve continuously. We conduct extensive experiments across various testing environments, demonstrating RetroAct has substantial improvements in task performance and decision-making processes.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.01490"
    },
    "a69dad462d5c2ed209611c5855132b06": {
        "title": "ATLaS: Agent Tuning via Learning Critical Steps",
        "authors": [
            "Zhixun Chen",
            "Ming Li",
            "Yuxuan Huang",
            "Yali Du",
            "Meng Fang",
            "Tianyi Zhou"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02197",
        "abstract": "Large Language Model (LLM) agents have demonstrated remarkable generalization capabilities across multi-domain tasks. Existing agent tuning approaches typically employ supervised finetuning on entire expert trajectories. However, behavior-cloning of full trajectories can introduce expert bias and weaken generalization to states not covered by the expert data. Additionally, critical steps, such as planning, complex reasoning for intermediate subtasks, and strategic decision-making, are essential to success in agent tasks, so learning these steps is the key to improving LLM agents. For more effective and efficient agent tuning, we propose ATLaS that identifies the critical steps in expert trajectories and finetunes LLMs solely on these steps with reduced costs. By steering the training&#39;s focus to a few critical steps, our method mitigates the risk of overfitting entire trajectories and promotes generalization across different environments and tasks. In extensive experiments, an LLM finetuned on only 30% critical steps selected by ATLaS outperforms the LLM finetuned on all steps and recent open-source LLM agents. ATLaS maintains and improves base LLM skills as generalist agents interacting with diverse environments.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02197"
    },
    "bf02b3247bcd402f5ac26cca0992eaa1": {
        "title": "Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent",
        "authors": [
            "Xingzuo Li",
            "Kehai Chen",
            "Yunfei Long",
            "Xuefeng Bai",
            "Yong Xu",
            "Min Zhang"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02519",
        "abstract": "Large language model (LLM) agents typically adopt a step-by-step reasoning framework, in which they interleave the processes of thinking and acting to accomplish the given task. However, this paradigm faces a deep-rooted one-pass issue whereby each generated intermediate thought is plugged into the trajectory regardless of its correctness, which can cause irreversible error propagation. To address the issue, this paper proposes a novel framework called Generator-Assistant Stepwise Rollback (GA-Rollback) to induce better decision-making for LLM agents. Particularly, GA-Rollback utilizes a generator to interact with the environment and an assistant to examine each action produced by the generator, where the assistant triggers a rollback operation upon detection of incorrect actions. Moreover, we introduce two additional strategies tailored for the rollback scenario to further improve its effectiveness. Extensive experiments show that GA-Rollback achieves significant improvements over several strong baselines on three widely used benchmarks. Our analysis further reveals that GA-Rollback can function as a robust plug-and-play module, integrating seamlessly with other methods.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02519"
    },
    "6c0c2a1aa25b10122be50884b469956a": {
        "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
        "authors": [
            "Weimin Xiong",
            "Yifan Song",
            "Qingxiu Dong",
            "Bingchan Zhao",
            "Feifan Song",
            "Xun Wang",
            "Sujian Li"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02682",
        "abstract": "Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent&#39;s task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02682"
    },
    "2a0ccb233db8bb358cf267a1d83129bb": {
        "title": "MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving",
        "authors": [
            "Ruida Wang",
            "Rui Pan",
            "Yuxin Li",
            "Jipeng Zhang",
            "Yizhen Jia",
            "Shizhe Diao",
            "Renjie Pi",
            "Junjie Hu",
            "Tong Zhang"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03205",
        "abstract": "Solving mathematical problems using computer-verifiable languages like Lean has significantly impacted mathematical and computer science communities. State-of-the-art methods utilize single Large Language Models (LLMs) as agents or provers to either generate complete proof or perform tree searches. However, single-agent methods inherently lack a structured way to combine high-level reasoning in Natural Language (NL) with Formal Language (FL) verification feedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought framework, (to the best of our knowledge), the first multi-agent framework for Lean4 theorem proving that balance high-level NL reasoning and FL verification in Long CoT. Using this structured interaction, our approach enables deeper insights and long-term coherence in proof generation, with which past methods struggle. We do this by leveraging emergent formal reasoning ability in Long CoT using our novel LoT-Transfer Learning training-inference pipeline. Extensive experiments show that our framework achieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset, largely outperforming GPT-4 (22.95%), single-agent tree search (InternLM-Step-Prover, 50.70%), and whole-proof generation (DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight the potential of combining Long CoT with formal verification for a more insightful generation in a broader perspective.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Math"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03205"
    },
    "91d1c46c1506ae6e222d3f66227a1cb5": {
        "title": "MAS-GPT: Training LLMs to Build LLM-based Multi-Agent Systems",
        "authors": [
            "Rui Ye",
            "Shuo Tang",
            "Rui Ge",
            "Yaxin Du",
            "Zhenfei Yin",
            "Siheng Chen",
            "Jing Shao"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03686",
        "abstract": "LLM-based multi-agent systems (MAS) have shown significant potential in tackling diverse tasks. However, to design effective MAS, existing approaches heavily rely on manual configurations or multiple calls of advanced LLMs, resulting in inadaptability and high inference costs. In this paper, we simplify the process of building an MAS by reframing it as a generative language task, where the input is a user query and the output is a corresponding MAS. To address this novel task, we unify the representation of MAS as executable code and propose a consistency-oriented data construction pipeline to create a high-quality dataset comprising coherent and consistent query-MAS pairs. Using this dataset, we train MAS-GPT, an open-source medium-sized LLM that is capable of generating query-adaptive MAS within a single LLM inference. The generated MAS can be seamlessly applied to process user queries and deliver high-quality responses. Extensive experiments on 9 benchmarks and 5 LLMs show that the proposed MAS-GPT consistently outperforms 10+ baseline MAS methods on diverse settings, indicating MAS-GPT&#39;s high effectiveness, efficiency and strong generalization ability. Code will be available at https://github.com/rui-ye/MAS-GPT.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03686"
    },
    "4fc071aaaffd1b4a23a5fdd07b601e42": {
        "title": "Measuring temporal effects of agent knowledge by date-controlled tool use",
        "authors": [
            "R. Patrick Xian",
            "Qiming Cui",
            "Stefan Bauer",
            "Reza Abbasi-Asl"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.04188",
        "abstract": "Temporal progression is an integral part of knowledge accumulation and update. Web search is frequently adopted as grounding for agent knowledge, yet its inappropriate configuration affects the quality of agent responses. Here, we construct a tool-based out-of-sample testing framework to measure the knowledge variability of large language model (LLM) agents from distinct date-controlled tools (DCTs). We demonstrate the temporal effects of an LLM agent as a writing assistant, which can use web search to help complete scientific publication abstracts. We show that temporal effects of the search engine translates into tool-dependent agent performance but can be alleviated with base model choice and explicit reasoning instructions such as chain-of-thought prompting. Our results indicate that agent evaluation should take a dynamical view and account for the temporal influence of tools and the updates of external resources.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04188"
    },
    "8eee22be7cee0f74a475f80af5cb6428": {
        "title": "PreMind: Multi-Agent Video Understanding for Advanced Indexing of Presentation-style Videos",
        "authors": [
            "Kangda Wei",
            "Zhengyu Zhou",
            "Bingqing Wang",
            "Jun Araki",
            "Lukas Lange",
            "Ruihong Huang",
            "Zhe Feng"
        ],
        "date": "2025/02/28",
        "pdf": "http://arxiv.org/pdf/2503.00162",
        "abstract": "In recent years, online lecture videos have become an increasingly popular resource for acquiring new knowledge. Systems capable of effectively understanding/indexing lecture videos are thus highly desirable, enabling downstream tasks like question answering to help users efficiently locate specific information within videos. This work proposes PreMind, a novel multi-agent multimodal framework that leverages various large models for advanced understanding/indexing of presentation-style videos. PreMind first segments videos into slide-presentation segments using a Vision-Language Model (VLM) to enhance modern shot-detection techniques. Each segment is then analyzed to generate multimodal indexes through three key steps: (1) extracting slide visual content, (2) transcribing speech narratives, and (3) consolidating these visual and speech contents into an integrated understanding. Three innovative mechanisms are also proposed to improve performance: leveraging prior lecture knowledge to refine visual understanding, detecting/correcting speech transcription errors using a VLM, and utilizing a critic agent for dynamic iterative self-reflection in vision analysis. Compared to traditional video indexing methods, PreMind captures rich, reliable multimodal information, allowing users to search for details like abbreviations shown only on slides. Systematic evaluations on the public LPM dataset and an internal enterprise dataset are conducted to validate PreMind&#39;s effectiveness, supported by detailed analyses.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.00162"
    },
    "2659d842cae295662ec22d68402d3e6b": {
        "title": "MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents",
        "authors": [
            "Kunlun Zhu",
            "Hongyi Du",
            "Zhaochen Hong",
            "Xiaocheng Yang",
            "Shuyi Guo",
            "Zhe Wang",
            "Zhenhailong Wang",
            "Cheng Qian",
            "Xiangru Tang",
            "Heng Ji",
            "Jiaxuan You"
        ],
        "date": "2025/03/03",
        "pdf": "http://arxiv.org/pdf/2503.01935",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities as autonomous agents, yet existing benchmarks either focus on single-agent tasks or are confined to narrow domains, failing to capture the dynamics of multi-agent coordination and competition. In this paper, we introduce MultiAgentBench, a comprehensive benchmark designed to evaluate LLM-based multi-agent systems across diverse, interactive scenarios. Our framework measures not only task completion but also the quality of collaboration and competition using novel, milestone-based key performance indicators. Moreover, we evaluate various coordination protocols (including star, chain, tree, and graph topologies) and innovative strategies such as group discussion and cognitive planning. Notably, gpt-4o-mini reaches the average highest task score, graph structure performs the best among coordination protocols in the research scenario, and cognitive planning improves milestone achievement rates by 3%. Code and datasets are public available at https://github.com/MultiagentBench/MARBLE.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.01935"
    },
    "65c60ff242d27a0ebd6709bae5e75ef2": {
        "title": "BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling",
        "authors": [
            "Hao Li",
            "Yu-Hao Huang",
            "Chang Xu",
            "Viktor Schlegel",
            "Ren-He Jiang",
            "Riza Batista-Navarro",
            "Goran Nenadic",
            "Jiang Bian"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02445",
        "abstract": "Time-series Generation (TSG) is a prominent research area with broad applications in simulations, data augmentation, and counterfactual analysis. While existing methods have shown promise in unconditional single-domain TSG, real-world applications demand for cross-domain approaches capable of controlled generation tailored to domain-specific constraints and instance-level requirements. In this paper, we argue that text can provide semantic insights, domain information and instance-specific temporal patterns, to guide and improve TSG. We introduce ``Text-Controlled TSG&#39;&#39;, a task focused on generating realistic time series by incorporating textual descriptions. To address data scarcity in this setting, we propose a novel LLM-based Multi-Agent framework that synthesizes diverse, realistic text-to-TS datasets. Furthermore, we introduce BRIDGE, a hybrid text-controlled TSG framework that integrates semantic prototypes with text description for supporting domain-level guidance. This approach achieves state-of-the-art generation fidelity on 11 of 12 datasets, and improves controllability by 12.52% on MSE and 6.34% MAE compared to no text input generation, highlighting its potential for generating tailored time-series data.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02445"
    },
    "8848ad517b3ee7778c1809830b35142f": {
        "title": "LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications",
        "authors": [
            "Danqing Zhang",
            "Balaji Rama",
            "Jingyi Ni",
            "Shiying He",
            "Fu Zhao",
            "Kunyu Chen",
            "Arnold Chen",
            "Junyu Cao"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.02950",
        "abstract": "We introduce LiteWebAgent, an open-source suite for VLM-based web agent applications. Our framework addresses a critical gap in the web agent ecosystem with a production-ready solution that combines minimal serverless backend configuration, intuitive user and browser interfaces, and extensible research capabilities in agent planning, memory, and tree search. For the core LiteWebAgent agent framework, we implemented a simple yet effective baseline using recursive function calling, providing with decoupled action generation and action grounding. In addition, we integrate advanced research components such as agent planning, agent workflow memory, and tree search in a modular and extensible manner. We then integrate the LiteWebAgent agent framework with frontend and backend as deployed systems in two formats: (1) a production Vercel-based web application, which provides users with an agent-controlled remote browser, (2) a Chrome extension leveraging LiteWebAgent&#39;s API to control an existing Chrome browser via CDP (Chrome DevTools Protocol). The LiteWebAgent framework is available at https://github.com/PathOnAI/LiteWebAgent, with deployed frontend at https://lite-web-agent.vercel.app/.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.02950"
    },
    "d7323d56eb84e078d18f7f314a606b04": {
        "title": "Unified Mind Model: Reimagining Autonomous Agents in the LLM Era",
        "authors": [
            "Pengbo Hu",
            "Xiang Ying"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03459",
        "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4), reviving the research of general autonomous agents with human-like cognitive abilities. Such human-level agents require semantic comprehension and instruction-following capabilities, which exactly fall into the strengths of LLMs. Although there have been several initial attempts to build human-level agents based on LLMs, the theoretical foundation remains a challenging open problem. In this paper, we propose a novel theoretical cognitive architecture, the Unified Mind Model (UMM), which offers guidance to facilitate the rapid creation of autonomous agents with human-level cognitive abilities. Specifically, our UMM starts with the global workspace theory and further leverage LLMs to enable the agent with various cognitive abilities, such as multi-modal perception, planning, reasoning, tool use, learning, memory, reflection and motivation. Building upon UMM, we then develop an agent-building engine, MindOS, which allows users to quickly create domain-/task-specific autonomous agents without any programming effort.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03459"
    },
    "b615ad93e66685fd45e5227addf662a6": {
        "title": "Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence",
        "authors": [
            "Cristian Jimenez-Romero",
            "Alper Yegenoglu",
            "Christian Blum"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.03800",
        "abstract": "This work examines the integration of large language models (LLMs) into multi-agent simulations by replacing the hard-coded programs of agents with LLM-driven prompts. The proposed approach is showcased in the context of two examples of complex systems from the field of swarm intelligence: ant colony foraging and bird flocking. Central to this study is a toolchain that integrates LLMs with the NetLogo simulation platform, leveraging its Python extension to enable communication with GPT-4o via the OpenAI API. This toolchain facilitates prompt-driven behavior generation, allowing agents to respond adaptively to environmental data. For both example applications mentioned above, we employ both structured, rule-based prompts and autonomous, knowledge-driven prompts. Our work demonstrates how this toolchain enables LLMs to study self-organizing processes and induce emergent behaviors within multi-agent environments, paving the way for new approaches to exploring intelligent systems and modeling swarm intelligence inspired by natural phenomena. We provide the code, including simulation files and data at https://github.com/crjimene/swarm_gpt.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.03800"
    },
    "3a433f91fa4ce9d60c858dbe2393bf8e": {
        "title": "Multi-Agent System for AI-Assisted Extraction of Narrative Arcs in TV Series",
        "authors": [
            "Roberto Balestri",
            "Guglielmo Pescatore"
        ],
        "date": "2025/03/04",
        "pdf": "http://arxiv.org/pdf/2503.04817",
        "abstract": "Serialized TV shows are built on complex storylines that can be hard to track and evolve in ways that defy straightforward analysis. This paper introduces a multi-agent system designed to extract and analyze these narrative arcs. Tested on the first season of Grey&#39;s Anatomy (ABC 2005-), the system identifies three types of arcs: Anthology (self-contained), Soap (relationship-focused), and Genre-Specific (strictly related to the series&#39; genre). Episodic progressions of these arcs are stored in both relational and semantic (vectorial) databases, enabling structured analysis and comparison. To bridge the gap between automation and critical interpretation, the system is paired with a graphical interface that allows for human refinement using tools to enhance and visualize the data. The system performed strongly in identifying Anthology Arcs and character entities, but its reliance on textual paratexts (such as episode summaries) revealed limitations in recognizing overlapping arcs and subtler dynamics. This approach highlights the potential of combining computational and human expertise in narrative analysis. Beyond television, it offers promise for serialized written formats, where the narrative resides entirely in the text. Future work will explore the integration of multimodal inputs, such as dialogue and visuals, and expand testing across a wider range of genres to refine the system further.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04817"
    },
    "93b0123769b5d1ad82af92e8ba53fbcc": {
        "title": "Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems",
        "authors": [
            "Mahfuz Ahmed Anik",
            "Abdur Rahman",
            "Azmine Toushik Wasi",
            "Md Manjurul Ahsan"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.04827",
        "abstract": "Language is a cornerstone of cultural identity, yet globalization and the dominance of major languages have placed nearly 3,000 languages at risk of extinction. Existing AI-driven translation models prioritize efficiency but often fail to capture cultural nuances, idiomatic expressions, and historical significance, leading to translations that marginalize linguistic diversity. To address these challenges, we propose a multi-agent AI framework designed for culturally adaptive translation in underserved language communities. Our approach leverages specialized agents for translation, interpretation, content synthesis, and bias evaluation, ensuring that linguistic accuracy and cultural relevance are preserved. Using CrewAI and LangChain, our system enhances contextual fidelity while mitigating biases through external validation. Comparative analysis shows that our framework outperforms GPT-4o, producing contextually rich and culturally embedded translations, a critical advancement for Indigenous, regional, and low-resource languages. This research underscores the potential of multi-agent AI in fostering equitable, sustainable, and culturally sensitive NLP technologies, aligning with the AI Governance, Cultural NLP, and Sustainable NLP pillars of Language Models for Underserved Communities. Our full experimental codebase is publicly available at: https://github.com/ciol-researchlab/Context-Aware_Translation_MAS",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04827"
    },
    "d10d79664866bf586b54857d871cdae1": {
        "title": "Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents",
        "authors": [
            "Jingying Zeng",
            "Hui Liu",
            "Zhenwei Dai",
            "Xianfeng Tang",
            "Chen Luo",
            "Samarth Varshney",
            "Zhen Li",
            "Qi He"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.04830",
        "abstract": "With the advancement of conversational large language models (LLMs), several LLM-based Conversational Shopping Agents (CSA) have been developed to help customers answer questions and smooth their shopping journey in e-commerce domain. The primary objective in building a trustworthy CSA is to ensure the agent&#39;s responses are accurate and factually grounded, which is essential for building customer trust and encouraging continuous engagement. However, two challenges remain. First, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk spreading misinformation and diminishing customer trust. Second, without providing knowledge source attribution in CSA response, customers struggle to verify LLM-generated information. To address these challenges, we present an easily productionized solution that enables a &#34;citation experience&#34; utilizing In-context Learning (ICL) and Multi-UX-Inference (MUI) to generate responses with citations to attribute its original sources without interfering other existing UX features. With proper UX design, these citation marks can be linked to the related product information and display the source to our customers. In this work, we also build auto-metrics and scalable benchmarks to holistically evaluate LLM&#39;s grounding and attribution capabilities. Our experiments demonstrate that incorporating this citation generation paradigm can substantially enhance the grounding of LLM responses by 13.83% on the real-world data. As such, our solution not only addresses the immediate challenges of LLM grounding issues but also adds transparency to conversational AI.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04830"
    },
    "2e343af2d215e8ed471330cb2f977c48": {
        "title": "Enhancing Collective Intelligence in Large Language Models Through Emotional Integration",
        "authors": [
            "Likith Kadiyala",
            "Ramteja Sajja",
            "Yusuf Sermet",
            "Ibrahim Demir"
        ],
        "date": "2025/03/05",
        "pdf": "http://arxiv.org/pdf/2503.04849",
        "abstract": "This research investigates the integration of emotional diversity into Large Language Models (LLMs) to enhance collective intelligence. Inspired by the human wisdom of crowds phenomenon, where group decisions often outperform individual judgments, we fine-tuned the DarkIdol-Llama-3.1-8B model using Google&#39;s GoEmotions dataset and Low-Rank Adaptation (LoRA) to simulate emotionally diverse responses. Evaluating the model on a distance estimation task between Fargo, ND, and Seattle, WA, across 15,064 unique persona configurations, we analyzed how emotional states and social attributes influence decision-making. Our findings demonstrate that emotional integration shapes response patterns while maintaining acceptable prediction accuracy, revealing its potential to enhance artificial collective intelligence. This study provides valuable insights into the interplay of emotional diversity and decision-making in LLMs, suggesting pathways for creating emotionally aware AI systems that balance emotional depth with analytical precision.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04849"
    },
    "5f941837291f315e0572978ad8229411": {
        "title": "VQEL: Enabling Self-Developed Symbolic Language in Agents through Vector Quantization in Emergent Language Games",
        "authors": [
            "Mohammad Mahdi Samiei Paqaleh",
            "Mahdieh Soleymani Baghshah"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.04940",
        "abstract": "In the field of emergent language, efforts have traditionally focused on developing communication protocols through interactions between agents in referential games. However, the aspect of internal language learning, where language serves not only as a communicative tool with others but also as a means for individual thinking, self-reflection, and problem-solving remains underexplored. Developing a language through self-play, without another agent&#39;s involvement, poses a unique challenge. It requires an agent to craft symbolic representations and train them using direct gradient methods. The challenge here is that if an agent attempts to learn symbolic representations through self-play using conventional modeling and techniques such as REINFORCE, the solution will offer no advantage over previous multi-agent approaches. We introduce VQEL, a novel method that incorporates Vector Quantization into the agents&#39; architecture, enabling them to autonomously invent and develop discrete symbolic representations in a self-play referential game. Following the self-play phase, agents can enhance their language through reinforcement learning and interactions with other agents in the mutual-play phase. Our experiments across various datasets demonstrate that VQEL not only outperforms the traditional REINFORCE method but also benefits from improved control and reduced susceptibility to collapse, thanks to the incorporation of vector quantization.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04940"
    },
    "84b36d72f7cbe637c81e22714582d943": {
        "title": "MM-StoryAgent: Immersive Narrated Storybook Video Generation with a Multi-Agent Paradigm across Text, Image and Audio",
        "authors": [
            "Xuenan Xu",
            "Jiahao Mei",
            "Chenliang Li",
            "Yuning Wu",
            "Ming Yan",
            "Shaopeng Lai",
            "Ji Zhang",
            "Mengyue Wu"
        ],
        "date": "2025/03/07",
        "pdf": "http://arxiv.org/pdf/2503.05242",
        "abstract": "The rapid advancement of large language models (LLMs) and artificial intelligence-generated content (AIGC) has accelerated AI-native applications, such as AI-based storybooks that automate engaging story production for children. However, challenges remain in improving story attractiveness, enriching storytelling expressiveness, and developing open-source evaluation benchmarks and frameworks. Therefore, we propose and opensource MM-StoryAgent, which creates immersive narrated video storybooks with refined plots, role-consistent images, and multi-channel audio. MM-StoryAgent designs a multi-agent framework that employs LLMs and diverse expert tools (generative models and APIs) across several modalities to produce expressive storytelling videos. The framework enhances story attractiveness through a multi-stage writing pipeline. In addition, it improves the immersive storytelling experience by integrating sound effects with visual, music and narrative assets. MM-StoryAgent offers a flexible, open-source platform for further development, where generative modules can be substituted. Both objective and subjective evaluation regarding textual story quality and alignment between modalities validate the effectiveness of our proposed MM-StoryAgent system. The demo and source code are available.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.05242"
    },
    "4f1fa0b4324132fd8d732715647f5513": {
        "title": "GEMA-Score: Granular Explainable Multi-Agent Score for Radiology Report Evaluation",
        "authors": [
            "Zhenxuan Zhang",
            "Kinhei Lee",
            "Weihang Deng",
            "Huichi Zhou",
            "Zihao Jin",
            "Jiahao Huang",
            "Zhifan Gao",
            "Dominic C Marshall",
            "Yingying Fang",
            "Guang Yang"
        ],
        "date": "2025/03/07",
        "pdf": "http://arxiv.org/pdf/2503.05347",
        "abstract": "Automatic medical report generation supports clinical diagnosis, reduces the workload of radiologists, and holds the promise of improving diagnosis consistency. However, existing evaluation metrics primarily assess the accuracy of key medical information coverage in generated reports compared to human-written reports, while overlooking crucial details such as the location and certainty of reported abnormalities. These limitations hinder the comprehensive assessment of the reliability of generated reports and pose risks in their selection for clinical use. Therefore, we propose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both objective quantification and subjective evaluation through a large language model-based multi-agent workflow. Our GEMA-Score parses structured reports and employs NER-F1 calculations through interactive exchanges of information among agents to assess disease diagnosis, location, severity, and uncertainty. Additionally, an LLM-based scoring agent evaluates completeness, readability, and clinical terminology while providing explanatory feedback. Extensive experiments validate that GEMA-Score achieves the highest correlation with human expert evaluations on a public dataset, demonstrating its effectiveness in clinical scoring (Kendall coefficient = 0.70 for Rexval dataset and Kendall coefficient = 0.54 for RadEvalX dataset). The anonymous project demo is available at: https://github.com/Zhenxuan-Zhang/GEMA_score.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ],
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.05347"
    },
    "1b886b0c530807ee9ffd2e564d21571c": {
        "title": "DatawiseAgent: A Notebook-Centric LLM Agent Framework for Automated Data Science",
        "authors": [
            "Ziming You",
            "Yumiao Zhang",
            "Dexuan Xu",
            "Yiwei Lou",
            "Yandong Yan",
            "Wei Wang",
            "Huaming Zhang",
            "Yu Huang"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07044",
        "abstract": "Data Science tasks are multifaceted, dynamic, and often domain-specific. Existing LLM-based approaches largely concentrate on isolated phases, neglecting the interdependent nature of many data science tasks and limiting their capacity for comprehensive end-to-end support. We propose DatawiseAgent, a notebook-centric LLM agent framework that unifies interactions among user, agent and the computational environment through markdown and executable code cells, supporting flexible and adaptive automated data science. Built on a Finite State Transducer(FST), DatawiseAgent orchestrates four stages, including DSF-like planning, incremental execution, self-debugging, and post-filtering. Specifically, the DFS-like planning stage systematically explores the solution space, while incremental execution harnesses real-time feedback and accommodates LLM&#39;s limited capabilities to progressively complete tasks. The self-debugging and post-filtering modules further enhance reliability by diagnosing and correcting errors and pruning extraneous information. Extensive experiments on diverse tasks, including data analysis, visualization, and data modeling, show that DatawiseAgent consistently outperforms or matches state-of-the-art methods across multiple model settings. These results highlight its potential to generalize across data science scenarios and lay the groundwork for more efficient, fully automated workflows.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07044"
    },
    "dd09b677a3dcc943afe402930cfd15a3": {
        "title": "ASTRA: A Negotiation Agent with Adaptive and Strategic Reasoning through Action in Dynamic Offer Optimization",
        "authors": [
            "Deuksin Kwon",
            "Jiwon Hae",
            "Emma Clift",
            "Daniel Shamsoddini",
            "Jonathan Gratch",
            "Gale M. Lucas"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07129",
        "abstract": "Negotiation requires dynamically balancing self-interest and cooperation to maximize one&#39;s own utility. Yet, existing agents struggle due to bounded rationality in human data, low adaptability to counterpart behavior, and limited strategic reasoning. To address this, we introduce principle-driven negotiation agents, powered by ASTRA, a novel framework for turn-level offer optimization grounded in two core principles: opponent modeling and Tit-for-Tat reciprocity. ASTRA operates in three stages: (1) interpreting counterpart behavior, (2) optimizing counteroffers via a linear programming (LP) solver, and (3) selecting offers based on negotiation tactics and the partner&#39;s acceptance probability. Through simulations and human evaluations, our agent effectively adapts to an opponent&#39;s shifting stance and achieves favorable outcomes through enhanced adaptability and strategic reasoning. Beyond improving negotiation performance, it also serves as a powerful coaching tool, offering interpretable strategic feedback and optimal offer recommendations.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07129"
    },
    "d45b96c8298e8580d4a8a23840b54b75": {
        "title": "MedAgentsBench: Benchmarking Thinking Models and Agent Frameworks for Complex Medical Reasoning",
        "authors": [
            "Xiangru Tang",
            "Daniel Shao",
            "Jiwoong Sohn",
            "Jiapeng Chen",
            "Jiayi Zhang",
            "Jinyu Xiang",
            "Fang Wu",
            "Yilun Zhao",
            "Chenglin Wu",
            "Wenqi Shi",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07459",
        "abstract": "Large Language Models (LLMs) have shown impressive performance on existing medical question-answering benchmarks. This high performance makes it increasingly difficult to meaningfully evaluate and differentiate advanced methods. We present MedAgentsBench, a benchmark that focuses on challenging medical questions requiring multi-step clinical reasoning, diagnosis formulation, and treatment planning-scenarios where current models still struggle despite their strong performance on standard tests. Drawing from seven established medical datasets, our benchmark addresses three key limitations in existing evaluations: (1) the prevalence of straightforward questions where even base models achieve high performance, (2) inconsistent sampling and evaluation protocols across studies, and (3) lack of systematic analysis of the interplay between performance, cost, and inference time. Through experiments with various base models and reasoning methods, we demonstrate that the latest thinking models, DeepSeek R1 and OpenAI o3, exhibit exceptional performance in complex medical reasoning tasks. Additionally, advanced search-based agent methods offer promising performance-to-cost ratios compared to traditional approaches. Our analysis reveals substantial performance gaps between model families on complex questions and identifies optimal model selections for different computational constraints. Our benchmark and evaluation framework are publicly available at https://github.com/gersteinlab/medagents-benchmark.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07459"
    },
    "bdc771b5efeac8623a2dd2e1d6c7bdb9": {
        "title": "In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents",
        "authors": [
            "Zhen Tan",
            "Jun Yan",
            "I-Hung Hsu",
            "Rujun Han",
            "Zifeng Wang",
            "Long T. Le",
            "Yiwen Song",
            "Yanfei Chen",
            "Hamid Palangi",
            "George Lee",
            "Anand Iyer",
            "Tianlong Chen",
            "Huan Liu",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "date": "2025/03/11",
        "pdf": "http://arxiv.org/pdf/2503.08026",
        "abstract": "Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities-utterances, turns, and sessions-into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs&#39; cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08026"
    },
    "7589557b1ebf73ece2a977f592b10c6a": {
        "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews",
        "authors": [
            "Xian Gao",
            "Jiacheng Ruan",
            "Jingsheng Gao",
            "Ting Liu",
            "Yuzhuo Fu"
        ],
        "date": "2025/03/11",
        "pdf": "http://arxiv.org/pdf/2503.08506",
        "abstract": "Academic paper review is a critical yet time-consuming task within the research community. With the increasing volume of academic publications, automating the review process has become a significant challenge. The primary issue lies in generating comprehensive, accurate, and reasoning-consistent review comments that align with human reviewers&#39; judgments. In this paper, we address this challenge by proposing ReviewAgents, a framework that leverages large language models (LLMs) to generate academic paper reviews. We first introduce a novel dataset, Review-CoT, consisting of 142k review comments, designed for training LLM agents. This dataset emulates the structured reasoning process of human reviewers-summarizing the paper, referencing relevant works, identifying strengths and weaknesses, and generating a review conclusion. Building upon this, we train LLM reviewer agents capable of structured reasoning using a relevant-paper-aware training method. Furthermore, we construct ReviewAgents, a multi-role, multi-LLM agent review framework, to enhance the review comment generation process. Additionally, we propose ReviewBench, a benchmark for evaluating the review comments generated by LLMs. Our experimental results on ReviewBench demonstrate that while existing LLMs exhibit a certain degree of potential for automating the review process, there remains a gap when compared to human-generated reviews. Moreover, our ReviewAgents framework further narrows this gap, outperforming advanced LLMs in generating review comments.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08506"
    },
    "5eea5e3f0e1815a1338d740b2ae9ad71": {
        "title": "AgentOrca: A Dual-System Framework to Evaluate Language Agents on Operational Routine and Constraint Adherence",
        "authors": [
            "Zekun Li",
            "Shinda Huang",
            "Jiangtian Wang",
            "Nathan Zhang",
            "Antonis Antoniades",
            "Wenyue Hua",
            "Kaijie Zhu",
            "Sirui Zeng",
            "William Yang Wang",
            "Xifeng Yan"
        ],
        "date": "2025/03/11",
        "pdf": "http://arxiv.org/pdf/2503.08669",
        "abstract": "As language agents progressively automate critical tasks across domains, their ability to operate within operational constraints and safety protocols becomes essential. While extensive research has demonstrated these agents&#39; effectiveness in downstream task completion, their reliability in following operational procedures and constraints remains largely unexplored. To this end, we present AgentOrca, a dual-system framework for evaluating language agents&#39; compliance with operational constraints and routines. Our framework encodes action constraints and routines through both natural language prompts for agents and corresponding executable code serving as ground truth for automated verification. Through an automated pipeline of test case generation and evaluation across five real-world domains, we quantitatively assess current language agents&#39; adherence to operational constraints. Our findings reveal notable performance gaps among state-of-the-art models, with large reasoning models like o1 demonstrating superior compliance while others show significantly lower performance, particularly when encountering complex constraints or user persuasion attempts.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08669"
    },
    "71e6a4bac3df7dda75ae7d06c814f628": {
        "title": "Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions",
        "authors": [
            "Mourad Gridach",
            "Jay Nanavati",
            "Khaldoun Zine El Abidine",
            "Lenon Mendes",
            "Christina Mack"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.08979",
        "abstract": "The integration of Agentic AI into scientific discovery marks a new frontier in research automation. These AI systems, capable of reasoning, planning, and autonomous decision-making, are transforming how scientists perform literature review, generate hypotheses, conduct experiments, and analyze results. This survey provides a comprehensive overview of Agentic AI for scientific discovery, categorizing existing systems and tools, and highlighting recent progress across fields such as chemistry, biology, and materials science. We discuss key evaluation metrics, implementation frameworks, and commonly used datasets to offer a detailed understanding of the current state of the field. Finally, we address critical challenges, such as literature review automation, system reliability, and ethical concerns, while outlining future research directions that emphasize human-AI collaboration and enhanced system calibration.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.08979"
    },
    "82a101ffb49d4dcadd90c77b90c3de64": {
        "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
        "authors": [
            "Lutfi Eren Erdogan",
            "Nicholas Lee",
            "Sehoon Kim",
            "Suhong Moon",
            "Hiroki Furuta",
            "Gopala Anumanchipalli",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09572",
        "abstract": "Large language models (LLMs) have shown remarkable advancements in enabling language agents to tackle simple tasks. However, applying them for complex, multi-step, long-horizon tasks remains a challenge. Recent work have found success by separating high-level planning from low-level execution, which enables the model to effectively balance high-level planning objectives and low-level execution details. However, generating accurate plans remains difficult since LLMs are not inherently trained for this task. To address this, we propose Plan-and-Act, a novel framework that incorporates explicit planning into LLM-based agents and introduces a scalable method to enhance plan generation through a novel synthetic data generation method. Plan-and-Act consists of a Planner model which generates structured, high-level plans to achieve user goals, and an Executor model that translates these plans into environment-specific actions. To train the Planner effectively, we introduce a synthetic data generation method that annotates ground-truth trajectories with feasible plans, augmented with diverse and extensive examples to enhance generalization. We evaluate Plan-and-Act using web navigation as a representative long-horizon planning environment, demonstrating a state-of the-art 54% success rate on the WebArena-Lite benchmark.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09572"
    },
    "3a03c1c0ddb611408734756cc711da9b": {
        "title": "Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents",
        "authors": [
            "Dongjun Lee",
            "Juyong Lee",
            "Kyuyoung Kim",
            "Jihoon Tack",
            "Jinwoo Shin",
            "Yee Whye Teh",
            "Kimin Lee"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.10689",
        "abstract": "Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 15.6%, and demonstrates a 23.7% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark. Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts. The relevant code materials are available at our project page: https://lcowiclr2025.github.io.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.10689"
    },
    "e22446cf9c1268138eb33d5b0fcae7c2": {
        "title": "DeskVision: Large Scale Desktop Region Captioning for Advanced GUI Agents",
        "authors": [
            "Yibin Xu",
            "Liang Yang",
            "Hao Chen",
            "Hua Wang",
            "Zhi Chen",
            "Yaohua Tang"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11170",
        "abstract": "The limitation of graphical user interface (GUI) data has been a significant barrier to the development of GUI agents today, especially for the desktop / computer use scenarios. To address this, we propose an automated GUI data generation pipeline, AutoCaptioner, which generates data with rich descriptions while minimizing human effort. Using AutoCaptioner, we created a novel large-scale desktop GUI dataset, DeskVision, along with the largest desktop test benchmark, DeskVision-Eval, which reflects daily usage and covers diverse systems and UI elements, each with rich descriptions. With DeskVision, we train a new GUI understanding model, GUIExplorer. Results show that GUIExplorer achieves state-of-the-art (SOTA) performance in understanding/grounding visual elements without the need for complex architectural designs. We further validated the effectiveness of the DeskVision dataset through ablation studies on various large visual language models (LVLMs). We believe that AutoCaptioner and DeskVision will significantly advance the development of GUI agents, and will open-source them for the community.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11170"
    },
    "f34ab1d71cd38e25cc1f194db27e012d": {
        "title": "GNNs as Predictors of Agentic Workflow Performances",
        "authors": [
            "Yuanshuo Zhang",
            "Yuchen Hou",
            "Bohan Tang",
            "Shuo Chen",
            "Muhan Zhang",
            "Xiaowen Dong",
            "Siheng Chen"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11301",
        "abstract": "Agentic workflows invoked by Large Language Models (LLMs) have achieved remarkable success in handling complex tasks. However, optimizing such workflows is costly and inefficient in real-world applications due to extensive invocations of LLMs. To fill this gap, this position paper formulates agentic workflows as computational graphs and advocates Graph Neural Networks (GNNs) as efficient predictors of agentic workflow performances, avoiding repeated LLM invocations for evaluation. To empirically ground this position, we construct FLORA-Bench, a unified platform for benchmarking GNNs for predicting agentic workflow performances. With extensive experiments, we arrive at the following conclusion: GNNs are simple yet effective predictors. This conclusion supports new applications of GNNs and a novel direction towards automating agentic workflow optimization. All codes, models, and data are available at https://github.com/youngsoul0731/Flora-Bench.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11301"
    },
    "f37f12bbfd5a31aee1107ecdcc90226b": {
        "title": "AIstorian lets AI be a historian: A KG-powered multi-agent system for accurate biography generation",
        "authors": [
            "Fengyu Li",
            "Yilin Li",
            "Junhao Zhu",
            "Lu Chen",
            "Yanfei Zhang",
            "Jia Zhou",
            "Hui Zu",
            "Jingwen Zhao",
            "Yunjun Gao"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11346",
        "abstract": "Huawei has always been committed to exploring the AI application in historical research. Biography generation, as a specialized form of abstractive summarization, plays a crucial role in historical research but faces unique challenges that existing large language models (LLMs) struggle to address. These challenges include maintaining stylistic adherence to historical writing conventions, ensuring factual fidelity, and handling fragmented information across multiple documents. We present AIstorian, a novel end-to-end agentic system featured with a knowledge graph (KG)-powered retrieval-augmented generation (RAG) and anti-hallucination multi-agents. Specifically, AIstorian introduces an in-context learning based chunking strategy and a KG-based index for accurate and efficient reference retrieval. Meanwhile, AIstorian orchestrates multi-agents to conduct on-the-fly hallucination detection and error-type-aware correction. Additionally, to teach LLMs a certain language style, we finetune LLMs based on a two-step training approach combining data augmentation-enhanced supervised fine-tuning with stylistic preference optimization. Extensive experiments on a real-life historical Jinshi dataset demonstrate that AIstorian achieves a 3.8x improvement in factual accuracy and a 47.6% reduction in hallucination rate compared to existing baselines. The data and code are available at: https://github.com/ZJU-DAILY/AIstorian.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11346"
    },
    "3b472d8f1d74627b8a4b76905acf9de6": {
        "title": "Large Reasoning Models in Agent Scenarios: Exploring the Necessity of Reasoning Capabilities",
        "authors": [
            "Xueyang Zhou",
            "Guiyao Tie",
            "Guowen Zhang",
            "Weidong Wang",
            "Zhigang Zuo",
            "Di Wu",
            "Duanfeng Chu",
            "Pan Zhou",
            "Lichao Sun",
            "Neil Zhenqiang Gong"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11074",
        "abstract": "The rise of Large Reasoning Models (LRMs) signifies a paradigm shift toward advanced computational reasoning. Yet, this progress disrupts traditional agent frameworks, traditionally anchored by execution-oriented Large Language Models (LLMs). To explore this transformation, we propose the LaRMA framework, encompassing nine tasks across Tool Usage, Plan Design, and Problem Solving, assessed with three top LLMs (e.g., Claude3.5-sonnet) and five leading LRMs (e.g., DeepSeek-R1). Our findings address four research questions: LRMs surpass LLMs in reasoning-intensive tasks like Plan Design, leveraging iterative reflection for superior outcomes; LLMs excel in execution-driven tasks such as Tool Usage, prioritizing efficiency; hybrid LLM-LRM configurations, pairing LLMs as actors with LRMs as reflectors, optimize agent performance by blending execution speed with reasoning depth; and LRMs&#39; enhanced reasoning incurs higher computational costs, prolonged processing, and behavioral challenges, including overthinking and fact-ignoring tendencies. This study fosters deeper inquiry into LRMs&#39; balance of deep thinking and overthinking, laying a critical foundation for future agent design advancements.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11074"
    },
    "68afc1df36640456afee26fa41632e72": {
        "title": "Cerebrum (AIOS SDK): A Platform for Agent Development, Deployment, Distribution, and Discovery",
        "authors": [
            "Balaji Rama",
            "Kai Mei",
            "Yongfeng Zhang"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11444",
        "abstract": "Autonomous LLM-based agents have emerged as a powerful paradigm for complex task execution, yet the field lacks standardized tools for development, deployment, distribution and discovery of agents. We present Cerebrum, an Agent SDK for AIOS that addresses this gap through three key components: (1) a comprehensive SDK featuring a modular four-layer architecture for agent development, encompassing LLM, memory, storage, and tool management; (2) a community-driven Agent Hub for sharing and discovering agents, complete with version control and dependency management; (3) an interactive web interface for testing and evaluating agents. The platform&#39;s effectiveness is demonstrated through implementations of various agent architectures, including Chain of Thought (CoT), ReAct, and tool-use agents. Cerebrum advances the field by providing a unified framework that standardizes agent development while maintaining flexibility for researchers and developers to innovate and distribute their agents. The live website is at https://app.aios.foundation, the code is at https://github.com/agiresearch/Cerebrum, and video is at https://app.aios.foundation/video-demo.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11444"
    },
    "e4a9de64131ed45863b1bbf22aff2382": {
        "title": "Prompt Injection Detection and Mitigation via AI Multi-Agent NLP Frameworks",
        "authors": [
            "Diego Gosmar",
            "Deborah A. Dahl",
            "Dario Gosmar"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11517",
        "abstract": "Prompt injection constitutes a significant challenge for generative AI systems by inducing unintended outputs. We introduce a multi-agent NLP framework specifically designed to address prompt injection vulnerabilities through layered detection and enforcement mechanisms. The framework orchestrates specialized agents for generating responses, sanitizing outputs, and enforcing policy compliance. Evaluation on 500 engineered injection prompts demonstrates a marked reduction in injection success and policy breaches. Novel metrics, including Injection Success Rate (ISR), Policy Override Frequency (POF), Prompt Sanitization Rate (PSR), and Compliance Consistency Score (CCS), are proposed to derive a composite Total Injection Vulnerability Score (TIVS). The system utilizes the OVON (Open Voice Network) framework for inter-agent communication via structured JSON messages, extending a previously established multi-agent architecture from hallucination mitigation to address the unique challenges of prompt injection.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11517"
    },
    "7f91926edeb6ef52ee6dae176a2bd7fa": {
        "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation",
        "authors": [
            "Kaiyuan Liu",
            "Youcheng Pan",
            "Jing Li",
            "Daojing He",
            "Yang Xiang",
            "Yexing Du",
            "Tianrun Gao"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07010",
        "abstract": "Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users&#39; perspective, and also lack the explainability of the results of LLM agents&#39; code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation&#39;s automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07010"
    },
    "0b0222a69bb4d3e9734f347c4889a86d": {
        "title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code",
        "authors": [
            "Dhruv Gautam",
            "Spandan Garg",
            "Jinu Jang",
            "Neel Sundaresan",
            "Roshanak Zilouchian Moghaddam"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07832",
        "abstract": "Recent advances in language model (LM) agents and function calling have enabled autonomous, feedback-driven systems to solve problems across various digital domains. To better understand the unique limitations of LM agents, we introduce RefactorBench, a benchmark consisting of 100 large handcrafted multi-file refactoring tasks in popular open-source repositories. Solving tasks within RefactorBench requires thorough exploration of dependencies across multiple files and strong adherence to relevant instructions. Every task is defined by 3 natural language instructions of varying specificity and is mutually exclusive, allowing for the creation of longer combined tasks on the same repository. Baselines on RefactorBench reveal that current LM agents struggle with simple compositional tasks, solving only 22% of tasks with base instructions, in contrast to a human developer with short time constraints solving 87%. Through trajectory analysis, we identify various unique failure modes of LM agents, and further explore the failure mode of tracking past actions. By adapting a baseline agent to condition on representations of state, we achieve a 43.9% improvement in solving RefactorBench tasks. We further extend our state-aware approach to encompass entire digital environments and outline potential directions for future research. RefactorBench aims to support the study of LM agents by providing a set of real-world, multi-hop tasks within the realm of code.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07832"
    },
    "fe786f6903fcdf3bb37e717b5d479c9f": {
        "title": "BEARCUBS: A benchmark for computer-using web agents",
        "authors": [
            "Yixiao Song",
            "Katherine Thai",
            "Chau Minh Pham",
            "Yapei Chang",
            "Mazin Nadaf",
            "Mohit Iyyer"
        ],
        "date": "2025/03/10",
        "pdf": "http://arxiv.org/pdf/2503.07919",
        "abstract": "Modern web agents possess computer use abilities that allow them to interact with webpages by sending commands to a virtual keyboard and mouse. While such agents have considerable potential to assist human users with complex tasks, evaluating their capabilities in real-world settings poses a major challenge. To this end, we introduce BEARCUBS, a &#34;small but mighty&#34; benchmark of 111 information-seeking questions designed to evaluate a web agent&#39;s ability to search, browse, and identify factual information from the web. Unlike prior web agent benchmarks, solving BEARCUBS requires (1) accessing live web content rather than synthetic or simulated pages, which captures the unpredictability of real-world web interactions; and (2) performing a broad range of multimodal interactions (e.g., video understanding, 3D navigation) that cannot be bypassed via text-based workarounds. Each question in BEARCUBS has a corresponding short, unambiguous answer and a human-validated browsing trajectory, allowing for transparent evaluation of agent performance and strategies. A human study confirms that BEARCUBS questions are solvable but non-trivial (84.7% human accuracy), revealing search inefficiencies and domain knowledge gaps as common failure points. By contrast, state-of-the-art computer-using agents underperform, with the best-scoring system (OpenAI&#39;s Operator) reaching only 24.3% accuracy. These results highlight critical areas for improvement, including reliable source selection and more powerful multimodal capabilities. To facilitate future research, BEARCUBS will be updated periodically to replace invalid or contaminated questions, keeping the benchmark fresh for future generations of web agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.07919"
    },
    "2808666d6676ab9e856d077010922df3": {
        "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
        "authors": [
            "Zhaoling Chen",
            "Xiangru Tang",
            "Gangda Deng",
            "Fang Wu",
            "Jialong Wu",
            "Zhiwei Jiang",
            "Viktor Prasanna",
            "Arman Cohan",
            "Xingyao Wang"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09089",
        "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09089"
    },
    "c7938c2a855107580bff21151d018254": {
        "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning",
        "authors": [
            "Ziyu Wan",
            "Yunxiang Li",
            "Yan Song",
            "Hanjing Wang",
            "Linyi Yang",
            "Mark Schmidt",
            "Jun Wang",
            "Weinan Zhang",
            "Shuyue Hu",
            "Ying Wen"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09501",
        "abstract": "Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking -- enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Experimental results demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09501"
    },
    "bac6b47f3aaeaab4718b9606271c4ce4": {
        "title": "Factorio Learning Environment",
        "authors": [
            "Jack Hopkins",
            "Mart Bakler",
            "Akbir Khan"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.09617",
        "abstract": "Large Language Models (LLMs) are rapidly saturating existing benchmarks, necessitating new open-ended evaluations. We introduce the Factorio Learning Environment (FLE), based on the game of Factorio, that tests agents in long-term planning, program synthesis, and resource optimization. FLE provides exponentially scaling challenges -- from basic automation to complex factories processing millions of resource units per second. We provide two settings: (1) lab-play consisting of eight structured tasks with fixed resources, and (2) open-play with the unbounded task of building the largest factory on an procedurally generated map. We demonstrate across both settings that models still lack strong spatial reasoning. In lab-play, we find that LLMs exhibit promising short-horizon skills, yet are unable to operate effectively in constrained environments, reflecting limitations in error analysis. In open-play, while LLMs discover automation strategies that improve growth (e.g electric-powered drilling), they fail to achieve complex automation (e.g electronic-circuit manufacturing).",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09617"
    },
    "56ff3940cfa84ad8b291b9e7c49f0625": {
        "title": "Can A Society of Generative Agents Simulate Human Behavior and Inform Public Health Policy? A Case Study on Vaccine Hesitancy",
        "authors": [
            "Abe Bohan Hou",
            "Hongru Du",
            "Yichen Wang",
            "Jingyu Zhang",
            "Zixiao Wang",
            "Paul Pu Liang",
            "Daniel Khashabi",
            "Lauren Gardner",
            "Tianxing He"
        ],
        "date": "2025/03/12",
        "pdf": "http://arxiv.org/pdf/2503.09639",
        "abstract": "Can we simulate a sandbox society with generative agents to model human behavior, thereby reducing the over-reliance on real human trials for assessing public policies? In this work, we investigate the feasibility of simulating health-related decision-making, using vaccine hesitancy, defined as the delay in acceptance or refusal of vaccines despite the availability of vaccination services (MacDonald, 2015), as a case study. To this end, we introduce the VacSim framework with 100 generative agents powered by Large Language Models (LLMs). VacSim simulates vaccine policy outcomes with the following steps: 1) instantiate a population of agents with demographics based on census data; 2) connect the agents via a social network and model vaccine attitudes as a function of social dynamics and disease-related information; 3) design and evaluate various public health interventions aimed at mitigating vaccine hesitancy. To align with real-world results, we also introduce simulation warmup and attitude modulation to adjust agents&#39; attitudes. We propose a series of evaluations to assess the reliability of various LLM simulations. Experiments indicate that models like Llama and Qwen can simulate aspects of human behavior but also highlight real-world alignment challenges, such as inconsistent responses with demographic profiles. This early exploration of LLM-driven simulations is not meant to serve as definitive policy guidance; instead, it serves as a call for action to examine social simulation for policy development.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.09639"
    },
    "18eafd063c50a85cd349cdabfd081a6b": {
        "title": "CoLLMLight: Cooperative Large Language Model Agents for Network-Wide Traffic Signal Control",
        "authors": [
            "Zirui Yuan",
            "Siqi Lai",
            "Hao Liu"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11739",
        "abstract": "Traffic Signal Control (TSC) plays a critical role in urban traffic management by optimizing traffic flow and mitigating congestion. While Large Language Models (LLMs) have recently emerged as promising tools for TSC due to their exceptional problem-solving and generalization capabilities, existing approaches fail to address the essential need for inter-agent coordination, limiting their effectiveness in achieving network-wide optimization. To bridge this gap, we propose CoLLMLight, a cooperative LLM agent framework for TSC. Specifically, we first construct a structured spatiotemporal graph to capture real-time traffic dynamics and spatial relationships among neighboring intersections, enabling the LLM to reason about complex traffic interactions. Moreover, we introduce a complexity-aware reasoning mechanism that dynamically adapts reasoning depth based on real-time traffic conditions, ensuring optimal computational efficiency without sacrificing decision quality. Besides, we propose a fine-tuning strategy that leverages iterative simulation-driven data collection and environmental feedback to build a lightweight LLM tailored for cooperative TSC. Extensive experiments on both synthetic and real-world datasets demonstrate that CoLLMLight outperforms state-of-the-art methods in diverse traffic scenarios, showcasing its effectiveness, scalability, and robustness.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11739"
    },
    "c6dfccc19688759367c8142f5669e60d": {
        "title": "VeriLA: A Human-Centered Evaluation Framework for Interpretable Verification of LLM Agent Failures",
        "authors": [
            "Yoo Yeon Sung",
            "Hannah Kim",
            "Dan Zhang"
        ],
        "date": "2025/03/16",
        "pdf": "http://arxiv.org/pdf/2503.12651",
        "abstract": "AI practitioners increasingly use large language model (LLM) agents in compound AI systems to solve complex reasoning tasks, these agent executions often fail to meet human standards, leading to errors that compromise the system&#39;s overall performance. Addressing these failures through human intervention is challenging due to the agents&#39; opaque reasoning processes, misalignment with human expectations, the complexity of agent dependencies, and the high cost of manual inspection. This paper thus introduces a human-centered evaluation framework for Verifying LLM Agent failures (VeriLA), which systematically assesses agent failures to reduce human effort and make these agent failures interpretable to humans. The framework first defines clear expectations of each agent by curating human-designed agent criteria. Then, it develops a human-aligned agent verifier module, trained with human gold standards, to assess each agent&#39;s execution output. This approach enables granular evaluation of each agent&#39;s performance by revealing failures from a human standard, offering clear guidelines for revision, and reducing human cognitive load. Our case study results show that VeriLA is both interpretable and efficient in helping practitioners interact more effectively with the system. By upholding accountability in human-agent collaboration, VeriLA paves the way for more trustworthy and human-aligned compound AI systems.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.12651"
    },
    "861743010ad121aeb0f4b32f3acc28c3": {
        "title": "Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering",
        "authors": [
            "Kenneth J. K. Ong",
            "Lye Jia Jun",
            "Hieu Minh &#34;Jord&#34; Nguyen",
            "Seong Hah Cho",
            "Natalia Pérez-Campanero Antolín"
        ],
        "date": "2025/03/17",
        "pdf": "http://arxiv.org/pdf/2503.12722",
        "abstract": "As Large Language Models (LLMs) gain autonomous capabilities, their coordination in multi-agent settings becomes increasingly important. However, they often struggle with cooperation, leading to suboptimal outcomes. Inspired by Axelrod&#39;s Iterated Prisoner&#39;s Dilemma (IPD) tournaments, we explore how personality traits influence LLM cooperation. Using representation engineering, we steer Big Five traits (e.g., Agreeableness, Conscientiousness) in LLMs and analyze their impact on IPD decision-making. Our results show that higher Agreeableness and Conscientiousness improve cooperation but increase susceptibility to exploitation, highlighting both the potential and limitations of personality-based steering for aligning AI agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.12722"
    },
    "2fe5b0ecc3fdbed27db943dc10707d9f": {
        "title": "MAP: Evaluation and Multi-Agent Enhancement of Large Language Models for Inpatient Pathways",
        "authors": [
            "Zhen Chen",
            "Zhihao Peng",
            "Xusheng Liang",
            "Cheng Wang",
            "Peigan Liang",
            "Linsheng Zeng",
            "Minjie Ju",
            "Yixuan Yuan"
        ],
        "date": "2025/03/17",
        "pdf": "http://arxiv.org/pdf/2503.13205",
        "abstract": "Inpatient pathways demand complex clinical decision-making based on comprehensive patient information, posing critical challenges for clinicians. Despite advancements in large language models (LLMs) in medical applications, limited research focused on artificial intelligence (AI) inpatient pathways systems, due to the lack of large-scale inpatient datasets. Moreover, existing medical benchmarks typically concentrated on medical question-answering and examinations, ignoring the multifaceted nature of clinical decision-making in inpatient settings. To address these gaps, we first developed the Inpatient Pathway Decision Support (IPDS) benchmark from the MIMIC-IV database, encompassing 51,274 cases across nine triage departments and 17 major disease categories alongside 16 standardized treatment options. Then, we proposed the Multi-Agent Inpatient Pathways (MAP) framework to accomplish inpatient pathways with three clinical agents, including a triage agent managing the patient admission, a diagnosis agent serving as the primary decision maker at the department, and a treatment agent providing treatment plans. Additionally, our MAP framework includes a chief agent overseeing the inpatient pathways to guide and promote these three clinician agents. Extensive experiments showed our MAP improved the diagnosis accuracy by 25.10% compared to the state-of-the-art LLM HuatuoGPT2-13B. It is worth noting that our MAP demonstrated significant clinical compliance, outperforming three board-certified clinicians by 10%-12%, establishing a foundation for inpatient pathways systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13205"
    },
    "126f4dfb493aec4dfd3e5e52a7f8f43c": {
        "title": "LLM-Mediated Guidance of MARL Systems",
        "authors": [
            "Philipp D. Siedler",
            "Ian Gemp"
        ],
        "date": "2025/03/16",
        "pdf": "http://arxiv.org/pdf/2503.13553",
        "abstract": "In complex multi-agent environments, achieving efficient learning and desirable behaviours is a significant challenge for Multi-Agent Reinforcement Learning (MARL) systems. This work explores the potential of combining MARL with Large Language Model (LLM)-mediated interventions to guide agents toward more desirable behaviours. Specifically, we investigate how LLMs can be used to interpret and facilitate interventions that shape the learning trajectories of multiple agents. We experimented with two types of interventions, referred to as controllers: a Natural Language (NL) Controller and a Rule-Based (RB) Controller. The NL Controller, which uses an LLM to simulate human-like interventions, showed a stronger impact than the RB Controller. Our findings indicate that agents particularly benefit from early interventions, leading to more efficient training and higher performance. Both intervention types outperform the baseline without interventions, highlighting the potential of LLM-mediated guidance to accelerate training and enhance MARL performance in challenging environments.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13553"
    },
    "54d02ff5b4ebf3895fdb5f95d0900bda": {
        "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection",
        "authors": [
            "Tittaya Mairittha",
            "Tanakon Sawanglok",
            "Panuwit Raden",
            "Sorrawit Treesuk"
        ],
        "date": "2025/03/19",
        "pdf": "http://arxiv.org/pdf/2503.15204",
        "abstract": "Swine disease surveillance is critical to the sustainability of global agriculture, yet its effectiveness is frequently undermined by limited veterinary resources, delayed identification of cases, and variability in diagnostic accuracy. To overcome these barriers, we introduce a novel AI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented Generation (RAG) to deliver timely, evidence-based disease detection and clinical guidance. By automatically classifying user inputs into either Knowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system ensures targeted information retrieval and facilitates precise diagnostic reasoning. An adaptive questioning protocol systematically collects relevant clinical signs, while a confidence-weighted decision fusion mechanism integrates multiple diagnostic hypotheses to generate robust disease predictions and treatment recommendations. Comprehensive evaluations encompassing query classification, disease diagnosis, and knowledge retrieval demonstrate that the system achieves high accuracy, rapid response times, and consistent reliability. By providing a scalable, AI-driven diagnostic framework, this approach enhances veterinary decision-making, advances sustainable livestock management practices, and contributes substantively to the realization of global food security.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.15204"
    },
    "caa5cbc1e61e7aa77360d88b8b61fb06": {
        "title": "DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments",
        "authors": [
            "Wenjie Tang",
            "Yuan Zhou",
            "Erqiang Xu",
            "Keyan Cheng",
            "Minne Li",
            "Liquan Xiao"
        ],
        "date": "2025/03/08",
        "pdf": "http://arxiv.org/pdf/2503.06047",
        "abstract": "Large Language Model~(LLM) based agents have been increasingly popular in solving complex and dynamic tasks, which requires proper evaluation systems to assess their capabilities. Nevertheless, existing benchmarks usually either focus on single-objective tasks or use overly broad assessing metrics, failing to provide a comprehensive inspection of the actual capabilities of LLM-based agents in complicated decision-making tasks. To address these issues, we introduce DSGBench, a more rigorous evaluation platform for strategic decision-making. Firstly, it incorporates six complex strategic games which serve as ideal testbeds due to their long-term and multi-dimensional decision-making demands and flexibility in customizing tasks of various difficulty levels or multiple targets. Secondly, DSGBench employs a fine-grained evaluation scoring system which examines the decision-making capabilities by looking into the performance in five specific dimensions and offering a comprehensive assessment in a well-designed way. Furthermore, DSGBench also incorporates an automated decision-tracking mechanism which enables in-depth analysis of agent behaviour patterns and the changes in their strategies. We demonstrate the advances of DSGBench by applying it to multiple popular LLM-based agents and our results suggest that DSGBench provides valuable insights in choosing LLM-based agents as well as improving their future development. DSGBench is available at https://github.com/DeciBrain-Group/DSGBench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.06047"
    },
    "66336244f893cb2a11dca17d2d0daed7": {
        "title": "SafeArena: Evaluating the Safety of Autonomous Web Agents",
        "authors": [
            "Ada Defne Tur",
            "Nicholas Meade",
            "Xing Han Lù",
            "Alejandra Zambrano",
            "Arkil Patel",
            "Esin Durmus",
            "Spandana Gella",
            "Karolina Stańczak",
            "Siva Reddy"
        ],
        "date": "2025/03/06",
        "pdf": "http://arxiv.org/pdf/2503.04957",
        "abstract": "LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.04957"
    },
    "ac343ad69210d689e37d4d1455f87f5c": {
        "title": "Multi Agent based Medical Assistant for Edge Devices",
        "authors": [
            "Sakharam Gawade",
            "Shivam Akhouri",
            "Chinmay Kulkarni",
            "Jagdish Samant",
            "Pragya Sahu",
            "Aastik",
            "Jai Pahal",
            "Saswat Meher"
        ],
        "date": "2025/03/07",
        "pdf": "http://arxiv.org/pdf/2503.05397",
        "abstract": "Large Action Models (LAMs) have revolutionized intelligent automation, but their application in healthcare faces challenges due to privacy concerns, latency, and dependency on internet access. This report introduces an ondevice, multi-agent healthcare assistant that overcomes these limitations. The system utilizes smaller, task-specific agents to optimize resources, ensure scalability and high performance. Our proposed system acts as a one-stop solution for health care needs with features like appointment booking, health monitoring, medication reminders, and daily health reporting. Powered by the Qwen Code Instruct 2.5 7B model, the Planner and Caller Agents achieve an average RougeL score of 85.5 for planning and 96.5 for calling for our tasks while being lightweight for on-device deployment. This innovative approach combines the benefits of ondevice systems with multi-agent architectures, paving the way for user-centric healthcare solutions.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.05397"
    },
    "0b1d0f2c8ea897885df0051048568fdf": {
        "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration",
        "authors": [
            "David Wan",
            "Justin Chih-Yao Chen",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "date": "2025/03/19",
        "pdf": "http://arxiv.org/pdf/2503.15272",
        "abstract": "Multi-agent collaboration among models has shown promise in reasoning tasks but is underexplored in long-form generation tasks like summarization and question-answering. We extend multi-agent multi-model reasoning to generation, specifically to improving faithfulness through refinement, i.e., revising model-generated outputs to remove factual inconsistencies. We investigate how iterative collaboration among multiple instances and types of large language models (LLMs) enhances subtasks in the refinement process, such as error detection, critiquing unfaithful sentences, and making corrections based on critiques. We design intrinsic evaluations for each subtask, with our findings indicating that both multi-agent (multiple instances) and multi-model (diverse LLM types) approaches benefit error detection and critiquing. Additionally, reframing critiquing and refinement as reranking rather than generation tasks improves multi-agent performance. We consolidate these insights into a final &#34;recipe&#34; called Multi-Agent Multi-Model Refinement (MAMM-Refine), where multi-agent and multi-model collaboration significantly boosts performance on three summarization datasets as well as on long-form question answering, demonstrating the effectiveness and generalizability of our recipe.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.15272"
    },
    "8b725539fe724dd9bee9773e1ff16fab": {
        "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement",
        "authors": [
            "Ruihan Yang",
            "Fanghua Ye",
            "Jian Li",
            "Siyu Yuan",
            "Yikai Zhang",
            "Zhaopeng Tu",
            "Xiaolong Li",
            "Deqing Yang"
        ],
        "date": "2025/03/20",
        "pdf": "http://arxiv.org/pdf/2503.16024",
        "abstract": "Large language models (LLMs) have recently transformed from text-based assistants to autonomous agents capable of planning, reasoning, and iteratively improving their actions. While numerical reward signals and verifiers can effectively rank candidate actions, they often provide limited contextual guidance. In contrast, natural language feedback better aligns with the generative capabilities of LLMs, providing richer and more actionable suggestions. However, parsing and implementing this feedback effectively can be challenging for LLM-based agents. In this work, we introduce Critique-Guided Improvement (CGI), a novel two-player framework, comprising an actor model that explores an environment and a critic model that generates detailed nature language feedback. By training the critic to produce fine-grained assessments and actionable revisions, and the actor to utilize these critiques, our approach promotes more robust exploration of alternative strategies while avoiding local optima. Experiments in three interactive environments show that CGI outperforms existing baselines by a substantial margin. Notably, even a small critic model surpasses GPT-4 in feedback quality. The resulting actor achieves state-of-the-art performance, demonstrating the power of explicit iterative guidance to enhance decision-making in LLM-based agents.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16024"
    },
    "c57e2e896e7e9441f17383a92b3e78b0": {
        "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration",
        "authors": [
            "Hong Qing Yu",
            "Frank McQuade"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.13514",
        "abstract": "This paper presents RAG-KG-IL, a novel multi-agent hybrid framework designed to enhance the reasoning capabilities of Large Language Models (LLMs) by integrating Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) with an Incremental Learning (IL) approach. Despite recent advancements, LLMs still face significant challenges in reasoning with structured data, handling dynamic knowledge evolution, and mitigating hallucinations, particularly in mission-critical domains. Our proposed RAG-KG-IL framework addresses these limitations by employing a multi-agent architecture that enables continuous knowledge updates, integrates structured knowledge, and incorporates autonomous agents for enhanced explainability and reasoning. The framework utilizes RAG to ensure the generated responses are grounded in verifiable information, while KGs provide structured domain knowledge for improved consistency and depth of understanding. The Incremental Learning approach allows for dynamic updates to the knowledge base without full retraining, significantly reducing computational overhead and improving the model&#39;s adaptability. We evaluate the framework using real-world case studies involving health-related queries, comparing it to state-of-the-art models like GPT-4o and a RAG-only baseline. Experimental results demonstrate that our approach significantly reduces hallucination rates and improves answer completeness and reasoning accuracy. The results underscore the potential of combining RAG, KGs, and multi-agent systems to create intelligent, adaptable systems capable of real-time knowledge integration and reasoning in complex domains.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13514"
    },
    "d36777a949aa2b6220dafae9287053c8": {
        "title": "Agent-Enhanced Large Language Models for Researching Political Institutions",
        "authors": [
            "Joseph R. Loffredo",
            "Suyeol Yun"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.13524",
        "abstract": "The applications of Large Language Models (LLMs) in political science are rapidly expanding. This paper demonstrates how LLMs, when augmented with predefined functions and specialized tools, can serve as dynamic agents capable of streamlining tasks such as data collection, preprocessing, and analysis. Central to this approach is agentic retrieval-augmented generation (Agentic RAG), which equips LLMs with action-calling capabilities for interaction with external knowledge bases. Beyond information retrieval, LLM agents may incorporate modular tools for tasks like document summarization, transcript coding, qualitative variable classification, and statistical modeling. To demonstrate the potential of this approach, we introduce CongressRA, an LLM agent designed to support scholars studying the U.S. Congress. Through this example, we highlight how LLM agents can reduce the costs of replicating, testing, and extending empirical research using the domain-specific data that drives the study of political institutions.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.13524"
    },
    "74e8b2991fcb479af68b03a9470c9684": {
        "title": "DARS: Dynamic Action Re-Sampling to Enhance Coding Agent Performance by Adaptive Tree Traversal",
        "authors": [
            "Vaibhav Aggarwal",
            "Ojasv Kamal",
            "Abhinav Japesh",
            "Zhijing Jin",
            "Bernhard Schölkopf"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14269",
        "abstract": "Large Language Models (LLMs) have revolutionized various domains, including natural language processing, data analysis, and software development, by enabling automation. In software engineering, LLM-powered coding agents have garnered significant attention due to their potential to automate complex development tasks, assist in debugging, and enhance productivity. However, existing approaches often struggle with sub-optimal decision-making, requiring either extensive manual intervention or inefficient compute scaling strategies. To improve coding agent performance, we present Dynamic Action Re-Sampling (DARS), a novel inference time compute scaling approach for coding agents, that is faster and more effective at recovering from sub-optimal decisions compared to baselines. While traditional agents either follow linear trajectories or rely on random sampling for scaling compute, our approach DARS works by branching out a trajectory at certain key decision points by taking an alternative action given the history of the trajectory and execution feedback of the previous attempt from that point. We evaluate our approach on SWE-Bench Lite benchmark, demonstrating that this scaling strategy achieves a pass@k score of 55% with Claude 3.5 Sonnet V2. Our framework achieves a pass@1 rate of 47%, outperforming state-of-the-art (SOTA) open-source frameworks.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14269"
    },
    "f94d953e86fdfa5c1fe6fcc313debd56": {
        "title": "PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play",
        "authors": [
            "Wei Fang",
            "Yang Zhang",
            "Kaizhi Qian",
            "James Glass",
            "Yada Zhu"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14432",
        "abstract": "Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically &#34;plays&#34; with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14432"
    },
    "8ea63e6e401e0f9b6287664f0b7f2b73": {
        "title": "Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations",
        "authors": [
            "Hikaru Shimadzu",
            "Takehito Utsuro",
            "Daisuke Kitayama"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14620",
        "abstract": "In the 2023 edition of the White Paper on Information and Communications, it is estimated that the population of social networking services in Japan will exceed 100 million by 2022, and the influence of social networking services in Japan is growing significantly. In addition, marketing using SNS and research on the propagation of emotions and information on SNS are being actively conducted, creating the need for a system for predicting trends in SNS interactions. We have already created a system that simulates the behavior of various communities on SNS by building a virtual SNS environment in which agents post and reply to each other in a chat community created by agents using a LLMs. In this paper, we evaluate the impact of the search extension generation mechanism used to create posts and replies in a virtual SNS environment using a simulation system on the ability to generate posts and replies. As a result of the evaluation, we confirmed that the proposed search extension generation mechanism, which mimics human search behavior, generates the most natural exchange.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ],
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14620"
    },
    "26f89c6e2b8e1b813c667c6f30ec185b": {
        "title": "Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs",
        "authors": [
            "Vincent Li",
            "Yule Fu",
            "Tim Knappe",
            "Kevin Han",
            "Kevin Zhu"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2503.11657",
        "abstract": "Large Language Models have demonstrated remarkable capabilities in natural language processing tasks, including mathematical problem-solving that requires multi-step logical reasoning. However, challenges persist in automating the identification of key mathematical concepts, understanding their interrelations, and formalizing proofs within a rigorous framework. We present a novel framework that leverages knowledge graphs to augment LLMs to construct and formalize mathematical proofs. Our results demonstrate significant performance improvements across multiple datasets, with using knowledge graphs, achieving up to a 34% success rate on the MUSTARDSAUCE dataset on o1-mini and consistently outperforming baseline approaches by 2-11% across different models. We show how this approach bridges the gap between natural language understanding and formal logic proof systems and achieve elevated results for foundation models over baseline.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11657"
    },
    "601ca0bd430e0e1367925b2b1bcbac03": {
        "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection",
        "authors": [
            "Tharindu Kumarage",
            "Cameron Johnson",
            "Jadie Adams",
            "Lin Ai",
            "Matthias Kirchner",
            "Anthony Hoogs",
            "Joshua Garland",
            "Julia Hirschberg",
            "Arslan Basharat",
            "Huan Liu"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.15552",
        "abstract": "The rapid advancement of conversational agents, particularly chatbots powered by Large Language Models (LLMs), poses a significant risk of social engineering (SE) attacks on social media platforms. SE detection in multi-turn, chat-based interactions is considerably more complex than single-instance detection due to the dynamic nature of these conversations. A critical factor in mitigating this threat is understanding the mechanisms through which SE attacks operate, specifically how attackers exploit vulnerabilities and how victims&#39; personality traits contribute to their susceptibility. In this work, we propose an LLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating multi-turn conversations. We model victim agents with varying personality traits to assess how psychological profiles influence susceptibility to manipulation. Using a dataset of over 1000 simulated conversations, we examine attack scenarios in which adversaries, posing as recruiters, funding agencies, and journalists, attempt to extract sensitive information. Based on this analysis, we present a proof of concept, SE-OmniGuard, to offer personalized protection to users by leveraging prior knowledge of the victims personality, evaluating attack strategies, and monitoring information exchanges in conversations to identify potential SE attempts.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.15552"
    },
    "e92acde6c28267f954d98f55f31a8234": {
        "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
        "authors": [
            "Akhil Perincherry",
            "Jacob Krantz",
            "Stefan Lee"
        ],
        "date": "2025/03/20",
        "pdf": "http://arxiv.org/pdf/2503.16394",
        "abstract": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or imaginations, we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of around 1 point and up to 0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone. Code and data for our work can be found at https://www.akhilperincherry.com/VLN-Imagine-website/.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16394"
    },
    "8c0786d88200f9dfc117d7eef0b8a500": {
        "title": "Survey on Evaluation of LLM-based Agents",
        "authors": [
            "Asaf Yehudai",
            "Lilach Eden",
            "Alan Li",
            "Guy Uziel",
            "Yilun Zhao",
            "Roy Bar-Haim",
            "Arman Cohan",
            "Michal Shmueli-Scheuer"
        ],
        "date": "2025/03/20",
        "pdf": "http://arxiv.org/pdf/2503.16416",
        "abstract": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16416"
    },
    "84e653c9399f57eecb52bcf44a237716": {
        "title": "LLM Agents for Education: Advances and Applications",
        "authors": [
            "Zhendong Chu",
            "Shen Wang",
            "Jian Xie",
            "Tinghui Zhu",
            "Yibo Yan",
            "Jinheng Ye",
            "Aoxiao Zhong",
            "Xuming Hu",
            "Jing Liang",
            "Philip S. Yu",
            "Qingsong Wen"
        ],
        "date": "2025/03/14",
        "pdf": "http://arxiv.org/pdf/2503.11733",
        "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) \\emph{Domain-Specific Educational Agents}, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.11733"
    },
    "8f6a31651270ccb2ce8a8ca378e3cb62": {
        "title": "Large Language Model Agent: A Survey on Methodology, Applications and Challenges",
        "authors": [
            "Junyu Luo",
            "Weizhi Zhang",
            "Ye Yuan",
            "Yusheng Zhao",
            "Junwei Yang",
            "Yiyang Gu",
            "Bohan Wu",
            "Binqi Chen",
            "Ziyue Qiao",
            "Qingqing Long",
            "Rongcheng Tu",
            "Xiao Luo",
            "Wei Ju",
            "Zhiping Xiao",
            "Yifan Wang",
            "Meng Xiao",
            "Chenwu Liu",
            "Jingyang Yuan",
            "Shichang Zhang",
            "Yiqiao Jin",
            "Fan Zhang",
            "Xian Wu",
            "Hanqing Zhao",
            "Dacheng Tao",
            "Philip S. Yu",
            "Ming Zhang"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21460",
        "abstract": "The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21460"
    },
    "f53122daba2b2f39df85deba36cf51b9": {
        "title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment",
        "authors": [
            "Souradip Chakraborty",
            "Sujay Bhatt",
            "Udari Madhushani Sehwag",
            "Soumya Suvra Ghosal",
            "Jiahao Qiu",
            "Mengdi Wang",
            "Dinesh Manocha",
            "Furong Huang",
            "Alec Koppel",
            "Sumitra Ganesh"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21720",
        "abstract": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21720"
    },
    "09441fc23ddc1d49f1819056c7a47c08": {
        "title": "MemInsight: Autonomous Memory Augmentation for LLM Agents",
        "authors": [
            "Rana Salama",
            "Jason Cai",
            "Michelle Yuan",
            "Anna Currey",
            "Monica Sunkara",
            "Yi Zhang",
            "Yassine Benajiba"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21760",
        "abstract": "Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21760"
    },
    "c4fdf661bf8928db15c3ac6bdad2b56d": {
        "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
        "authors": [
            "Shengyue Guan",
            "Haoyi Xiong",
            "Jindong Wang",
            "Jiang Bian",
            "Bin Zhu",
            "Jian-guang Lou"
        ],
        "date": "2025/03/28",
        "pdf": "http://arxiv.org/pdf/2503.22458",
        "abstract": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22458"
    },
    "7e76098cde9cedbbd7fd621f39a8c57f": {
        "title": "WorkTeam: Constructing Workflows from Natural Language with Multi-Agents",
        "authors": [
            "Hanchao Liu",
            "Rongjun Li",
            "Weimin Xiong",
            "Ziyu Zhou",
            "Wei Peng"
        ],
        "date": "2025/03/28",
        "pdf": "http://arxiv.org/pdf/2503.22473",
        "abstract": "Workflows play a crucial role in enhancing enterprise efficiency by orchestrating complex processes with multiple tools or components. However, hand-crafted workflow construction requires expert knowledge, presenting significant technical barriers. Recent advancements in Large Language Models (LLMs) have improved the generation of workflows from natural language instructions (aka NL2Workflow), yet existing single LLM agent-based methods face performance degradation on complex tasks due to the need for specialized knowledge and the strain of task-switching. To tackle these challenges, we propose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor, orchestrator, and filler agent, each with distinct roles that collaboratively enhance the conversion process. As there are currently no publicly available NL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which includes 3,695 real-world business samples for training and evaluation. Experimental results show that our approach significantly increases the success rate of workflow construction, providing a novel and effective solution for enterprise NL2Workflow services.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22473"
    },
    "7a72c0080495957f1480c962b3f24423": {
        "title": "Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions",
        "authors": [
            "Mohammad Almansoori",
            "Komal Kumar",
            "Hisham Cholakkal"
        ],
        "date": "2025/03/28",
        "pdf": "http://arxiv.org/pdf/2503.22678",
        "abstract": "In this work, we introduce MedAgentSim, an open-source simulated clinical environment with doctor, patient, and measurement agents designed to evaluate and enhance LLM performance in dynamic diagnostic settings. Unlike prior approaches, our framework requires doctor agents to actively engage with patients through multi-turn conversations, requesting relevant medical examinations (e.g., temperature, blood pressure, ECG) and imaging results (e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnostic process. Additionally, we incorporate self improvement mechanisms that allow models to iteratively refine their diagnostic strategies. We enhance LLM performance in our simulated setting by integrating multi-agent discussions, chain-of-thought reasoning, and experience-based knowledge retrieval, facilitating progressive learning as doctor agents interact with more patients. We also introduce an evaluation benchmark for assessing the LLM&#39;s ability to engage in dynamic, context-aware diagnostic interactions. While MedAgentSim is fully automated, it also supports a user-controlled mode, enabling human interaction with either the doctor or patient agent. Comprehensive evaluations in various simulated diagnostic scenarios demonstrate the effectiveness of our approach. Our code, simulation tool, and benchmark are available at \\href{https://medagentsim.netlify.app/}.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22678"
    },
    "f67387b5779ea69017fdaef549bd7fe3": {
        "title": "A Survey of Large Language Model Agents for Question Answering",
        "authors": [
            "Murong Yue"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.19213",
        "abstract": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19213"
    },
    "3e64c47554e09749e9d5fa9be3c0fb66": {
        "title": "MARS: Memory-Enhanced Agents with Reflective Self-improvement",
        "authors": [
            "Xuechen Liang",
            "Meiling Tao",
            "Yinghui Xia",
            "Jianhui Wang",
            "Kun Li",
            "Yijin Wang",
            "Jingsong Yang",
            "Tianyu Shi",
            "Yuantao Wang",
            "Miao Zhang",
            "Xueqian Wang"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19271",
        "abstract": "Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments. To address these issues, this paper proposes an innovative framework Memory-Enhanced Agents with Reflective Self-improvement. The MARS framework comprises three agents: the User, the Assistant, and the Checker. By integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents capabilities in handling multi-tasking and long-span information.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19271"
    },
    "7ff8f93830a7e8402a1d098309aed2bc": {
        "title": "CoMAC: Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions",
        "authors": [
            "Junfeng Liu",
            "Christopher T. Symons",
            "Ranga Raju Vatsavai"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19274",
        "abstract": "Recent advancements in AI-driven conversational agents have exhibited immense potential of AI applications. Effective response generation is crucial to the success of these agents. While extensive research has focused on leveraging multiple auxiliary data sources (e.g., knowledge bases and personas) to enhance response generation, existing methods often struggle to efficiently extract relevant information from these sources. There are still clear limitations in the ability to combine versatile conversational capabilities with adherence to known facts and adaptation to large variations in user preferences and belief systems, which continues to hinder the wide adoption of conversational AI tools. This paper introduces a novel method, Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions (CoMAC), for conversation generation, which employs specialized encoding streams and post-fusion grounding networks for multiple data sources to identify relevant persona and knowledge information for the conversation. CoMAC also leverages a novel text similarity metric that allows bi-directional information sharing among multiple sources and focuses on a selective subset of meaningful words. Our experiments show that CoMAC improves the relevant persona and knowledge prediction accuracies and response generation quality significantly over two state-of-the-art methods.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19274"
    },
    "617c1a6b61b00a6ac9ffd650e4533b87": {
        "title": "Substance over Style: Evaluating Proactive Conversational Coaching Agents",
        "authors": [
            "Vidya Srinivas",
            "Xuhai Xu",
            "Xin Liu",
            "Kumar Ayush",
            "Isaac Galatzer-Levy",
            "Shwetak Patel",
            "Daniel McDuff",
            "Tim Althoff"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19328",
        "abstract": "While NLP research has made strides in conversational tasks, many approaches focus on single-turn responses with well-defined objectives or evaluation criteria. In contrast, coaching presents unique challenges with initially undefined goals that evolve through multi-turn interactions, subjective evaluation criteria, mixed-initiative dialogue. In this work, we describe and implement five multi-turn coaching agents that exhibit distinct conversational styles, and evaluate them through a user study, collecting first-person feedback on 155 conversations. We find that users highly value core functionality, and that stylistic components in absence of core components are viewed negatively. By comparing user feedback with third-person evaluations from health experts and an LM, we reveal significant misalignment across evaluation approaches. Our findings provide insights into design and evaluation of conversational coaching agents and contribute toward improving human-centered NLP applications.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19328"
    },
    "13fc78ed52933fba3f818696e3ac75a1": {
        "title": "Writing as a testbed for open ended agents",
        "authors": [
            "Sian Gooding",
            "Lucia Lopez-Rivilla",
            "Edward Grefenstette"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19711",
        "abstract": "Open-ended tasks are particularly challenging for LLMs due to the vast solution space, demanding both expansive exploration and adaptable strategies, especially when success lacks a clear, objective definition. Writing, with its vast solution space and subjective evaluation criteria, provides a compelling testbed for studying such problems. In this paper, we investigate the potential of LLMs to act as collaborative co-writers, capable of suggesting and implementing text improvements autonomously. We analyse three prominent LLMs - Gemini 1.5 Pro, Claude 3.5 Sonnet, and GPT-4o - focusing on how their action diversity, human alignment, and iterative improvement capabilities impact overall performance. This work establishes a framework for benchmarking autonomous writing agents and, more broadly, highlights fundamental challenges and potential solutions for building systems capable of excelling in diverse open-ended domains.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19711"
    },
    "dd94fe9dbeb1bbf821b3cd196aa31edf": {
        "title": "sudo rm -rf agentic_security",
        "authors": [
            "Sejin Lee",
            "Jian Kim",
            "Haon Park",
            "Ashkan Yousefpour",
            "Sangyoon Yu",
            "Min Song"
        ],
        "date": "2025/03/26",
        "pdf": "http://arxiv.org/pdf/2503.20279",
        "abstract": "Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.20279"
    },
    "ccbd8bb4a4dd80af4b255d06a33770f6": {
        "title": "EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues",
        "authors": [
            "Yuhan Liu",
            "Yunbo Long"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21080",
        "abstract": "While large language model (LLM)-based chatbots have been applied for effective engagement in credit dialogues, their capacity for dynamic emotional expression remains limited. Current agents primarily rely on passive empathy rather than affective reasoning. For instance, when faced with persistent client negativity, the agent should employ strategic emotional adaptation by expressing measured anger to discourage counterproductive behavior and guide the conversation toward resolution. This context-aware emotional modulation is essential for imitating the nuanced decision-making of human negotiators. This paper introduces an EQ-negotiator that combines emotion sensing from pre-trained language models (PLMs) with emotional reasoning based on Game Theory and Hidden Markov Models. It takes into account both the current and historical emotions of the client to better manage and address negative emotions during interactions. By fine-tuning pre-trained language models (PLMs) on public emotion datasets and validating them on the credit dialogue datasets, our approach enables LLM-based agents to effectively capture shifts in client emotions and dynamically adjust their response tone based on our emotion decision policies in real-world financial negotiations. This EQ-negotiator can also help credit agencies foster positive client relationships, enhancing satisfaction in credit services.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21080"
    },
    "a56dcf15ff339dc5ac74c58a256e9a7f": {
        "title": "ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach",
        "authors": [
            "Reem Gody",
            "Mahmoud Goudy",
            "Ahmed Y. Tawfik"
        ],
        "date": "2025/03/21",
        "pdf": "http://arxiv.org/pdf/2503.17460",
        "abstract": "In this paper, we present ConvoGen: an innovative framework for generating synthetic conversational data using multi-agent systems. Our method leverages few-shot learning and introduces iterative sampling from a dynamically updated few-shot hub to create diverse and realistic conversational scenarios. The generated data has numerous applications, including training and evaluating conversational AI models, and augmenting existing datasets for tasks like conversational intent classification or conversation summarization. Our experiments demonstrate the effectiveness of this method in producing high-quality diverse synthetic conversational data, highlighting its potential to enhance the development and evaluation of conversational AI systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.17460"
    },
    "31567ecef9d254394aadbdeb260767ac": {
        "title": "Building Resource-Constrained Language Agents: A Korean Case Study on Chemical Toxicity Information",
        "authors": [
            "Hojun Cho",
            "Donghu Kim",
            "Soyoung Yang",
            "Chan Lee",
            "Hunjoo Lee",
            "Jaegul Choo"
        ],
        "date": "2025/03/22",
        "pdf": "http://arxiv.org/pdf/2503.17753",
        "abstract": "Language agents powered by large language models (LLMs) face significant deployment challenges in resource-constrained environments, particularly for specialized domains and less-common languages. This paper presents Tox-chat, a Korean chemical toxicity information agent devised within these limitations. We propose two key innovations: a context-efficient architecture that reduces token consumption through hierarchical section search, and a scenario-based dialogue generation methodology that effectively distills tool-using capabilities from larger models. Experimental evaluations demonstrate that our fine-tuned 8B parameter model substantially outperforms both untuned models and baseline approaches, in terms of DB faithfulness and preference. Our work offers valuable insights for researchers developing domain-specific language agents under practical constraints.",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.17753"
    },
    "d838fec710c0454fbba7c27a691344c5": {
        "title": "MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection",
        "authors": [
            "Yibo Yan",
            "Shen Wang",
            "Jiahao Huo",
            "Philip S. Yu",
            "Xuming Hu",
            "Qingsong Wen"
        ],
        "date": "2025/03/23",
        "pdf": "http://arxiv.org/pdf/2503.18132",
        "abstract": "Mathematical error detection in educational settings presents a significant challenge for Multimodal Large Language Models (MLLMs), requiring a sophisticated understanding of both visual and textual mathematical content along with complex reasoning capabilities. Though effective in mathematical problem-solving, MLLMs often struggle with the nuanced task of identifying and categorizing student errors in multimodal mathematical contexts. Therefore, we introduce MathAgent, a novel Mixture-of-Math-Agent framework designed specifically to address these challenges. Our approach decomposes error detection into three phases, each handled by a specialized agent: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer. This architecture enables more accurate processing of mathematical content by explicitly modeling relationships between multimodal problems and student solution steps. We evaluate MathAgent on real-world educational data, demonstrating approximately 5% higher accuracy in error step identification and 3% improvement in error categorization compared to baseline models. Besides, MathAgent has been successfully deployed in an educational platform that has served over one million K-12 students, achieving nearly 90% student satisfaction while generating significant cost savings by reducing manual error detection.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18132"
    },
    "24a2a5e1718557cf809a8e87b23439ab": {
        "title": "AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration",
        "authors": [
            "Zhexuan Wang",
            "Yutong Wang",
            "Xuebo Liu",
            "Liang Ding",
            "Miao Zhang",
            "Jie Liu",
            "Min Zhang"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18891",
        "abstract": "Multi-agent systems (MAS) based on large language models (LLMs) have demonstrated significant potential in collaborative problem-solving. However, they still face substantial challenges of low communication efficiency and suboptimal task performance, making the careful design of the agents&#39; communication topologies particularly important. Inspired by the management theory that roles in an efficient team are often dynamically adjusted, we propose AgentDropout, which identifies redundant agents and communication across different communication rounds by optimizing the adjacency matrices of the communication graphs and eliminates them to enhance both token efficiency and task performance. Compared to state-of-the-art methods, AgentDropout achieves an average reduction of 21.6% in prompt token consumption and 18.4% in completion token consumption, along with a performance improvement of 1.14 on the tasks. Furthermore, the extended experiments demonstrate that AgentDropout achieves notable domain transferability and structure robustness, revealing its reliability and effectiveness. We release our code at https://github.com/wangzx1219/AgentDropout.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18891"
    },
    "cecfefa9723e716554aa5e3297d135df": {
        "title": "MARS: A Multi-Agent Framework Incorporating Socratic Guidance for Automated Prompt Optimization",
        "authors": [
            "Jian Zhang",
            "Zhangqi Wang",
            "Haiping Zhu",
            "Jun Liu",
            "Qika Lin",
            "Erik Cambria"
        ],
        "date": "2025/03/21",
        "pdf": "http://arxiv.org/pdf/2503.16874",
        "abstract": "The basic question-answering format of large language models involves inputting a prompt and receiving a response, and the quality of the prompt directly impacts the effectiveness of the response. Automated Prompt Optimization (APO) aims to break free from the cognitive biases of manually designed prompts and explores a broader design space for prompts. However, existing APO methods suffer from limited flexibility of fixed templates and inefficient search in prompt spaces as key issues. To this end, we propose a Multi-Agent framework Incorporating Socratic guidance (MARS), which utilizes multi-agent fusion technology for automatic planning, with gradual continuous optimization and evaluation. Specifically, MARS comprises seven agents, each with distinct functionalities, which autonomously use the Planner to devise an optimization path that ensures flexibility. Additionally, it employs a Teacher-Critic-Student Socratic dialogue pattern to iteratively optimize the prompts while conducting effective search. We conduct extensive experiments on various datasets to validate the effectiveness of our method, and perform additional analytical experiments to assess the model&#39;s advancement as well as the interpretability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16874"
    },
    "a0a87a7ff5e8224f802c82812bdd2143": {
        "title": "Gricean Norms as a Basis for Effective Collaboration",
        "authors": [
            "Fardin Saad",
            "Pradeep K. Murukannaiah",
            "Munindar P. Singh"
        ],
        "date": "2025/03/18",
        "pdf": "http://arxiv.org/pdf/2503.14484",
        "abstract": "Effective human-AI collaboration hinges not only on the AI agent&#39;s ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks -- common ground, relevance theory, and theory of mind -- into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent&#39;s pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.14484"
    },
    "9d6f8e56edbca6455c7f3eb4f3732bf2": {
        "title": "Multimodal Transformer Models for Turn-taking Prediction: Effects on Conversational Dynamics of Human-Agent Interaction during Cooperative Gameplay",
        "authors": [
            "Young-Ho Bae",
            "Casey C. Bennett"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2503.16432",
        "abstract": "This study investigates multimodal turn-taking prediction within human-agent interactions (HAI), particularly focusing on cooperative gaming environments. It comprises both model development and subsequent user study, aiming to refine our understanding and improve conversational dynamics in spoken dialogue systems (SDSs). For the modeling phase, we introduce a novel transformer-based deep learning (DL) model that simultaneously integrates multiple modalities - text, vision, audio, and contextual in-game data to predict turn-taking events in real-time. Our model employs a Crossmodal Transformer architecture to effectively fuse information from these diverse modalities, enabling more comprehensive turn-taking predictions. The model demonstrates superior performance compared to baseline models, achieving 87.3% accuracy and 83.0% macro F1 score. A human user study was then conducted to empirically evaluate the turn-taking DL model in an interactive scenario with a virtual avatar while playing the game &#34;Dont Starve Together&#34;, comparing a control condition without turn-taking prediction (n=20) to an experimental condition with our model deployed (n=40). Both conditions included a mix of English and Korean speakers, since turn-taking cues are known to vary by culture. We then analyzed the interaction quality, examining aspects such as utterance counts, interruption frequency, and participant perceptions of the avatar. Results from the user study suggest that our multimodal turn-taking model not only enhances the fluidity and naturalness of human-agent conversations, but also maintains a balanced conversational dynamic without significantly altering dialogue frequency. The study provides in-depth insights into the influence of turn-taking abilities on user perceptions and interaction quality, underscoring the potential for more contextually adaptive and responsive conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16432"
    },
    "14d37e57f142fb713f00584b2a688dda": {
        "title": "The Application of MATEC (Multi-AI Agent Team Care) Framework in Sepsis Care",
        "authors": [
            "Andrew Cho",
            "Jason M. Woo",
            "Brian Shi",
            "Aishwaryaa Udeshi",
            "Jonathan S. H. Woo"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2503.16433",
        "abstract": "Under-resourced or rural hospitals have limited access to medical specialists and healthcare professionals, which can negatively impact patient outcomes in sepsis. To address this gap, we developed the MATEC (Multi-AI Agent Team Care) framework, which integrates a team of specialized AI agents for sepsis care. The sepsis AI agent team includes five doctor agents, four health professional agents, and a risk prediction model agent, with an additional 33 doctor agents available for consultations. Ten attending physicians at a teaching hospital evaluated this framework, spending approximately 40 minutes on the web-based MATEC application and participating in the 5-point Likert scale survey (rated from 1-unfavorable to 5-favorable). The physicians found the MATEC framework very useful (Median=4, P=0.01), and very accurate (Median=4, P&lt;0.01). This pilot study demonstrates that a Multi-AI Agent Team Care framework (MATEC) can potentially be useful in assisting medical professionals, particularly in under-resourced hospital settings.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16433"
    },
    "ce7b16fc0bccbeecaadef583b9b67b86": {
        "title": "Improving Interactive Diagnostic Ability of a Large Language Model Agent Through Clinical Experience Learning",
        "authors": [
            "Zhoujian Sun",
            "Ziyi Liu",
            "Cheng Luo",
            "Jiebin Chu",
            "Zhengxing Huang"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2503.16463",
        "abstract": "Recent advances in large language models (LLMs) have shown promising results in medical diagnosis, with some studies indicating superior performance compared to human physicians in specific scenarios. However, the diagnostic capabilities of LLMs are often overestimated, as their performance significantly deteriorates in interactive diagnostic settings that require active information gathering. This study investigates the underlying mechanisms behind the performance degradation phenomenon and proposes a solution. We identified that the primary deficiency of LLMs lies in the initial diagnosis phase, particularly in information-gathering efficiency and initial diagnosis formation, rather than in the subsequent differential diagnosis phase. To address this limitation, we developed a plug-and-play method enhanced (PPME) LLM agent, leveraging over 3.5 million electronic medical records from Chinese and American healthcare facilities. Our approach integrates specialized models for initial disease diagnosis and inquiry into the history of the present illness, trained through supervised and reinforcement learning techniques. The experimental results indicate that the PPME LLM achieved over 30% improvement compared to baselines. The final diagnostic accuracy of the PPME LLM in interactive diagnostic scenarios approached levels comparable to those achieved using complete clinical data. These findings suggest a promising potential for developing autonomous diagnostic systems, although further validation studies are needed.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16463"
    },
    "d770b8c68188c38012236a5219526f91": {
        "title": "EmpathyAgent: Can Embodied Agents Conduct Empathetic Actions?",
        "authors": [
            "Xinyan Chen",
            "Jiaxin Ge",
            "Hongming Dai",
            "Qiang Zhou",
            "Qiuxuan Feng",
            "Jingtong Hu",
            "Yizhou Wang",
            "Jiaming Liu",
            "Shanghang Zhang"
        ],
        "date": "2025/03/19",
        "pdf": "http://arxiv.org/pdf/2503.16545",
        "abstract": "Empathy is fundamental to human interactions, yet it remains unclear whether embodied agents can provide human-like empathetic support. Existing works have studied agents&#39; tasks solving and social interactions abilities, but whether agents can understand empathetic needs and conduct empathetic behaviors remains overlooked. To address this, we introduce EmpathyAgent, the first benchmark to evaluate and enhance agents&#39; empathetic actions across diverse scenarios. EmpathyAgent contains 10,000 multimodal samples with corresponding empathetic task plans and three different challenges. To systematically evaluate the agents&#39; empathetic actions, we propose an empathy-specific evaluation suite that evaluates the agents&#39; empathy process. We benchmark current models and found that exhibiting empathetic actions remains a significant challenge. Meanwhile, we train Llama3-8B using EmpathyAgent and find it can potentially enhance empathetic behavior. By establishing a standard benchmark for evaluating empathetic actions, we hope to advance research in empathetic embodied agents. Our code and data are publicly available at https://github.com/xinyan-cxy/EmpathyAgent.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.16545"
    },
    "a9b278e1b45d901ea8a302a9f21e9354": {
        "title": "Autonomous Radiotherapy Treatment Planning Using DOLA: A Privacy-Preserving, LLM-Based Optimization Agent",
        "authors": [
            "Humza Nusrat",
            "Bing Luo",
            "Ryan Hall",
            "Joshua Kim",
            "Hassan Bagher-Ebadian",
            "Anthony Doemer",
            "Benjamin Movsas",
            "Kundan Thind"
        ],
        "date": "2025/03/21",
        "pdf": "http://arxiv.org/pdf/2503.17553",
        "abstract": "Radiotherapy treatment planning is a complex and time-intensive process, often impacted by inter-planner variability and subjective decision-making. To address these challenges, we introduce Dose Optimization Language Agent (DOLA), an autonomous large language model (LLM)-based agent designed for optimizing radiotherapy treatment plans while rigorously protecting patient privacy. DOLA integrates the LLaMa3.1 LLM directly with a commercial treatment planning system, utilizing chain-of-thought prompting, retrieval-augmented generation (RAG), and reinforcement learning (RL). Operating entirely within secure local infrastructure, this agent eliminates external data sharing. We evaluated DOLA using a retrospective cohort of 18 prostate cancer patients prescribed 60 Gy in 20 fractions, comparing model sizes (8 billion vs. 70 billion parameters) and optimization strategies (No-RAG, RAG, and RAG+RL) over 10 planning iterations. The 70B model demonstrated significantly improved performance, achieving approximately 16.4% higher final scores than the 8B model. The RAG approach outperformed the No-RAG baseline by 19.8%, and incorporating RL accelerated convergence, highlighting the synergy of retrieval-based memory and reinforcement learning. Optimal temperature hyperparameter analysis identified 0.4 as providing the best balance between exploration and exploitation. This proof of concept study represents the first successful deployment of locally hosted LLM agents for autonomous optimization of treatment plans within a commercial radiotherapy planning system. By extending human-machine interaction through interpretable natural language reasoning, DOLA offers a scalable and privacy-conscious framework, with significant potential for clinical implementation and workflow improvement.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.17553"
    },
    "7babf881d292a7545391dbdaaa6d7eba": {
        "title": "AgentRxiv: Towards Collaborative Autonomous Research",
        "authors": [
            "Samuel Schmidgall",
            "Michael Moor"
        ],
        "date": "2025/03/23",
        "pdf": "http://arxiv.org/pdf/2503.18102",
        "abstract": "Progress in scientific discovery is rarely the result of a single &#34;Eureka&#34; moment, but is rather the product of hundreds of scientists incrementally working together toward a common goal. While existing agent workflows are capable of producing research autonomously, they do so in isolation, without the ability to continuously improve upon prior research results. To address these challenges, we introduce AgentRxiv-a framework that lets LLM agent laboratories upload and retrieve reports from a shared preprint server in order to collaborate, share insights, and iteratively build on each other&#39;s research. We task agent laboratories to develop new reasoning and prompting techniques and find that agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500). We find that the best performing strategy generalizes to benchmarks in other domains (improving on average by 3.3%). Multiple agent laboratories sharing research through AgentRxiv are able to work together towards a common goal, progressing more rapidly than isolated laboratories, achieving higher overall accuracy (13.7% relative improvement over baseline on MATH-500). These findings suggest that autonomous agents may play a role in designing future AI systems alongside humans. We hope that AgentRxiv allows agents to collaborate toward research goals and enables researchers to accelerate discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18102"
    },
    "487afe7e9a6604888b28f55e5e96f22d": {
        "title": "Safeguarding Mobile GUI Agent via Logic-based Action Verification",
        "authors": [
            "Jungjae Lee",
            "Dongjae Lee",
            "Chihun Choi",
            "Youngmin Im",
            "Jaeyoung Wi",
            "Kihong Heo",
            "Sangeun Oh",
            "Sunjae Lee",
            "Insik Shin"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18492",
        "abstract": "Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interpreting GUIs. These agents promise to revolutionize mobile computing by allowing users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA): a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA is designed to deterministically ensure that an agent&#39;s actions strictly align with user intent before conducting an action. At its core, VSA introduces a novel autoformalization technique that translates natural language user instructions into a formally verifiable specification, expressed in our domain-specific language (DSL). This enables runtime, rule-based verification, allowing VSA to detect and prevent erroneous actions executing an action, either by providing corrective feedback or halting unsafe behavior. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agent. effectively bridging the gap between LFM-driven automation and formal software verification. We implement VSA using off-the-shelf LLM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.3%-98.33% accuracy in verifying agent actions, representing a significant 20.4%-25.6% improvement over existing LLM-based verification methods, and consequently increases the GUI agent&#39;s task completion rate by 90%-130%.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18492"
    },
    "d3941739f04e589fa8d9b0a6c0e8b673": {
        "title": "Verbal Process Supervision Elicits Better Coding Agents",
        "authors": [
            "Hao-Yuan Chen",
            "Cheng-Pong Huang",
            "Jui-Ming Yao"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18494",
        "abstract": "The emergence of large language models and their applications as AI agents have significantly advanced state-of-the-art code generation benchmarks, transforming modern software engineering tasks. However, even with test-time computed reasoning models, these systems still struggle with complex software engineering challenges. This work introduces CURA, a code understanding and reasoning agent system enhanced with verbal process supervision (VPS), achieving a 3.65\\% improvement over baseline models on challenging benchmarks like BigCodeBench. Furthermore, CURA, when paired with the o3-mini model and VPS techniques, attains state-of-the-art performance. This work represents a step forward in integrating reasoning-driven architectures with LLM-based code generation, enabling agentic reasoning for language models to solve complex software engineering tasks.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18494"
    },
    "b197193c35af6b85a40cc3c4c96ef159": {
        "title": "AgentSpec: Customizable Runtime Enforcement for Safe and Reliable LLM Agents",
        "authors": [
            "Haoyu Wang",
            "Christopher M. Poskitt",
            "Jun Sun"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18666",
        "abstract": "Agents built on LLMs are increasingly deployed across diverse domains, automating complex decision-making and task execution. However, their autonomy introduces safety risks, including security vulnerabilities, legal violations, and unintended harmful actions. Existing mitigation methods, such as model-based safeguards and early enforcement strategies, fall short in robustness, interpretability, and adaptability. To address these challenges, we propose AgentSpec, a lightweight domain-specific language for specifying and enforcing runtime constraints on LLM agents. With AgentSpec, users define structured rules that incorporate triggers, predicates, and enforcement mechanisms, ensuring agents operate within predefined safety boundaries. We implement AgentSpec across multiple domains, including code execution, embodied agents, and autonomous driving, demonstrating its adaptability and effectiveness. Our evaluation shows that AgentSpec successfully prevents unsafe executions in over 90% of code agent cases, eliminates all hazardous actions in embodied agent tasks, and enforces 100% compliance by autonomous vehicles (AVs). Despite its strong safety guarantees, AgentSpec remains computationally lightweight, with overheads in milliseconds. By combining interpretability, modularity, and efficiency, AgentSpec provides a practical and scalable solution for enforcing LLM agent safety across diverse applications. We also automate the generation of rules using LLMs and assess their effectiveness. Our evaluation shows that the rules generated by OpenAI o1 achieve a precision of 95.56% and recall of 70.96% for embodied agents, successfully identifying 87.26% of the risky code, and prevent AVs from breaking laws in 5 out of 8 scenarios.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18666"
    },
    "c480a8219a40bd84955386135c5ca67b": {
        "title": "EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments",
        "authors": [
            "Sara Fish",
            "Julia Shephard",
            "Minkai Li",
            "Ran I. Shorrer",
            "Yannai A. Gonczarowski"
        ],
        "date": "2025/03/24",
        "pdf": "http://arxiv.org/pdf/2503.18825",
        "abstract": "We develop benchmarks for LLM agents that act in, learn from, and strategize in unknown environments, the specifications of which the LLM agent must learn over time from deliberate exploration. Our benchmarks consist of decision-making tasks derived from key problems in economics. To forestall saturation, the benchmark tasks are synthetically generated with scalable difficulty levels. Additionally, we propose litmus tests, a new kind of quantitative measure for LLMs and LLM agents. Unlike benchmarks, litmus tests quantify differences in character, values, and tendencies of LLMs and LLM agents, by considering their behavior when faced with tradeoffs (e.g., efficiency versus equality) where there is no objectively right or wrong behavior. Overall, our benchmarks and litmus tests assess the abilities and tendencies of LLM agents in tackling complex economic problems in diverse settings spanning procurement, scheduling, task allocation, and pricing -- applications that should grow in importance as such agents are further integrated into the economy.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.18825"
    },
    "8d093e6b11f15e74cfcd9bbb2bac0a35": {
        "title": "Multi-agent Application System in Office Collaboration Scenarios",
        "authors": [
            "Songtao Sun",
            "Jingyi Li",
            "Yuanfei Dong",
            "Haoguang Liu",
            "Chenxin Xu",
            "Fuyang Li",
            "Qiang Liu"
        ],
        "date": "2025/03/25",
        "pdf": "http://arxiv.org/pdf/2503.19584",
        "abstract": "This paper introduces a multi-agent application system designed to enhance office collaboration efficiency and work quality. The system integrates artificial intelligence, machine learning, and natural language processing technologies, achieving functionalities such as task allocation, progress monitoring, and information sharing. The agents within the system are capable of providing personalized collaboration support based on team members&#39; needs and incorporate data analysis tools to improve decision-making quality. The paper also proposes an intelligent agent architecture that separates Plan and Solver, and through techniques such as multi-turn query rewriting and business tool retrieval, it enhances the agent&#39;s multi-intent and multi-turn dialogue capabilities. Furthermore, the paper details the design of tools and multi-turn dialogue in the context of office collaboration scenarios, and validates the system&#39;s effectiveness through experiments and evaluations. Ultimately, the system has demonstrated outstanding performance in real business applications, particularly in query understanding, task planning, and tool calling. Looking forward, the system is expected to play a more significant role in addressing complex interaction issues within dynamic environments and large-scale multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.19584"
    },
    "041b86faedf7fe252d13a1480bae33b9": {
        "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents",
        "authors": [
            "Salaheddin Alzubi",
            "Creston Brooks",
            "Purva Chiniya",
            "Edoardo Contente",
            "Chiara von Gerlach",
            "Lucas Irwin",
            "Yihan Jiang",
            "Arda Kaz",
            "Windsor Nguyen",
            "Sewoong Oh",
            "Himanshu Tyagi",
            "Pramod Viswanath"
        ],
        "date": "2025/03/26",
        "pdf": "http://arxiv.org/pdf/2503.20201",
        "abstract": "We introduce Open Deep Search (ODS) to close the increasing gap between the proprietary search AI solutions, such as Perplexity&#39;s Sonar Reasoning Pro and OpenAI&#39;s GPT-4o Search Preview, and their open-source counterparts. The main innovation introduced in ODS is to augment the reasoning capabilities of the latest open-source LLMs with reasoning agents that can judiciously use web search tools to answer queries. Concretely, ODS consists of two components that work with a base LLM chosen by the user: Open Search Tool and Open Reasoning Agent. Open Reasoning Agent interprets the given task and completes it by orchestrating a sequence of actions that includes calling tools, one of which is the Open Search Tool. Open Search Tool is a novel web search tool that outperforms proprietary counterparts. Together with powerful open-source reasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses the existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES. For example, on the FRAMES evaluation benchmark, ODS improves the best existing baseline of the recently released GPT-4o Search Preview by 9.7% in accuracy. ODS is a general framework for seamlessly augmenting any LLMs -- for example, DeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search and reasoning capabilities to achieve state-of-the-art performance: 88.3% on SimpleQA and 75.3% on FRAMES.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.20201"
    },
    "31163d24a54eb32676b64f7e919b255f": {
        "title": "TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews",
        "authors": [
            "Huimin Xu",
            "Seungjun Yi",
            "Terence Lim",
            "Jiawei Xu",
            "Andrew Well",
            "Carlos Mery",
            "Aidong Zhang",
            "Yuji Zhang",
            "Heng Ji",
            "Keshav Pingali",
            "Yan Leng",
            "Ying Ding"
        ],
        "date": "2025/03/26",
        "pdf": "http://arxiv.org/pdf/2503.20666",
        "abstract": "Thematic analysis (TA) is a widely used qualitative approach for uncovering latent meanings in unstructured text data. TA provides valuable insights in healthcare but is resource-intensive. Large Language Models (LLMs) have been introduced to perform TA, yet their applications in healthcare remain unexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis framework using Multi-Agent LLMs for clinical interviews. We leverage the scalability and coherence of multi-agent systems through structured conversations between agents and coordinate the expertise of cardiac experts in TA. Using interview transcripts from parents of children with Anomalous Aortic Origin of a Coronary Artery (AAOCA), a rare congenital heart disease, we demonstrate that TAMA outperforms existing LLM-assisted TA approaches, achieving higher thematic hit rate, coverage, and distinctiveness. TAMA demonstrates strong potential for automated TA in clinical settings by leveraging multi-agent LLM systems with human-in-the-loop integration by enhancing quality while significantly reducing manual workload.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ],
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.20666"
    },
    "1784972f2e5bc05dd88a3a48000f1531": {
        "title": "Bias-Aware Agent: Enhancing Fairness in AI-Driven Knowledge Retrieval",
        "authors": [
            "Karanbir Singh",
            "William Ngu"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21237",
        "abstract": "Advancements in retrieving accessible information have evolved faster in the last few years compared to the decades since the internet&#39;s creation. Search engines, like Google, have been the number one way to find relevant data. They have always relied on the user&#39;s abilities to find the best information in its billions of links and sources at everybody&#39;s fingertips. The advent of large language models (LLMs) has completely transformed the field of information retrieval. The LLMs excel not only at retrieving relevant knowledge but also at summarizing it effectively, making information more accessible and consumable for users. On top of it, the rise of AI Agents has introduced another aspect to information retrieval i.e. dynamic information retrieval which enables the integration of real-time data such as weather forecasts, and financial data with the knowledge base to curate context-aware knowledge. However, despite these advancements the agents remain susceptible to issues of bias and fairness, challenges deeply rooted within the knowledge base and training of LLMs. This study introduces a novel approach to bias-aware knowledge retrieval by leveraging agentic framework and the innovative use of bias detectors as tools to identify and highlight inherent biases in the retrieved content. By empowering users with transparency and awareness, this approach aims to foster more equitable information systems and promote the development of responsible AI.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21237"
    },
    "d77e4c32a22fd63bd02e87399958d736": {
        "title": "GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics",
        "authors": [
            "Arsham Gholamzadeh Khoee",
            "Shuai Wang",
            "Yinan Yu",
            "Robert Feldt",
            "Dhasarathy Parthasarathy"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.21735",
        "abstract": "Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.21735"
    },
    "5d15d8ea90e2bd8389937e46fc7983a3": {
        "title": "Debate-Driven Multi-Agent LLMs for Phishing Email Detection",
        "authors": [
            "Ngoc Tuong Vy Nguyen",
            "Felix D Childress",
            "Yunting Yin"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2503.22038",
        "abstract": "Phishing attacks remain a critical cybersecurity threat. Attackers constantly refine their methods, making phishing emails harder to detect. Traditional detection methods, including rule-based systems and supervised machine learning models, either rely on predefined patterns like blacklists, which can be bypassed with slight modifications, or require large datasets for training and still can generate false positives and false negatives. In this work, we propose a multi-agent large language model (LLM) prompting technique that simulates debates among agents to detect whether the content presented on an email is phishing. Our approach uses two LLM agents to present arguments for or against the classification task, with a judge agent adjudicating the final verdict based on the quality of reasoning provided. This debate mechanism enables the models to critically analyze contextual cue and deceptive patterns in text, which leads to improved classification accuracy. The proposed framework is evaluated on multiple phishing email datasets and demonstrate that mixed-agent configurations consistently outperform homogeneous configurations. Results also show that the debate structure itself is sufficient to yield accurate decisions without extra prompting strategies.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2503.22038"
    },
    "c3d5c0cddc1bcca3c3ff3a6628c40791": {
        "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers",
        "authors": [
            "Yanzheng Xiang",
            "Hanqi Yan",
            "Shuyin Ouyang",
            "Lin Gui",
            "Yulan He"
        ],
        "date": "2025/03/31",
        "pdf": "http://arxiv.org/pdf/2504.00255",
        "abstract": "This study evaluates large language models (LLMs) in generating code from algorithm descriptions from recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a multi-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implement solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful Non-Reasoning LLMs and Reasoning LLMs as foundational models. The best-performing LLM using Sci-Reproducer achieves only 39% execution accuracy, highlighting the benchmark&#39;s difficulty.Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We will open-source our benchmark, and code at https://github.com/xyzCS/SciReplicate-Bench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00255"
    },
    "869bee540143ad8df01c0ae32f1dc629": {
        "title": "When Persuasion Overrides Truth in Multi-Agent LLM Debates: Introducing a Confidence-Weighted Persuasion Override Rate (CW-POR)",
        "authors": [
            "Mahak Agarwal",
            "Divyam Khanna"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00374",
        "abstract": "In many real-world scenarios, a single Large Language Model (LLM) may encounter contradictory claims-some accurate, others forcefully incorrect-and must judge which is true. We investigate this risk in a single-turn, multi-agent debate framework: one LLM-based agent provides a factual answer from TruthfulQA, another vigorously defends a falsehood, and the same LLM architecture serves as judge. We introduce the Confidence-Weighted Persuasion Override Rate (CW-POR), which captures not only how often the judge is deceived but also how strongly it believes the incorrect choice. Our experiments on five open-source LLMs (3B-14B parameters), where we systematically vary agent verbosity (30-300 words), reveal that even smaller models can craft persuasive arguments that override truthful answers-often with high confidence. These findings underscore the importance of robust calibration and adversarial testing to prevent LLMs from confidently endorsing misinformation.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00374"
    },
    "651adb5d58cb69adcddabac87519905a": {
        "title": "VerifiAgent: a Unified Verification Agent in Language Model Reasoning",
        "authors": [
            "Jiuzhou Han",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00406",
        "abstract": "Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00406"
    },
    "e598e2e2537ec06e7c4cb01ab5b2b3cb": {
        "title": "On the Robustness of Agentic Function Calling",
        "authors": [
            "Ella Rabinovich",
            "Ateret Anaby-Tavor"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00914",
        "abstract": "Large Language Models (LLMs) are increasingly acting as autonomous agents, with function calling (FC) capabilities enabling them to invoke specific tools for tasks. While prior research has primarily focused on improving FC accuracy, little attention has been given to the robustness of these agents to perturbations in their input. We introduce a benchmark assessing FC robustness in two key areas: resilience to naturalistic query variations, and stability in function calling when the toolkit expands with semantically related tools. Evaluating best-performing FC models on a carefully expanded subset of the Berkeley function calling leaderboard (BFCL), we identify critical weaknesses in existing evaluation methodologies, and highlight areas for improvement in real-world agentic deployments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00914"
    },
    "8e058bc67633854b1e830ffd69c3e49f": {
        "title": "Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection",
        "authors": [
            "Souradip Chakraborty",
            "Mohammadreza Pourreza",
            "Ruoxi Sun",
            "Yiwen Song",
            "Nino Scherrer",
            "Jindong Gu",
            "Furong Huang",
            "Amrit Singh Bedi",
            "Ahmad Beirami",
            "Hamid Palangi",
            "Tomas Pfister"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.01931",
        "abstract": "While AI agents have shown remarkable performance at various tasks, they still struggle with complex multi-modal applications, structured generation and strategic planning. Improvements via standard fine-tuning is often impractical, as solving agentic tasks usually relies on black box API access without control over model parameters. Inference-time methods such as Best-of-N (BON) sampling offer a simple yet effective alternative to improve performance. However, BON lacks iterative feedback integration mechanism. Hence, we propose Iterative Agent Decoding (IAD) which combines iterative refinement with dynamic candidate evaluation and selection guided by a verifier. IAD differs in how feedback is designed and integrated, specifically optimized to extract maximal signal from reward scores. We conduct a detailed comparison of baselines across key metrics on Sketch2Code, Text2SQL, and Webshop where IAD consistently outperforms baselines, achieving 3--6% absolute gains on Sketch2Code and Text2SQL (with and without LLM judges) and 8--10% gains on Webshop across multiple metrics. To better understand the source of IAD&#39;s gains, we perform controlled experiments to disentangle the effect of adaptive feedback from stochastic sampling, and find that IAD&#39;s improvements are primarily driven by verifier-guided refinement, not merely sampling diversity. We also show that both IAD and BON exhibit inference-time scaling with increased compute when guided by an optimal verifier. Our analysis highlights the critical role of verifier quality in effective inference-time optimization and examines the impact of noisy and sparse rewards on scaling behavior. Together, these findings offer key insights into the trade-offs and principles of effective inference-time optimization.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.01931"
    },
    "cb60bbcab95e7d11cc2b6339a2a8f004": {
        "title": "LLMs as Deceptive Agents: How Role-Based Prompting Induces Semantic Ambiguity in Puzzle Tasks",
        "authors": [
            "Seunghyun Yoo"
        ],
        "date": "2025/04/03",
        "pdf": "http://arxiv.org/pdf/2504.02254",
        "abstract": "Recent advancements in Large Language Models (LLMs) have not only showcased impressive creative capabilities but also revealed emerging agentic behaviors that exploit linguistic ambiguity in adversarial settings. In this study, we investigate how an LLM, acting as an autonomous agent, leverages semantic ambiguity to generate deceptive puzzles that mislead and challenge human users. Inspired by the popular puzzle game &#34;Connections&#34;, we systematically compare puzzles produced through zero-shot prompting, role-injected adversarial prompts, and human-crafted examples, with an emphasis on understanding the underlying agent decision-making processes. Employing computational analyses with HateBERT to quantify semantic ambiguity, alongside subjective human evaluations, we demonstrate that explicit adversarial agent behaviors significantly heighten semantic ambiguity -- thereby increasing cognitive load and reducing fairness in puzzle solving. These findings provide critical insights into the emergent agentic qualities of LLMs and underscore important ethical considerations for evaluating and safely deploying autonomous language systems in both educational technologies and entertainment.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02254"
    },
    "dd08795743487f7e05abb8dc97d2fb5d": {
        "title": "Multi-Agent LLM Judge: automatic personalized LLM judge design for evaluating natural language generation applications",
        "authors": [
            "Hongliu Cao",
            "Ilias Driouich",
            "Robin Singh",
            "Eoin Thomas"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.02867",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across diverse domains, yet they still encounter challenges such as insufficient domain-specific knowledge, biases, and hallucinations. This underscores the need for robust evaluation methodologies to accurately assess LLM-based applications. Traditional evaluation methods, which rely on word overlap or text embeddings, are inadequate for capturing the nuanced semantic information necessary to evaluate dynamic, open-ended text generation. Recent research has explored leveraging LLMs to mimic human reasoning and decision-making processes for evaluation purposes known as LLM-as-a-judge framework. However, these existing frameworks have two significant limitations. First, they lack the flexibility to adapt to different text styles, including various answer and ground truth styles, thereby reducing their generalization performance. Second, the evaluation scores produced by these frameworks are often skewed and hard to interpret, showing a low correlation with human judgment. To address these challenges, we propose a novel dynamic multi-agent system that automatically designs personalized LLM judges for various natural language generation applications. This system iteratively refines evaluation prompts and balances the trade-off between the adaptive requirements of downstream tasks and the alignment with human perception. Our experimental results show that the proposed multi-agent LLM Judge framework not only enhances evaluation accuracy compared to existing methods but also produces evaluation scores that better align with human perception.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02867"
    },
    "5b4e54c0d1b19adb3af364d5dede720e": {
        "title": "AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening",
        "authors": [
            "Frank P. -W. Lo",
            "Jianing Qiu",
            "Zeyu Wang",
            "Haibao Yu",
            "Yeming Chen",
            "Gao Zhang",
            "Benny Lo"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.02870",
        "abstract": "Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02870"
    },
    "a70c1b702a56414e2cc1382419bb06eb": {
        "title": "Automated Survey Collection with LLM-based Conversational Agents",
        "authors": [
            "Kurmanbek Kaiyrbekov",
            "Nicholas J Dobbins",
            "Sean D Mooney"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.02891",
        "abstract": "Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs). Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience. Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction. Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02891"
    },
    "8e41e0ff92cd02812d3fcf7135b9e855": {
        "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of Language Agents",
        "authors": [
            "Jaymari Chua",
            "Chen Wang",
            "Lina Yao"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03185",
        "abstract": "Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03185"
    },
    "f1e7f650d24b258f5ca46a80709373e3": {
        "title": "Agentic Knowledgeable Self-awareness",
        "authors": [
            "Shuofei Qiao",
            "Zhisong Qiu",
            "Baochang Ren",
            "Xiaobin Wang",
            "Xiangyuan Ru",
            "Ningyu Zhang",
            "Xiang Chen",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03553",
        "abstract": "Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a &#34;flood irrigation&#34; methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent&#39;s self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03553"
    },
    "7f41b5e225a4622d908b1745fc77dee3": {
        "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
        "authors": [
            "Runnan Fang",
            "Xiaobin Wang",
            "Yuan Liang",
            "Shuofei Qiao",
            "Jialong Wu",
            "Zekun Xi",
            "Ningyu Zhang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03561",
        "abstract": "In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03561"
    },
    "a0851d5e71d927ddf0522d160534778d": {
        "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
        "authors": [
            "Akshara Prabhakar",
            "Zuxin Liu",
            "Weiran Yao",
            "Jianguo Zhang",
            "Ming Zhu",
            "Shiyu Wang",
            "Zhiwei Liu",
            "Tulika Awalgaonkar",
            "Haolin Chen",
            "Thai Hoang",
            "Juan Carlos Niebles",
            "Shelby Heinecke",
            "Huan Wang",
            "Silvio Savarese",
            "Caiming Xiong"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03601",
        "abstract": "Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03601"
    },
    "12764ab8f05768ddb96724bc25ecf7f4": {
        "title": "$\\textit{Agents Under Siege}$: Breaking Pragmatic Multi-Agent LLM Systems with Optimized Prompt Attacks",
        "authors": [
            "Rana Muhammad Shahroz Khan",
            "Zhen Tan",
            "Sukwon Yun",
            "Charles Flemming",
            "Tianlong Chen"
        ],
        "date": "2025/03/31",
        "pdf": "http://arxiv.org/pdf/2504.00218",
        "abstract": "Most discussions about Large Language Model (LLM) safety have focused on single-agent settings but multi-agent LLM systems now create novel adversarial risks because their behavior depends on communication between agents and decentralized reasoning. In this work, we innovatively focus on attacking pragmatic systems that have constrains such as limited token bandwidth, latency between message delivery, and defense mechanisms. We design a $\\textit{permutation-invariant adversarial attack}$ that optimizes prompt distribution across latency and bandwidth-constraint network topologies to bypass distributed safety mechanisms within the system. Formulating the attack path as a problem of $\\textit{maximum-flow minimum-cost}$, coupled with the novel $\\textit{Permutation-Invariant Evasion Loss (PIEL)}$, we leverage graph-based optimization to maximize attack success rate while minimizing detection risk. Evaluating across models including $\\texttt{Llama}$, $\\texttt{Mistral}$, $\\texttt{Gemma}$, $\\texttt{DeepSeek}$ and other variants on various datasets like $\\texttt{JailBreakBench}$ and $\\texttt{AdversarialBench}$, our method outperforms conventional attacks by up to $7\\times$, exposing critical vulnerabilities in multi-agent systems. Moreover, we demonstrate that existing defenses, including variants of $\\texttt{Llama-Guard}$ and $\\texttt{PromptGuard}$, fail to prohibit our attack, emphasizing the urgent need for multi-agent specific safety mechanisms.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00218"
    },
    "d431300dd5ec7f09b642bdc1a951c612": {
        "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems",
        "authors": [
            "Yingxuan Yang",
            "Huacan Chai",
            "Shuai Shao",
            "Yuanyi Song",
            "Siyuan Qi",
            "Renting Rui",
            "Weinan Zhang"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00587",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of multi-agent systems, where multiple LLM-based agents collaborate to solve complex tasks. However, existing systems predominantly rely on centralized coordination, which introduces scalability bottlenecks, limits adaptability, and creates single points of failure. Additionally, concerns over privacy and proprietary knowledge sharing hinder cross-organizational collaboration, leading to siloed expertise. To address these challenges, we propose AgentNet, a decentralized, Retrieval-Augmented Generation (RAG)-based framework that enables LLM-based agents to autonomously evolve their capabilities and collaborate efficiently in a Directed Acyclic Graph (DAG)-structured network. Unlike traditional multi-agent systems that depend on static role assignments or centralized control, AgentNet allows agents to specialize dynamically, adjust their connectivity, and route tasks without relying on predefined workflows. AgentNet&#39;s core design is built upon several key innovations: (1) Fully Decentralized Paradigm: Removing the central orchestrator, allowing agents to coordinate and specialize autonomously, fostering fault tolerance and emergent collective intelligence. (2) Dynamically Evolving Graph Topology: Real-time adaptation of agent connections based on task demands, ensuring scalability and resilience.(3) Adaptive Learning for Expertise Refinement: A retrieval-based memory system that enables agents to continuously update and refine their specialized skills. By eliminating centralized control, AgentNet enhances fault tolerance, promotes scalable specialization, and enables privacy-preserving collaboration across organizations. Through decentralized coordination and minimal data exchange, agents can leverage diverse knowledge sources while safeguarding sensitive information.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00587"
    },
    "96a937d20277044f3eac65e33ebe67a5": {
        "title": "Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents",
        "authors": [
            "Saaket Agashe",
            "Kyle Wong",
            "Vincent Tu",
            "Jiachen Yang",
            "Ang Li",
            "Xin Eric Wang"
        ],
        "date": "2025/04/01",
        "pdf": "http://arxiv.org/pdf/2504.00906",
        "abstract": "Computer use agents automate digital tasks by directly interacting with graphical user interfaces (GUIs) on computers and mobile devices, offering significant potential to enhance human productivity by completing an open-ended space of user queries. However, current agents face significant challenges: imprecise grounding of GUI elements, difficulties with long-horizon task planning, and performance bottlenecks from relying on single generalist models for diverse cognitive tasks. To this end, we introduce Agent S2, a novel compositional framework that delegates cognitive responsibilities across various generalist and specialist models. We propose a novel Mixture-of-Grounding technique to achieve precise GUI localization and introduce Proactive Hierarchical Planning, dynamically refining action plans at multiple temporal scales in response to evolving observations. Evaluations demonstrate that Agent S2 establishes new state-of-the-art (SOTA) performance on three prominent computer use benchmarks. Specifically, Agent S2 achieves 18.9% and 32.7% relative improvements over leading baseline agents such as Claude Computer Use and UI-TARS on the OSWorld 15-step and 50-step evaluation. Moreover, Agent S2 generalizes effectively to other operating systems and applications, surpassing previous best methods by 52.8% on WindowsAgentArena and by 16.52% on AndroidWorld relatively. Code available at https://github.com/simular-ai/Agent-S.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.00906"
    },
    "5e77490cfe3998c72dcaacb6832c305d": {
        "title": "An Illusion of Progress? Assessing the Current State of Web Agents",
        "authors": [
            "Tianci Xue",
            "Weijian Qi",
            "Tianneng Shi",
            "Chan Hee Song",
            "Boyu Gou",
            "Dawn Song",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.01382",
        "abstract": "As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.01382"
    },
    "c0935b16b561f9ad742d8149e1d32ef6": {
        "title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
        "authors": [
            "R. M. Aratchige",
            "W. M. K. S. Ilmini"
        ],
        "date": "2025/03/13",
        "pdf": "http://arxiv.org/pdf/2504.01963",
        "abstract": "This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.01963"
    },
    "d320b94a2f0a9406ba6bc990ca004ebd": {
        "title": "Self-Resource Allocation in Multi-Agent LLM Systems",
        "authors": [
            "Alfonso Amayuelas",
            "Jingbo Yang",
            "Saaket Agashe",
            "Ashwin Nagarajan",
            "Antonis Antoniades",
            "Xin Eric Wang",
            "William Wang"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.02051",
        "abstract": "With the development of LLMs as agents, there is a growing interest in connecting multiple agents into multi-agent systems to solve tasks concurrently, focusing on their role in task assignment and coordination. This paper explores how LLMs can effectively allocate computational tasks among multiple agents, considering factors such as cost, efficiency, and performance. In this work, we address key questions, including the effectiveness of LLMs as orchestrators and planners, comparing their effectiveness in task assignment and coordination. Our experiments demonstrate that LLMs can achieve high validity and accuracy in resource allocation tasks. We find that the planner method outperforms the orchestrator method in handling concurrent actions, resulting in improved efficiency and better utilization of agents. Additionally, we show that providing explicit information about worker capabilities enhances the allocation strategies of planners, particularly when dealing with suboptimal workers.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02051"
    },
    "bb24a3a2b05728444d59fd3bca4010b1": {
        "title": "Achieving Unanimous Consensus in Decision Making Using Multi-Agents",
        "authors": [
            "Apurba Pokharel",
            "Ram Dantu",
            "Shakila Zaman",
            "Sirisha Talapuru",
            "Vinh Quach"
        ],
        "date": "2025/04/02",
        "pdf": "http://arxiv.org/pdf/2504.02128",
        "abstract": "Blockchain consensus mechanisms have relied on algorithms such as Proof-of-Work (PoW) and Proof-of-Stake (PoS) to ensure network functionality and integrity. However, these approaches struggle with adaptability for decision-making where the opinions of each matter rather than reaching an agreement based on honest majority or weighted consensus. This paper introduces a novel deliberation-based consensus mechanism where Large Language Models (LLMs) act as rational agents engaging in structured discussions to reach a unanimous consensus. By leveraging graded consensus and a multi-round deliberation process, our approach ensures both unanimous consensus for definitive problems and graded confidence for prioritized decisions and policies. We provide a formalization of our system and use it to show that the properties of blockchains: consistency, agreement, liveness, and determinism are maintained. Moreover, experimental results demonstrate our system&#39;s feasibility, showcasing how our deliberation method&#39;s convergence, block properties, and accuracy enable decision-making on blockchain networks. We also address key challenges with this novel approach such as degeneration of thoughts, hallucinations, malicious models and nodes, resource consumption, and scalability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.02128"
    },
    "2e1105998bfe886b039781b55af1b916": {
        "title": "Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective",
        "authors": [
            "Garry A. Gabison",
            "R. Patrick Xian"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03255",
        "abstract": "Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention over effective governance policies, monitoring and control protocols. Based on emerging landscapes of the agentic market, we analyze the potential liability issues stemming from delegated use of LLM agents and their extended systems from a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing and monitoring approaches to enhancing transparency and accountability.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03255"
    },
    "4304608933a6b567340702fb3726f01e": {
        "title": "YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization",
        "authors": [
            "Dongsuk Jang",
            "Alan Li",
            "Arman Cohan"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03932",
        "abstract": "Automated summarization of healthcare community question-answering forums is challenging due to diverse perspectives presented across multiple user responses to each question. The PerAnsSumm Shared Task was therefore proposed to tackle this challenge by identifying perspectives from different answers and then generating a comprehensive answer to the question. In this study, we address the PerAnsSumm Shared Task using two complementary paradigms: (i) a training-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct, and (ii) agentic approaches including zero- and few-shot prompting with frontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA) framework that leverages a diverse set of LLMs by combining outputs from multi-layer feedback aggregation. For perspective span identification/classification, GPT-4o zero-shot achieves an overall score of 0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a 2-layer MoA configuration, we were able to improve LLaMA performance up by 28 percent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains an overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our 2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37. Furthermore, in few-shot setting, our results show that the sentence-transformer embedding-based exemplar selection provides more gain than manually selected exemplars on LLaMA models, although the few-shot prompting is not always helpful for GPT-4o. The YaleNLP team&#39;s approach ranked the overall second place in the shared task.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03932"
    },
    "46b7ba6cea59476d0d699c4b4f30720e": {
        "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization",
        "authors": [
            "Weiwei Sun",
            "Shengyu Feng",
            "Shanda Li",
            "Yiming Yang"
        ],
        "date": "2025/04/06",
        "pdf": "http://arxiv.org/pdf/2504.04310",
        "abstract": "Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems-a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agent frameworks against established human-designed algorithms, revealing key strengths and limitations of current approaches and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.04310"
    },
    "32a7978023ee5560285f9c4e3422152f": {
        "title": "scAgent: Universal Single-Cell Annotation via a LLM Agent",
        "authors": [
            "Yuren Mao",
            "Yu Mi",
            "Peigen Liu",
            "Mengfei Zhang",
            "Hanqing Liu",
            "Yunjun Gao"
        ],
        "date": "2025/04/07",
        "pdf": "http://arxiv.org/pdf/2504.04698",
        "abstract": "Cell type annotation is critical for understanding cellular heterogeneity. Based on single-cell RNA-seq data and deep learning models, good progress has been made in annotating a fixed number of cell types within a specific tissue. However, universal cell annotation, which can generalize across tissues, discover novel cell types, and extend to novel cell types, remains less explored. To fill this gap, this paper proposes scAgent, a universal cell annotation framework based on Large Language Models (LLMs). scAgent can identify cell types and discover novel cell types in diverse tissues; furthermore, it is data efficient to learn novel cell types. Experimental studies in 160 cell types and 35 tissues demonstrate the superior performance of scAgent in general cell-type annotation, novel cell discovery, and extensibility to novel cell type.",
        "code": "",
        "category": [
            [
                "Application",
                "Biology"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.04698"
    },
    "6c0d24ef48c97344a1595b6fcd138183": {
        "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments",
        "authors": [
            "Saeid Ario Vaghefi",
            "Aymane Hachcham",
            "Veronica Grasso",
            "Jiska Manicus",
            "Nakiete Msemo",
            "Chiara Colesanti Senni",
            "Markus Leippold"
        ],
        "date": "2025/04/07",
        "pdf": "http://arxiv.org/pdf/2504.05104",
        "abstract": "Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.05104"
    },
    "86ccdbad00f5f489af483287e734c540": {
        "title": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation",
        "authors": [
            "Xinglin Lyu",
            "Wei Tang",
            "Yuang Li",
            "Xiaofeng Zhao",
            "Ming Zhu",
            "Junhui Li",
            "Yunfei Lu",
            "Min Zhang",
            "Daimeng Wei",
            "Hao Yang",
            "Min Zhang"
        ],
        "date": "2025/04/07",
        "pdf": "http://arxiv.org/pdf/2504.05122",
        "abstract": "Document-level context is crucial for handling discourse challenges in text-to-text document-level machine translation (MT). Despite the increased discourse challenges introduced by noise from automatic speech recognition (ASR), the integration of document-level context in speech translation (ST) remains insufficiently explored. In this paper, we develop DoCIA, an online framework that enhances ST performance by incorporating document-level context. DoCIA decomposes the ST pipeline into four stages. Document-level context is integrated into the ASR refinement, MT, and MT refinement stages through auxiliary LLM (large language model)-based modules. Furthermore, DoCIA leverages document-level information in a multi-level manner while minimizing computational overhead. Additionally, a simple yet effective determination mechanism is introduced to prevent hallucinations from excessive refinement, ensuring the reliability of the final results. Experimental results show that DoCIA significantly outperforms traditional ST baselines in both sentence and discourse metrics across four LLMs, demonstrating its effectiveness in improving ST performance.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.05122"
    },
    "48939eba1921b0a5fb1d45aab3507c65": {
        "title": "Bridging Industrial Expertise and XR with LLM-Powered Conversational Agents",
        "authors": [
            "Despina Tomkou",
            "George Fatouros",
            "Andreas Andreou",
            "Georgios Makridis",
            "Fotis Liarokapis",
            "Dimitrios Dardanis",
            "Athanasios Kiourtis",
            "John Soldatos",
            "Dimosthenis Kyriazis"
        ],
        "date": "2025/04/07",
        "pdf": "http://arxiv.org/pdf/2504.05527",
        "abstract": "This paper introduces a novel integration of Retrieval-Augmented Generation (RAG) enhanced Large Language Models (LLMs) with Extended Reality (XR) technologies to address knowledge transfer challenges in industrial environments. The proposed system embeds domain-specific industrial knowledge into XR environments through a natural language interface, enabling hands-free, context-aware expert guidance for workers. We present the architecture of the proposed system consisting of an LLM Chat Engine with dynamic tool orchestration and an XR application featuring voice-driven interaction. Performance evaluation of various chunking strategies, embedding models, and vector databases reveals that semantic chunking, balanced embedding models, and efficient vector stores deliver optimal performance for industrial knowledge retrieval. The system&#39;s potential is demonstrated through early implementation in multiple industrial use cases, including robotic assembly, smart infrastructure maintenance, and aerospace component servicing. Results indicate potential for enhancing training efficiency, remote assistance capabilities, and operational guidance in alignment with Industry 5.0&#39;s human-centric and resilient approach to industrial development.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.05527"
    },
    "9256c9601d03199cd764506049963c8c": {
        "title": "FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction",
        "authors": [
            "Qian-Wen Zhang",
            "Fang Li",
            "Jie Wang",
            "Lingfeng Qiao",
            "Yifei Yu",
            "Di Yin",
            "Xing Sun"
        ],
        "date": "2025/04/08",
        "pdf": "http://arxiv.org/pdf/2504.05607",
        "abstract": "Extractive reading comprehension systems are designed to locate the correct answer to a question within a given text. However, a persistent challenge lies in ensuring these models maintain high accuracy in answering questions while reliably recognizing unanswerable queries. Despite significant advances in large language models (LLMs) for reading comprehension, this issue remains critical, particularly as the length of supported contexts continues to expand. To address this challenge, we propose an innovative data augmentation methodology grounded in a multi-agent collaborative framework. Unlike traditional methods, such as the costly human annotation process required for datasets like SQuAD 2.0, our method autonomously generates evidence-based question-answer pairs and systematically constructs unanswerable questions. Using this methodology, we developed the FactGuard-Bench dataset, which comprises 25,220 examples of both answerable and unanswerable question scenarios, with context lengths ranging from 8K to 128K. Experimental evaluations conducted on seven popular LLMs reveal that even the most advanced models achieve only 61.79% overall accuracy. Furthermore, we emphasize the importance of a model&#39;s ability to reason about unanswerable questions to avoid generating plausible but incorrect answers. By implementing efficient data selection and generation within the multi-agent collaborative framework, our method significantly reduces the traditionally high costs associated with manual annotation and provides valuable insights for the training and optimization of LLMs.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.05607"
    },
    "12cece4a248f402632f6074f72b52318": {
        "title": "Inducing Programmatic Skills for Agentic Tasks",
        "authors": [
            "Zora Zhiruo Wang",
            "Apurva Gandhi",
            "Graham Neubig",
            "Daniel Fried"
        ],
        "date": "2025/04/09",
        "pdf": "http://arxiv.org/pdf/2504.06821",
        "abstract": "To succeed in common digital tasks such as web navigation, agents must carry out a variety of specialized tasks such as searching for products or planning a travel route. To tackle these tasks, agents can bootstrap themselves by learning task-specific skills online through interaction with the web environment. In this work, we demonstrate that programs are an effective representation for skills. We propose agent skill induction (ASI), which allows agents to adapt themselves by inducing, verifying, and utilizing program-based skills on the fly. We start with an evaluation on the WebArena agent benchmark and show that ASI outperforms the static baseline agent and its text-skill counterpart by 23.5% and 11.3% in success rate, mainly thanks to the programmatic verification guarantee during the induction phase. ASI also improves efficiency by reducing 10.7-15.3% of the steps over baselines, by composing primitive actions (e.g., click) into higher-level skills (e.g., search product). We then highlight the efficacy of ASI in remaining efficient and accurate under scaled-up web activities. Finally, we examine the generalizability of induced skills when transferring between websites, and find that ASI can effectively reuse common skills, while also updating incompatible skills to versatile website changes.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.06821"
    },
    "0cccdaa9e928156ce3f9b9b7493548b2": {
        "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games",
        "authors": [
            "Seungwon Lim",
            "Seungbeen Lee",
            "Dongjun Min",
            "Youngjae Yu"
        ],
        "date": "2025/04/09",
        "pdf": "http://arxiv.org/pdf/2504.06868",
        "abstract": "Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: PersonalityAdapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent&#39;s actions exhibit, and (ii) we integrate the personality profiles directly into the agent&#39;s policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent&#39;s action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.06868"
    },
    "b7f47247380b82e946509b4214574bba": {
        "title": "AgentAda: Skill-Adaptive Data Analytics for Tailored Insight Discovery",
        "authors": [
            "Amirhossein Abaskohi",
            "Amrutha Varshini Ramesh",
            "Shailesh Nanisetty",
            "Chirag Goel",
            "David Vazquez",
            "Christopher Pal",
            "Spandana Gella",
            "Giuseppe Carenini",
            "Issam H. Laradji"
        ],
        "date": "2025/04/10",
        "pdf": "http://arxiv.org/pdf/2504.07421",
        "abstract": "We introduce AgentAda, the first LLM-powered analytics agent that can learn and use new analytics skills to extract more specialized insights. Unlike existing methods that require users to manually decide which data analytics method to apply, AgentAda automatically identifies the skill needed from a library of analytical skills to perform the analysis. This also allows AgentAda to use skills that existing LLMs cannot perform out of the box. The library covers a range of methods, including clustering, predictive modeling, and NLP techniques like BERT, which allow AgentAda to handle complex analytics tasks based on what the user needs. AgentAda&#39;s dataset-to-insight extraction strategy consists of three key steps: (I) a question generator to generate queries relevant to the user&#39;s goal and persona, (II) a hybrid Retrieval-Augmented Generation (RAG)-based skill matcher to choose the best data analytics skill from the skill library, and (III) a code generator that produces executable code based on the retrieved skill&#39;s documentation to extract key patterns. We also introduce KaggleBench, a benchmark of curated notebooks across diverse domains, to evaluate AgentAda&#39;s performance. We conducted a human evaluation demonstrating that AgentAda provides more insightful analytics than existing tools, with 48.78% of evaluators preferring its analyses, compared to 27.67% for the unskilled agent. We also propose a novel LLM-as-a-judge approach that we show is aligned with human evaluation as a way to automate insight quality evaluation at larger scale.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.07421"
    },
    "2b0ea1ad90e5a6ffc47493c5d67c2c00": {
        "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations",
        "authors": [
            "Genglin Liu",
            "Salman Rahman",
            "Elisa Kreiss",
            "Marzyeh Ghassemi",
            "Saadia Gabriel"
        ],
        "date": "2025/04/10",
        "pdf": "http://arxiv.org/pdf/2504.07830",
        "abstract": "We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents&#39; articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.07830"
    },
    "8863638e2b6f7de82d61144ada15d8e0": {
        "title": "Are Generative AI Agents Effective Personalized Financial Advisors?",
        "authors": [
            "Takehiro Takayanagi",
            "Kiyoshi Izumi",
            "Javier Sanz-Cruzado",
            "Richard McCreadie",
            "Iadh Ounis"
        ],
        "date": "2025/04/08",
        "pdf": "http://arxiv.org/pdf/2504.05862",
        "abstract": "Large language model-based agents are becoming increasingly popular as a low-cost mechanism to provide personalized, conversational advice, and have demonstrated impressive capabilities in relatively simple scenarios, such as movie recommendations. But how do these agents perform in complex high-stakes domains, where domain expertise is essential and mistakes carry substantial risk? This paper investigates the effectiveness of LLM-advisors in the finance domain, focusing on three distinct challenges: (1) eliciting user preferences when users themselves may be unsure of their needs, (2) providing personalized guidance for diverse investment preferences, and (3) leveraging advisor personality to build relationships and foster trust. Via a lab-based user study with 64 participants, we show that LLM-advisors often match human advisor performance when eliciting preferences, although they can struggle to resolve conflicting user needs. When providing personalized advice, the LLM was able to positively influence user behavior, but demonstrated clear failure modes. Our results show that accurate preference elicitation is key, otherwise, the LLM-advisor has little impact, or can even direct the investor toward unsuitable assets. More worryingly, users appear insensitive to the quality of advice being given, or worse these can have an inverse relationship. Indeed, users reported a preference for and increased satisfaction as well as emotional trust with LLMs adopting an extroverted persona, even though those agents provided worse advice.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.05862"
    },
    "867ad89f67f76d491fbaebcdc6cb902b": {
        "title": "SkillFlow: Efficient Skill and Code Transfer Through Communication in Adapting AI Agents",
        "authors": [
            "Pagkratios Tagkopoulos",
            "Fangzhou Li",
            "Ilias Tagkopoulos"
        ],
        "date": "2025/04/08",
        "pdf": "http://arxiv.org/pdf/2504.06188",
        "abstract": "AI agents are autonomous systems that can execute specific tasks based on predefined programming. Here, we present SkillFlow, a modular, technology-agnostic framework that allows agents to expand their functionality in an ad-hoc fashion by acquiring new skills from their environment or other agents. We present a theoretical model that examines under which conditions this framework would be beneficial, and we then explore SkillFlow&#39;s ability to accelerate task completion and lead to lower cumulative costs in a real-world application, namely scheduling agents for calendar events. We demonstrate that within a few iterations, SkillFlow leads to considerable (24.8%, p-value = $6.4\\times10^{-3}$) gains in time and cost, especially when the communication cost is high. Finally, we draw analogies from well-studied biological systems and compare this framework to that of lateral gene transfer, a significant process of adaptation and evolution in novel environments.",
        "code": "",
        "category": [
            [
                "Application",
                "Biology"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.06188"
    },
    "17ee0472ff52c81203c7508aea0aff04": {
        "title": "TxGemma: Efficient and Agentic LLMs for Therapeutics",
        "authors": [
            "Eric Wang",
            "Samuel Schmidgall",
            "Paul F. Jaeger",
            "Fan Zhang",
            "Rory Pilgrim",
            "Yossi Matias",
            "Joelle Barral",
            "David Fleet",
            "Shekoofeh Azizi"
        ],
        "date": "2025/04/08",
        "pdf": "http://arxiv.org/pdf/2504.06196",
        "abstract": "Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity&#39;s Last Exam benchmark (Chemistry &amp; Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.06196"
    },
    "e89c7a252e38cd195e762099845ca32b": {
        "title": "EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation Detection with Hierarchical Retrieval",
        "authors": [
            "Yin Wu",
            "Zhengxuan Zhang",
            "Fuling Wang",
            "Yuyu Luo",
            "Hui Xiong",
            "Nan Tang"
        ],
        "date": "2025/03/01",
        "pdf": "http://arxiv.org/pdf/2504.06269",
        "abstract": "Misinformation continues to pose a significant challenge in today&#39;s information ecosystem, profoundly shaping public perception and behavior. Among its various manifestations, Out-of-Context (OOC) misinformation is particularly obscure, as it distorts meaning by pairing authentic images with misleading textual narratives. Existing methods for detecting OOC misinformation predominantly rely on coarse-grained similarity metrics between image-text pairs, which often fail to capture subtle inconsistencies or provide meaningful explainability. While multi-modal large language models (MLLMs) demonstrate remarkable capabilities in visual reasoning and explanation generation, they have not yet demonstrated the capacity to address complex, fine-grained, and cross-modal distinctions necessary for robust OOC detection. To overcome these limitations, we introduce EXCLAIM, a retrieval-based framework designed to leverage external knowledge through multi-granularity index of multi-modal events and entities. Our approach integrates multi-granularity contextual analysis with a multi-agent reasoning architecture to systematically evaluate the consistency and integrity of multi-modal news content. Comprehensive experiments validate the effectiveness and resilience of EXCLAIM, demonstrating its ability to detect OOC misinformation with 4.3% higher accuracy compared to state-of-the-art approaches, while offering explainable and actionable insights.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ],
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.06269"
    },
    "c633f57a16dc85d5677b9ef3ef6ab51d": {
        "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
        "authors": [
            "Jifang Wang",
            "Xue Yang",
            "Longyue Wang",
            "Zhenran Xu",
            "Yiyu Wang",
            "Yaowei Wang",
            "Weihua Luo",
            "Kaifu Zhang",
            "Baotian Hu",
            "Min Zhang"
        ],
        "date": "2025/04/09",
        "pdf": "http://arxiv.org/pdf/2504.07046",
        "abstract": "Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval&#39;s capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.07046"
    },
    "0915bfa10e67699468f8c8db7d03f19b": {
        "title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills",
        "authors": [
            "Boyuan Zheng",
            "Michael Y. Fatemi",
            "Xiaolong Jin",
            "Zora Zhiruo Wang",
            "Apurva Gandhi",
            "Yueqi Song",
            "Yu Gu",
            "Jayanth Srinivasa",
            "Gaowen Liu",
            "Graham Neubig",
            "Yu Su"
        ],
        "date": "2025/04/09",
        "pdf": "http://arxiv.org/pdf/2504.07079",
        "abstract": "To survive and thrive in complex environments, humans have evolved sophisticated self-improvement mechanisms through environment exploration, hierarchical abstraction of experiences into reuseable skills, and collaborative construction of an ever-growing skill repertoire. Despite recent advancements, autonomous web agents still lack crucial self-improvement capabilities, struggling with procedural knowledge abstraction, refining skills, and skill composition. In this work, we introduce SkillWeaver, a skill-centric framework enabling agents to self-improve by autonomously synthesizing reusable skills as APIs. Given a new website, the agent autonomously discovers skills, executes them for practice, and distills practice experiences into robust APIs. Iterative exploration continually expands a library of lightweight, plug-and-play APIs, significantly enhancing the agent&#39;s capabilities. Experiments on WebArena and real-world websites demonstrate the efficacy of SkillWeaver, achieving relative success rate improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized by strong agents substantially enhance weaker agents through transferable skills, yielding improvements of up to 54.3% on WebArena. These results demonstrate the effectiveness of honing diverse website interactions into APIs, which can be seamlessly shared among various web agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.07079"
    },
    "95165b186634c9078d95a794e3353b26": {
        "title": "R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
        "authors": [
            "Naman Jain",
            "Jaskirat Singh",
            "Manish Shetty",
            "Liang Zheng",
            "Koushik Sen",
            "Ion Stoica"
        ],
        "date": "2025/04/09",
        "pdf": "http://arxiv.org/pdf/2504.07164",
        "abstract": "Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce AgentGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is powered by two main contributions: 1) SYNGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.07164"
    },
    "e07cc067d09d9cbc1da2044afd40bc63": {
        "title": "CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections",
        "authors": [
            "Florian Schneider",
            "Narges Baba Ahmadi",
            "Niloufar Baba Ahmadi",
            "Iris Vogel",
            "Martin Semmann",
            "Chris Biemann"
        ],
        "date": "2025/04/10",
        "pdf": "http://arxiv.org/pdf/2504.07643",
        "abstract": "In this paper, we introduce CollEx, an innovative multimodal agentic Retrieval-Augmented Generation (RAG) system designed to enhance interactive exploration of extensive scientific collections. Given the overwhelming volume and inherent complexity of scientific collections, conventional search systems often lack necessary intuitiveness and interactivity, presenting substantial barriers for learners, educators, and researchers. CollEx addresses these limitations by employing state-of-the-art Large Vision-Language Models (LVLMs) as multimodal agents accessible through an intuitive chat interface. By abstracting complex interactions via specialized agents equipped with advanced tools, CollEx facilitates curiosity-driven exploration, significantly simplifying access to diverse scientific collections and records therein. Our system integrates textual and visual modalities, supporting educational scenarios that are helpful for teachers, pupils, students, and researchers by fostering independent exploration as well as scientific excitement and curiosity. Furthermore, CollEx serves the research community by discovering interdisciplinary connections and complementing visual data. We illustrate the effectiveness of our system through a proof-of-concept application containing over 64,000 unique records across 32 collections from a local scientific collection from a public university.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.07643"
    },
    "07d3bee7098268616c2aada2f9273902": {
        "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models",
        "authors": [
            "Siddharth Srikanth",
            "Varun Bhatt",
            "Boshen Zhang",
            "Werner Hager",
            "Charles Michael Lewis",
            "Katia P. Sycara",
            "Aaquib Tabrez",
            "Stefanos Nikolaidis"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2504.03991",
        "abstract": "Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.03991"
    },
    "6da5ea664090dc428d5fb7efcdce8028": {
        "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
        "authors": [
            "Yin Jou Huang",
            "Rafik Hadfi"
        ],
        "date": "2025/04/11",
        "pdf": "http://arxiv.org/pdf/2504.08399",
        "abstract": "There is a growing interest in assessing the personality traits of Large language models (LLMs). However, traditional personality assessments based on self-report questionnaires may fail to capture their true behavioral nuances due to inherent biases and meta-knowledge contamination. This paper introduces a novel multi-observer framework for LLM personality assessment that draws inspiration from informant-report methods in psychology. Instead of relying solely on self-assessments, our approach employs multiple observer agents configured with a specific relationship context (e.g., family, friend, or workplace) to simulate interactive scenarios with a subject LLM. These observers engage in dialogues and subsequently provide ratings across the Big Five personality dimensions. Our experiments reveal that LLMs possess systematic biases in self-report personality ratings. Moreover, aggregating observer ratings effectively reduces non-systematic biases and achieves optimal reliability with 5-7 observers. The findings highlight the significant impact of relationship context on personality perception and demonstrate that a multi-observer paradigm yields a more robust and context-sensitive evaluation of LLM personality traits.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.08399"
    },
    "160e0d2ce771c77c1061302076f63b83": {
        "title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning",
        "authors": [
            "Hang Ni",
            "Fan Liu",
            "Xinyu Ma",
            "Lixin Su",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Hui Xiong",
            "Hao Liu"
        ],
        "date": "2025/04/11",
        "pdf": "http://arxiv.org/pdf/2504.08694",
        "abstract": "Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs&#39; intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.08694"
    },
    "461bfbd678b76474e544199679172f48": {
        "title": "A Multi-view Discourse Framework for Integrating Semantic and Syntactic Features in Dialog Agents",
        "authors": [
            "Akanksha Mehndiratta",
            "Krishna Asawa"
        ],
        "date": "2025/04/12",
        "pdf": "http://arxiv.org/pdf/2504.09073",
        "abstract": "Multiturn dialogue models aim to generate human-like responses by leveraging conversational context, consisting of utterances from previous exchanges. Existing methods often neglect the interactions between these utterances or treat all of them as equally significant. This paper introduces a discourse-aware framework for response selection in retrieval-based dialogue systems. The proposed model first encodes each utterance and response with contextual, positional, and syntactic features using Multi-view Canonical Correlation Analysis (MCCA). It then learns discourse tokens that capture relationships between an utterance and its surrounding turns in a shared subspace via Canonical Correlation Analysis (CCA). This two-step approach effectively integrates semantic and syntactic features to build discourse-level understanding. Experiments on the Ubuntu Dialogue Corpus demonstrate that our model achieves significant improvements in automatic evaluation metrics, highlighting its effectiveness in response selection.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.09073"
    },
    "b5102a429a76797267aebe18cb0c14bc": {
        "title": "UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents",
        "authors": [
            "Yuxuan Lu",
            "Bingsheng Yao",
            "Hansu Gu",
            "Jing Huang",
            "Jessie Wang",
            "Yang Li",
            "Jiri Gesi",
            "Qi He",
            "Toby Jia-Jun Li",
            "Dakuo Wang"
        ],
        "date": "2025/04/13",
        "pdf": "http://arxiv.org/pdf/2504.09407",
        "abstract": "Usability testing is a fundamental research method that user experience (UX) researchers use to evaluate and iterate a web design, but\\textbf{ how to evaluate and iterate the usability testing study design } itself? Recent advances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research inspired us to design \\textbf{UXAgent} to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features a Persona Generator module, an LLM Agent module, and a Universal Browser Connector module to automatically generate thousands of simulated users to interactively test the target website. The system also provides an Agent Interview Interface and a Video Replay Interface so that the UX researchers can easily review and analyze the generated qualitative and quantitative log data. Through a heuristic evaluation, five UX researcher participants praised the innovation of our system but also expressed concerns about the future of LLM Agent usage in UX studies.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.09407"
    },
    "1c1af06b4c613331d8e6a9fdd16309ed": {
        "title": "SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users",
        "authors": [
            "Xinnong Zhang",
            "Jiayu Lin",
            "Xinyi Mou",
            "Shiyue Yang",
            "Xiawei Liu",
            "Libo Sun",
            "Hanjia Lyu",
            "Yihang Yang",
            "Weihong Qi",
            "Yue Chen",
            "Guanying Li",
            "Ling Yan",
            "Yao Hu",
            "Siming Chen",
            "Yu Wang",
            "Xuanjing Huang",
            "Jiebo Luo",
            "Shiping Tang",
            "Libo Wu",
            "Baohua Zhou",
            "Zhongyu Wei"
        ],
        "date": "2025/04/14",
        "pdf": "http://arxiv.org/pdf/2504.10157",
        "abstract": "Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.10157"
    },
    "18c8519869e643039a530ab9676a0f33": {
        "title": "BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents",
        "authors": [
            "Jason Wei",
            "Zhiqing Sun",
            "Spencer Papay",
            "Scott McKinney",
            "Jeffrey Han",
            "Isa Fulford",
            "Hyung Won Chung",
            "Alex Tachard Passos",
            "William Fedus",
            "Amelia Glaese"
        ],
        "date": "2025/04/16",
        "pdf": "http://arxiv.org/pdf/2504.12516",
        "abstract": "We present BrowseComp, a simple yet challenging benchmark for measuring the ability for agents to browse the web. BrowseComp comprises 1,266 questions that require persistently navigating the internet in search of hard-to-find, entangled information. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers. BrowseComp for browsing agents can be seen as analogous to how programming competitions are an incomplete but useful benchmark for coding agents. While BrowseComp sidesteps challenges of a true user query distribution, like generating long answers or resolving ambiguity, it measures the important core capability of exercising persistence and creativity in finding information. BrowseComp can be found at https://github.com/openai/simple-evals.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.12516"
    },
    "635731b8b31cb4e59cd8cb98cf37980c": {
        "title": "MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation",
        "authors": [
            "Haris Riaz",
            "Sourav Bhabesh",
            "Vinayak Arannil",
            "Miguel Ballesteros",
            "Graham Horwood"
        ],
        "date": "2025/04/17",
        "pdf": "http://arxiv.org/pdf/2504.12563",
        "abstract": "Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple &#34;expert&#34; LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora. Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.12563"
    },
    "60e0dd85b6fe05db82391e9ec5c3589f": {
        "title": "Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge",
        "authors": [
            "Yongrui Chen",
            "Junhao He",
            "Linbo Fu",
            "Shenyu Zhang",
            "Rihui Jin",
            "Xinbang Dai",
            "Jiaqi Li",
            "Dehai Min",
            "Nan Hu",
            "Yuxin Zhang",
            "Guilin Qi",
            "Yi Huang",
            "Tongtong Wu"
        ],
        "date": "2025/04/17",
        "pdf": "http://arxiv.org/pdf/2504.12734",
        "abstract": "Unified Structured Knowledge Reasoning (USKR) aims to answer natural language questions (NLQs) by using structured sources such as tables, databases, and knowledge graphs in a unified way. Existing USKR methods either rely on employing task-specific strategies or custom-defined representations, which struggle to leverage the knowledge transfer between different SKR tasks or align with the prior of LLMs, thereby limiting their performance. This paper proposes a novel USKR framework named \\textsc{Pandora}, which takes advantage of \\textsc{Python}&#39;s \\textsc{Pandas} API to construct a unified knowledge representation for alignment with LLM pre-training. It employs an LLM to generate textual reasoning steps and executable Python code for each question. Demonstrations are drawn from a memory of training examples that cover various SKR tasks, facilitating knowledge transfer. Extensive experiments on four benchmarks involving three SKR tasks demonstrate that \\textsc{Pandora} outperforms existing unified frameworks and competes effectively with task-specific methods.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.12734"
    },
    "df5f21a5feebebf0178d8b9c72779186": {
        "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication",
        "authors": [
            "Vicent Briva-Iglesias"
        ],
        "date": "2025/04/17",
        "pdf": "http://arxiv.org/pdf/2504.12891",
        "abstract": "The rapid evolution of artificial intelligence (AI) has introduced AI agents as a disruptive paradigm across various industries, yet their application in machine translation (MT) remains underexplored. This paper describes and analyses the potential of single- and multi-agent systems for MT, reflecting on how they could enhance multilingual digital communication. While single-agent systems are well-suited for simpler translation tasks, multi-agent systems, which involve multiple specialized AI agents collaborating in a structured manner, may offer a promising solution for complex scenarios requiring high accuracy, domain-specific knowledge, and contextual awareness. To demonstrate the feasibility of multi-agent workflows in MT, we are conducting a pilot study in legal MT. The study employs a multi-agent system involving four specialized AI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and (iv) final editing. Our findings suggest that multi-agent systems may have the potential to significantly improve domain-adaptability and contextual awareness, with superior translation quality to traditional MT or single-agent systems. This paper also sets the stage for future research into multi-agent applications in MT, integration into professional translation workflows, and shares a demo of the system analyzed in the paper.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.12891"
    },
    "aa3a36bc627bea863f8d19eab8f5c15a": {
        "title": "DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue",
        "authors": [
            "Xiang Li",
            "Duyi Pan",
            "Hongru Xiao",
            "Jiale Han",
            "Jing Tang",
            "Jiabao Ma",
            "Wei Wang",
            "Bo Cheng"
        ],
        "date": "2025/04/20",
        "pdf": "http://arxiv.org/pdf/2504.14482",
        "abstract": "Speech synthesis is crucial for human-computer interaction, enabling natural and intuitive communication. However, existing datasets involve high construction costs due to manual annotation and suffer from limited character diversity, contextual scenarios, and emotional expressiveness. To address these issues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis framework, which integrates three specialized agents -- a script writer, a speech synthesizer, and a dialogue critic -- to collaboratively generate dialogues. Grounded in a diverse character pool, the framework iteratively refines dialogue scripts and synthesizes speech based on speech review, boosting emotional expressiveness and paralinguistic features of the synthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a bilingual, multi-party, multi-turn speech dialogue dataset covering diverse topics. Extensive experiments demonstrate the effectiveness of our framework and the high quality of the MultiTalk dataset. We release the dataset and code https://github.com/uirlx/DialogueAgents to facilitate future research on advanced speech synthesis models and customized data generation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14482"
    },
    "262671ea1108ed868ed838dcebea4bbf": {
        "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation",
        "authors": [
            "Yiting Ran",
            "Xintao Wang",
            "Tian Qiu",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Deqing Yang"
        ],
        "date": "2025/04/20",
        "pdf": "http://arxiv.org/pdf/2504.14538",
        "abstract": "Recent advances in large language models (LLMs) have enabled social simulation through multi-agent systems. Prior efforts focus on agent societies created from scratch, assigning agents with newly defined personas. However, simulating established fictional worlds and characters remain largely underexplored, despite its significant practical value. In this paper, we introduce BookWorld, a comprehensive system for constructing and simulating book-based multi-agent societies. BookWorld&#39;s design covers comprehensive real-world intricacies, including diverse and dynamic characters, fictional worldviews, geographical constraints and changes, e.t.c. BookWorld enables diverse applications including story generation, interactive games and social simulation, offering novel ways to extend and explore beloved fictional works. Through extensive experiments, we demonstrate that BookWorld generates creative, high-quality stories while maintaining fidelity to the source books, surpassing previous methods with a win rate of 75.36%. The code of this paper can be found at the project page: https://bookworld2025.github.io/.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14538"
    },
    "80742f80499f4b7420d06ca7ea71b511": {
        "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web",
        "authors": [
            "Manya Wadhwa",
            "Zayne Sprague",
            "Chaitanya Malaviya",
            "Philippe Laban",
            "Junyi Jessy Li",
            "Greg Durrett"
        ],
        "date": "2025/04/21",
        "pdf": "http://arxiv.org/pdf/2504.15219",
        "abstract": "Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like &#34;Help me draft an academic talk on coffee intake vs research productivity&#34;, a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user&#39;s prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.15219"
    },
    "08eda25b86e5ec3f4e3211acf50ecd77": {
        "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
        "authors": [
            "Yutaro Yamada",
            "Robert Tjarko Lange",
            "Cong Lu",
            "Shengran Hu",
            "Chris Lu",
            "Jakob Foerster",
            "Jeff Clune",
            "David Ha"
        ],
        "date": "2025/04/10",
        "pdf": "http://arxiv.org/pdf/2504.08066",
        "abstract": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ],
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.08066"
    },
    "a1ee3d31d7616d0b4308388fd25d0dd0": {
        "title": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks",
        "authors": [
            "Ye Ye"
        ],
        "date": "2025/04/11",
        "pdf": "http://arxiv.org/pdf/2504.08525",
        "abstract": "Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. A reference implementation of the core TME components is available at https://github.com/biubiutomato/TME-Agent, including basic examples and structured memory integration. While the current implementation uses a tree-based structure, TME is designed to be graph-aware, supporting reusable substeps, converging task paths, and shared dependencies. This lays the groundwork for future DAG-based memory architectures.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.08525"
    },
    "543210d11e4939d646fb7d91d10a4514": {
        "title": "DocAgent: A Multi-Agent System for Automated Code Documentation Generation",
        "authors": [
            "Dayu Yang",
            "Antoine Simoulin",
            "Xin Qian",
            "Xiaoyi Liu",
            "Yuwei Cao",
            "Zhaopu Teng",
            "Grey Yang"
        ],
        "date": "2025/04/11",
        "pdf": "http://arxiv.org/pdf/2504.08725",
        "abstract": "High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.08725"
    },
    "53fc0c31be6b71ada54bc7262ea7f686": {
        "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
        "authors": [
            "Xing Han Lù",
            "Amirhossein Kazemnejad",
            "Nicholas Meade",
            "Arkil Patel",
            "Dongchan Shin",
            "Alejandra Zambrano",
            "Karolina Stańczak",
            "Peter Shaw",
            "Christopher J. Pal",
            "Siva Reddy"
        ],
        "date": "2025/04/11",
        "pdf": "http://arxiv.org/pdf/2504.08942",
        "abstract": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.08942"
    },
    "201f3921e46ba0d65dacc0229adb21cb": {
        "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
        "authors": [
            "Zixuan Ke",
            "Fangkai Jiao",
            "Yifei Ming",
            "Xuan-Phi Nguyen",
            "Austin Xu",
            "Do Xuan Long",
            "Minzhi Li",
            "Chengwei Qin",
            "Peifeng Wang",
            "Silvio Savarese",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "date": "2025/04/12",
        "pdf": "http://arxiv.org/pdf/2504.09037",
        "abstract": "Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ...",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.09037"
    },
    "1866d47555e344f3e4482ded8d8de7f1": {
        "title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
        "authors": [
            "Jiahao Qiu",
            "Yinghui He",
            "Xinzhe Juan",
            "Yimin Wang",
            "Yuhan Liu",
            "Zixin Yao",
            "Yue Wu",
            "Xun Jiang",
            "Ling Yang",
            "Mengdi Wang"
        ],
        "date": "2025/04/13",
        "pdf": "http://arxiv.org/pdf/2504.09689",
        "abstract": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users&#39; mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.09689"
    },
    "abb8936c73c933f1c5b5cd5d6f768c19": {
        "title": "AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents",
        "authors": [
            "Dakuo Wang",
            "Ting-Yao Hsu",
            "Yuxuan Lu",
            "Hansu Gu",
            "Limeng Cui",
            "Yaochen Xie",
            "William Headean",
            "Bingsheng Yao",
            "Akash Veeragouni",
            "Jiapeng Liu",
            "Sreyashi Nag",
            "Jessie Wang"
        ],
        "date": "2025/04/13",
        "pdf": "http://arxiv.org/pdf/2504.09723",
        "abstract": "A/B testing experiment is a widely adopted method for evaluating UI/UX design decisions in modern web applications. Yet, traditional A/B testing remains constrained by its dependence on the large-scale and live traffic of human participants, and the long time of waiting for the testing result. Through formative interviews with six experienced industry practitioners, we identified critical bottlenecks in current A/B testing workflows. In response, we present AgentA/B, a novel system that leverages Large Language Model-based autonomous agents (LLM Agents) to automatically simulate user interaction behaviors with real webpages. AgentA/B enables scalable deployment of LLM agents with diverse personas, each capable of navigating the dynamic webpage and interactively executing multi-step interactions like search, clicking, filtering, and purchasing. In a demonstrative controlled experiment, we employ AgentA/B to simulate a between-subject A/B testing with 1,000 LLM agents Amazon.com, and compare agent behaviors with real human shopping behaviors at a scale. Our findings suggest AgentA/B can emulate human-like behavior patterns.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.09723"
    },
    "556e227b7819455ff440fdf559579f78": {
        "title": "A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science",
        "authors": [
            "Jie Feng",
            "Jinwei Zeng",
            "Qingyue Long",
            "Hongyi Chen",
            "Jie Zhao",
            "Yanxin Xi",
            "Zhilun Zhou",
            "Yuan Yuan",
            "Shengyuan Wang",
            "Qingbin Zeng",
            "Songwei Li",
            "Yunke Zhang",
            "Yuming Lin",
            "Tong Li",
            "Jingtao Ding",
            "Chen Gao",
            "Fengli Xu",
            "Yong Li"
        ],
        "date": "2025/04/14",
        "pdf": "http://arxiv.org/pdf/2504.09848",
        "abstract": "Over the past year, the development of large language models (LLMs) has brought spatial intelligence into focus, with much attention on vision-based embodied intelligence. However, spatial intelligence spans a broader range of disciplines and scales, from navigation and urban planning to remote sensing and earth science. What are the differences and connections between spatial intelligence across these fields? In this paper, we first review human spatial cognition and its implications for spatial intelligence in LLMs. We then examine spatial memory, knowledge representations, and abstract reasoning in LLMs, highlighting their roles and connections. Finally, we analyze spatial intelligence across scales -- from embodied to urban and global levels -- following a framework that progresses from spatial memory and understanding to spatial reasoning and intelligence. Through this survey, we aim to provide insights into interdisciplinary spatial intelligence research and inspire future studies.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.09848"
    },
    "4774f10bebb60dd8a17409abefe535f9": {
        "title": "Breaking the Data Barrier -- Building GUI Agents Through Task Generalization",
        "authors": [
            "Junlei Zhang",
            "Zichen Ding",
            "Chang Ma",
            "Zijie Chen",
            "Qiushi Sun",
            "Zhenzhong Lan",
            "Junxian He"
        ],
        "date": "2025/04/14",
        "pdf": "http://arxiv.org/pdf/2504.10127",
        "abstract": "Graphical User Interface (GUI) agents offer cross-platform solutions for automating complex digital tasks, with significant potential to transform productivity workflows. However, their performance is often constrained by the scarcity of high-quality trajectory data. To address this limitation, we propose training Vision Language Models (VLMs) on data-rich, reasoning-intensive tasks during a dedicated mid-training stage, and then examine how incorporating these tasks facilitates generalization to GUI planning scenarios. Specifically, we explore a range of tasks with readily available instruction-tuning data, including GUI perception, multimodal reasoning, and textual reasoning. Through extensive experiments across 11 mid-training tasks, we demonstrate that: (1) Task generalization proves highly effective, yielding substantial improvements across most settings. For instance, multimodal mathematical reasoning enhances performance on AndroidWorld by an absolute 6.3%. Remarkably, text-only mathematical data significantly boosts GUI web agent performance, achieving a 5.6% improvement on WebArena and 5.4% improvement on AndroidWorld, underscoring notable cross-modal generalization from text-based to visual domains; (2) Contrary to prior assumptions, GUI perception data - previously considered closely aligned with GUI agent tasks and widely utilized for training - has a comparatively limited impact on final performance; (3) Building on these insights, we identify the most effective mid-training tasks and curate optimized mixture datasets, resulting in absolute performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Our work provides valuable insights into cross-domain knowledge transfer for GUI agents and offers a practical approach to addressing data scarcity challenges in this emerging field. The code, data and models will be available at https://github.com/hkust-nlp/GUIMid.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.10127"
    },
    "45d33e17a8f4c0482b3ea4bb6ed222b2": {
        "title": "GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents",
        "authors": [
            "Run Luo",
            "Lu Wang",
            "Wanwei He",
            "Xiaobo Xia"
        ],
        "date": "2025/04/14",
        "pdf": "http://arxiv.org/pdf/2504.10458",
        "abstract": "Existing efforts in building Graphical User Interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning on Large Vision-Language Models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by Reinforcement Fine-Tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose \\name, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and employing policy optimization algorithms such as Group Relative Policy Optimization (GRPO) to update the model, \\name achieves superior performance using only 0.02\\% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.10458"
    },
    "c18b697f68b6bc1d709c167ff41ed85a": {
        "title": "Toward Super Agent System with Hybrid AI Routers",
        "authors": [
            "Yuhang Yao",
            "Haixin Wang",
            "Yibo Chen",
            "Jiawen Wang",
            "Min Chang Jordan Ren",
            "Bosheng Ding",
            "Salman Avestimehr",
            "Chaoyang He"
        ],
        "date": "2025/04/11",
        "pdf": "http://arxiv.org/pdf/2504.10519",
        "abstract": "AI Agents powered by Large Language Models are transforming the world through enormous applications. A super agent has the potential to fulfill diverse user needs, such as summarization, coding, and research, by accurately understanding user intent and leveraging the appropriate tools to solve tasks. However, to make such an agent viable for real-world deployment and accessible at scale, significant optimizations are required to ensure high efficiency and low cost. This paper presents a design of the Super Agent System. Upon receiving a user prompt, the system first detects the intent of the user, then routes the request to specialized task agents with the necessary tools or automatically generates agentic workflows. In practice, most applications directly serve as AI assistants on edge devices such as phones and robots. As different language models vary in capability and cloud-based models often entail high computational costs, latency, and privacy concerns, we then explore the hybrid mode where the router dynamically selects between local and cloud models based on task complexity. Finally, we introduce the blueprint of an on-device super agent enhanced with cloud. With advances in multi-modality models and edge hardware, we envision that most computations can be handled locally, with cloud collaboration only as needed. Such architecture paves the way for super agents to be seamlessly integrated into everyday life in the near future.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.10519"
    },
    "67bf38dfa8d80db1994b567ab90daf97": {
        "title": "Towards Automated Safety Requirements Derivation Using Agent-based RAG",
        "authors": [
            "Balahari Vignesh Balu",
            "Florian Geissler",
            "Francesco Carella",
            "Joao-Vitor Zacchi",
            "Josef Jiru",
            "Nuria Mata",
            "Reinhard Stolle"
        ],
        "date": "2025/04/15",
        "pdf": "http://arxiv.org/pdf/2504.11243",
        "abstract": "We study the automated derivation of safety requirements in a self-driving vehicle use case, leveraging LLMs in combination with agent-based retrieval-augmented generation. Conventional approaches that utilise pre-trained LLMs to assist in safety analyses typically lack domain-specific knowledge. Existing RAG approaches address this issue, yet their performance deteriorates when handling complex queries and it becomes increasingly harder to retrieve the most relevant information. This is particularly relevant for safety-relevant applications. In this paper, we propose the use of agent-based RAG to derive safety requirements and show that the retrieved information is more relevant to the queries. We implement an agent-based approach on a document pool of automotive standards and the Apollo case study, as a representative example of an automated driving perception system. Our solution is tested on a data set of safety requirement questions and answers, extracted from the Apollo data. Evaluating a set of selected RAG metrics, we present and discuss advantages of a agent-based approach compared to default RAG methods.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.11243"
    },
    "a9fe48fa73f5946f76dfadb945fa4310": {
        "title": "The Obvious Invisible Threat: LLM-Powered GUI Agents&#39; Vulnerability to Fine-Print Injections",
        "authors": [
            "Chaoran Chen",
            "Zhiping Zhang",
            "Bingcan Guo",
            "Shang Ma",
            "Ibrahim Khalilov",
            "Simret A Gebreegziabher",
            "Yanfang Ye",
            "Ziang Xiao",
            "Yaxing Yao",
            "Tianshi Li",
            "Toby Jia-Jun Li"
        ],
        "date": "2025/04/15",
        "pdf": "http://arxiv.org/pdf/2504.11281",
        "abstract": "A Large Language Model (LLM) powered GUI agent is a specialized autonomous system that performs tasks on the user&#39;s behalf according to high-level instructions. It does so by perceiving and interpreting the graphical user interfaces (GUIs) of relevant apps, often visually, inferring necessary sequences of actions, and then interacting with GUIs by executing the actions such as clicking, typing, and tapping. To complete real-world tasks, such as filling forms or booking services, GUI agents often need to process and act on sensitive user data. However, this autonomy introduces new privacy and security risks. Adversaries can inject malicious content into the GUIs that alters agent behaviors or induces unintended disclosures of private information. These attacks often exploit the discrepancy between visual saliency for agents and human users, or the agent&#39;s limited ability to detect violations of contextual integrity in task automation. In this paper, we characterized six types of such attacks, and conducted an experimental study to test these attacks with six state-of-the-art GUI agents, 234 adversarial webpages, and 39 human participants. Our findings suggest that GUI agents are highly vulnerable, particularly to contextually embedded threats. Moreover, human users are also susceptible to many of these attacks, indicating that simple human oversight may not reliably prevent failures. This misalignment highlights the need for privacy-aware agent design. We propose practical defense strategies to inform the development of safer and more reliable GUI agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.11281"
    },
    "06f89a14e28121c8914689b62d424a46": {
        "title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents",
        "authors": [
            "Dayeon Ki",
            "Tianyi Zhou",
            "Marine Carpuat",
            "Gang Wu",
            "Puneet Mathur",
            "Viswanathan Swaminathan"
        ],
        "date": "2025/04/15",
        "pdf": "http://arxiv.org/pdf/2504.11571",
        "abstract": "Large Language Model (LLM)-powered agents have unlocked new possibilities for automating human tasks. While prior work has focused on well-defined tasks with specified goals, the capabilities of agents in creative design tasks with open-ended goals remain underexplored. We introduce GraphicBench, a new planning benchmark for graphic design that covers 1,079 user queries and input images across four design types. We further present GraphicTown, an LLM agent framework with three design experts and 46 actions (tools) to choose from for executing each step of the planned workflows in web environments. Experiments with six LLMs demonstrate their ability to generate workflows that integrate both explicit design constraints from user queries and implicit commonsense constraints. However, these workflows often do not lead to successful execution outcomes, primarily due to challenges in: (1) reasoning about spatial relationships, (2) coordinating global dependencies across experts, and (3) retrieving the most appropriate action per step. We envision GraphicBench as a challenging yet valuable testbed for advancing LLM-agent planning and execution in creative design tasks.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.11571"
    },
    "d03e29b1c480eded4d859495d91496e2": {
        "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents",
        "authors": [
            "Arth Bohra",
            "Manvel Saroyan",
            "Danil Melkozerov",
            "Vahe Karufanyan",
            "Gabriel Maher",
            "Pascal Weinberger",
            "Artem Harutyunyan",
            "Giovanni Campagna"
        ],
        "date": "2025/04/17",
        "pdf": "http://arxiv.org/pdf/2504.12682",
        "abstract": "Most recent web agent research has focused on navigation and transaction tasks, with little emphasis on extracting structured data at scale. We present WebLists, a benchmark of 200 data-extraction tasks across four common business and enterprise use-cases. Each task requires an agent to navigate to a webpage, configure it appropriately, and extract complete datasets with well-defined schemas. We show that both LLMs with search capabilities and SOTA web agents struggle with these tasks, with a recall of 3% and 31%, respectively, despite higher performance on question-answering tasks. To address this challenge, we propose BardeenAgent, a novel framework that enables web agents to convert their execution into repeatable programs, and replay them at scale across pages with similar structure. BardeenAgent is also the first LLM agent to take advantage of the regular structure of HTML. In particular BardeenAgent constructs a generalizable CSS selector to capture all relevant items on the page, then fits the operations to extract the data. On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more than doubling the performance of SOTA web agents, and reducing cost per output row by 3x.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.12682"
    },
    "bc64c55c097df561047a31ac93752bf7": {
        "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents",
        "authors": [
            "Salman Rahman",
            "Liwei Jiang",
            "James Shiffer",
            "Genglin Liu",
            "Sheriff Issaka",
            "Md Rizwan Parvez",
            "Hamid Palangi",
            "Kai-Wei Chang",
            "Yejin Choi",
            "Saadia Gabriel"
        ],
        "date": "2025/04/15",
        "pdf": "http://arxiv.org/pdf/2504.13203",
        "abstract": "Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.13203"
    },
    "5f2ed4d4715002b02ccf383096b33aae": {
        "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation",
        "authors": [
            "Xinchen Wang",
            "Pengfei Gao",
            "Chao Peng",
            "Ruida Hu",
            "Cuiyun Gao"
        ],
        "date": "2025/04/18",
        "pdf": "http://arxiv.org/pdf/2504.13472",
        "abstract": "Large language models (LLMs) have demonstrated strong capabilities in code generation, underscoring the critical need for rigorous and comprehensive evaluation. Existing evaluation approaches fall into three categories, including human-centered, metric-based, and LLM-based. Considering that human-centered approaches are labour-intensive and metric-based ones overly rely on reference answers, LLM-based approaches are gaining increasing attention due to their stronger contextual understanding capabilities and superior efficiency. However, the performance of LLM-based approaches remains limited due to: (1) lack of multisource domain knowledge, and (2) insufficient comprehension of complex code. To mitigate the limitations, we propose CodeVisionary, the first LLM-based agent framework for evaluating LLMs in code generation. CodeVisionary consists of two stages: (1) Multiscore knowledge analysis stage, which aims to gather multisource and comprehensive domain knowledge by formulating and executing a stepwise evaluation plan. (2) Negotiation-based scoring stage, which involves multiple judges engaging in discussions to better comprehend the complex code and reach a consensus on the evaluation score. Extensive experiments demonstrate that CodeVisionary achieves the best performance for evaluating LLMs in code generation, outperforming the best baseline methods with average improvements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau coefficients, respectively. Besides, CodeVisionary provides detailed evaluation reports, which assist developers in identifying shortcomings and making improvements. The resources of CodeVisionary are available at https://anonymous.4open.science/r/CodeVisionary.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.13472"
    },
    "65e942f4423049e5440ba0b97a261ca0": {
        "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark",
        "authors": [
            "Ivan Sviridov",
            "Amina Miftakhova",
            "Artemiy Tereshchenko",
            "Galina Zubkova",
            "Pavel Blinov",
            "Andrey Savchenko"
        ],
        "date": "2025/03/26",
        "pdf": "http://arxiv.org/pdf/2504.13861",
        "abstract": "Large Vision-Language Models (LVLMs) are increasingly being explored for applications in telemedicine, yet their ability to engage with diverse patient behaviors remains underexplored. We introduce 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark), an open-source evaluation framework designed to assess LLM-driven medical consultations. Unlike existing benchmarks, 3MDBench simulates real-world patient variability by incorporating four temperament-driven Patient Agents and an Assessor Agent that evaluates diagnostic accuracy and dialogue quality. The benchmark integrates textual and image-based patient data across 34 common diagnoses, mirroring real-world telemedicine interactions. Under different diagnostic strategies, we evaluate state-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue improves the F1 score from 50.4 to 54.2 compared to non-dialogue settings, underscoring the value of context-driven, information-seeking questioning. Additionally, we demonstrate that multimodal inputs enhance diagnostic efficiency. Image-supported models outperform text-only counterparts by raising the diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting. Finally, we suggest an approach that improves the diagnostic F1-score to 70.3 by training the CNN model on the diagnosis prediction task and incorporating its top-3 predictions into the LVLM context. 3MDBench provides a reproducible and extendable evaluation framework for AI-driven medical assistants. It offers insights into how patient temperament, dialogue strategies, and multimodal reasoning influence diagnosis quality. By addressing real-world complexities in telemedicine, our benchmark paves the way for more empathetic, reliable, and context-aware AI-driven healthcare solutions. The source code of our benchmark is publicly available: https://github.com/univanxx/3mdbench",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.13861"
    },
    "90f3dd3bd25715d6fc040f5f256dc3b7": {
        "title": "A Survey on (M)LLM-Based GUI Agents",
        "authors": [
            "Fei Tang",
            "Haolei Xu",
            "Hang Zhang",
            "Siqi Chen",
            "Xingyu Wu",
            "Yongliang Shen",
            "Wenqi Zhang",
            "Guiyang Hou",
            "Zeqi Tan",
            "Yuchen Yan",
            "Kaitao Song",
            "Jian Shao",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "date": "2025/03/27",
        "pdf": "http://arxiv.org/pdf/2504.13865",
        "abstract": "Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents&#39; capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field&#39;s current state and offers insights into future developments in intelligent interface automation.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.13865"
    },
    "a6ed736863c0aa412804cdb28055e20d": {
        "title": "System of Agentic AI for the Discovery of Metal-Organic Frameworks",
        "authors": [
            "Theo Jaffrelot Inizan",
            "Sherry Yang",
            "Aaron Kaplan",
            "Yen-hsu Lin",
            "Jian Yin",
            "Saber Mirzaei",
            "Mona Abdelgaid",
            "Ali H. Alawadhi",
            "KwangHwan Cho",
            "Zhiling Zheng",
            "Ekin Dogus Cubuk",
            "Christian Borgs",
            "Jennifer T. Chayes",
            "Kristin A. Persson",
            "Omar M. Yaghi"
        ],
        "date": "2025/04/18",
        "pdf": "http://arxiv.org/pdf/2504.14110",
        "abstract": "Generative models and machine learning promise accelerated material discovery in MOFs for CO2 capture and water harvesting but face significant challenges navigating vast chemical spaces while ensuring synthetizability. Here, we present MOFGen, a system of Agentic AI comprising interconnected agents: a large language model that proposes novel MOF compositions, a diffusion model that generates crystal structures, quantum mechanical agents that optimize and filter candidates, and synthetic-feasibility agents guided by expert rules and machine learning. Trained on all experimentally reported MOFs and computational databases, MOFGen generated hundreds of thousands of novel MOF structures and synthesizable organic linkers. Our methodology was validated through high-throughput experiments and the successful synthesis of five &#34;AI-dreamt&#34; MOFs, representing a major step toward automated synthesizable material discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14110"
    },
    "4d597a4fbf497e77d7cdf4f41b96e065": {
        "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners",
        "authors": [
            "Yuhang Liu",
            "Pengxiang Li",
            "Congkai Xie",
            "Xavier Hu",
            "Xiaotian Han",
            "Shengyu Zhang",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "date": "2025/04/19",
        "pdf": "http://arxiv.org/pdf/2504.14239",
        "abstract": "Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14239"
    },
    "0419e0702a7509a437d567ff228797e5": {
        "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey",
        "authors": [
            "Ahsan Bilal",
            "Muhammad Ahmed Mohsin",
            "Muhammad Umer",
            "Muhammad Awais Khan Bangash",
            "Muhammad Ali Jamshed"
        ],
        "date": "2025/04/20",
        "pdf": "http://arxiv.org/pdf/2504.14520",
        "abstract": "This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14520"
    },
    "78a5ce217186995eab8defe503191315": {
        "title": "PLANET: A Collection of Benchmarks for Evaluating LLMs&#39; Planning Capabilities",
        "authors": [
            "Haoming Li",
            "Zhaoliang Chen",
            "Jonathan Zhang",
            "Fei Liu"
        ],
        "date": "2025/04/21",
        "pdf": "http://arxiv.org/pdf/2504.14773",
        "abstract": "Planning is central to agents and agentic AI. The ability to plan, e.g., creating travel itineraries within a budget, holds immense potential in both scientific and commercial contexts. Moreover, optimal plans tend to require fewer resources compared to ad-hoc methods. To date, a comprehensive understanding of existing planning benchmarks appears to be lacking. Without it, comparing planning algorithms&#39; performance across domains or selecting suitable algorithms for new scenarios remains challenging. In this paper, we examine a range of planning benchmarks to identify commonly used testbeds for algorithm development and highlight potential gaps. These benchmarks are categorized into embodied environments, web navigation, scheduling, games and puzzles, and everyday task automation. Our study recommends the most appropriate benchmarks for various algorithms and offers insights to guide future benchmark development.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14773"
    },
    "88e1823c2cf70a5006d97185bcf9eec0": {
        "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents",
        "authors": [
            "Rui Qiu",
            "Shijie Chen",
            "Yu Su",
            "Po-Yin Yen",
            "Han-Wei Shen"
        ],
        "date": "2025/04/21",
        "pdf": "http://arxiv.org/pdf/2504.14822",
        "abstract": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes disciplines, such as healthcare, but are often impeded by intensive labors and lengthy processes that can take months to complete. Due to the high demand for domain expertise, existing automatic summarization methods fail to accurately identify relevant studies and generate high-quality summaries. To that end, we introduce InsightAgent, a human-centered interactive AI agent powered by large language models that revolutionize this workflow. InsightAgent partitions a large literature corpus based on semantics and employs a multi-agent design for more focused processing of literature, leading to significant improvement in the quality of generated SRs. InsightAgent also provides intuitive visualizations of the corpus and agent trajectories, allowing users to effortlessly monitor the actions of the agent and provide real-time feedback based on their expertise. Our user studies with 9 medical professionals demonstrate that the visualization and interaction mechanisms can effectively improve the quality of synthesized SRs by 27.2%, reaching 79.7% of human-written quality. At the same time, user satisfaction is improved by 34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather than months, to complete a high-quality systematic review.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14822"
    },
    "74f1ff3dbc67c636d9e2b760dd8825fa": {
        "title": "EducationQ: Evaluating LLMs&#39; Teaching Capabilities Through Multi-Agent Dialogue Framework",
        "authors": [
            "Yao Shi",
            "Rongkeng Liang",
            "Yong Xu"
        ],
        "date": "2025/04/21",
        "pdf": "http://arxiv.org/pdf/2504.14928",
        "abstract": "Large language models (LLMs) increasingly serve as educational tools, yet evaluating their teaching capabilities remains challenging due to the resource-intensive, context-dependent, and methodologically complex nature of teacher-student interactions. We introduce EducationQ, a multi-agent dialogue framework that efficiently assesses teaching capabilities through simulated dynamic educational scenarios, featuring specialized agents for teaching, learning, and evaluation. Testing 14 LLMs across major AI Organizations (OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13 disciplines and 10 difficulty levels reveals that teaching effectiveness does not correlate linearly with model scale or general reasoning capabilities - with some smaller open-source models outperforming larger commercial counterparts in teaching contexts. This finding highlights a critical gap in current evaluations that prioritize knowledge recall over interactive pedagogy. Our mixed-methods evaluation, combining quantitative metrics with qualitative analysis and expert case studies, identifies distinct pedagogical strengths employed by top-performing models (e.g., sophisticated questioning strategies, adaptive feedback mechanisms). Human expert evaluations show 78% agreement with our automated qualitative analysis of effective teaching behaviors, validating our methodology. EducationQ demonstrates that LLMs-as-teachers require specialized optimization beyond simple scaling, suggesting next-generation educational AI prioritize targeted enhancement of specific pedagogical effectiveness.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.14928"
    },
    "81c137c15c7a21b7522b862ced6da4ce": {
        "title": "TextArena",
        "authors": [
            "Leon Guertler",
            "Bobby Cheng",
            "Simon Yu",
            "Bo Liu",
            "Leshem Choshen",
            "Cheston Tan"
        ],
        "date": "2025/04/15",
        "pdf": "http://arxiv.org/pdf/2504.11442",
        "abstract": "TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ],
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.11442"
    },
    "a450e72fad06fbe38b9c400957db5f83": {
        "title": "Enhancing Web Agents with Explicit Rollback Mechanisms",
        "authors": [
            "Zhisong Zhang",
            "Tianqing Fang",
            "Kaixin Ma",
            "Wenhao Yu",
            "Hongming Zhang",
            "Haitao Mi",
            "Dong Yu"
        ],
        "date": "2025/04/16",
        "pdf": "http://arxiv.org/pdf/2504.11788",
        "abstract": "With recent advancements in large language models, web agents have been greatly improved. However, dealing with complex and dynamic web environments requires more advanced planning and search abilities. Previous studies usually adopt a greedy one-way search strategy, which may struggle to recover from erroneous states. In this work, we enhance web agents with an explicit rollback mechanism, enabling the agent to revert back to a previous state in its navigation trajectory. This mechanism gives the model the flexibility to directly control the search process, leading to an effective and efficient web navigation method. We conduct experiments on two live web navigation benchmarks with zero-shot and fine-tuning settings. The results demonstrate the effectiveness of our proposed approach.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.11788"
    },
    "0fed4c3c952260463070d645f81afc44": {
        "title": "HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation",
        "authors": [
            "Pei Liu",
            "Xin Liu",
            "Ruoyu Yao",
            "Junming Liu",
            "Siyuan Meng",
            "Ding Wang",
            "Jun Ma"
        ],
        "date": "2025/04/13",
        "pdf": "http://arxiv.org/pdf/2504.12330",
        "abstract": "While Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge, conventional single-agent RAG remains fundamentally limited in resolving complex queries demanding coordinated reasoning across heterogeneous data ecosystems. We present HM-RAG, a novel Hierarchical Multi-agent Multimodal RAG framework that pioneers collaborative intelligence for dynamic knowledge synthesis across structured, unstructured, and graph-based data. The framework is composed of three-tiered architecture with specialized agents: a Decomposition Agent that dissects complex queries into contextually coherent sub-tasks via semantic-aware query rewriting and schema-guided context augmentation; Multi-source Retrieval Agents that carry out parallel, modality-specific retrieval using plug-and-play modules designed for vector, graph, and web-based databases; and a Decision Agent that uses consistency voting to integrate multi-source answers and resolve discrepancies in retrieval results through Expert Model Refinement. This architecture attains comprehensive query understanding by combining textual, graph-relational, and web-derived evidence, resulting in a remarkable 12.95% improvement in answer accuracy and a 3.56% boost in question classification accuracy over baseline RAG systems on the ScienceQA and CrisisMMD benchmarks. Notably, HM-RAG establishes state-of-the-art results in zero-shot settings on both datasets. Its modular architecture ensures seamless integration of new data modalities while maintaining strict data governance, marking a significant advancement in addressing the critical challenges of multimodal reasoning and knowledge synthesis in RAG systems. Code is available at https://github.com/ocean-luna/HMRAG.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.12330"
    },
    "8e00fae527a67bdbc7cbdf117abe6a7f": {
        "title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation",
        "authors": [
            "Zhiyuan Hu",
            "Shiyun Xiong",
            "Yifan Zhang",
            "See-Kiong Ng",
            "Anh Tuan Luu",
            "Bo An",
            "Shuicheng Yan",
            "Bryan Hooi"
        ],
        "date": "2025/04/22",
        "pdf": "http://arxiv.org/pdf/2504.16073",
        "abstract": "Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in three GUI navigation tasks, achieving a 3.4% improvement in single step action accuracy for static environments, along with a around 33% increase in task success rate in one dynamic environment. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.16073"
    },
    "016ce932b472f51d913796f228fde87e": {
        "title": "Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation",
        "authors": [
            "Jiahao Yuan",
            "Xingzhe Sun",
            "Xing Yu",
            "Jingwen Wang",
            "Dehui Du",
            "Zhiqing Cui",
            "Zixiang Di"
        ],
        "date": "2025/04/23",
        "pdf": "http://arxiv.org/pdf/2504.16408",
        "abstract": "The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.16408"
    },
    "9af9ec59bf78701bb1c7a92216c994d6": {
        "title": "Monte Carlo Planning with Large Language Model for Text-Based Game Agents",
        "authors": [
            "Zijing Shi",
            "Meng Fang",
            "Ling Chen"
        ],
        "date": "2025/04/23",
        "pdf": "http://arxiv.org/pdf/2504.16855",
        "abstract": "Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.16855"
    },
    "63a227eb2f33025f363ffecf0f4c5e72": {
        "title": "OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents",
        "authors": [
            "Raghav Thind",
            "Youran Sun",
            "Ling Liang",
            "Haizhao Yang"
        ],
        "date": "2025/04/23",
        "pdf": "http://arxiv.org/pdf/2504.16918",
        "abstract": "Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems described in natural language by leveraging LLM-powered \\underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \\emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \\emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \\emph{coder} and a \\emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\\times$ and $3.1\\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\\% accuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\\% and 50\\% respectively over prior best results.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.16918"
    },
    "3705d5ebaa6ae685ef47e12ab3a61ff5": {
        "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation",
        "authors": [
            "Yangxinyu Xie",
            "Bowen Jiang",
            "Tanwi Mallick",
            "Joshua David Bergerson",
            "John K. Hutchison",
            "Duane R. Verner",
            "Jordan Branham",
            "M. Ross Alexander",
            "Robert B. Ross",
            "Yan Feng",
            "Leslie-Anne Levy",
            "Weijie Su",
            "Camillo J. Taylor"
        ],
        "date": "2025/04/24",
        "pdf": "http://arxiv.org/pdf/2504.17200",
        "abstract": "Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire hazards. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating natural hazard and extreme weather projection data, observational datasets, and scientific literature through an RAG framework, the system ensures both the accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.17200"
    },
    "63f6ac566e172de14415ddc193116101": {
        "title": "Exploring Personality-Aware Interactions in Salesperson Dialogue Agents",
        "authors": [
            "Sijia Cheng",
            "Wen-Yu Chang",
            "Yun-Nung Chen"
        ],
        "date": "2025/04/25",
        "pdf": "http://arxiv.org/pdf/2504.18058",
        "abstract": "The integration of dialogue agents into the sales domain requires a deep understanding of how these systems interact with users possessing diverse personas. This study explores the influence of user personas, defined using the Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance of sales-oriented dialogue agents. Through large-scale testing and analysis, we assess the pre-trained agent&#39;s effectiveness, adaptability, and personalization capabilities across a wide range of MBTI-defined user types. Our findings reveal significant patterns in interaction dynamics, task completion rates, and dialogue naturalness, underscoring the future potential for dialogue agents to refine their strategies to better align with varying personality traits. This work not only provides actionable insights for building more adaptive and user-centric conversational systems in the sales domain but also contributes broadly to the field by releasing persona-defined user simulators. These simulators, unconstrained by domain, offer valuable tools for future research and demonstrate the potential for scaling personalized dialogue systems across diverse applications.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.18058"
    },
    "583fb871d3127858377fb2e3b8ba9e10": {
        "title": "MAGI: Multi-Agent Guided Interview for Psychiatric Assessment",
        "authors": [
            "Guanqun Bi",
            "Zhuang Chen",
            "Zhoufu Liu",
            "Hongkai Wang",
            "Xiyao Xiao",
            "Yuqiang Xie",
            "Wen Zhang",
            "Yongkang Huang",
            "Yuxuan Chen",
            "Libiao Peng",
            "Yi Feng",
            "Minlie Huang"
        ],
        "date": "2025/04/25",
        "pdf": "http://arxiv.org/pdf/2504.18260",
        "abstract": "Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI&#39;s branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.18260"
    },
    "c58aa398cf2f88e27603d797c3430ee6": {
        "title": "Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant",
        "authors": [
            "Lei Shen",
            "Xiaoyu Shen"
        ],
        "date": "2025/04/25",
        "pdf": "http://arxiv.org/pdf/2504.18373",
        "abstract": "In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.18373"
    },
    "be86094e23f648c96b479e37e8b0851c": {
        "title": "Stealing Creator&#39;s Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation",
        "authors": [
            "Jong Inn Park",
            "Maanas Taneja",
            "Qianwen Wang",
            "Dongyeop Kang"
        ],
        "date": "2025/04/26",
        "pdf": "http://arxiv.org/pdf/2504.18805",
        "abstract": "Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators&#39; workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators&#39; quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.18805"
    },
    "5e896770ead42a91807a8c201991bd84": {
        "title": "AndroidGen: Building an Android Language Agent under Data Scarcity",
        "authors": [
            "Hanyu Lai",
            "Junjie Gao",
            "Xiao Liu",
            "Yifan Xu",
            "Shudan Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "date": "2025/04/27",
        "pdf": "http://arxiv.org/pdf/2504.19298",
        "abstract": "Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.19298"
    },
    "79048dc11a1cb71839bfab33ae69c9fe": {
        "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory",
        "authors": [
            "Prateek Chhikara",
            "Dev Khant",
            "Saket Aryan",
            "Taranjeet Singh",
            "Deshraj Yadav"
        ],
        "date": "2025/04/28",
        "pdf": "http://arxiv.org/pdf/2504.19413",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.19413"
    },
    "c06dfe4a2c4d6fc07712608d187a87e6": {
        "title": "m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training",
        "authors": [
            "Meng Xiao",
            "Xunxin Cai",
            "Chengrui Wang",
            "Yuanchun Zhou"
        ],
        "date": "2025/04/28",
        "pdf": "http://arxiv.org/pdf/2504.19565",
        "abstract": "The rapid progress of large language models (LLMs) in biomedical research has underscored the limitations of existing open-source annotated scientific corpora, which are often insufficient in quantity and quality. Addressing the challenge posed by the complex hierarchy of biomedical knowledge, we propose a knowledge-driven, multi-agent framework for scientific corpus distillation tailored for LLM training in the biomedical domain. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. These agents collectively generate and refine domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.",
        "code": "",
        "category": [
            [
                "Application",
                "Biology"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.19565"
    },
    "e0831da57aae9b36ad8c7e25934803d8": {
        "title": "Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking",
        "authors": [
            "Luigia Costabile",
            "Gian Marco Orlando",
            "Valerio La Gatta",
            "Vincenzo Moscato"
        ],
        "date": "2025/04/24",
        "pdf": "http://arxiv.org/pdf/2504.19940",
        "abstract": "The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches. In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds. Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments. Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.19940"
    },
    "20cb08221d76d17458c388249b919883": {
        "title": "MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools",
        "authors": [
            "Nishant Subramani",
            "Jason Eisner",
            "Justin Svegliato",
            "Benjamin Van Durme",
            "Yu Su",
            "Sam Thomson"
        ],
        "date": "2025/04/28",
        "pdf": "http://arxiv.org/pdf/2504.20168",
        "abstract": "Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer&#39;s generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.20168"
    },
    "07c0be96eca60360f9c45d0dc47eec1d": {
        "title": "BrAIcht, a theatrical agent that speaks like Bertolt Brecht&#39;s characters",
        "authors": [
            "Baz Roland",
            "Kristina Malyseva",
            "Anna Pappa",
            "Tristan Cazenave"
        ],
        "date": "2025/04/29",
        "pdf": "http://arxiv.org/pdf/2504.20552",
        "abstract": "This project introduces BrAIcht, an AI conversational agent that creates dialogues in the distinctive style of the famous German playwright Bertolt Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7 billion parameters and a modified version of the base Llama2 suitable for German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of other German plays that are stylistically similar to Bertolt Brecht are used to form a more di-erse dataset. Due to the limited memory capacity, a parameterefficient fine-tuning technique called QLoRA is implemented to train the large language model. The results, based on BLEU score and perplexity, show very promising performance of BrAIcht in generating dialogues in the style of Bertolt Brecht.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.20552"
    },
    "819dc150c611aa47a22ccc884a8aa029": {
        "title": "WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model",
        "authors": [
            "Tianqing Fang",
            "Hongming Zhang",
            "Zhisong Zhang",
            "Kaixin Ma",
            "Wenhao Yu",
            "Haitao Mi",
            "Dong Yu"
        ],
        "date": "2025/04/23",
        "pdf": "http://arxiv.org/pdf/2504.21024",
        "abstract": "Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs&#39; pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent&#39;s policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.21024"
    },
    "1efa7643e0746e7574ee210f8847b779": {
        "title": "Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA",
        "authors": [
            "Xuanzhao Dong",
            "Wenhui Zhu",
            "Hao Wang",
            "Xiwen Chen",
            "Peijie Qiu",
            "Rui Yin",
            "Yi Su",
            "Yalin Wang"
        ],
        "date": "2025/04/30",
        "pdf": "http://arxiv.org/pdf/2504.21252",
        "abstract": "Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.21252"
    },
    "ac00884693e04cf3fe78260fab71c204": {
        "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment",
        "authors": [
            "Kun Wang",
            "Guibin Zhang",
            "Zhenhong Zhou",
            "Jiahao Wu",
            "Miao Yu",
            "Shiqian Zhao",
            "Chenlong Yin",
            "Jinhu Fu",
            "Yibo Yan",
            "Hanjun Luo",
            "Liang Lin",
            "Zhihao Xu",
            "Haolang Lu",
            "Xinye Cao",
            "Xinyun Zhou",
            "Weifei Jin",
            "Fanci Meng",
            "Junyuan Mao",
            "Hao Wu",
            "Minghe Wang",
            "Fan Zhang",
            "Junfeng Fang",
            "Chengwei Liu",
            "Yifan Zhang",
            "Qiankun Li",
            "Chongye Guo",
            "Yalan Qin",
            "Yi Ding",
            "Donghai Hong",
            "Jiaming Ji",
            "Xinfeng Li",
            "Yifan Jiang",
            "Dongxia Wang",
            "Yihao Huang",
            "Yufei Guo",
            "Jen-tse Huang",
            "Yanwei Yue",
            "Wenke Huang",
            "Guancheng Wan",
            "Tianlin Li",
            "Lei Bai",
            "Jie Zhang",
            "Qing Guo",
            "Jingyi Wang",
            "Tianlong Chen",
            "Joey Tianyi Zhou",
            "Xiaojun Jia",
            "Weisong Sun",
            "Cong Wu",
            "Jing Chen",
            "Xuming Hu",
            "Yiming Li",
            "Xiao Wang",
            "Ningyu Zhang",
            "Luu Anh Tuan",
            "Guowen Xu",
            "Tianwei Zhang",
            "Xingjun Ma",
            "Xiang Wang",
            "Bo An",
            "Jun Sun",
            "Mohit Bansal",
            "Shirui Pan",
            "Yuval Elovici",
            "Bhavya Kailkhura",
            "Bo Li",
            "Yaodong Yang",
            "Hongwei Li",
            "Wenyuan Xu",
            "Yizhou Sun",
            "Wei Wang",
            "Qing Li",
            "Ke Tang",
            "Yu-Gang Jiang",
            "Felix Juefei-Xu",
            "Hui Xiong",
            "Xiaofeng Wang",
            "Shuicheng Yan",
            "Dacheng Tao",
            "Philip S. Yu",
            "Qingsong Wen",
            "Yang Liu"
        ],
        "date": "2025/04/22",
        "pdf": "http://arxiv.org/pdf/2504.15585",
        "abstract": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire &#34;lifechain&#34; of LLMs. To address this gap, this paper introduces, for the first time, the concept of &#34;full-stack&#34; safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2504.15585"
    },
    "b4bb0624e7f1bae97e469f03b6875f64": {
        "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions",
        "authors": [
            "Emre Can Acikgoz",
            "Cheng Qian",
            "Hongru Wang",
            "Vardhan Dongre",
            "Xiusi Chen",
            "Heng Ji",
            "Dilek Hakkani-Tür",
            "Gokhan Tur"
        ],
        "date": "2025/04/07",
        "pdf": "http://arxiv.org/pdf/2504.16939",
        "abstract": "Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. Yet, fundamental questions about their capabilities, limitations, and paths forward remain open. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. To that end, we systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following. Building upon this, we introduce a novel taxonomy by classifying recent work on Conversational Agents around our proposed desideratum. We identify critical research gaps and outline key directions, including realistic evaluations, long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. This work aims to provide a structured foundation, highlight existing limitations, and offer insights into potential future research directions for Conversational Agents, ultimately advancing progress toward Artificial General Intelligence (AGI). We maintain a curated repository of papers at: https://github.com/emrecanacikgoz/awesome-conversational-agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.16939"
    },
    "9a7d2a5749e682f91707ffe42e4c3f5e": {
        "title": "Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents",
        "authors": [
            "Chaoran Chen",
            "Zhiping Zhang",
            "Ibrahim Khalilov",
            "Bingcan Guo",
            "Simret A Gebreegziabher",
            "Yanfang Ye",
            "Ziang Xiao",
            "Yaxing Yao",
            "Tianshi Li",
            "Toby Jia-Jun Li"
        ],
        "date": "2025/04/24",
        "pdf": "http://arxiv.org/pdf/2504.17934",
        "abstract": "The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.17934"
    },
    "e70cae08f6a03cde1d9dbc64b1c1f113": {
        "title": "Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning",
        "authors": [
            "Isadora White",
            "Kolby Nottingham",
            "Ayush Maniar",
            "Max Robinson",
            "Hansen Lillemark",
            "Mehul Maheshwari",
            "Lianhui Qin",
            "Prithviraj Ammanabrolu"
        ],
        "date": "2025/04/24",
        "pdf": "http://arxiv.org/pdf/2504.17950",
        "abstract": "Collaboration is ubiquitous and essential in day-to-day life -- from exchanging ideas, to delegating tasks, to generating plans together. This work studies how LLMs can adaptively collaborate to perform complex embodied reasoning tasks. To this end we introduce MINDcraft, an easily extensible platform built to enable LLM agents to control characters in the open-world game of Minecraft; and MineCollab, a benchmark to test the different dimensions of embodied and collaborative reasoning. An experimental study finds that the primary bottleneck in collaborating effectively for current state-of-the-art agents is efficient natural language communication, with agent performance dropping as much as 15% when they are required to communicate detailed task completion plans. We conclude that existing LLM agents are ill-optimized for multi-agent collaboration, especially in embodied scenarios, and highlight the need to employ methods beyond in-context and imitation learning. Our website can be found here: https://mindcraft-minecollab.github.io/",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.17950"
    },
    "3543f039b8d65d85a66fbf9bda09c353": {
        "title": "Anyprefer: An Agentic Framework for Preference Data Synthesis",
        "authors": [
            "Yiyang Zhou",
            "Zhaoyang Wang",
            "Tianle Wang",
            "Shangyu Xing",
            "Peng Xia",
            "Bo Li",
            "Kaiyuan Zheng",
            "Zijian Zhang",
            "Zhaorun Chen",
            "Wenhao Zheng",
            "Xuchao Zhang",
            "Chetan Bansal",
            "Weitong Zhang",
            "Ying Wei",
            "Mohit Bansal",
            "Huaxiu Yao"
        ],
        "date": "2025/04/27",
        "pdf": "http://arxiv.org/pdf/2504.19276",
        "abstract": "High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model&#39;s responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.19276"
    },
    "2b3cec49f65e66f7e8c0919f155085ba": {
        "title": "RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning",
        "authors": [
            "Zihan Wang",
            "Kangrui Wang",
            "Qineng Wang",
            "Pingyue Zhang",
            "Linjie Li",
            "Zhengyuan Yang",
            "Kefan Yu",
            "Minh Nhat Nguyen",
            "Licheng Liu",
            "Eli Gottlieb",
            "Monica Lam",
            "Yiping Lu",
            "Kyunghyun Cho",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Lijuan Wang",
            "Yejin Choi",
            "Manling Li"
        ],
        "date": "2025/04/24",
        "pdf": "http://arxiv.org/pdf/2504.20073",
        "abstract": "Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.20073"
    },
    "dd65064eec75522926627d3ece2c91c3": {
        "title": "MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?",
        "authors": [
            "Zheng Hui",
            "Xiaokai Wei",
            "Yexi Jiang",
            "Kevin Gao",
            "Chen Wang",
            "Frank Ong",
            "Se-eun Yoon",
            "Rachit Pareek",
            "Michelle Gong"
        ],
        "date": "2025/04/26",
        "pdf": "http://arxiv.org/pdf/2504.20094",
        "abstract": "In this paper, we propose a multi-agent collaboration framework called MATCHA for conversational recommendation system, leveraging large language models (LLMs) to enhance personalization and user engagement. Users can request recommendations via free-form text and receive curated lists aligned with their interests, preferences, and constraints. Our system introduces specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards. These agents collaboratively improve recommendations accuracy, diversity, and safety. On eight metrics, our model achieves superior or comparable performance to the current state-of-the-art. Through comparisons with six baseline models, our approach addresses key challenges in conversational recommendation systems for game recommendations, including: (1) handling complex, user-specific requests, (2) enhancing personalization through multi-agent collaboration, (3) empirical evaluation and deployment, and (4) ensuring safe and trustworthy interactions.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.20094"
    },
    "001c74efba70cde5d98f87ad30fc45b2": {
        "title": "ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies",
        "authors": [
            "Shubham Gandhi",
            "Dhruv Shah",
            "Manasi Patwardhan",
            "Lovekesh Vig",
            "Gautam Shroff"
        ],
        "date": "2025/04/28",
        "pdf": "http://arxiv.org/pdf/2504.20117",
        "abstract": "In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system&#39;s effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.20117"
    },
    "9fde1be049c67faa1a55aaa4428a0c28": {
        "title": "LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics",
        "authors": [
            "Marc Glocker",
            "Peter Hönig",
            "Matthias Hirschmanner",
            "Markus Vincze"
        ],
        "date": "2025/04/30",
        "pdf": "http://arxiv.org/pdf/2504.21716",
        "abstract": "We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.21716"
    },
    "8c7fd1c6daa401bb83ed8cebd3fc3050": {
        "title": "SWE-smith: Scaling Data for Software Engineering Agents",
        "authors": [
            "John Yang",
            "Kilian Leret",
            "Carlos E. Jimenez",
            "Alexander Wettig",
            "Kabir Khandpur",
            "Yanzhe Zhang",
            "Binyuan Hui",
            "Ofir Press",
            "Ludwig Schmidt",
            "Diyi Yang"
        ],
        "date": "2025/04/30",
        "pdf": "http://arxiv.org/pdf/2504.21798",
        "abstract": "Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.21798"
    },
    "6cbfea86fa5ac20415473c7d850781ed": {
        "title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation",
        "authors": [
            "Nicolas Bougie",
            "Narimasa Watanabe"
        ],
        "date": "2025/04/17",
        "pdf": "http://arxiv.org/pdf/2504.12722",
        "abstract": "Recommender systems play a central role in numerous real-life applications, yet evaluating their performance remains a significant challenge due to the gap between offline metrics and online behaviors. Given the scarcity and limits (e.g., privacy issues) of real user data, we introduce SimUSER, an agent framework that serves as believable and cost-effective human proxies. SimUSER first identifies self-consistent personas from historical data, enriching user profiles with unique backgrounds and personalities. Then, central to this evaluation are users equipped with persona, memory, perception, and brain modules, engaging in interactions with the recommender system. SimUSER exhibits closer alignment with genuine humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments to explore the effects of thumbnails on click rates, the exposure effect, and the impact of reviews on user engagement. Finally, we refine recommender system parameters based on offline A/B test results, resulting in improved user engagement in the real world.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2504.12722"
    },
    "4baea3c1e563f0c9277c5c8c35b8e012": {
        "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets",
        "authors": [
            "Yuzhe Yang",
            "Yifei Zhang",
            "Minghao Wu",
            "Kaidi Zhang",
            "Yunmiao Zhang",
            "Honghai Yu",
            "Yan Hu",
            "Benyou Wang"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01506",
        "abstract": "The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01506"
    },
    "f07a3317672b4794826308491dd60e77": {
        "title": "A Survey on Large Language Model based Human-Agent Systems",
        "authors": [
            "Henry Peng Zou",
            "Wei-Chieh Huang",
            "Yaozu Wu",
            "Yankai Chen",
            "Chunyu Miao",
            "Hoang Nguyen",
            "Yue Zhou",
            "Weizhi Zhang",
            "Liancheng Fang",
            "Langzhou He",
            "Yangning Li",
            "Yuwei Cao",
            "Dongyuan Li",
            "Renhe Jiang",
            "Philip S. Yu"
        ],
        "date": "2025/05/01",
        "pdf": "http://arxiv.org/pdf/2505.00753",
        "abstract": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment &amp; profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.",
        "code": "https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.00753"
    },
    "38b543418035578f4fdead0af7da8c3b": {
        "title": "VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language",
        "authors": [
            "Sijin Sun",
            "Liangbin Zhao",
            "Ming Deng",
            "Xiuju Fu"
        ],
        "date": "2025/05/02",
        "pdf": "http://arxiv.org/pdf/2505.00989",
        "abstract": "Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.00989"
    },
    "9c241dca9b28884fbd9559877847c5ca": {
        "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains",
        "authors": [
            "Vicent Briva Iglesias",
            "Gokhan Dogru"
        ],
        "date": "2025/05/02",
        "pdf": "http://arxiv.org/pdf/2505.01560",
        "abstract": "Large language models (LLMs) and multi-agent orchestration are touted as the next leap in machine translation (MT), but their benefits relative to conventional neural MT (NMT) remain unclear. This paper offers an empirical reality check. We benchmark five paradigms, Google Translate (strong NMT baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM), and two GPT-4o-powered agentic workflows (sequential three-stage and iterative refinement), on test data drawn from a legal contract and news prose in three English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with expert ratings of adequacy and fluency; efficiency with total input-plus-output token counts mapped to April 2025 pricing. Automatic scores still favour the mature NMT system, which ranks first in seven of twelve metric-language combinations; o1-preview ties or places second in most remaining cases, while both multi-agent workflows trail. Human evaluation reverses part of this narrative: o1-preview produces the most adequate and fluent output in five of six comparisons, and the iterative agent edges ahead once, indicating that reasoning layers capture semantic nuance undervalued by surface metrics. Yet these qualitative gains carry steep costs. The sequential agent consumes roughly five times, and the iterative agent fifteen times, the tokens used by NMT or single-pass LLMs. We advocate multidimensional, cost-aware evaluation protocols and highlight research directions that could tip the balance: leaner coordination strategies, selective agent activation, and hybrid pipelines combining single-pass LLMs with targeted agent intervention.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.01560"
    },
    "223946a126642a58a196f966e41b6e74": {
        "title": "PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents",
        "authors": [
            "Takyoung Kim",
            "Janvijay Singh",
            "Shuhaib Mehri",
            "Emre Can Acikgoz",
            "Sagnik Mukherjee",
            "Nimet Beyza Bozdag",
            "Sumuk Shashidhar",
            "Gokhan Tur",
            "Dilek Hakkani-Tür"
        ],
        "date": "2025/05/02",
        "pdf": "http://arxiv.org/pdf/2505.01592",
        "abstract": "The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose PIPA, a unified evaluation protocol that conceptualizes the behavioral process of interactive task planning agents within a partially observable Markov Decision Process (POMDP) paradigm. The proposed protocol offers a comprehensive assessment of agent performance through a set of atomic evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent&#39;s decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.01592"
    },
    "eb649ce6cf118458bb5f8df94644746b": {
        "title": "Adaptive Thinking via Mode Policy Optimization for Social Language Agents",
        "authors": [
            "Minzheng Wang",
            "Yongbin Li",
            "Haobo Wang",
            "Xinghua Zhang",
            "Nan Xu",
            "Bingli Wu",
            "Fei Huang",
            "Haiyang Yu",
            "Wenji Mao"
        ],
        "date": "2025/05/04",
        "pdf": "http://arxiv.org/pdf/2505.02156",
        "abstract": "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current studies. Existing methods either lack this kind of reasoning capability or enforce Long Chain-of-Thought reasoning uniformly across all scenarios, resulting in excessive token usage and inflexible social simulation. To address this, we propose an $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning ($\\textbf{AML}$) framework in this paper, aiming to improve the adaptive thinking ability of language agents in dynamic social interactions. To this end, we first identify hierarchical thinking modes ranging from intuitive response to deep deliberation based on the cognitive control theory. We then develop the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm to optimize the context-aware mode switching and reasoning. Our framework advances existing research in three key aspects: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence benchmarks verify that AML achieves 15.6% higher task performance than GPT-4o. Notably, our AMPO outperforms GRPO by 7.0% with 32.8% shorter reasoning chains, demonstrating the advantage of adaptive thinking mode selection and optimization mechanism in AMPO over GRPO&#39;s fixed-depth solution.",
        "code": "",
        "category": [
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.02156"
    },
    "d8fa3ada4f7e6102754417e509ffea4d": {
        "title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models",
        "authors": [
            "Bang Zhang",
            "Ruotian Ma",
            "Qingxuan Jiang",
            "Peisong Wang",
            "Jiaqi Chen",
            "Zheng Xie",
            "Xingyu Chen",
            "Yue Wang",
            "Fanghua Ye",
            "Jian Li",
            "Yifan Yang",
            "Zhaopeng Tu",
            "Xiaolong Li"
        ],
        "date": "2025/05/01",
        "pdf": "http://arxiv.org/pdf/2505.02847",
        "abstract": "Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM&#39;s higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.02847"
    },
    "6766ce08cb9b6e125c09e161a2fc736a": {
        "title": "Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale",
        "authors": [
            "Jiale Liu",
            "Yifan Zeng",
            "Shaokun Zhang",
            "Chi Zhang",
            "Malte Højmark-Bertelsen",
            "Marie Normann Gadeberg",
            "Huazheng Wang",
            "Qingyun Wu"
        ],
        "date": "2025/05/06",
        "pdf": "http://arxiv.org/pdf/2505.03973",
        "abstract": "LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.03973"
    },
    "30c79843ae40ff127e6f2f3b7b5c9049": {
        "title": "How Social is It? A Benchmark for LLMs&#39; Capabilities in Multi-user Multi-turn Social Agent Tasks",
        "authors": [
            "Yusen Wu",
            "Junwu Xiong",
            "Xiaotie Deng"
        ],
        "date": "2025/04/04",
        "pdf": "http://arxiv.org/pdf/2505.04628",
        "abstract": "Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs&#39; capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM&#39;s social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs&#39; social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.04628"
    },
    "86d23a22b9f3403a7c75c375f88a7245": {
        "title": "FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights",
        "authors": [
            "Chengzhang Yu",
            "Yiming Zhang",
            "Zhixin Liu",
            "Zenghui Ding",
            "Yining Sun",
            "Zhanpeng Jin"
        ],
        "date": "2025/05/06",
        "pdf": "http://arxiv.org/pdf/2505.04649",
        "abstract": "The automation of scientific research through large language models (LLMs) presents significant opportunities but faces critical challenges in knowledge synthesis and quality assurance. We introduce Feedback-Refined Agent Methodology (FRAME), a novel framework that enhances medical paper generation through iterative refinement and structured feedback. Our approach comprises three key innovations: (1) A structured dataset construction method that decomposes 4,287 medical papers into essential research components through iterative refinement; (2) A tripartite architecture integrating Generator, Evaluator, and Reflector agents that progressively improve content quality through metric-driven feedback; and (3) A comprehensive evaluation framework that combines statistical metrics with human-grounded benchmarks. Experimental results demonstrate FRAME&#39;s effectiveness, achieving significant improvements over conventional approaches across multiple models (9.91% average gain with DeepSeek V3, comparable improvements with GPT-4o Mini) and evaluation dimensions. Human evaluation confirms that FRAME-generated papers achieve quality comparable to human-authored works, with particular strength in synthesizing future research directions. The results demonstrated our work could efficiently assist medical research by building a robust foundation for automated medical research paper generation while maintaining rigorous academic standards.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.04649"
    },
    "73e566850121757a663e85fc05ffda6d": {
        "title": "ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents",
        "authors": [
            "Elias Lumer",
            "Anmol Gulati",
            "Vamse Kumar Subbiah",
            "Pradeep Honaganahalli Basavaraju",
            "James A. Burke"
        ],
        "date": "2025/05/09",
        "pdf": "http://arxiv.org/pdf/2505.06416",
        "abstract": "Recent advancements in Large Language Models (LLMs) and the introduction of the Model Context Protocol (MCP) have significantly expanded LLM agents&#39; capability to interact dynamically with external tools and APIs. However, existing tool selection frameworks do not integrate MCP servers, instead relying heavily on error-prone manual updates to monolithic local tool repositories, leading to duplication, inconsistencies, and inefficiencies. Additionally, current approaches abstract tool selection before the LLM agent is invoked, limiting its autonomy and hindering dynamic re-querying capabilities during multi-turn interactions. To address these issues, we introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM agents with a MCP tool retriever, giving agents the autonomy to add tools into their memory, as well as an auto-synchronizing tool storage system pipeline through CRUD (create, read, update, delete) operations with MCP servers as the single source of truth. We also propose a novel embedding strategy, Tool Document Weighted Average (TDWA), designed to selectively emphasize critical components of tool documents (e.g. tool name or synthetic questions) during the embedding process. Comprehensive evaluations conducted on a created dataset of 5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models, and 5 retriever types, demonstrate substantial improvements in tool retrieval and agent invocation performance, emphasizing ScaleMCP&#39;s effectiveness in scalable, dynamic tool selection and invocation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.06416"
    },
    "408d270c5aad1944907c5b7bb0e3aef2": {
        "title": "EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation",
        "authors": [
            "Xinyi Mou",
            "Chen Qian",
            "Wei Liu",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "date": "2025/05/11",
        "pdf": "http://arxiv.org/pdf/2505.06904",
        "abstract": "Large language models (LLMs) have demonstrated an impressive ability to role-play humans and replicate complex social dynamics. While large-scale social simulations are gaining increasing attention, they still face significant challenges, particularly regarding high time and computation costs. Existing solutions, such as distributed mechanisms or hybrid agent-based model (ABM) integrations, either fail to address inference costs or compromise accuracy and generalizability. To this end, we propose EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation. EcoLANG operates in two stages: (1) language evolution, where we filter synonymous words and optimize sentence-level rules through natural selection, and (2) language utilization, where agents in social simulations communicate using the evolved language. Experimental results demonstrate that EcoLANG reduces token consumption by over 20%, enhancing efficiency without sacrificing simulation accuracy.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.06904"
    },
    "4b33332ff17929a762c3bd1fbb80ca89": {
        "title": "Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs",
        "authors": [
            "Yifan Wei",
            "Xiaoyan Yu",
            "Tengfei Pan",
            "Angsheng Li",
            "Li Du"
        ],
        "date": "2025/05/12",
        "pdf": "http://arxiv.org/pdf/2505.07184",
        "abstract": "Large language models (LLMs) have achieved unprecedented performance by leveraging vast pretraining corpora, yet their performance remains suboptimal in knowledge-intensive domains such as medicine and scientific research, where high factual precision is required. While synthetic data provides a promising avenue for augmenting domain knowledge, existing methods frequently generate redundant samples that do not align with the model&#39;s true knowledge gaps. To overcome this limitation, we propose a novel Structural Entropy-guided Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge deficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree Search (MCTS) to selectively explore regions where the model lacks domain-specific knowledge. Guided by these insights, the framework generates targeted synthetic data for supervised fine-tuning, enabling continuous self-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple domain-specific benchmarks show that SENATOR effectively detects and repairs knowledge deficiencies, achieving notable performance improvements. The code and data for our methods and experiments are available at https://github.com/weiyifan1023/senator.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.07184"
    },
    "eb9dd896dd2000a393d83f43df249b81": {
        "title": "Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study",
        "authors": [
            "Baixuan Xu",
            "Chunyang Li",
            "Weiqi Wang",
            "Wei Fan",
            "Tianshi Zheng",
            "Haochen Shi",
            "Tao Fan",
            "Yangqiu Song",
            "Qiang Yang"
        ],
        "date": "2025/05/12",
        "pdf": "http://arxiv.org/pdf/2505.07313",
        "abstract": "Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.07313"
    },
    "f2a896ebc19d2b35e8742396f5cf6ef9": {
        "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent",
        "authors": [
            "Ziyang Huang",
            "Xiaowei Yuan",
            "Yiming Ju",
            "Jun Zhao",
            "Kang Liu"
        ],
        "date": "2025/05/12",
        "pdf": "http://arxiv.org/pdf/2505.07596",
        "abstract": "Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.07596"
    },
    "cf50a6f802ee44ed49d4ac58dbae8035": {
        "title": "Putting It All into Context: Simplifying Agents with LCLMs",
        "authors": [
            "Mingjian Jiang",
            "Yangjun Ruan",
            "Luis Lastras",
            "Pavan Kapanipathi",
            "Tatsunori Hashimoto"
        ],
        "date": "2025/05/12",
        "pdf": "http://arxiv.org/pdf/2505.08120",
        "abstract": "Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. In this work, we investigate whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, we demonstrate that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.08120"
    },
    "aa27b16685b8237c4e276cd809b35786": {
        "title": "ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval",
        "authors": [
            "Mingxu Tao",
            "Bowen Tang",
            "Mingxuan Ma",
            "Yining Zhang",
            "Hourun Li",
            "Feifan Wen",
            "Hao Ma",
            "Jia Yang"
        ],
        "date": "2025/05/13",
        "pdf": "http://arxiv.org/pdf/2505.08130",
        "abstract": "The rise of Large Language Models~(LLMs) revolutionizes information retrieval, allowing users to obtain required answers through complex instructions within conversations. However, publicly available services remain inadequate in addressing the needs of faculty and students to search campus-specific information. It is primarily due to the LLM&#39;s lack of domain-specific knowledge and the limitation of search engines in supporting multilingual and timely scenarios. To tackle these challenges, we introduce ALOHA, a multilingual agent enhanced by hierarchical retrieval for university orientation. We also integrate external APIs into the front-end interface to provide interactive service. The human evaluation and case study show our proposed system has strong capabilities to yield correct, timely, and user-friendly responses to the queries in multiple languages, surpassing commercial chatbots and search engines. The system has been deployed and has provided service for more than 12,000 people.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.08130"
    },
    "26b91a3702ebf713125a37db316792a7": {
        "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?",
        "authors": [
            "Ada Chen",
            "Yongjiang Wu",
            "Junyuan Zhang",
            "Jingyu Xiao",
            "Shu Yang",
            "Jen-tse Huang",
            "Kun Wang",
            "Wenxuan Wang",
            "Shuai Wang"
        ],
        "date": "2025/05/16",
        "pdf": "http://arxiv.org/pdf/2505.10924",
        "abstract": "Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety analysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs; \\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.10924"
    },
    "129e074f64608b0fc544ab58625d792f": {
        "title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents",
        "authors": [
            "Jiaxing Zhao",
            "Hongbin Xie",
            "Yuzhen Lei",
            "Xuan Song",
            "Zhuoran Shi",
            "Lianxin Li",
            "Shuangxue Liu",
            "Haoran Zhang"
        ],
        "date": "2025/05/16",
        "pdf": "http://arxiv.org/pdf/2505.10936",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.10936"
    },
    "3bb4b8a0b8cfbe47046019cc01865d03": {
        "title": "SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution",
        "authors": [
            "Hanlin Wang",
            "Chak Tou Leong",
            "Jiashuo Wang",
            "Jian Wang",
            "Wenjie Li"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.20732",
        "abstract": "Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\\% on average) and grounding accuracy (+1.9\\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.",
        "code": "https://github.com/WangHanLinHenry/SPA-RL-Agent",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20732"
    },
    "701dab78513be48b5e23ac1e10bc8431": {
        "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents",
        "authors": [
            "Lingxiao Diao",
            "Xinyue Xu",
            "Wanxuan Sun",
            "Cheng Yang",
            "Zhuosheng Zhang"
        ],
        "date": "2025/05/16",
        "pdf": "http://arxiv.org/pdf/2505.11368",
        "abstract": "Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.11368"
    },
    "72061bc48b55de694d249a7b02f9af35": {
        "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks",
        "authors": [
            "Yuxuan Li",
            "Aoi Naito",
            "Hirokazu Shirado"
        ],
        "date": "2025/05/15",
        "pdf": "http://arxiv.org/pdf/2505.11556",
        "abstract": "Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but also risk replicating collective reasoning failures observed in human groups. Yet, no theory-grounded benchmark exists to systematically evaluate such failures. In this paper, we introduce the Hidden Profile paradigm from social psychology as a diagnostic testbed for multi-agent LLM systems. By distributing critical information asymmetrically across agents, the paradigm reveals how inter-agent dynamics support or hinder collective reasoning. We first formalize the paradigm for multi-agent decision-making under distributed knowledge and instantiate it as a benchmark with nine tasks spanning diverse scenarios, including adaptations from prior human studies. We then conduct experiments with GPT-4.1 and five other leading LLMs, including reasoning-enhanced variants, showing that multi-agent systems across all models fail to match the accuracy of single agents given complete information. While agents&#39; collective performance is broadly comparable to that of human groups, nuanced behavioral differences emerge, such as increased sensitivity to social desirability. Finally, we demonstrate the paradigm&#39;s diagnostic utility by exploring a cooperation-contradiction trade-off in multi-agent LLM systems. We find that while cooperative agents are prone to over-coordination in collective settings, increased contradiction impairs group convergence. This work contributes a reproducible framework for evaluating multi-agent LLM systems and motivates future research on artificial collective intelligence and human-AI interaction.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.11556"
    },
    "e3a9edc1e4c1c34dec3b0c6c8df6ef2d": {
        "title": "Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing",
        "authors": [
            "Kyudan Jung",
            "Hojun Cho",
            "Jooyeol Yun",
            "Soyoung Yang",
            "Jaehyeok Jang",
            "Jaegul Choo"
        ],
        "date": "2025/05/16",
        "pdf": "http://arxiv.org/pdf/2505.11604",
        "abstract": "Editing presentation slides remains one of the most common and time-consuming tasks faced by millions of users daily, despite significant advances in automated slide generation. Existing approaches have successfully demonstrated slide editing via graphic user interface (GUI)-based agents, offering intuitive visual control. However, such methods often suffer from high computational cost and latency. In this paper, we propose Talk-to-Your-Slides, an LLM-powered agent designed to edit slides %in active PowerPoint sessions by leveraging structured information about slide objects rather than relying on image modality. The key insight of our work is designing the editing process with distinct high-level and low-level layers to facilitate interaction between user commands and slide objects. By providing direct access to application objects rather than screen pixels, our system enables 34.02% faster processing, 34.76% better instruction fidelity, and 87.42% cheaper operation than baselines. To evaluate slide editing capabilities, we introduce TSBench, a human-annotated dataset comprising 379 diverse editing instructions paired with corresponding slide variations in four categories. Our code, benchmark and demos are available at https://anonymous.4open.science/r/Talk-to-Your-Slides-0F4C.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.11604"
    },
    "1ba4e26948c9a0d15aad0e22ea9b910e": {
        "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic",
        "authors": [
            "Yufei Xiang",
            "Yiqun Shen",
            "Yeqin Zhang",
            "Cam-Tu Nguyen"
        ],
        "date": "2025/05/17",
        "pdf": "http://arxiv.org/pdf/2505.11807",
        "abstract": "Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM&#39;s context. Instead, it combines the LLM&#39;s action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline &#39;&#39;retrospection&#39;&#39; process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.11807"
    },
    "65374e325f40dfec661a0fa2826d7008": {
        "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering",
        "authors": [
            "Taolin Zhang",
            "Dongyang Li",
            "Qizhou Chen",
            "Chengyu Wang",
            "Xiaofeng He"
        ],
        "date": "2025/05/17",
        "pdf": "http://arxiv.org/pdf/2505.11811",
        "abstract": "Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an &#39;&#39;operator&#39;&#39; by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined &#39;&#39;operators&#39;&#39; to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.11811"
    },
    "45edea0eeef0661f877297dc45a910d9": {
        "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents",
        "authors": [
            "Weikai Xu",
            "Zhizheng Jiang",
            "Yuxuan Liu",
            "Pengzhi Gao",
            "Wei Liu",
            "Jian Luan",
            "Yuanchun Li",
            "Yunxin Liu",
            "Bin Wang",
            "Bo An"
        ],
        "date": "2025/05/17",
        "pdf": "http://arxiv.org/pdf/2505.11891",
        "abstract": "VLM-based mobile agents are increasingly popular due to their capabilities to interact with smartphone GUIs and XML-structured texts and to complete daily tasks. However, existing online benchmarks struggle with obtaining stable reward signals due to dynamic environmental changes. Offline benchmarks evaluate the agents through single-path trajectories, which stands in contrast to the inherently multi-solution characteristics of GUI tasks. Additionally, both types of benchmarks fail to assess whether mobile agents can handle noise or engage in proactive interactions due to a lack of noisy apps or overly full instructions during the evaluation process. To address these limitations, we use a slot-based instruction generation method to construct a more realistic and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a common task split, with offline multi-path evaluation to assess the agent&#39;s ability to obtain step rewards during task execution. It contains a noisy split based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to formulate a real noisy environment. Furthermore, an ambiguous instruction split with preset Q\\&amp;A interactions is released to evaluate the agent&#39;s proactive interaction capabilities. We conduct evaluations on these splits using the single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2, as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.11891"
    },
    "f683f73389ff2ef5e634a80b8106d7ba": {
        "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning",
        "authors": [
            "Kun Huang",
            "Weikai Xu",
            "Yuxuan Liu",
            "Quandong Wang",
            "Pengzhi Gao",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang",
            "Bo An"
        ],
        "date": "2025/05/18",
        "pdf": "http://arxiv.org/pdf/2505.12299",
        "abstract": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to improve the reasoning performance of VLM-based mobile agents in GUI tasks. However, the scarcity of diverse CoaT trajectories limits the expressiveness and generalization ability of such agents. While self-training is commonly employed to address data scarcity, existing approaches either overlook the correctness of intermediate reasoning steps or depend on expensive process-level annotations to construct process reward models (PRM). To address the above problems, we propose an Iterative Preference Learning (IPL) that constructs a CoaT-tree through interative sampling, scores leaf nodes using rule-based reward, and backpropagates feedback to derive Thinking-level Direct Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up supervised fine-tuning, we further introduce a three-stage instruction evolution, which leverages GPT-4o to generate diverse Q\\&amp;A pairs based on real mobile UI screenshots, enhancing both generality and layout understanding. Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our agent MobileIPL outperforms strong baselines, including continual pretraining models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance across three standard Mobile GUI-Agents benchmarks and shows strong generalization to out-of-domain scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12299"
    },
    "6d2a6d582d633e16a1ad03d9607b1267": {
        "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents",
        "authors": [
            "Navid Madani",
            "Rohini Srihari"
        ],
        "date": "2025/05/18",
        "pdf": "http://arxiv.org/pdf/2505.12531",
        "abstract": "Large language models (LLMs) increasingly power mental-health chatbots, yet the field still lacks a scalable, theory-grounded way to decide which model is most effective to deploy. We present ESC-Judge, the first end-to-end evaluation framework that (i) grounds head-to-head comparisons of emotional-support LLMs in Clara Hill&#39;s established Exploration-Insight-Action counseling model, providing a structured and interpretable view of performance, and (ii) fully automates the evaluation pipeline at scale. ESC-Judge operates in three stages: first, it synthesizes realistic help-seeker roles by sampling empirically salient attributes such as stressors, personality, and life history; second, it has two candidate support agents conduct separate sessions with the same role, isolating model-specific strategies; and third, it asks a specialized judge LLM to express pairwise preferences across rubric-anchored skills that span the Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and 86 percent of Action decisions, demonstrating human-level reliability at a fraction of the cost. All code, prompts, synthetic roles, transcripts, and judgment scripts are released to promote transparent progress in emotionally supportive AI.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12531"
    },
    "0e602149566137888bdedc73eb9366c6": {
        "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection",
        "authors": [
            "Tiankai Yang",
            "Junjun Liu",
            "Wingchun Siu",
            "Jiahang Wang",
            "Zhuangzhuang Qian",
            "Chanjuan Song",
            "Cheng Cheng",
            "Xiyang Hu",
            "Yue Zhao"
        ],
        "date": "2025/05/19",
        "pdf": "http://arxiv.org/pdf/2505.12594",
        "abstract": "Anomaly detection (AD) is essential in areas such as fraud detection, network monitoring, and scientific research. However, the diversity of data modalities and the increasing number of specialized AD libraries pose challenges for non-expert users who lack in-depth library-specific knowledge and advanced programming skills. To tackle this, we present AD-AGENT, an LLM-driven multi-agent framework that turns natural-language instructions into fully executable AD pipelines. AD-AGENT coordinates specialized agents for intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging. Using a shared short-term workspace and a long-term cache, the agents integrate popular AD libraries like PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that AD-AGENT produces reliable scripts and recommends competitive models across libraries. The system is open-sourced to support further research and practical applications in AD.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12594"
    },
    "c773f06285d2f9fc72a81b62d002687d": {
        "title": "Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making",
        "authors": [
            "Jacob Kleiman",
            "Kevin Frank",
            "Joseph Voyles",
            "Sindy Campagna"
        ],
        "date": "2025/05/19",
        "pdf": "http://arxiv.org/pdf/2505.13761",
        "abstract": "Simulations, although powerful in accurately replicating real-world systems, often remain inaccessible to non-technical users due to their complexity. Conversely, large language models (LLMs) provide intuitive, language-based interactions but can lack the structured, causal understanding required to reliably model complex real-world dynamics. We introduce our simulation agent framework, a novel approach that integrates the strengths of both simulation models and LLMs. This framework helps empower users by leveraging the conversational capabilities of LLMs to interact seamlessly with sophisticated simulation systems, while simultaneously utilizing the simulations to ground the LLMs in accurate and structured representations of real-world phenomena. This integrated approach helps provide a robust and generalizable foundation for empirical validation and offers broad applicability across diverse domains.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.13761"
    },
    "487ef8912e7701f8d8adba0cbd6b8305": {
        "title": "CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring",
        "authors": [
            "Jiamin Su",
            "Yibo Yan",
            "Zhuoran Gao",
            "Han Zhang",
            "Xiang Liu",
            "Xuming Hu"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.13965",
        "abstract": "Automated Essay Scoring (AES) is crucial for modern education, particularly with the increasing prevalence of multimodal assessments. However, traditional AES methods struggle with evaluation generalizability and multimodal perception, while even recent Multimodal Large Language Model (MLLM)-based approaches can produce hallucinated justifications and scores misaligned with human judgment. To address the limitations, we introduce CAFES, the first collaborative multi-agent framework specifically designed for AES. It orchestrates three specialized agents: an Initial Scorer for rapid, trait-specific evaluations; a Feedback Pool Manager to aggregate detailed, evidence-grounded strengths; and a Reflective Scorer that iteratively refines scores based on this feedback to enhance human alignment. Extensive experiments, using state-of-the-art MLLMs, achieve an average relative improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth, especially for grammatical and lexical diversity. Our proposed CAFES framework paves the way for an intelligent multimodal AES system. The code will be available upon acceptance.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.13965"
    },
    "341a28a88345137853a42f9fe6e5a306": {
        "title": "BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks",
        "authors": [
            "Weihong Du",
            "Wenrui Liao",
            "Binyu Yan",
            "Hongru Liang",
            "Anthony G. Cohn",
            "Wenqiang Lei"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.14079",
        "abstract": "Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agent&#39;s initial state. However, this forward reasoning paradigm doesn&#39;t work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agent&#39;s initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.14079"
    },
    "caaa6c11650340b6220bb078384d1c42": {
        "title": "Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents",
        "authors": [
            "Pengzhou Cheng",
            "Haowen Hu",
            "Zheng Wu",
            "Zongru Wu",
            "Tianjie Ju",
            "Zhuosheng Zhang",
            "Gongshen Liu"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.14418",
        "abstract": "Graphical user interface (GUI) agents powered by multimodal large language models (MLLMs) have shown greater promise for human-interaction. However, due to the high fine-tuning cost, users often rely on open-source GUI agents or APIs offered by AI providers, which introduces a critical but underexplored supply chain threat: backdoor attacks. In this work, we first unveil that MLLM-powered GUI agents naturally expose multiple interaction-level triggers, such as historical steps, environment states, and task progress. Based on this observation, we introduce AgentGhost, an effective and stealthy framework for red-teaming backdoor attacks. Specifically, we first construct composite triggers by combining goal and interaction levels, allowing GUI agents to unintentionally activate backdoors while ensuring task utility. Then, we formulate backdoor injection as a Min-Max optimization problem that uses supervised contrastive learning to maximize the feature difference across sample classes at the representation space, improving flexibility of the backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the discrepancy between backdoor and clean behavior generation, enhancing effectiveness and utility. Extensive evaluations of various agent models in two established mobile benchmarks show that AgentGhost is effective and generic, with attack accuracy that reaches 99.7\\% on three attack objectives, and shows stealthiness with only 1\\% utility degradation. Furthermore, we tailor a defense method against AgentGhost that reduces the attack accuracy to 22.1\\%. Our code is available at \\texttt{anonymous}.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.14418"
    },
    "b8b8057e263f0c1f4f35bd8a01d7984a": {
        "title": "MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation",
        "authors": [
            "Xi Wang",
            "Jiaqian Hu",
            "Safinah Ali"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.14848",
        "abstract": "We present MAATS, a Multi Agent Automated Translation System that leverages the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal for error detection and refinement. MAATS employs multiple specialized AI agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency, Style, Terminology), followed by a synthesis agent that integrates the annotations to iteratively refine translations. This design contrasts with conventional single-agent methods that rely on self-correction. Evaluated across diverse language pairs and Large Language Models (LLMs), MAATS outperforms zero-shot and single-agent baselines with statistically significant gains in both automatic metrics and human assessments. It excels particularly in semantic accuracy, locale adaptation, and linguistically distant language pairs. Qualitative analysis highlights its strengths in multi-layered error diagnosis, omission detection across perspectives, and context-aware refinement. By aligning modular agent roles with interpretable MQM dimensions, MAATS narrows the gap between black-box LLMs and human translation workflows, shifting focus from surface fluency to deeper semantic and contextual fidelity.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.14848"
    },
    "e2b27c29dd29b686306a50b3b30743f0": {
        "title": "MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision",
        "authors": [
            "Zixuan Ke",
            "Austin Xu",
            "Yifei Ming",
            "Xuan-Phi Nguyen",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.14996",
        "abstract": "Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs&#39; strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation set for tuning and yield static MAS designs lacking adaptability during inference. We introduce MAS-ZERO, the first self-evolved, inference-time framework for automatic MAS design. MAS-ZERO employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM backbones of varying sizes, demonstrate that MAS-ZERO outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-evolved design for creating effective and adaptive MAS.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.14996"
    },
    "9edc0d987893b41193ef30b2dfe471ee": {
        "title": "UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking",
        "authors": [
            "Sarfraz Ahmad",
            "Hasan Iqbal",
            "Momina Ahsan",
            "Numaan Naeem",
            "Muhammad Ahsan Riaz Khan",
            "Arham Riaz",
            "Muhammad Arslan Manzoor",
            "Yuxia Wang",
            "Preslav Nakov"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15063",
        "abstract": "The rapid use of large language models (LLMs) has raised critical concerns regarding the factual reliability of their outputs, especially in low-resource languages such as Urdu. Existing automated fact-checking solutions overwhelmingly focus on English, leaving a significant gap for the 200+ million Urdu speakers worldwide. In this work, we introduce UrduFactCheck, the first comprehensive, modular fact-checking framework specifically tailored for Urdu. Our system features a dynamic, multi-strategy evidence retrieval pipeline that combines monolingual and translation-based approaches to address the scarcity of high-quality Urdu evidence. We curate and release two new hand-annotated benchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating LLM factuality. Extensive experiments demonstrate that UrduFactCheck, particularly its translation-augmented variants, consistently outperforms baselines and open-source alternatives on multiple metrics. We further benchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in Urdu, highlighting persistent gaps between proprietary and open-source models. UrduFactCheck&#39;s code and datasets are open-sourced and publicly available at https://github.com/mbzuai-nlp/UrduFactCheck.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15063"
    },
    "c5c1e638bc519743b79a0bbcfac46a92": {
        "title": "A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents",
        "authors": [
            "Ian Steenstra",
            "Timothy W. Bickmore"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15108",
        "abstract": "The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk taxonomy specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this taxonomy, detailing its grounding, and discuss potential use cases. We discuss two use cases in detail: monitoring cognitive model-based risk factors during a counseling conversation to detect unsafe deviations, in both human-AI counseling sessions and in automated benchmarking of AI psychotherapists with simulated patients. The proposed taxonomy offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15108"
    },
    "61e61d19f1e3d4f7970bb5b869f1ea02": {
        "title": "An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents",
        "authors": [
            "Bowen Jin",
            "Jinsung Yoon",
            "Priyanka Kargupta",
            "Sercan O. Arik",
            "Jiawei Han"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15117",
        "abstract": "Reinforcement learning (RL) has demonstrated strong potential in training large language models (LLMs) capable of complex reasoning for real-world problem solving. More recently, RL has been leveraged to create sophisticated LLM-based search agents that adeptly combine reasoning with search engine use. While the use of RL for training search agents is promising, the optimal design of such agents remains not fully understood. In particular, key factors -- such as (1) reward formulation, (2) the choice and characteristics of the underlying LLM, and (3) the role of the search engine in the RL process -- require further investigation. In this work, we conduct comprehensive empirical studies to systematically investigate these and offer actionable insights. We highlight several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoning-specialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference. These establish important guidelines for successfully building and deploying LLM-based search agents in real-world applications. Code is available at https://github.com/PeterGriffinJin/Search-R1.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15117"
    },
    "768ddc264a6cc3f9ecc8ad33c77c4ed4": {
        "title": "ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection",
        "authors": [
            "Jeonghye Kim",
            "Sojeong Rhee",
            "Minbeom Kim",
            "Dohyung Kim",
            "Sangmook Lee",
            "Youngchul Sung",
            "Kyomin Jung"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15182",
        "abstract": "Recent advances in LLM agents have largely built on reasoning backbones like ReAct, which interleave thought and action in complex environments. However, ReAct often produces ungrounded or incoherent reasoning steps, leading to misalignment between the agent&#39;s actual state and goal. Our analysis finds that this stems from ReAct&#39;s inability to maintain consistent internal beliefs and goal alignment, causing compounding errors and hallucinations. To address this, we introduce ReflAct, a novel backbone that shifts reasoning from merely planning next actions to continuously reflecting on the agent&#39;s state relative to its goal. By explicitly grounding decisions in states and enforcing ongoing goal alignment, ReflAct dramatically improves strategic reliability. This design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7% on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM), showing that strengthening the core reasoning backbone is key to reliable agent performance.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15182"
    },
    "c4070c75fc1547ec353177717ab800e4": {
        "title": "AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection",
        "authors": [
            "Jiatao Li",
            "Mao Ye",
            "Cheng Peng",
            "Xunjian Yin",
            "Xiaojun Wan"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15261",
        "abstract": "Existing AI-generated text detection methods heavily depend on large annotated datasets and external threshold tuning, restricting interpretability, adaptability, and zero-shot effectiveness. To address these limitations, we propose AGENT-X, a zero-shot multi-agent framework informed by classical rhetoric and systemic functional linguistics. Specifically, we organize detection guidelines into semantic, stylistic, and structural dimensions, each independently evaluated by specialized linguistic agents that provide explicit reasoning and robust calibrated confidence via semantic steering. A meta agent integrates these assessments through confidence-aware aggregation, enabling threshold-free, interpretable classification. Additionally, an adaptive Mixture-of-Agent router dynamically selects guidelines based on inferred textual characteristics. Experiments on diverse datasets demonstrate that AGENT-X substantially surpasses state-of-the-art supervised and zero-shot approaches in accuracy, interpretability, and generalization.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15261"
    },
    "8a30a2710e56d6d78c87ac17d24ec1ba": {
        "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
        "authors": [
            "Hyungjoo Chae",
            "Sunghwan Kim",
            "Junhee Cho",
            "Seungone Kim",
            "Seungjun Moon",
            "Gyeom Hwangbo",
            "Dongha Lim",
            "Minjin Kim",
            "Yeonjun Hwang",
            "Minju Gwak",
            "Dongwook Choi",
            "Minseok Kang",
            "Gwanhoon Im",
            "ByeongUng Cho",
            "Hyojun Kim",
            "Jun Hee Han",
            "Taeyoon Kwon",
            "Minju Kim",
            "Beong-woo Kwak",
            "Dongjin Kang",
            "Jinyoung Yeo"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15277",
        "abstract": "Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10 less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at LINK.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15277"
    },
    "b4f1fdaa4251b1ebb49a679bfb993139": {
        "title": "X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System",
        "authors": [
            "Peng Wang",
            "Ruihan Tao",
            "Qiguang Chen",
            "Mengkang Hu",
            "Libo Qin"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15372",
        "abstract": "Recently, large language model (LLM)-based agents have achieved significant success in interactive environments, attracting significant academic and industrial attention. Despite these advancements, current research predominantly focuses on English scenarios. In reality, there are over 7,000 languages worldwide, all of which demand access to comparable agentic services. Nevertheless, the development of language agents remains inadequate for meeting the diverse requirements of multilingual agentic applications. To fill this gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an interactive web environment, which evaluates the planning and interaction performance of language agents across multiple languages, thereby contributing to the advancement of global agent intelligence. Additionally, we assess the performance of various LLMs and cross-lingual alignment methods, examining their effectiveness in enhancing agents. Our findings reveal that even advanced models like GPT-4o, when combined with cross-lingual techniques, fail to achieve satisfactory results. We hope that X-WebAgentBench can serve as a valuable benchmark for multilingual agent scenario in real-world applications.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15372"
    },
    "b184a96a237d9500619fe49079b5b979": {
        "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents",
        "authors": [
            "Yuqi Zhou",
            "Sunhao Dai",
            "Shuai Wang",
            "Kaiwen Zhou",
            "Qinglin Jia",
            "Jun Xu"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15810",
        "abstract": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update-each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a Fast Thinking Template that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding. The project repository is available at https://github.com/Yuqi-Zhou/GUI-G1.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15810"
    },
    "8e93123d61db4231aef2484b1e860e47": {
        "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition",
        "authors": [
            "Dong Won Lee",
            "Hae Won Park",
            "Cynthia Breazeal",
            "Louis-Philippe Morency"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15922",
        "abstract": "We propose a large language model based reward decomposition framework for aligning dialogue agents using only a single session-level feedback signal. We leverage the reasoning capabilities of a frozen, pretrained large language model (LLM) to infer fine-grained local implicit rewards by decomposing global, session-level feedback. Our first text-only variant prompts the LLM to perform reward decomposition using only the dialogue transcript. The second multimodal variant incorporates additional behavioral cues, such as pitch, gaze, and facial affect, expressed as natural language descriptions. These inferred turn-level rewards are distilled into a lightweight reward model, which we utilize for RL-based fine-tuning for dialogue generation. We evaluate both text-only and multimodal variants against state-of-the-art reward decomposition methods and demonstrate notable improvements in human evaluations of conversation quality, suggesting that LLMs are strong reward decomposers that obviate the need for manual reward shaping and granular human feedback.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15922"
    },
    "11cdbc9a86ea47db990d39b7293b1d13": {
        "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation",
        "authors": [
            "Shijie Zhang",
            "Renhao Li",
            "Songsheng Wang",
            "Philipp Koehn",
            "Min Yang",
            "Derek F. Wong"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16281",
        "abstract": "The advancement of Large Language Models (LLMs) enables flexible and interpretable automatic evaluations. In the field of machine translation evaluation, utilizing LLMs with translation error annotations based on Multidimensional Quality Metrics (MQM) yields more human-aligned judgments. However, current LLM-based evaluation methods still face challenges in accurately identifying error spans and assessing their severity. In this paper, we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation Evaluation. We argue that existing approaches inadequately exploit the fine-grained structural and semantic information within the MQM hierarchy. To address this, we develop a hierarchical multi-agent system grounded in the MQM error typology, enabling granular evaluation of subtype errors. Two key strategies are incorporated to further mitigate systemic hallucinations within the framework: the utilization of the model&#39;s self-reflection capability and the facilitation of agent discussion involving asymmetric information. Empirically, HiMATE outperforms competitive baselines across different datasets in conducting human-aligned evaluations. Further analyses underscore its significant advantage in error span detection and severity assessment, achieving an average F1-score improvement of 89% over the best-performing baseline. We make our code and data publicly available at https://anonymous.4open.science/r/HiMATE-Anony.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16281"
    },
    "d014116d21edae38d2252e44dc1569bc": {
        "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance",
        "authors": [
            "Taeyoon Kwon",
            "Dongwook Choi",
            "Sunghwan Kim",
            "Hyojun Kim",
            "Seungjun Moon",
            "Beong-woo Kwak",
            "Kuan-Hao Huang",
            "Jinyoung Yeo"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16348",
        "abstract": "Embodied agents empowered by large language models (LLMs) have shown strong performance in household object rearrangement tasks. However, these tasks primarily focus on single-turn interactions with simplified instructions, which do not truly reflect the challenges of providing meaningful assistance to users. To provide personalized assistance, embodied agents must understand the unique semantics that users assign to the physical world (e.g., favorite cup, breakfast routine) by leveraging prior interaction history to interpret dynamic, real-world instructions. Yet, the effectiveness of embodied agents in utilizing memory for personalized assistance remains largely underexplored. To address this gap, we present MEMENTO, a personalized embodied agent evaluation framework designed to comprehensively assess memory utilization capabilities to provide personalized assistance. Our framework consists of a two-stage memory evaluation process design that enables quantifying the impact of memory utilization on task performance. This process enables the evaluation of agents&#39; understanding of personalized knowledge in object rearrangement tasks by focusing on its role in goal interpretation: (1) the ability to identify target objects based on personal meaning (object semantics), and (2) the ability to infer object-location configurations from consistent user patterns, such as routines (user patterns). Our experiments across various LLMs reveal significant limitations in memory utilization, with even frontier models like GPT-4o experiencing a 30.5% performance drop when required to reference multiple memories, particularly in tasks involving user patterns. These findings, along with our detailed analyses and case studies, provide valuable insights for future research in developing more effective personalized embodied agents. Project website: https://connoriginal.github.io/MEMENTO",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16348"
    },
    "715e1edb4b690534c854640c1f9fd6a8": {
        "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning",
        "authors": [
            "Zhepei Wei",
            "Wenlin Yao",
            "Yao Liu",
            "Weizhi Zhang",
            "Qin Lu",
            "Liang Qiu",
            "Changlong Yu",
            "Puyang Xu",
            "Chao Zhang",
            "Bing Yin",
            "Hyokun Yun",
            "Lihong Li"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16421",
        "abstract": "While reinforcement learning (RL) has demonstrated remarkable success in enhancing large language models (LLMs), it has primarily focused on single-turn tasks such as solving math problems. Training effective web agents for multi-turn interactions remains challenging due to the complexity of long-horizon decision-making across dynamic web interfaces. In this work, we present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework for training web agents. It learns directly from online interactions with web environments by asynchronously generating diverse trajectories, entirely guided by binary rewards depending on task success. Experiments on the WebArena-Lite benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to 44.8%, significantly outperforming existing state-of-the-art methods and strong proprietary models such as OpenAI o3. In-depth analyses reveal the effectiveness of the thinking-based prompting strategy and test-time scaling through increased interactions for web tasks. We further investigate different RL initialization policies by introducing two variants, namely WebAgent-R1-Zero and WebAgent-R1-CoT, which highlight the importance of the warm-up training stage (i.e., behavior cloning) and provide insights on incorporating long chain-of-thought (CoT) reasoning in web agents.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16421"
    },
    "489925cb4830bb8f1e18d55a31c4ec1c": {
        "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems",
        "authors": [
            "Song Jin",
            "Juntian Zhang",
            "Yuhan Liu",
            "Xun Zhang",
            "Yufei Zhang",
            "Guojun Yin",
            "Fei Jiang",
            "Wei Lin",
            "Rui Yan"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16429",
        "abstract": "Evaluating and iterating upon recommender systems is crucial, yet traditional A/B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16429"
    },
    "e4116f00f5b94ce5a5ea4f05683ae97c": {
        "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions",
        "authors": [
            "Spencer Hong",
            "Meng Luo",
            "Xinyi Wan"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16576",
        "abstract": "Determining the veracity of atomic claims is an imperative component of many recently proposed fact-checking systems. Many approaches tackle this problem by first retrieving evidence by querying a search engine and then performing classification by providing the evidence set and atomic claim to a large language model, but this process deviates from what a human would do in order to perform the task. Recent work attempted to address this issue by proposing iterative evidence retrieval, allowing for evidence to be collected several times and only when necessary. Continuing along this line of research, we propose a novel claim verification system, called EMULATE, which is designed to better emulate human actions through the use of a multi-agent framework where each agent performs a small part of the larger task, such as ranking search results according to predefined criteria or evaluating webpage content. Extensive experiments on several benchmarks show clear improvements over prior work, demonstrating the efficacy of our new multi-agent framework.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16576"
    },
    "566c4738dd93b086b40ab7bdb1c0cf7d": {
        "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering",
        "authors": [
            "Jianbiao Mei",
            "Tao Hu",
            "Daocheng Fu",
            "Licheng Wen",
            "Xuemeng Yang",
            "Rong Wu",
            "Pinlong Cai",
            "Xinyu Cai",
            "Xing Gao",
            "Yu Yang",
            "Chengjun Xie",
            "Botian Shi",
            "Yong Liu",
            "Yu Qiao"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16582",
        "abstract": "Large Language Models (LLMs), despite their advancements, are fundamentally limited by their static parametric knowledge, hindering performance on tasks requiring open-domain up-to-date information. While enabling LLMs to interact with external knowledge environments is a promising solution, current efforts primarily address closed-end problems. Open-ended questions, which characterized by lacking a standard answer or providing non-unique and diverse answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a novel search agent leveraging reinforcement learning to effectively tackle both open-ended and closed-ended questions in the open domain. O$^2$-Searcher leverages an efficient, locally simulated search environment for dynamic knowledge acquisition, effectively decoupling the external world knowledge from model&#39;s sophisticated reasoning processes. It employs a unified training mechanism with meticulously designed reward functions, enabling the agent to identify problem types and adapt different answer generation strategies. Furthermore, to evaluate performance on complex open-ended tasks, we construct O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain open-ended questions with associated web page caches. Extensive experiments show that O$^2$-Searcher, using only a 3B model, significantly surpasses leading LLM agents on O$^2$-QA. It also achieves SOTA results on various closed-ended QA benchmarks against similarly-sized models, while performing on par with much larger ones.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16582"
    },
    "f7a83e34788eed8255e5edc80d485c0e": {
        "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning",
        "authors": [
            "Amartya Chakraborty",
            "Paresh Dashore",
            "Nadia Bathaee",
            "Anmol Jain",
            "Anirban Das",
            "Shi-Xiong Zhang",
            "Sambit Sahu",
            "Milind Naphade",
            "Genta Indra Winata"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16986",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities as intelligent agents capable of solving complex problems. However, effective planning in scenarios involving dependencies between API or tool calls-particularly in multi-turn conversations-remains a significant challenge. To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn conversational dataset specifically designed to capture and manage inter-tool dependencies across diverse domains. T1 enables rigorous evaluation of agents&#39; ability to coordinate tool use across nine distinct domains (4 single domain and 5 multi-domain) with the help of an integrated caching mechanism for both short- and long-term memory, while supporting dynamic replanning-such as deciding whether to recompute or reuse cached results. Beyond facilitating research on tool use and planning, T1 also serves as a benchmark for evaluating the performance of open-source language models. We present results powered by T1-Agent, highlighting their ability to plan and reason in complex, tool-dependent scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16986"
    },
    "7252fd64ed5aad1729c62ac347957434": {
        "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems",
        "authors": [
            "Rui Ye",
            "Keduan Huang",
            "Qimin Wu",
            "Yuzhu Cai",
            "Tian Jin",
            "Xianghe Pang",
            "Xiangrui Liu",
            "Jiaqi Su",
            "Chen Qian",
            "Bohan Tang",
            "Kaiqu Liang",
            "Jiaao Chen",
            "Yue Hu",
            "Zhenfei Yin",
            "Rongye Shi",
            "Bo An",
            "Yang Gao",
            "Wenjun Wu",
            "Lei Bai",
            "Siheng Chen"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16988",
        "abstract": "LLM-based multi-agent systems (MAS) have demonstrated significant potential in enhancing single LLMs to address complex and diverse tasks in practical applications. Despite considerable advancements, the field lacks a unified codebase that consolidates existing methods, resulting in redundant re-implementation efforts, unfair comparisons, and high entry barriers for researchers. To address these challenges, we introduce MASLab, a unified, comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab integrates over 20 established methods across multiple domains, each rigorously validated by comparing step-by-step outputs with its official implementation. (2) MASLab provides a unified environment with various benchmarks for fair comparisons among methods, ensuring consistent inputs and standardized evaluation protocols. (3) MASLab implements methods within a shared streamlined structure, lowering the barriers for understanding and extension. Building on MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models, offering researchers a clear and comprehensive view of the current landscape of MAS methods. MASLab will continue to evolve, tracking the latest developments in the field, and invite contributions from the broader open-source community.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16988"
    },
    "6de5ce13b0a00b5fc26ff4844ca8f12a": {
        "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization",
        "authors": [
            "Yihong Wu",
            "Liheng Ma",
            "Muzhi Li",
            "Jiaming Zhou",
            "Jianye Hao",
            "Ho-fung Leung",
            "Irwin King",
            "Yingxue Zhang",
            "Jian-Yun Nie"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.17086",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility, due to the lack of factual knowledge, their application to Question Answering (QA) tasks remains hindered by hallucination. While Retrieval-Augmented Generation mitigates these issues by integrating external knowledge, existing approaches rely heavily on in-context learning, whose performance is constrained by the fundamental reasoning capabilities of LLMs. In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex Question Answering, comprising a planner that decomposes questions into a directed acyclic graph of subquestions and a worker that resolves questions via retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy Gradient Optimization), a novel reinforcement learning method that replaces traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by sampling trajectories from an asymptotically optimal policy. MyGO eliminates the need for gradient rescaling and reference models, ensuring stable and efficient training. Empirical results across multiple datasets demonstrate the effectiveness of Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a scalable and resource-efficient solution for complex QA tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.17086"
    },
    "399f2910cae63b0e2acb123836f884ff": {
        "title": "Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)",
        "authors": [
            "Clayton Cohn",
            "Surya Rayala",
            "Caitlin Snyder",
            "Joyce Fonteles",
            "Shruti Jain",
            "Naveeduddin Mohammed",
            "Umesh Timalsina",
            "Sarah K. Burriss",
            "Ashwin T S",
            "Namrata Srivastava",
            "Menton Deweese",
            "Angela Eeds",
            "Gautam Biswas"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.17238",
        "abstract": "Collaborative dialogue offers rich insights into students&#39; learning and critical thinking. This is essential for adapting pedagogical agents to students&#39; learning and problem-solving skills in STEM+C settings. While large language models (LLMs) facilitate dynamic pedagogical interactions, potential hallucinations can undermine confidence, trust, and instructional value. Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge, but its effectiveness depends on clear semantic links between user input and a knowledge base, which are often weak in student dialogue. We propose log-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating environment logs to contextualize collaborative discourse. Our findings show that LC-RAG improves retrieval over a discourse-only baseline and allows our collaborative peer agent, Copa, to deliver relevant, personalized guidance that supports students&#39; critical thinking and epistemic decision-making in a collaborative computational modeling environment, XYZ.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.17238"
    },
    "fd3209b9785ff435864394308737b026": {
        "title": "Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty",
        "authors": [
            "Peilin Wu",
            "Mian Zhang",
            "Xinlu Zhang",
            "Xinya Du",
            "Zhiyu Zoey Chen"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.17281",
        "abstract": "Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models&#39; uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model&#39;s uncertainty in its search decisions. To address this, we propose $\\beta$-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that $\\beta$-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.17281"
    },
    "f1e0b123670e397da94dad3387f24af9": {
        "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
        "authors": [
            "Minki Kang",
            "Jongwon Jeong",
            "Seanie Lee",
            "Jaewoong Cho",
            "Sung Ju Hwang"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.17612",
        "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.17612"
    },
    "7f9a199a76b44eaee6a12893d8de9610": {
        "title": "Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments",
        "authors": [
            "Qingyu Lu",
            "Liang Ding",
            "Siyi Cao",
            "Xuebo Liu",
            "Kanjian Zhang",
            "Jinxia Zhang",
            "Dacheng Tao"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.17616",
        "abstract": "Agents powered by large language models (LLMs) have demonstrated strong planning and decision-making capabilities in complex embodied environments. However, such agents often suffer from inefficiencies in multi-turn interactions, frequently trapped in repetitive loops or issuing ineffective commands, leading to redundant computational overhead. Instead of relying solely on learning from trajectories, we take a first step toward exploring the early-exit behavior for LLM-based agents. We propose two complementary approaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions during generation, and 2. an $\\textbf{extrinsic}$ method that verifies task completion to determine when to halt an agent&#39;s trial. To evaluate early-exit mechanisms, we introduce two metrics: one measures the reduction of $\\textbf{redundant steps}$ as a positive effect, and the other evaluates $\\textbf{progress degradation}$ as a negative effect. Experiments with 4 different LLMs across 5 embodied environments show significant efficiency improvements, with only minor drops in agent performance. We also validate a practical strategy where a stronger agent assists after an early-exit agent, achieving better performance with the same total steps. We will release our code to support further research.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.17616"
    },
    "f7c59dfa54d744a9ae7236d147a76716": {
        "title": "The Real Barrier to LLM Agent Usability is Agentic ROI",
        "authors": [
            "Weiwen Liu",
            "Jiarui Qin",
            "Xu Huang",
            "Xingshan Zeng",
            "Yunjia Xi",
            "Jianghao Lin",
            "Chuhan Wu",
            "Yasheng Wang",
            "Lifeng Shang",
            "Ruiming Tang",
            "Defu Lian",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.17767",
        "abstract": "Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications. This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use. Hence, we call for a shift from solely optimizing model performance to a broader, utility-driven perspective: evaluating agents through the lens of the overall agentic return on investment (Agent ROI). By identifying key factors that determine Agentic ROI--information quality, agent time, and cost--we posit a zigzag development trajectory in optimizing agentic ROI: first scaling up to improve the information quality, then scaling down to minimize the time and cost. We outline the roadmap across different development stages to bridge the current usability gaps, aiming to make LLM agents truly scalable, accessible, and effective in real-world contexts.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.17767"
    },
    "7c8ee6f1e6bd0dc01957d903cc691061": {
        "title": "ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework",
        "authors": [
            "Lisheng Huang",
            "Yichen Liu",
            "Jinhao Jiang",
            "Rongxiang Zhang",
            "Jiahao Yan",
            "Junyi Li",
            "Wayne Xin Zhao"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.18105",
        "abstract": "Recent advances in web-augmented large language models (LLMs) have exhibited strong performance in complex reasoning tasks, yet these capabilities are mostly locked in proprietary systems with opaque architectures. In this work, we propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework designed to democratize deep search for LLMs. ManuSearch decomposes the search and reasoning process into three collaborative agents: (1) a solution planning agent that iteratively formulates sub-queries, (2) an Internet search agent that retrieves relevant documents via real-time web search, and (3) a structured webpage reading agent that extracts key evidence from raw web content. To rigorously evaluate deep reasoning abilities, we introduce \\textbf{ORION}, a challenging benchmark focused on open-web reasoning over long-tail entities, covering both English and Chinese. Experimental results show that ManuSearch substantially outperforms prior open-source baselines and even surpasses leading closed-source systems. Our work paves the way for reproducible, extensible research in open deep search systems. We release the data and code in https://github.com/RUCAIBox/ManuSearch",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18105"
    },
    "ddd9042f07145a31f83aa95686b53434": {
        "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games",
        "authors": [
            "Shuhang Xu",
            "Fangwei Zhong"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.18218",
        "abstract": "Metaphors are a crucial way for humans to express complex or subtle ideas by comparing one concept to another, often from a different domain. However, many large language models (LLMs) struggle to interpret and apply metaphors in multi-agent language games, hindering their ability to engage in covert communication and semantic evasion, which are crucial for strategic communication. To address this challenge, we introduce CoMet, a framework that enables LLM-based agents to engage in metaphor processing. CoMet combines a hypothesis-based metaphor reasoner with a metaphor generator that improves through self-reflection and knowledge integration. This enhances the agents&#39; ability to interpret and apply metaphors, improving the strategic and nuanced quality of their interactions. We evaluate CoMet on two multi-agent language games - Undercover and Adversarial Taboo - which emphasize Covert Communication and Semantic Evasion. Experimental results demonstrate that CoMet significantly enhances the agents&#39; ability to communicate strategically using metaphors.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18218"
    },
    "c2d77dae79b3a258e1de636fb86def86": {
        "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation",
        "authors": [
            "Zhihao Jia",
            "Mingyi Jia",
            "Junwen Duan",
            "Jianxin Wang"
        ],
        "date": "2025/05/24",
        "pdf": "http://arxiv.org/pdf/2505.18630",
        "abstract": "Large Language Models (LLMs) demonstrate strong generalization and reasoning abilities, making them well-suited for complex decision-making tasks such as medical consultation (MC). However, existing LLM-based methods often fail to capture the dual nature of MC, which entails two distinct sub-tasks: symptom inquiry, a sequential decision-making process, and disease diagnosis, a classification problem. This mismatch often results in ineffective symptom inquiry and unreliable disease diagnosis. To address this, we propose \\textbf{DDO}, a novel LLM-based framework that performs \\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and independently optimizing the the two sub-tasks through a collaborative multi-agent workflow. Experiments on three real-world MC datasets show that DDO consistently outperforms existing LLM-based approaches and achieves competitive performance with state-of-the-art generation-based methods, demonstrating its effectiveness in the MC task.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18630"
    },
    "4d2e188198468ea9cdf6d904cd5946a0": {
        "title": "Multi-Party Conversational Agents: A Survey",
        "authors": [
            "Sagar Sapkota",
            "Mohammad Saqib Hasan",
            "Mubarak Shah",
            "Santu Karmaker"
        ],
        "date": "2025/05/24",
        "pdf": "http://arxiv.org/pdf/2505.18845",
        "abstract": "Multi-party Conversational Agents (MPCAs) are systems designed to engage in dialogue with more than two participants simultaneously. Unlike traditional two-party agents, designing MPCAs faces additional challenges due to the need to interpret both utterance semantics and social dynamics. This survey explores recent progress in MPCAs by addressing three key questions: 1) Can agents model each participants&#39; mental states? (State of Mind Modeling); 2) Can they properly understand the dialogue content? (Semantic Understanding); and 3) Can they reason about and predict future conversation flow? (Agent Action Modeling). We review methods ranging from classical machine learning to Large Language Models (LLMs) and multi-modal systems. Our analysis underscores Theory of Mind (ToM) as essential for building intelligent MPCAs and highlights multi-modal understanding as a promising yet underexplored direction. Finally, this survey offers guidance to future researchers on developing more capable MPCAs.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18845"
    },
    "3c187f9b70150fd1bbdc0e0bd3dc3b98": {
        "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions",
        "authors": [
            "Kung-Hsiang Huang",
            "Akshara Prabhakar",
            "Onkar Thorat",
            "Divyansh Agarwal",
            "Prafulla Kumar Choubey",
            "Yixin Mao",
            "Silvio Savarese",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "date": "2025/05/24",
        "pdf": "http://arxiv.org/pdf/2505.18878",
        "abstract": "While AI agents hold transformative potential in business, effective performance benchmarking is hindered by the scarcity of public, realistic business data on widely used platforms. Existing benchmarks often lack fidelity in their environments, data, and agent-user interactions, with limited coverage of diverse business scenarios and industries. To address these gaps, we introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena with nineteen expert-validated tasks across sales, service, and &#39;configure, price, and quote&#39; processes, for both Business-to-Business and Business-to-Customer scenarios. It distinctively incorporates multi-turn interactions guided by diverse personas and robust confidentiality awareness assessments. Experiments reveal leading LLM agents achieve only around 58% single-turn success on CRMArena-Pro, with performance dropping significantly to approximately 35% in multi-turn settings. While Workflow Execution proves more tractable for top agents (over 83% single-turn success), other evaluated business skills present greater challenges. Furthermore, agents exhibit near-zero inherent confidentiality awareness; though targeted prompting can improve this, it often compromises task performance. These findings highlight a substantial gap between current LLM capabilities and enterprise demands, underscoring the need for advancements in multi-turn reasoning, confidentiality adherence, and versatile skill acquisition.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18878"
    },
    "f19178389dcafc03e59fd824b3b4bbce": {
        "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems",
        "authors": [
            "Xuanming Zhang",
            "Yuxuan Chen",
            "Min-Hsuan Yeh",
            "Yixuan Li"
        ],
        "date": "2025/05/25",
        "pdf": "http://arxiv.org/pdf/2505.18943",
        "abstract": "Human social interactions depend on the ability to infer others&#39; unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework&#39;s ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18943"
    },
    "072ed79e4e81d48fadbcfc15a2d14c00": {
        "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas",
        "authors": [
            "Steffen Backmann",
            "David Guzman Piedrahita",
            "Emanuel Tewolde",
            "Rada Mihalcea",
            "Bernhard Schölkopf",
            "Zhijing Jin"
        ],
        "date": "2025/05/25",
        "pdf": "http://arxiv.org/pdf/2505.19212",
        "abstract": "Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs&#39; moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner&#39;s dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent&#39;s &#34;self-interest&#34; may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ],
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19212"
    },
    "370ede17ee91fbc9deadd37412f89721": {
        "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems",
        "authors": [
            "Yan Wen",
            "Junfeng Guo",
            "Heng Huang"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19405",
        "abstract": "As large language models (LLMs) evolve into autonomous agents capable of collaborative reasoning and task execution, multi-agent LLM systems have emerged as a powerful paradigm for solving complex problems. However, these systems pose new challenges for copyright protection, particularly when sensitive or copyrighted content is inadvertently recalled through inter-agent communication and reasoning. Existing protection techniques primarily focus on detecting content in final outputs, overlooking the richer, more revealing reasoning processes within the agents themselves. In this paper, we introduce CoTGuard, a novel framework for copyright protection that leverages trigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically, we can activate specific CoT segments and monitor intermediate reasoning steps for unauthorized content reproduction by embedding specific trigger queries into agent prompts. This approach enables fine-grained, interpretable detection of copyright violations in collaborative agent scenarios. We evaluate CoTGuard on various benchmarks in extensive experiments and show that it effectively uncovers content leakage with minimal interference to task performance. Our findings suggest that reasoning-level monitoring offers a promising direction for safeguarding intellectual property in LLM-based agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19405"
    },
    "bcafe2f9f7c0332943a5b90dcfdffcea": {
        "title": "Frictional Agent Alignment Framework: Slow Down and Don&#39;t Break Things",
        "authors": [
            "Abhijnan Nath",
            "Carine Graff",
            "Andrei Bachinin",
            "Nikhil Krishnaswamy"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19428",
        "abstract": "AI support of collaborative interactions entails mediating potential misalignment between interlocutor beliefs. Common preference alignment methods like DPO excel in static settings, but struggle in dynamic collaborative tasks where the explicit signals of interlocutor beliefs are sparse and skewed. We propose the Frictional Agent Alignment Framework (FAAF), to generate precise, context-aware &#34;friction&#34; that prompts for deliberation and re-examination of existing evidence. FAAF&#39;s two-player objective decouples from data skew: a frictive-state policy identifies belief misalignments, while an intervention policy crafts collaborator-preferred responses. We derive an analytical solution to this objective, enabling training a single policy via a simple supervised loss. Experiments on three benchmarks show FAAF outperforms competitors in producing concise, interpretable friction and in OOD generalization. By aligning LLMs to act as adaptive &#34;thought partners&#34; -- not passive responders -- FAAF advances scalable, dynamic human-AI collaboration. Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19428"
    },
    "a850f1c81b6da2d8c993a5b4be6730df": {
        "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents",
        "authors": [
            "Derong Xu",
            "Yi Wen",
            "Pengyue Jia",
            "Yingyi Zhang",
            "wenlin zhang",
            "Yichao Wang",
            "Huifeng Guo",
            "Ruiming Tang",
            "Xiangyu Zhao",
            "Enhong Chen",
            "Tong Xu"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19549",
        "abstract": "Large Language Models (LLMs) have recently been widely adopted in conversational agents. However, the increasingly long interactions between users and agents accumulate extensive dialogue records, making it difficult for LLMs with limited context windows to maintain a coherent long-term dialogue memory and deliver personalized responses. While retrieval-augmented memory systems have emerged to address this issue, existing methods often depend on single-granularity memory segmentation and retrieval. This approach falls short in capturing deep memory connections, leading to partial retrieval of useful information or substantial noise, resulting in suboptimal performance. To tackle these limits, we propose MemGAS, a framework that enhances memory consolidation by constructing multi-granularity association, adaptive selection, and retrieval. MemGAS is based on multi-granularity memory units and employs Gaussian Mixture Models to cluster and associate new memories with historical ones. An entropy-based router adaptively selects optimal granularity by evaluating query relevance distributions and balancing information completeness and noise. Retrieved memories are further refined via LLM-based filtering. Experiments on four long-term memory benchmarks demonstrate that MemGAS outperforms state-of-the-art methods on both question answer and retrieval tasks, achieving superior performance across different query types and top-K settings.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19549"
    },
    "8d9140b2ee24d24bfaa62927f1248a5f": {
        "title": "Multi-Agent Collaboration via Evolving Orchestration",
        "authors": [
            "Yufan Dang",
            "Chen Qian",
            "Xueheng Luo",
            "Jingru Fan",
            "Zihao Xie",
            "Ruijie Shi",
            "Weize Chen",
            "Cheng Yang",
            "Xiaoyin Che",
            "Ye Tian",
            "Xuantang Xiong",
            "Lei Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19591",
        "abstract": "Large language models (LLMs) have achieved remarkable results across diverse downstream tasks, but their monolithic nature restricts scalability and efficiency in complex problem-solving. While recent research explores multi-agent collaboration among LLMs, most approaches rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, resulting in coordination overhead and inefficiencies. To this end, we propose a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator (&#34;puppeteer&#34;) dynamically directs agents (&#34;puppets&#34;) in response to evolving task states. This orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents, enabling flexible and evolvable collective reasoning. Experiments on closed- and open-domain scenarios show that this method achieves superior performance with reduced computational costs. Analyses further reveal that the key improvements consistently stem from the emergence of more compact, cyclic reasoning structures under the orchestrator&#39;s evolution.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19591"
    },
    "584e3f2b0742412a71dfd32a4757ec3a": {
        "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue",
        "authors": [
            "Yichun Feng",
            "Jiawei Wang",
            "Lu Zhou",
            "Yixue Li"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19630",
        "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19630"
    },
    "0283695bd841b26c106310c377138cdd": {
        "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation",
        "authors": [
            "Xiaochuan Liu",
            "Ruihua Song",
            "Xiting Wang",
            "Xu Chen"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19647",
        "abstract": "Automatic related work generation (RWG) can save people&#39;s time and effort when writing a draft of related work section (RWS) for further revision. However, existing methods for RWG always suffer from shallow comprehension due to taking the limited portions of references papers as input and isolated explanation for each reference due to ineffective capturing the relationships among them. To address these issues, we focus on full-text-based RWG task and propose a novel multi-agent framework. Our framework consists of three agents: a selector that decides which section of the papers is going to read next, a reader that digests the selected section and updates a shared working memory, and a writer that generates RWS based on the final curated memory. To better capture the relationships among references, we also propose two graph-aware strategies for selector, enabling to optimize the reading order with constrains of the graph structure. Extensive experiments demonstrate that our framework consistently improves performance across three base models and various input configurations. The graph-aware selectors outperform alternative selectors, achieving state-of-the-art results. The code and data are available at https://github.com/1190200817/Full_Text_RWG.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19647"
    },
    "f9a7cdfba4d8b9416d9c7195862e071e": {
        "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search",
        "authors": [
            "Xing Cui",
            "Yueying Zou",
            "Zekun Li",
            "Peipei Li",
            "Xinyuan Xu",
            "Xuannan Liu",
            "Huaibo Huang",
            "Ran He"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19768",
        "abstract": "Real-world multimodal misinformation often arises from mixed forgery sources, requiring dynamic reasoning and adaptive verification. However, existing methods mainly rely on static pipelines and limited tool usage, limiting their ability to handle such complexity and diversity. To address this challenge, we propose T2Agent, a novel misinformation detection agent that incorporates an extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of modular tools such as web search, forgery detection, and consistency analysis. Each tool is described using standardized templates, enabling seamless integration and future expansion. To avoid inefficiency from using all tools simultaneously, a Bayesian optimization-based selector is proposed to identify a task-relevant subset. This subset then serves as the action space for MCTS to dynamically collect evidence and perform multi-source verification. To better align MCTS with the multi-source nature of misinformation detection, T2Agent extends traditional MCTS with multi-source verification, which decomposes the task into coordinated subtasks targeting different forgery sources. A dual reward mechanism containing a reasoning trajectory score and a confidence score is further proposed to encourage a balance between exploration across mixed forgery sources and exploitation for more reliable evidence. We conduct ablation studies to confirm the effectiveness of the tree search mechanism and tool usage. Extensive experiments further show that T2Agent consistently outperforms existing baselines on challenging mixed-source multimodal misinformation benchmarks, demonstrating its strong potential as a training-free approach for enhancing detection accuracy. The code will be released.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19768"
    },
    "d7d78b5b08e3ba568e6c46f3b474300f": {
        "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback",
        "authors": [
            "Minda Hu",
            "Tianqing Fang",
            "Jianshu Zhang",
            "Junyu Ma",
            "Zhisong Zhang",
            "Jingyan Zhou",
            "Hongming Zhang",
            "Haitao Mi",
            "Dong Yu",
            "Irwin King"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20013",
        "abstract": "Web agents powered by Large Language Models (LLMs) show promise for next-generation AI, but their limited reasoning in uncertain, dynamic web environments hinders robust deployment. In this paper, we identify key reasoning skills essential for effective web agents, i.e., reflection &amp; lookahead, branching, and rollback, and curate trajectory data that exemplifies these abilities by reconstructing the agent&#39;s (inference-time) reasoning algorithms into chain-of-thought rationales. We conduct experiments in the agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling salient reasoning patterns into the backbone LLM via simple fine-tuning can substantially enhance its performance. Our approach yields significant improvements across multiple benchmarks, including WebVoyager, Mind2web-live, and SimpleQA (web search), highlighting the potential of targeted reasoning skill enhancement for web agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20013"
    },
    "16e32bd9f6404dc76f239f7ba6aa03d6": {
        "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking",
        "authors": [
            "Yihan Chen",
            "Benfeng Xu",
            "Xiaorui Wang",
            "Yongdong Zhang",
            "Zhendong Mao"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20023",
        "abstract": "Autonomous agents, which perceive environments and take actions to achieve goals, have become increasingly feasible with the advancements in large language models (LLMs). However, current powerful agents often depend on sophisticated prompt engineering combined with closed-source LLMs like GPT-4. Although training open-source LLMs using expert trajectories from teacher models has yielded some improvements in agent capabilities, this approach still faces limitations such as performance plateauing and error propagation. To mitigate these challenges, we propose STeP, a novel method for improving LLM-based agent training. We synthesize self-reflected trajectories that include reflections and corrections of error steps, which enhance the effectiveness of LLM agents in learning from teacher models, enabling them to become agents capable of self-reflecting and correcting. We also introduce partial masking strategy that prevents the LLM from internalizing incorrect or suboptimal steps. Experiments demonstrate that our method improves agent performance across three representative tasks: ALFWorld, WebShop, and SciWorld. For the open-source model LLaMA2-7B-Chat, when trained using self-reflected trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it achieves comprehensive improvements with less training data compared to agents trained exclusively on expert trajectories.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20023"
    },
    "065c706517645562c52dee2998450a24": {
        "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning",
        "authors": [
            "Thang Nguyen",
            "Peter Chin",
            "Yu-Wing Tai"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20096",
        "abstract": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on either end-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates a collaborative set of specialized AI agents: Planner, Step Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline with task-aware reasoning. Ambiguities may arise from underspecified queries, sparse or indirect evidence in retrieved documents, or the need to integrate information scattered across multiple sources. MA-RAG mitigates these challenges by decomposing the problem into subtasks, such as query disambiguation, evidence extraction, and answer synthesis, and dispatching them to dedicated agents equipped with chain-of-thought prompting. These agents communicate intermediate reasoning and progressively refine the retrieval and synthesis process. Our design allows fine-grained control over information flow without any model fine-tuning. Crucially, agents are invoked on demand, enabling a dynamic and efficient workflow that avoids unnecessary computation. This modular and reasoning-driven architecture enables MA-RAG to deliver robust, interpretable results. Experiments on multi-hop and ambiguous QA benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free baselines and rivals fine-tuned systems, validating the effectiveness of collaborative agent-based reasoning in RAG.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20096"
    },
    "c0652759e901b67bb80da1708d13ca1e": {
        "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent",
        "authors": [
            "Dominik Meier",
            "Jan Philip Wahle",
            "Paul Röttger",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20118",
        "abstract": "As large language models (LLMs) become integrated into sensitive workflows, concerns grow over their potential to leak confidential information. We propose TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to embed sensitive context information into natural-looking outputs via linguistic steganography, without requiring explicit control over inference inputs. We introduce a taxonomy outlining risk factors for compromised LLMs, and use it to evaluate the risk profile of the threat. To implement TrojanStego, we propose a practical encoding scheme based on vocabulary partitioning learnable by LLMs via fine-tuning. Experimental results show that compromised models reliably transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over 97% accuracy using majority voting across three generations. Further, they maintain high utility, can evade human detection, and preserve coherence. These results highlight a new class of LLM data exfiltration attacks that are passive, covert, practical, and dangerous.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20118"
    },
    "b39d5890792f2d38d182e48e2560b09c": {
        "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction",
        "authors": [
            "Haonan Zhang",
            "Run Luo",
            "Xiong Liu",
            "Yuchuan Wu",
            "Ting-En Lin",
            "Pengpeng Zeng",
            "Qiang Qu",
            "Feiteng Fang",
            "Min Yang",
            "Lianli Gao",
            "Jingkuan Song",
            "Fei Huang",
            "Yongbin Li"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20277",
        "abstract": "Role-Playing Agents (RPAs), benefiting from large language models, is an emerging interactive AI system that simulates roles or characters with diverse personalities. However, existing methods primarily focus on mimicking dialogues among roles in textual form, neglecting the role&#39;s voice traits (e.g., voice style and emotions) as playing a crucial effect in interaction, which tends to be more immersive experiences in realistic scenarios. Towards this goal, we propose OmniCharacter, a first seamless speech-language personality interaction model to achieve immersive RPAs with low latency. Specifically, OmniCharacter enables agents to consistently exhibit role-specific personality traits and vocal traits throughout the interaction, enabling a mixture of speech and language responses. To align the model with speech-language scenarios, we construct a dataset named OmniCharacter-10K, which involves more distinctive characters (20), richly contextualized multi-round dialogue (10K), and dynamic speech response (135K). Experimental results showcase that our method yields better responses in terms of both content and style compared to existing RPAs and mainstream speech-language models, with a response latency as low as 289ms. Code and dataset are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20277"
    },
    "a1ba087b5394e63ee0dfedc406b756c8": {
        "title": "MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search Capability",
        "authors": [
            "Weiqi Wu",
            "Xin Guan",
            "Shen Huang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Jiuxin Cao",
            "Hai Zhao",
            "Jingren Zhou"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20285",
        "abstract": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm where models enhance generative capabilities using external knowledge retrieved via a specialized module. Recent advancements in Agent techniques enable Large Language Models (LLMs) to autonomously utilize tools for retrieval, planning, and reasoning. While existing training-based methods show promise, their agentic abilities are limited by inherent characteristics of the task-specific data used during training. To further enhance the universal search capability of agents, we propose a novel pre-training framework, MaskSearch. In the pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to leverage search tools to fill masked spans on a large number of pre-training data, thus acquiring universal retrieval and reasoning capabilities for LLMs. After that, the model is trained on downstream tasks to achieve further improvement. We apply both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training. For SFT, we combine agent-based and distillation-based methods to generate training data, starting with a multi-agent system consisting of a planner, rewriter, observer, and followed by a self-evolving teacher model. While for RL, we employ DAPO as the training framework and adopt a hybrid reward system consisting of answer rewards and format rewards. Additionally, we introduce a curriculum learning approach that allows the model to learn progressively from easier to more challenging instances based on the number of masked spans. We evaluate the effectiveness of our framework in the scenario of open-domain multi-hop question answering. Through extensive experiments, we demonstrate that MaskSearch significantly enhances the performance of LLM-based search agents on both in-domain and out-of-domain downstream tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ],
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20285"
    },
    "cb99cb71859318a3efc9aeab3bbbc08a": {
        "title": "Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration",
        "authors": [
            "Sibo Xiao",
            "Zixin Lin",
            "Wenyang Gao",
            "Yue Zhang"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.20625",
        "abstract": "Processing long contexts has become a critical capability for modern large language models (LLMs). Existing works leverage agent-based divide-and-conquer methods for processing long contexts. But these methods face crucial limitations, including prohibitive accumulated latency and amplified information loss from excessive agent invocations, and the disruption of inherent textual dependencies by immoderate partitioning. In this paper, we propose a novel multi-agent framework XpandA (Expand-Agent) coupled with question-driven workflow and dynamic partitioning for robust long-context processing. XpandA overcomes these limitations through: 1) dynamic partitioning of long texts, which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths; 2) question-guided protocol to update flat information ensembles within centralized shared memory, constructing consistent inter-agent knowledge across partitions; and 3) selectively replaying specific partitions based on the state-tracking of question-information couples to promote the resolution of inverted-order structures across partitions (e.g., flashbacks). We perform a comprehensive evaluation of XpandA on multiple long-context benchmarks with length varying from 1k to 1M, demonstrating XpandA&#39;s feasibility for processing ultra-long sequences and its significant effectiveness in enhancing the long-context capabilities of various LLMs by achieving 20\\% improvements and 1.5x inference speedup over baselines of full-context, RAG and previous agent-based methods.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20625"
    },
    "ba4fa483c7219ecf7dcebc400857c9fa": {
        "title": "BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism",
        "authors": [
            "Qinzhuo Wu",
            "Pengzhi Gao",
            "Wei Liu",
            "Jian Luan"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.20660",
        "abstract": "Graphical User Interface (GUI) agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within GUI environments. However, existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors. To address these shortcomings, we propose the BacktrackAgent, a robust framework that incorporates a backtracking mechanism to improve task completion efficiency. BacktrackAgent includes verifier, judger, and reflector components as modules for error detection and recovery, while also applying judgment rewards to further enhance the agent&#39;s performance. Additionally, we develop a training dataset specifically designed for the backtracking mechanism, which considers the outcome pages after action executions. Experimental results show that BacktrackAgent has achieved performance improvements in both task success rate and step accuracy on Mobile3M and Auto-UI benchmarks. Our data and code will be released upon acceptance.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20660"
    },
    "ee95a8728174727d0e9bd022efee21df": {
        "title": "Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective",
        "authors": [
            "Krishna Singh Rajput",
            "Tejas Anvekar",
            "Chitta Baral",
            "Vivek Gupta"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.20816",
        "abstract": "Recent advances in multimodal question answering have primarily focused on combining heterogeneous modalities or fine-tuning multimodal large language models. While these approaches have shown strong performance, they often rely on a single, generalized reasoning strategy, overlooking the unique characteristics of each modality ultimately limiting both accuracy and interpretability. To address these limitations, we propose MAMMQA, a multi-agent QA framework for multimodal inputs spanning text, tables, and images. Our system includes two Visual Language Model (VLM) agents and one text-based Large Language Model (LLM) agent. The first VLM decomposes the user query into sub-questions and sequentially retrieves partial answers from each modality. The second VLM synthesizes and refines these results through cross-modal reasoning. Finally, the LLM integrates the insights into a cohesive answer. This modular design enhances interpretability by making the reasoning process transparent and allows each agent to operate within its domain of expertise. Experiments on diverse multimodal QA benchmarks demonstrate that our cooperative, multi-agent framework consistently outperforms existing baselines in both accuracy and robustness.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20816"
    },
    "933104f9766306fd827ca49358fd57f6": {
        "title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs",
        "authors": [
            "Xuanwen Ding",
            "Chengjun Pan",
            "Zejun Li",
            "Jiwen Zhang",
            "Siyuan Wang",
            "Zhongyu Wei"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21389",
        "abstract": "Evaluating multimodal large language models (MLLMs) is increasingly expensive, as the growing size and cross-modality complexity of benchmarks demand significant scoring efforts. To tackle with this difficulty, we introduce AutoJudger, an agent-driven framework for efficient and adaptive benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the Item Response Theory (IRT) to estimate the question difficulty and an autonomous evaluation agent to dynamically select the most informative test questions based on the model&#39;s real-time performance. Specifically, AutoJudger incorporates two pivotal components: a semantic-aware retrieval mechanism to ensure that selected questions cover diverse and challenging scenarios across both vision and language modalities, and a dynamic memory that maintains contextual statistics of previously evaluated questions to guide coherent and globally informed question selection throughout the evaluation process. Extensive experiments on four representative multimodal benchmarks demonstrate that our adaptive framework dramatically reduces evaluation expenses, i.e. AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with the full benchmark evaluation on MMT-Bench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21389"
    },
    "f863da7d4031076a008eb8df7e43188a": {
        "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration",
        "authors": [
            "Zijun Liu",
            "Zhennan Wan",
            "Peng Li",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21471",
        "abstract": "With the rapid advancement of post-training techniques for reasoning and information seeking, large language models (LLMs) can incorporate a large quantity of retrieved knowledge to solve complex tasks. However, the limited context window of LLMs obstructs scaling the amount of external knowledge input, prohibiting further improvement, especially for tasks requiring significant amount of external knowledge. Existing context window extension methods inevitably cause information loss. LLM-based multi-agent methods emerge as a new paradigm to handle massive input in a distributional manner, where we identify two core bottlenecks in existing knowledge synchronization and reasoning processes. In this work, we develop a multi-agent framework, $\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability in inference-time knowledge integration without longer-context training. Benchmarked with our enhanced multi-hop question answering test, $\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including long survey generation, ExtAgents significantly enhances the performance over existing non-training methods with the same amount of external knowledge input, regardless of whether it falls $\\textit{within or exceeds the context window}$. Moreover, the method maintains high efficiency due to high parallelism. Further study in the coordination of LLM agents on increasing external knowledge input could benefit real-world applications.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21471"
    },
    "453d0cac6f6e611da7217ae2bfb88a8b": {
        "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents",
        "authors": [
            "Han Xiao",
            "Guozhi Wang",
            "Yuxiang Chai",
            "Zimu Lu",
            "Weifeng Lin",
            "Hao He",
            "Lue Fan",
            "Liuyang Bian",
            "Rui Hu",
            "Liang Liu",
            "Shuai Ren",
            "Yafei Wen",
            "Xiaoxin Chen",
            "Aojun Zhou",
            "Hongsheng Li"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21496",
        "abstract": "In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21496"
    },
    "8f53ca58b8d7c7b156c9fad5eeb395c6": {
        "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making",
        "authors": [
            "Yihan Wang",
            "Qiao Yan",
            "Zhenghao Xing",
            "Lihao Liu",
            "Junjun He",
            "Chi-Wing Fu",
            "Xiaowei Hu",
            "Pheng-Ann Heng"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21503",
        "abstract": "Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect&#39;&#39; in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&amp;A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21503"
    },
    "3749eb2d76609148ac7128a32ea88ed0": {
        "title": "BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum",
        "authors": [
            "Yubin Kim",
            "Zhiyuan Hu",
            "Hyewon Jeong",
            "Eugene Park",
            "Shuyue Stella Li",
            "Chanwoo Park",
            "Shiyun Xiong",
            "MingYu Lu",
            "Hyeonhoon Lee",
            "Xin Liu",
            "Daniel McDuff",
            "Cynthia Breazeal",
            "Samir Tulebaev",
            "Hae Won Park"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21757",
        "abstract": "Large Language Models (LLMs) as clinical agents require careful behavioral adaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs often struggle with proactive engagement, like unprompted identification of critical missing information or risks. We introduce BehaviorBench, a comprehensive dataset to evaluate agent behaviors across a clinical assistance spectrum, ranging from reactive query responses to proactive interventions (e.g., clarifying ambiguities, flagging overlooked critical data). Our BehaviorBench experiments reveal LLMs&#39; inconsistent proactivity. To address this, we propose BehaviorSFT, a novel training strategy using behavioral tokens to explicitly condition LLMs for dynamic behavioral selection along this spectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro F1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to 96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a superior balance between helpful proactivity (e.g., timely, relevant suggestions) and necessary restraint (e.g., avoiding over-intervention) versus standard fine-tuning or explicit instructed agents.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21757"
    },
    "25f5b0f4676ed13f86d76bc241a22064": {
        "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development",
        "authors": [
            "Rennai Qiu",
            "Chen Qian",
            "Ran Li",
            "Yufan Dang",
            "Weize Chen",
            "Cheng Yang",
            "Yingli Zhang",
            "Ye Tian",
            "Xuantang Xiong",
            "Lei Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.21898",
        "abstract": "Recent advancements in Large Language Models (LLMs) and autonomous agents have demonstrated remarkable capabilities across various domains. However, standalone agents frequently encounter limitations when handling complex tasks that demand extensive interactions and substantial computational resources. Although Multi-Agent Systems (MAS) alleviate some of these limitations through collaborative mechanisms like task decomposition, iterative communication, and role specialization, they typically remain resource-unaware, incurring significant inefficiencies due to high token consumption and excessive execution time. To address these limitations, we propose a resource-aware multi-agent system -- Co-Saving (meaning that multiple agents collaboratively engage in resource-saving activities), which leverages experiential knowledge to enhance operational efficiency and solution quality. Our key innovation is the introduction of &#34;shortcuts&#34; -- instructional transitions learned from historically successful trajectories -- which allows to bypass redundant reasoning agents and expedite the collective problem-solving process. Experiments for software development tasks demonstrate significant advantages over existing methods. Specifically, compared to the state-of-the-art MAS ChatDev, our method achieves an average reduction of 50.85% in token usage, and improves the overall code quality by 10.06%.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21898"
    },
    "f0e90f8c1066d0451bd20dfcc2e767a6": {
        "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments",
        "authors": [
            "Zeyi Liao",
            "Jaylen Jones",
            "Linxi Jiang",
            "Eric Fosler-Lussier",
            "Yu Su",
            "Zhiqiang Lin",
            "Huan Sun"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.21936",
        "abstract": "Computer-use agents (CUAs) promise to automate complex tasks across operating systems (OS) and the web, but remain vulnerable to indirect prompt injection. Current evaluations of this threat either lack support realistic but controlled environments or ignore hybrid web-OS attack scenarios involving both interfaces. To address this, we propose RedTeamCUA, an adversarial testing framework featuring a novel hybrid sandbox that integrates a VM-based OS environment with Docker-based web platforms. Our sandbox supports key features tailored for red teaming, such as flexible adversarial scenario configuration, and a setting that decouples adversarial evaluation from navigational limitations of CUAs by initializing tests directly at the point of an adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive benchmark with 864 examples that investigate realistic, hybrid web-OS attack scenarios and fundamental security vulnerabilities. Benchmarking current frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated, still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute adversarial tasks with an Attempt Rate as high as 92.5%, although failing to complete them due to capability limitations. Nevertheless, we observe concerning ASRs of up to 50% in realistic end-to-end settings, with the recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%, demonstrating that indirect prompt injection presents tangible risks for even advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA provides an essential framework for advancing realistic, controlled, and systematic analysis of CUA vulnerabilities, highlighting the urgent need for robust defenses to indirect prompt injection prior to real-world deployment.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21936"
    },
    "cfd8ac7e100f33671cea009540a0b899": {
        "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents",
        "authors": [
            "Taro Yano",
            "Yoichi Ishibashi",
            "Masafumi Oyamada"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.21963",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21963"
    },
    "8c5fbaafe4ddd0f811ee7e225a6b918e": {
        "title": "EvolveSearch: An Iterative Self-Evolving Search Agent",
        "authors": [
            "Dingchu Zhang",
            "Yida Zhao",
            "Jialong Wu",
            "Baixuan Li",
            "Wenbiao Yin",
            "Liwen Zhang",
            "Yong Jiang",
            "Yufeng Li",
            "Kewei Tu",
            "Pengjun Xie",
            "Fei Huang"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.22501",
        "abstract": "The rapid advancement of large language models (LLMs) has transformed the landscape of agentic information seeking capabilities through the integration of tools such as search engines and web browsers. However, current mainstream approaches for enabling LLM web search proficiency face significant challenges: supervised fine-tuning struggles with data production in open-search domains, while RL converges quickly, limiting their data utilization efficiency. To address these issues, we propose EvolveSearch, a novel iterative self-evolution framework that combines SFT and RL to enhance agentic web search capabilities without any external human-annotated reasoning data. Extensive experiments on seven multi-hop question-answering (MHQA) benchmarks demonstrate that EvolveSearch consistently improves performance across iterations, ultimately achieving an average improvement of 4.7\\% over the current state-of-the-art across seven benchmarks, opening the door to self-evolution agentic capabilities in open web search domains.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.22501"
    },
    "7459fa691fb1735e1a70c02a9b19b992": {
        "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems",
        "authors": [
            "Hoang Pham",
            "Thuy-Duong Nguyen",
            "Khac-Hoai Nam Bui"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.22571",
        "abstract": "This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.22571"
    },
    "fd7cb821c2aa426bd7466de54a21bca2": {
        "title": "First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons &amp; Dragons Gameplay",
        "authors": [
            "Andrew Zhu",
            "Evan Osgood",
            "Chris Callison-Burch"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.22809",
        "abstract": "Much work has been done on conversational LLM agents which directly assist human users with tasks. We present an alternative paradigm for interacting with LLM agents, which we call &#34;overhearing agents&#34;. These overhearing agents do not actively participate in conversation -- instead, they &#34;listen in&#34; on human-to-human conversations and perform background tasks or provide suggestions to assist the user. In this work, we explore the overhearing agents paradigm through the lens of Dungeons &amp; Dragons gameplay. We present an in-depth study using large multimodal audio-language models as overhearing agents to assist a Dungeon Master. We perform a human evaluation to examine the helpfulness of such agents and find that some large audio-language models have the emergent ability to perform overhearing agent tasks using implicit audio cues. Finally, we release Python libraries and our project code to support further research into the overhearing agents paradigm at https://github.com/zhudotexe/overhearing_agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.22809"
    },
    "90ae5453ad456a6532d05d2517e66c6c": {
        "title": "WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning",
        "authors": [
            "Yuchen Zhuang",
            "Di Jin",
            "Jiaao Chen",
            "Wenqi Shi",
            "Hanrui Wang",
            "Chao Zhang"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.22942",
        "abstract": "Large language models (LLMs)-empowered web agents enables automating complex, real-time web navigation tasks in enterprise environments. However, existing web agents relying on supervised fine-tuning (SFT) often struggle with generalization and robustness due to insufficient reasoning capabilities when handling the inherently dynamic nature of web interactions. In this study, we introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based R1-style reinforcement learning framework designed explicitly to enhance single-step reasoning and planning for business-oriented web navigation tasks. We employ a structured reward function that evaluates both adherence to output formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly learn robust intermediate reasoning without explicit annotations or extensive expert demonstrations. Extensive experiments on the WorkArena benchmark demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by 10.26-16.59%, achieving competitive performance relative to proprietary LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.22942"
    },
    "c57db5281270295367c192849b85fd90": {
        "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs",
        "authors": [
            "Chiwan Park",
            "Wonjun Jang",
            "Daeryong Kim",
            "Aelim Ahn",
            "Kichang Yang",
            "Woosung Hwang",
            "Jihyeon Roh",
            "Hyerin Park",
            "Hyosun Wang",
            "Min Seok Kim",
            "Jihoon Kang"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23006",
        "abstract": "The advancement of Large Language Models (LLMs) has led to significant improvements in various service domains, including search, recommendation, and chatbot applications. However, applying state-of-the-art (SOTA) research to industrial settings presents challenges, as it requires maintaining flexible conversational abilities while also strictly complying with service-specific constraints. This can be seen as two conflicting requirements due to the probabilistic nature of LLMs. In this paper, we propose our approach to addressing this challenge and detail the strategies we employed to overcome their inherent limitations in real-world applications. We conduct a practical case study of a conversational agent designed for the e-commerce domain, detailing our implementation workflow and optimizations. Our findings provide insights into bridging the gap between academic research and real-world application, introducing a framework for developing scalable, controllable, and reliable AI-driven agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23006"
    },
    "d2a6191afd41eb4e7ba779addebe0e55": {
        "title": "Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration",
        "authors": [
            "Yilong Li",
            "Chen Qian",
            "Yu Xia",
            "Ruijie Shi",
            "Yufan Dang",
            "Zihao Xie",
            "Ziming You",
            "Weize Chen",
            "Cheng Yang",
            "Weichuan Liu",
            "Ye Tian",
            "Xuantang Xiong",
            "Lei Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23187",
        "abstract": "Large Language Model-based multi-agent systems (MAS) have shown remarkable progress in solving complex tasks through collaborative reasoning and inter-agent critique. However, existing approaches typically treat each task in isolation, resulting in redundant computations and limited generalization across structurally similar tasks. To address this, we introduce multi-agent cross-task experiential learning (MAEL), a novel framework that endows LLM-driven agents with explicit cross-task learning and experience accumulation. We model the task-solving workflow on a graph-structured multi-agent collaboration network, where agents propagate information and coordinate via explicit connectivity. During the experiential learning phase, we quantify the quality for each step in the task-solving workflow and store the resulting rewards along with the corresponding inputs and outputs into each agent&#39;s individual experience pool. During inference, agents retrieve high-reward, task-relevant experiences as few-shot examples to enhance the effectiveness of each reasoning step, thereby enabling more accurate and efficient multi-agent collaboration. Experimental results on diverse datasets demonstrate that MAEL empowers agents to learn from prior task experiences effectively-achieving faster convergence and producing higher-quality solutions on current tasks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23187"
    },
    "0fa794cd4edbd54b54874fa5d2530201": {
        "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering",
        "authors": [
            "Zexi Liu",
            "Jingyi Chai",
            "Xinyu Zhu",
            "Shuo Tang",
            "Rui Ye",
            "Bo Zhang",
            "Lei Bai",
            "Siheng Chen"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23723",
        "abstract": "The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23723"
    },
    "29a44e32ab7a3fadb78bb6ecdbd25b0d": {
        "title": "CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language",
        "authors": [
            "Lin Zhong",
            "Lingzhi Wang",
            "Xu Yang",
            "Qing Liao"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.23837",
        "abstract": "Large Language Models (LLMs) offer new opportunities for the next Point-Of-Interest (POI) prediction task, leveraging their capabilities in semantic understanding of POI trajectories. However, previous LLM-based methods, which are superficially adapted to next POI prediction, largely overlook critical challenges associated with applying LLMs to this task. Specifically, LLMs encounter two critical challenges: (1) a lack of intrinsic understanding of numeric spatiotemporal data, which hinders accurate modeling of users&#39; spatiotemporal distributions and preferences; and (2) an excessively large and unconstrained candidate POI space, which often results in random or irrelevant predictions. To address these issues, we propose a Collaborative Multi Agent Framework for Next POI Prediction, named CoMaPOI. Through the close interaction of three specialized agents (Profiler, Forecaster, and Predictor), CoMaPOI collaboratively addresses the two critical challenges. The Profiler agent is responsible for converting numeric data into language descriptions, enhancing semantic understanding. The Forecaster agent focuses on dynamically constraining and refining the candidate POI space. The Predictor agent integrates this information to generate high-precision predictions. Extensive experiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that CoMaPOI achieves state of the art performance, improving all metrics by 5% to 10% compared to SOTA baselines. This work pioneers the investigation of challenges associated with applying LLMs to complex spatiotemporal tasks by leveraging tailored collaborative agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23837"
    },
    "e16dc838eca9cc7fcf710e616c3a2051": {
        "title": "Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations",
        "authors": [
            "Atanu Barai",
            "Stephan Eidenbenz",
            "Nandakishore Santhi"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.23846",
        "abstract": "To fully leverage the potential of artificial intelligence (AI) systems in a trustworthy manner, it is desirable to couple multiple AI and non-AI systems together seamlessly for constraining and ensuring correctness of the output. This paper introduces a novel parallel discrete event simulation (PDES) based methodology to combine multiple AI and non-AI agents in a causal, rule-based way. Our approach tightly integrates the concept of passage of time, with each agent considered as an entity in the PDES framework and responding to prior requests from other agents. Such coupling mechanism enables the agents to work in a co-operative environment towards a common goal while many tasks run in parallel throughout the simulation. It further enables setting up boundaries to the outputs of the AI agents by applying necessary dynamic constraints using non-AI agents while allowing for scalability through deployment of hundreds of such agents in a larger compute cluster. Distributing smaller AI agents can enable extremely scalable simulations in the future, addressing local memory bottlenecks for model parameter storage. Within a PDES involving both AI and non-AI agents, we break down the problem at hand into structured steps, when necessary, providing a set of multiple choices to the AI agents, and then progressively solve these steps towards a final goal. At each step, the non-AI agents act as unbiased auditors, verifying each action by the AI agents so that certain rules of engagement are followed. We evaluate our approach by solving four problems from four different domains and comparing the results with those from AI models alone. Our results show greater accuracy in solving problems from various domains where the AI models struggle to solve the problems solely by themselves. Results show that overall accuracy of our approach is 68% where as the accuracy of vanilla models is less than 23%.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23846"
    },
    "c40dab37f2fa869d5eda2264f5663953": {
        "title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer&#39;s Disease",
        "authors": [
            "Nic Dobbins",
            "Christelle Xiong",
            "Kristine Lan",
            "Meliha Yetisgen"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23852",
        "abstract": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as autonomous agents to reproduce findings of published research studies using the same or similar dataset. Materials and Methods: We used the &#34;Quick Access&#34; dataset of the National Alzheimer&#39;s Coordinating Center (NACC). We identified highly cited published research manuscripts using NACC data and selected five studies that appeared reproducible using this dataset alone. Using GPT-4o, we created a simulated research team of LLM-based autonomous agents tasked with writing and executing code to dynamically reproduce the findings of each study, given only study Abstracts, Methods sections, and data dictionary descriptions of the dataset. Results: We extracted 35 key findings described in the Abstracts across 5 Alzheimer&#39;s studies. On average, LLM agents approximately reproduced 53.2% of findings per study. Numeric values and range-based findings often differed between studies and agents. The agents also applied statistical methods or parameters that varied from the originals, though overall trends and significance were sometimes similar. Discussion: In some cases, LLM-based agents replicated research techniques and findings. In others, they failed due to implementation flaws or missing methodological detail. These discrepancies show the current limits of LLMs in fully automating reproducibility assessments. Still, this early investigation highlights the potential of structured agent-based systems to provide scalable evaluation of scientific rigor. Conclusion: This exploratory work illustrates both the promise and limitations of LLMs as autonomous agents for automating reproducibility in biomedical research.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23852"
    },
    "5c0a6204eea4d867a557f53ea8811c1b": {
        "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents",
        "authors": [
            "Feiteng Fang",
            "Ting-En Lin",
            "Yuchuan Wu",
            "Xiong Liu",
            "Xiang Huang",
            "Dingwei Chen",
            "Jing Ye",
            "Haonan Zhang",
            "Liang Zhu",
            "Hamid Alinejad-Rokny",
            "Min Yang",
            "Fei Huang",
            "Yongbin Li"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23923",
        "abstract": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic and engaging human-computer interactions. However, traditional reward models often struggle with scalability and adapting to subjective conversational preferences. We propose ChARM, a Character-based Act-adaptive Reward Model, addressing these challenges through two innovations: (1) an act-adaptive margin that significantly enhances learning efficiency and generalizability, and (2) a self-evolution mechanism leveraging large-scale unlabeled data to improve training coverage. Additionally, we introduce RoleplayPref, the first large-scale preference dataset specifically for RPLAs, featuring 1,108 characters, 13 subcategories, and 16,888 bilingual dialogues, alongside RoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13% improvement over the conventional Bradley-Terry model in preference rankings. Furthermore, applying ChARM-generated rewards to preference learning techniques (e.g., direct preference optimization) achieves state-of-the-art results on CharacterEval and RoleplayEval. Code and dataset are available at https://github.com/calubkk/ChARM.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23923"
    },
    "fb0ccbe38af72a0d3d1701ed8363439c": {
        "title": "Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents",
        "authors": [
            "Fanhang Man",
            "Huandong Wang",
            "Jianjie Fang",
            "Zhaoyi Deng",
            "Baining Zhao",
            "Xinlei Chen",
            "Yong Li"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24331",
        "abstract": "User sentiment on social media reveals the underlying social trends, crises, and needs. Researchers have analyzed users&#39; past messages to trace the evolution of sentiments and reconstruct sentiment dynamics. However, predicting the imminent sentiment of an ongoing event is rarely studied. In this paper, we address the problem of \\textbf{sentiment forecasting} on social media to predict the user&#39;s future sentiment in response to the development of the event. We extract sentiment-related features to enhance the modeling skill and propose a multi-perspective role-playing framework to simulate the process of human response. Our preliminary results show significant improvement in sentiment forecasting on both microscopic and macroscopic levels.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24331"
    },
    "0491bbe4820aca1d0f2c28192eb61ee0": {
        "title": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research",
        "authors": [
            "Qianqian Zhang",
            "Jiajia Liao",
            "Heting Ying",
            "Yibo Ma",
            "Haozhan Shen",
            "Jingcheng Li",
            "Peng Liu",
            "Lu Zhang",
            "Chunxin Fang",
            "Kyusong Lee",
            "Ruochen Xu",
            "Tiancheng Zhao"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24354",
        "abstract": "Language agents powered by large language models (LLMs) have demonstrated remarkable capabilities in understanding, reasoning, and executing complex tasks. However, developing robust agents presents significant challenges: substantial engineering overhead, lack of standardized components, and insufficient evaluation frameworks for fair comparison. We introduce Agent Graph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and extensible framework that addresses these challenges through three key contributions: (1) a modular architecture with a graph-based workflow engine, efficient memory management, and clean component abstraction; (2) a comprehensive suite of reusable agent algorithms implementing state-of-the-art reasoning approaches; and (3) a rigorous evaluation framework enabling systematic comparison across multiple dimensions. Through extensive experiments on mathematical reasoning and multimodal tasks, we evaluate various agent algorithms across different LLMs, revealing important insights about their relative strengths and applicability. Our results demonstrate that while sophisticated reasoning approaches can enhance agent capabilities, simpler methods like Chain-of-Thought often exhibit robust performance with significantly lower computational overhead. AGORA not only simplifies language agent development but also establishes a foundation for reproducible agent research through standardized evaluation protocols.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24354"
    },
    "052e0936ab7eb906c6bf725294e80dbc": {
        "title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction",
        "authors": [
            "Ye Eun Chun",
            "Taeyoon Hwang",
            "Seung-won Hwang",
            "Byung-Hak Kim"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24553",
        "abstract": "Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24553"
    },
    "1eccb0bf39f1afd45dc60ee8d1b53951": {
        "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization",
        "authors": [
            "Hyuntak Kim",
            "Byung-Hak Kim"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24575",
        "abstract": "Summarizing long-form narratives--such as books, movies, and TV scripts--requires capturing intricate plotlines, character interactions, and thematic coherence, a task that remains challenging for existing LLMs. We introduce NexusSum, a multi-agent LLM framework for narrative summarization that processes long-form text through a structured, sequential pipeline--without requiring fine-tuning. Our approach introduces two key innovations: (1) Dialogue-to-Description Transformation: A narrative-specific preprocessing method that standardizes character dialogue and descriptive text into a unified format, improving coherence. (2) Hierarchical Multi-LLM Summarization: A structured summarization pipeline that optimizes chunk processing and controls output length for accurate, high-quality summaries. Our method establishes a new state-of-the-art in narrative summarization, achieving up to a 30.0% improvement in BERTScore (F1) across books, movies, and TV scripts. These results demonstrate the effectiveness of multi-agent LLMs in handling long-form content, offering a scalable approach for structured summarization in diverse storytelling domains.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24575"
    },
    "3ae79e6eb63813b59e4e0c427c785b6e": {
        "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment",
        "authors": [
            "Dayeon Ki",
            "Rachel Rudinger",
            "Tianyi Zhou",
            "Marine Carpuat"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24671",
        "abstract": "Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24671"
    },
    "c62742b469bf411ea1eaeb973c5a3254": {
        "title": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",
        "authors": [
            "Wenhan Dong",
            "Yuemeng Zhao",
            "Zhen Sun",
            "Yule Liu",
            "Zifan Peng",
            "Jingyi Zheng",
            "Zongmin Zhang",
            "Ziyi Zhang",
            "Jun Wu",
            "Ruiming Wang",
            "Shengmin Xu",
            "Xinyi Huang",
            "Xinlei He"
        ],
        "date": "2025/04/30",
        "pdf": "http://arxiv.org/pdf/2505.00049",
        "abstract": "As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, several important areas have not been systematically discussed, including detailed discussions of diverse psychological tests, LLM-specific psychological datasets, and the applications of LLMs with psychological traits. To address this gap, we systematically review six key dimensions of applying psychological theories to LLMs: (1) assessment tools; (2) LLM-specific datasets; (3) evaluation metrics (consistency and stability); (4) empirical findings; (5) personality simulation methods; and (6) LLM-based behavior simulation. Our analysis highlights both the strengths and limitations of current methods. While some LLMs exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Recognizing methodological challenges such as mismatches between psychological tools and LLMs&#39; capabilities, as well as inconsistencies in evaluation practices, this study aims to propose future directions for developing more interpretable, robust, and generalizable psychological assessment frameworks for LLMs.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.00049"
    },
    "5da8b8eec47dbaf1146dfb8add0fcf63": {
        "title": "Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems",
        "authors": [
            "Shaokun Zhang",
            "Ming Yin",
            "Jieyu Zhang",
            "Jiale Liu",
            "Zhiguang Han",
            "Jingyang Zhang",
            "Beibin Li",
            "Chi Wang",
            "Huazheng Wang",
            "Yiran Chen",
            "Qingyun Wu"
        ],
        "date": "2025/04/30",
        "pdf": "http://arxiv.org/pdf/2505.00212",
        "abstract": "Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task&#39;s complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.00212"
    },
    "633c963f8e54e5eb0b3e4eb967690f04": {
        "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks",
        "authors": [
            "Vishnu Sarukkai",
            "Zhiqiang Xie",
            "Kayvon Fatahalian"
        ],
        "date": "2025/05/01",
        "pdf": "http://arxiv.org/pdf/2505.00234",
        "abstract": "Improving Large Language Model (LLM) agents for sequential decision-making tasks typically requires extensive task-specific knowledge engineering--custom prompts, curated examples, and specialized observation/action spaces. We investigate a different approach where agents automatically improve by learning from their own successful experiences without human intervention. Our method constructs and refines a database of self-generated trajectories that serve as in-context examples for future tasks. Even naive accumulation of successful trajectories yields substantial performance gains across three diverse benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%). These improvements exceed those achieved by upgrading from gpt-4o-mini to gpt-4o and match the performance of allowing multiple attempts per task. We further enhance this approach with two innovations: database-level curation using population-based training to propagate high-performing example collections, and exemplar-level curation that selectively retains trajectories based on their empirical utility as in-context examples. With these enhancements, our method achieves 93% success on ALFWorld--surpassing approaches that use more powerful LLMs and hand-crafted components. Our trajectory bootstrapping technique demonstrates that agents can autonomously improve through experience, offering a scalable alternative to labor-intensive knowledge engineering.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.00234"
    },
    "d80c22d1df497f0f80b030f178c34d3b": {
        "title": "Interpretable Emergent Language Using Inter-Agent Transformers",
        "authors": [
            "Mannan Bhardwaj"
        ],
        "date": "2025/05/04",
        "pdf": "http://arxiv.org/pdf/2505.02215",
        "abstract": "This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.02215"
    },
    "8de295f286b864cd4f2aad7e3be891dc": {
        "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback",
        "authors": [
            "Hao Zhu",
            "Phil Cuvin",
            "Xinkai Yu",
            "Charlotte Ka Yee Yan",
            "Jason Zhang",
            "Diyi Yang"
        ],
        "date": "2025/05/05",
        "pdf": "http://arxiv.org/pdf/2505.02820",
        "abstract": "Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., &#34;If you find that the button is disabled, don&#39;t click it again&#34;, or &#34;This agent has too much autonomy to decide what to do on its own&#34;, into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent&#39;s behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: &#34;coverage&#34; and &#34;redundancy&#34;. Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra&#39;s ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.02820"
    },
    "27baec348afa0a9fa10b1d41a97460d8": {
        "title": "The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete",
        "authors": [
            "Gerrit Großmann",
            "Larisa Ivanova",
            "Sai Leela Poduru",
            "Mohaddeseh Tabrizian",
            "Islam Mesabah",
            "David A. Selby",
            "Sebastian J. Vollmer"
        ],
        "date": "2025/05/06",
        "pdf": "http://arxiv.org/pdf/2505.03961",
        "abstract": "According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM agents toward collaboration. We use a finitely repeated public goods game in which LLM agents choose either cooperative or egoistic spending strategies. We prime agents with stories highlighting teamwork to different degrees and test how this influences negotiation outcomes. Our experiments explore four questions:(1) How do narratives influence negotiation behavior? (2) What differs when agents share the same story versus different ones? (3) What happens when the agent numbers grow? (4) Are agents resilient against self-serving negotiators? We find that story-based priming significantly affects negotiation strategies and success rates. Common stories improve collaboration, benefiting each agent. By contrast, priming agents with different stories reverses this effect, and those agents primed toward self-interest prevail. We hypothesize that these results carry implications for multi-agent system design and AI alignment.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.03961"
    },
    "42ad252cdb139ea6d0e19d2ce8608688": {
        "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
        "authors": [
            "Darshan Deshpande",
            "Varun Gangal",
            "Hersh Mehta",
            "Jitin Krishnan",
            "Anand Kannappan",
            "Rebecca Qian"
        ],
        "date": "2025/05/13",
        "pdf": "http://arxiv.org/pdf/2505.08638",
        "abstract": "The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.08638"
    },
    "4c57eb4f2c8cb281078a1aafdf727718": {
        "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries",
        "authors": [
            "Zekun Wu",
            "Seonglae Cho",
            "Umar Mohammed",
            "Cristian Munoz",
            "Kleyton Costa",
            "Xin Guan",
            "Theo King",
            "Ze Wang",
            "Emre Kazim",
            "Adriano Koshiyama"
        ],
        "date": "2025/05/13",
        "pdf": "http://arxiv.org/pdf/2505.08842",
        "abstract": "Open-source AI libraries are foundational to modern AI systems but pose significant, underexamined risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance. We present LibVulnWatch, a graph-based agentic assessment framework that performs deep, source-grounded evaluations of these libraries. Built on LangGraph, the system coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases. LibVulnWatch generates reproducible, governance-aligned scores across five critical domains, publishing them to a public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely used libraries, including ML frameworks, LLM inference engines, and agent orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library. These include critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability. By translating high-level governance principles into practical, verifiable metrics, LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.08842"
    },
    "56d54b131c4d8ee2d284a5d93ccda0c1": {
        "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?",
        "authors": [
            "Anthony GX-Chen",
            "Dongyan Lin",
            "Mandana Samiei",
            "Doina Precup",
            "Blake A. Richards",
            "Rob Fergus",
            "Kenneth Marino"
        ],
        "date": "2025/05/14",
        "pdf": "http://arxiv.org/pdf/2505.09614",
        "abstract": "Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs&#39; ability to explore and infer causal relationships, using the well-established &#34;Blicket Test&#34; paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This &#34;disjunctive bias&#34; persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.09614"
    },
    "13bb4dd8a075dccf2afb30c51309848c": {
        "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language Agents",
        "authors": [
            "JieHao Wu",
            "Ziwei Wang",
            "Junjie Sheng",
            "Wenhao Li",
            "Xiangfeng Wang",
            "Jun Luo"
        ],
        "date": "2025/05/15",
        "pdf": "http://arxiv.org/pdf/2505.10117",
        "abstract": "In cloud services, virtual machine (VM) scheduling is a typical Online Dynamic Multidimensional Bin Packing (ODMBP) problem, characterized by large-scale complexity and fluctuating demands. Traditional optimization methods struggle to adapt to real-time changes, domain-expert-designed heuristic approaches suffer from rigid strategies, and existing learning-based methods often lack generalizability and interpretability. To address these limitations, this paper proposes a hierarchical language agent framework named MiCo, which provides a large language model (LLM)-driven heuristic design paradigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov Decision Process with Options (SMDP-Option), enabling dynamic scheduling through a two-stage architecture, i.e., Option Miner and Option Composer. Option Miner utilizes LLMs to discover diverse and useful non-context-aware strategies by interacting with constructed environments. Option Composer employs LLMs to discover a composing strategy that integrates the non-context-aware strategies with the contextual ones. Extensive experiments on real-world enterprise datasets demonstrate that MiCo achieves a 96.9\\% competitive ratio in large-scale scenarios involving more than 10,000 virtual machines. It maintains high performance even under nonstationary request flows and diverse configurations, thus validating its effectiveness in complex and large-scale cloud environments.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.10117"
    },
    "741ffd56bd5cf2a44a1e4569a87da2cb": {
        "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?",
        "authors": [
            "Chenxi Jiang",
            "Chuhao Zhou",
            "Jianfei Yang"
        ],
        "date": "2025/05/16",
        "pdf": "http://arxiv.org/pdf/2505.10872",
        "abstract": "Robot task planning decomposes human instructions into executable action sequences that enable robots to complete a series of complex tasks. Although recent large language model (LLM)-based task planners achieve amazing performance, they assume that human instructions are clear and straightforward. However, real-world users are not experts, and their instructions to robots often contain significant vagueness. Linguists suggest that such vagueness frequently arises from referring expressions (REs), whose meanings depend heavily on dialogue context and environment. This vagueness is even more prevalent among the elderly and children, who robots should serve more. This paper studies how such vagueness in REs within human instructions affects LLM-based robot task planning and how to overcome this issue. To this end, we propose the first robot task planning benchmark with vague REs (REI-Bench), where we discover that the vagueness of REs can severely degrade robot planning performance, leading to success rate drops of up to 77.9%. We also observe that most failure cases stem from missing objects in planners. To mitigate the REs issue, we propose a simple yet effective approach: task-oriented context cognition, which generates clear instructions for robots, achieving state-of-the-art performance compared to aware prompt and chains of thought. This work contributes to the research community of human-robot interaction (HRI) by making robot task planning more practical, particularly for non-expert users, e.g., the elderly and children.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.10872"
    },
    "f2db57230ebc1aaf6fcb61661f76cb73": {
        "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents",
        "authors": [
            "Xilong Wang",
            "John Bloch",
            "Zedian Shao",
            "Yuepeng Hu",
            "Shuyan Zhou",
            "Neil Zhenqiang Gong"
        ],
        "date": "2025/05/16",
        "pdf": "http://arxiv.org/pdf/2505.11717",
        "abstract": "Multi-modal large language model (MLLM)-based web agents interact with webpage environments by generating actions based on screenshots of the webpages. Environmental prompt injection attacks manipulate the environment to induce the web agent to perform a specific, attacker-chosen action--referred to as the target action. However, existing attacks suffer from limited effectiveness or stealthiness, or are impractical in real-world settings. In this work, we propose EnvInjection, a new attack that addresses these limitations. Our attack adds a perturbation to the raw pixel values of the rendered webpage, which can be implemented by modifying the webpage&#39;s source code. After these perturbed pixels are mapped into a screenshot, the perturbation induces the web agent to perform the target action. We formulate the task of finding the perturbation as an optimization problem. A key challenge in solving this problem is that the mapping between raw pixel values and screenshot is non-differentiable, making it difficult to backpropagate gradients to the perturbation. To overcome this, we train a neural network to approximate the mapping and apply projected gradient descent to solve the reformulated optimization problem. Extensive evaluation on multiple webpage datasets shows that EnvInjection is highly effective and significantly outperforms existing baselines.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.11717"
    },
    "e2dbcd1baf1282e96bf1954837a9b660": {
        "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents",
        "authors": [
            "Tiannuo Yang",
            "Zebin Yao",
            "Bowen Jin",
            "Lixiao Cui",
            "Yusen Li",
            "Gang Wang",
            "Xiaoguang Liu"
        ],
        "date": "2025/05/17",
        "pdf": "http://arxiv.org/pdf/2505.12065",
        "abstract": "Large Language Model (LLM)-based search agents have shown remarkable capabilities in solving complex tasks by dynamically decomposing problems and addressing them through interleaved reasoning and retrieval. However, this interleaved paradigm introduces substantial efficiency bottlenecks. First, we observe that both highly accurate and overly approximate retrieval methods degrade system efficiency: exact search incurs significant retrieval overhead, while coarse retrieval requires additional reasoning steps during generation. Second, we identify inefficiencies in system design, including improper scheduling and frequent retrieval stalls, which lead to cascading latency -- where even minor delays in retrieval amplify end-to-end inference time. To address these challenges, we introduce SearchAgent-X, a high-efficiency inference framework for LLM-based search agents. SearchAgent-X leverages high-recall approximate retrieval and incorporates two key techniques: priority-aware scheduling and non-stall retrieval. Extensive experiments demonstrate that SearchAgent-X consistently outperforms state-of-the-art systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving up to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without compromising generation quality. SearchAgent-X is available at https://github.com/tiannuo-yang/SearchAgent-X.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12065"
    },
    "0bece07c4f5eee9dd9cc87519b44a1ef": {
        "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks",
        "authors": [
            "Yinghao Zhu",
            "Ziyi He",
            "Haoran Hu",
            "Xiaochen Zheng",
            "Xichen Zhang",
            "Zixiang Wang",
            "Junyi Gao",
            "Liantao Ma",
            "Lequan Yu"
        ],
        "date": "2025/05/18",
        "pdf": "http://arxiv.org/pdf/2505.12371",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently understood. Existing evaluations often lack generalizability, failing to cover diverse tasks reflective of real-world clinical practice, and frequently omit rigorous comparisons against both single-LLM-based and established conventional methods. To address this critical gap, we introduce MedAgentBoard, a comprehensive benchmark for the systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches. MedAgentBoard encompasses four diverse medical task categories: (1) medical (visual) question answering, (2) lay summary generation, (3) structured Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow automation, across text, medical images, and structured EHR data. Our extensive experiments reveal a nuanced landscape: while multi-agent collaboration demonstrates benefits in specific scenarios, such as enhancing task completeness in clinical workflow automation, it does not consistently outperform advanced single LLMs (e.g., in textual medical QA) or, critically, specialized conventional methods that generally maintain better performance in tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital resource and actionable insights, emphasizing the necessity of a task-specific, evidence-based approach to selecting and developing AI solutions in medicine. It underscores that the inherent complexity and overhead of multi-agent collaboration must be carefully weighed against tangible performance gains. All code, datasets, detailed prompts, and experimental results are open-sourced at https://medagentboard.netlify.app/.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12371"
    },
    "5286ff3ca9a013c22eb23d0367fb09f8": {
        "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems",
        "authors": [
            "Liwen Wang",
            "Wenxuan Wang",
            "Shuai Wang",
            "Zongjie Li",
            "Zhenlan Ji",
            "Zongyi Lyu",
            "Daoyuan Wu",
            "Shing-Chi Cheung"
        ],
        "date": "2025/05/18",
        "pdf": "http://arxiv.org/pdf/2505.12442",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has led to the emergence of Multi-Agent Systems (MAS) to perform complex tasks through collaboration. However, the intricate nature of MAS, including their architecture and agent interactions, raises significant concerns regarding intellectual property (IP) protection. In this paper, we introduce MASLEAK, a novel attack framework designed to extract sensitive information from MAS applications. MASLEAK targets a practical, black-box setting, where the adversary has no prior knowledge of the MAS architecture or agent configurations. The adversary can only interact with the MAS through its public API, submitting attack query $q$ and observing outputs from the final agent. Inspired by how computer worms propagate and infect vulnerable network hosts, MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain responses from each MAS agent that reveal a full set of proprietary components, including the number of agents, system topology, system prompts, task instructions, and tool usages. We construct the first synthetic dataset of MAS applications with 810 applications and also evaluate MASLEAK against real-world MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in extracting MAS IP, with an average attack success rate of 87% for system prompts and task instructions, and 92% for system architecture in most cases. We conclude by discussing the implications of our findings and the potential defenses.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12442"
    },
    "adfa4e1e30b0e5d0ad6b4e2139cbc7c4": {
        "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents",
        "authors": [
            "Yunseok Jang",
            "Yeda Song",
            "Sungryull Sohn",
            "Lajanugen Logeswaran",
            "Tiange Luo",
            "Dong-Ki Kim",
            "Kyunghoon Bae",
            "Honglak Lee"
        ],
        "date": "2025/05/19",
        "pdf": "http://arxiv.org/pdf/2505.12632",
        "abstract": "Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have sparked significant interest in developing GUI visual agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from YouTube), a large-scale dataset of 313K annotated frames from 20K instructional videos capturing diverse real-world mobile OS navigation across multiple platforms. Models that include MONDAY in their pre-training phases demonstrate robust cross-platform generalization capabilities, consistently outperforming models trained on existing single OS datasets while achieving an average performance gain of 18.11%p on an unseen mobile OS platform. To enable continuous dataset expansion as mobile platforms evolve, we present an automated framework that leverages publicly available video content to create comprehensive task datasets without manual annotation. Our framework comprises robust OCR-based scene detection (95.04% F1score), near-perfect UI element detection (99.87% hit ratio), and novel multi-step action identification to extract reliable action sequences across diverse interface configurations. We contribute both the MONDAY dataset and our automated collection framework to facilitate future research in mobile OS navigation.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12632"
    },
    "f8de9551cfb7a6311d0360f36b8db642": {
        "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents",
        "authors": [
            "Zheng Wu",
            "Pengzhou Cheng",
            "Zongru Wu",
            "Lingzhong Dong",
            "Zhuosheng Zhang"
        ],
        "date": "2025/05/19",
        "pdf": "http://arxiv.org/pdf/2505.12842",
        "abstract": "Graphical user interface (GUI) agents have recently emerged as an intriguing paradigm for human-computer interaction, capable of automatically executing user instructions to operate intelligent terminal devices. However, when encountering out-of-distribution (OOD) instructions that violate environmental constraints or exceed the current capabilities of agents, GUI agents may suffer task breakdowns or even pose security threats. Therefore, effective OOD detection for GUI agents is essential. Traditional OOD detection methods perform suboptimally in this domain due to the complex embedding space and evolving GUI environments. In this work, we observe that the in-distribution input semantic space of GUI agents exhibits a clustering pattern with respect to the distance from the centroid. Based on the finding, we propose GEM, a novel method based on fitting a Gaussian mixture model over input embedding distances extracted from the GUI Agent that reflect its capability boundary. Evaluated on eight datasets spanning smartphones, computers, and web browsers, our method achieves an average accuracy improvement of 23.70\\% over the best-performing baseline. Analysis verifies the generalization ability of our method through experiments on nine different backbones. The codes are available at https://github.com/Wuzheng02/GEM-OODforGUIagents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.12842"
    },
    "af036e5578fe4923ad0ad3ea3c5ce564": {
        "title": "Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents",
        "authors": [
            "Karina Zainullina",
            "Alexander Golubev",
            "Maria Trofimova",
            "Sergei Polezhaev",
            "Ibragim Badertdinov",
            "Daria Litvintseva",
            "Simon Karasik",
            "Filipp Fisin",
            "Sergei Skvortsov",
            "Maksim Nekrashevich",
            "Anton Shevtsov",
            "Boris Yangel"
        ],
        "date": "2025/05/19",
        "pdf": "http://arxiv.org/pdf/2505.13652",
        "abstract": "Large language models (LLMs) have recently achieved remarkable results in complex multi-step tasks, such as mathematical reasoning and agentic software engineering. However, they often struggle to maintain consistent performance across multiple solution attempts. One effective approach to narrow the gap between average-case and best-case performance is guided test-time search, which explores multiple solution paths to identify the most promising one. Unfortunately, effective search techniques (e.g. MCTS) are often unsuitable for non-serializable RL environments, such as Docker containers, where intermediate environment states cannot be easily saved and restored. We investigate two complementary search strategies applicable to such environments: 1-step lookahead and trajectory selection, both guided by a learned action-value function estimator. On the SWE-bench Verified benchmark, a key testbed for agentic software engineering, we find these methods to double the average success rate of a fine-tuned Qwen-72B model, achieving 40.8%, the new state-of-the-art for open-weights models. Additionally, we show that these techniques are transferable to more advanced closed models, yielding similar improvements with GPT-4o.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.13652"
    },
    "dad4d21a0f22af38c01e9ca262a43ef8": {
        "title": "Structured Agent Distillation for Large Language Model",
        "authors": [
            "Jun Liu",
            "Zhenglun Kong",
            "Peiyan Dong",
            "Changdi Yang",
            "Tianqi Li",
            "Hao Tang",
            "Geng Yuan",
            "Wei Niu",
            "Wenbin Zhang",
            "Pu Zhao",
            "Xue Lin",
            "Dong Huang",
            "Yanzhi Wang"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.13820",
        "abstract": "Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher&#39;s behavior. This structure-aware supervision enables compact agents to better replicate the teacher&#39;s decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.13820"
    },
    "4980f4d0afb62836598dffbbbef4c5f0": {
        "title": "Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Xi Zhang",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.13887",
        "abstract": "The exponential rise in mobile device usage necessitates streamlined automation for effective task management, yet many AI frameworks fall short due to inadequate operational expertise. While manually written knowledge can bridge this gap, it is often burdensome and inefficient. We introduce Mobile-Agent-V, an innovative framework that utilizes video as a guiding tool to effortlessly and efficiently inject operational knowledge into mobile automation processes. By deriving knowledge directly from video content, Mobile-Agent-V eliminates manual intervention, significantly reducing the effort and time required for knowledge acquisition. To rigorously evaluate this approach, we propose Mobile-Knowledge, a benchmark tailored to assess the impact of external knowledge on mobile agent performance. Our experimental findings demonstrate that Mobile-Agent-V enhances performance by 36% compared to existing methods, underscoring its effortless and efficient advantages in mobile automation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.13887"
    },
    "9027ec8640d6c3561432a6e803cc76ad": {
        "title": "Efficient Agent Training for Computer Use",
        "authors": [
            "Yanheng He",
            "Jiahe Jin",
            "Pengfei Liu"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.13909",
        "abstract": "Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.13909"
    },
    "f87ad4f24f1c28b090f11ccf16a86c24": {
        "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation",
        "authors": [
            "Haoyang Fang",
            "Boran Han",
            "Nick Erickson",
            "Xiyuan Zhang",
            "Su Zhou",
            "Anirudh Dagar",
            "Jiani Zhang",
            "Ali Caner Turkmen",
            "Cuixiong Hu",
            "Huzefa Rangwala",
            "Ying Nian Wu",
            "Bernie Wang",
            "George Karypis"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.13941",
        "abstract": "Existing AutoML systems have advanced the automation of machine learning (ML); however, they still require substantial manual configuration and expert input, particularly when handling multimodal data. We introduce MLZero, a novel multi-agent framework powered by Large Language Models (LLMs) that enables end-to-end ML automation across diverse data modalities with minimal human intervention. A cognitive perception module is first employed, transforming raw multimodal inputs into perceptual context that effectively guides the subsequent workflow. To address key limitations of LLMs, such as hallucinated code generation and outdated API knowledge, we enhance the iterative code generation process with semantic and episodic memory. MLZero demonstrates superior performance on MLE-Bench Lite, outperforming all competitors in both success rate and solution quality, securing six gold medals. Additionally, when evaluated on our Multimodal AutoML Agent Benchmark, which includes 25 more challenging tasks spanning diverse data modalities, MLZero outperforms the competing methods by a large margin with a success rate of 0.92 (+263.6\\%) and an average rank of 2.28. Our approach maintains its robust effectiveness even with a compact 8B LLM, outperforming full-size systems from existing solutions.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.13941"
    },
    "c23caef6b00cdd6efb2f155a76e0730e": {
        "title": "s3: You Don&#39;t Need That Much Data to Train a Search Agent via RL",
        "authors": [
            "Pengcheng Jiang",
            "Xueqiang Xu",
            "Jiacheng Lin",
            "Jinfeng Xiao",
            "Zifeng Wang",
            "Jimeng Sun",
            "Jiawei Han"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.14146",
        "abstract": "Retrieval-augmented generation (RAG) systems empower large language models (LLMs) to access external knowledge during inference. Recent advances have enabled LLMs to act as search agents via reinforcement learning (RL), improving information acquisition through multi-turn interactions with retrieval engines. However, existing approaches either optimize retrieval using search-only metrics (e.g., NDCG) that ignore downstream utility or fine-tune the entire LLM to jointly reason and retrieve-entangling retrieval with generation and limiting the real search utility and compatibility with frozen or proprietary models. In this work, we propose s3, a lightweight, model-agnostic framework that decouples the searcher from the generator and trains the searcher using a Gain Beyond RAG reward: the improvement in generation accuracy over naive RAG. s3 requires only 2.4k training samples to outperform baselines trained on over 70x more data, consistently delivering stronger downstream performance across six general QA and five medical QA benchmarks.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.14146"
    },
    "2e6d3a37203db0b3a6510876ff081f09": {
        "title": "Agent Context Protocols Enhance Collective Inference",
        "authors": [
            "Devansh Bhardwaj",
            "Arjun Beniwal",
            "Shreyas Chaudhari",
            "Ashwin Kalyan",
            "Tanmay Rajpurohit",
            "Karthik R. Narasimhan",
            "Ameet Deshpande",
            "Vishvak Murahari"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.14569",
        "abstract": "AI agents have become increasingly adept at complex tasks such as coding, reasoning, and multimodal understanding. However, building generalist systems requires moving beyond individual agents to collective inference -- a paradigm where multi-agent systems with diverse, task-specialized agents complement one another through structured communication and collaboration. Today, coordination is usually handled with imprecise, ad-hoc natural language, which limits complex interaction and hinders interoperability with domain-specific agents. We introduce Agent context protocols (ACPs): a domain- and agent-agnostic family of structured protocols for agent-agent communication, coordination, and error handling. ACPs combine (i) persistent execution blueprints -- explicit dependency graphs that store intermediate agent outputs -- with (ii) standardized message schemas, enabling robust and fault-tolerant multi-agent collective inference. ACP-powered generalist systems reach state-of-the-art performance: 28.3 % accuracy on AssistantBench for long-horizon web assistance and best-in-class multimodal technical reports, outperforming commercial AI systems in human evaluation. ACPs are highly modular and extensible, allowing practitioners to build top-tier generalist agents quickly.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.14569"
    },
    "08dfb707118bde417358138d900c4287": {
        "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions",
        "authors": [
            "Bufang Yang",
            "Lilin Xu",
            "Liekang Zeng",
            "Kaiwei Liu",
            "Siyang Jiang",
            "Wenrui Lu",
            "Hongkai Chen",
            "Xiaofan Jiang",
            "Guoliang Xing",
            "Zhenyu Yan"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2505.14668",
        "abstract": "Recent advances in Large Language Models (LLMs) have propelled intelligent agents from reactive responses to proactive support. While promising, existing proactive agents either rely exclusively on observations from enclosed environments (e.g., desktop UIs) with direct LLM inference or employ rule-based proactive notifications, leading to suboptimal user intent understanding and limited functionality for proactive service. In this paper, we introduce ContextAgent, the first context-aware proactive agent that incorporates extensive sensory contexts to enhance the proactive capabilities of LLM agents. ContextAgent first extracts multi-dimensional contexts from massive sensory perceptions on wearables (e.g., video and audio) to understand user intentions. ContextAgent then leverages the sensory contexts and the persona contexts from historical data to predict the necessity for proactive services. When proactive assistance is needed, ContextAgent further automatically calls the necessary tools to assist users unobtrusively. To evaluate this new task, we curate ContextAgentBench, the first benchmark for evaluating context-aware proactive LLM agents, covering 1,000 samples across nine daily scenarios and twenty tools. Experiments on ContextAgentBench show that ContextAgent outperforms baselines by achieving up to 8.5% and 6.0% higher accuracy in proactive predictions and tool calling, respectively. We hope our research can inspire the development of more advanced, human-centric, proactive AI assistants.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.14668"
    },
    "d14ed3113227873c22e06fd42447d38e": {
        "title": "ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges",
        "authors": [
            "Cheng Qian",
            "Hongyi Du",
            "Hongru Wang",
            "Xiusi Chen",
            "Yuji Zhang",
            "Avirup Sil",
            "Chengxiang Zhai",
            "Kathleen McKeown",
            "Heng Ji"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15068",
        "abstract": "Recent progress in large language models (LLMs) has enabled substantial advances in solving mathematical problems. However, existing benchmarks often fail to reflect the complexity of real-world problems, which demand open-ended, interdisciplinary reasoning and integration of computational tools. To address this gap, we introduce ModelingBench, a novel benchmark featuring real-world-inspired, open-ended problems from math modeling competitions across diverse domains, ranging from urban traffic optimization to ecosystem resource planning. These tasks require translating natural language into formal mathematical formulations, applying appropriate tools, and producing structured, defensible reports. ModelingBench also supports multiple valid solutions, capturing the ambiguity and creativity of practical modeling. We also present ModelingAgent, a multi-agent framework that coordinates tool use, supports structured workflows, and enables iterative self-refinement to generate well-grounded, creative solutions. To evaluate outputs, we further propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as domain-specialized judges assessing solutions from multiple expert perspectives. Empirical results show that ModelingAgent substantially outperforms strong baselines and often produces solutions indistinguishable from those of human experts. Together, our work provides a comprehensive framework for evaluating and advancing real-world problem-solving in open-ended, interdisciplinary modeling challenges.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15068"
    },
    "9a5434786ff3c0a2bf5a090bf2408d0c": {
        "title": "BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems",
        "authors": [
            "Andy K. Zhang",
            "Joey Ji",
            "Celeste Menders",
            "Riya Dulepet",
            "Thomas Qin",
            "Ron Y. Wang",
            "Junrong Wu",
            "Kyleen Liao",
            "Jiliang Li",
            "Jinghan Hu",
            "Sara Hong",
            "Nardos Demilew",
            "Shivatmica Murgai",
            "Jason Tran",
            "Nishka Kacheria",
            "Ethan Ho",
            "Denis Liu",
            "Lauren McLane",
            "Olivia Bruvik",
            "Dai-Rong Han",
            "Seungwoo Kim",
            "Akhil Vyas",
            "Cuiyuanxiu Chen",
            "Ryan Li",
            "Weiran Xu",
            "Jonathan Z. Ye",
            "Prerit Choudhary",
            "Siddharth M. Bhatia",
            "Vikram Sivashankar",
            "Yuxuan Bao",
            "Dawn Song",
            "Dan Boneh",
            "Daniel E. Ho",
            "Percy Liang"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15216",
        "abstract": "AI agents have the potential to significantly alter the cybersecurity landscape. To help us understand this change, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a specific vulnerability), and Patch (patching a specific vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \\$10 to \\$30,485, and cover 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code, OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing agents are Claude Code (5% on Detect, mapping to \\$1,350), Custom Agent with Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \\$1,025; 67.5% on Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \\$2,400; 90% on Patch, mapping to \\$14,422). OpenAI Codex CLI and Claude Code are more capable at defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit scores of 32.5% and 57.5% respectively; in contrast, the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 40-67.5% and Patch scores of 45-60%.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15216"
    },
    "e8f21c6d9ecd2d36a7250e8e252a8856": {
        "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving",
        "authors": [
            "Kangan Qian",
            "Sicong Jiang",
            "Yang Zhong",
            "Ziang Luo",
            "Zilin Huang",
            "Tianze Zhu",
            "Kun Jiang",
            "Mengmeng Yang",
            "Zheng Fu",
            "Jinyu Miao",
            "Yining Shi",
            "He Zhe Lim",
            "Li Liu",
            "Tianbao Zhou",
            "Hongyi Wang",
            "Huang Yu",
            "Yifei Hu",
            "Guang Li",
            "Guang Chen",
            "Hao Ye",
            "Lijun Sun",
            "Diange Yang"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15298",
        "abstract": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their struggle with hallucinations, inefficient reasoning, and limited real-world validation hinders accurate perception and robust step-by-step reasoning. To overcome this, we introduce \\textbf{AgentThink}, a pioneering unified framework that, for the first time, integrates Chain-of-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink&#39;s core innovations include: \\textbf{(i) Structured Data Generation}, by establishing an autonomous driving tool library to automatically construct structured, self-verified reasoning data explicitly incorporating tool usage for diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline}, employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to equip VLMs with the capability for autonomous tool invocation; and \\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel multi-tool assessment protocol to rigorously evaluate the model&#39;s tool invocation and utilization. Experiments on the DriveLMM-o1 benchmark demonstrate AgentThink significantly boosts overall reasoning scores by \\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while markedly improving reasoning quality and consistency. Furthermore, ablation studies and robust zero-shot/few-shot generalization experiments across various benchmarks underscore its powerful capabilities. These findings highlight a promising trajectory for developing trustworthy and tool-aware autonomous driving models.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15298"
    },
    "53866fb9e9fa5dfe426a3e5ba4fa0250": {
        "title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation",
        "authors": [
            "Yunjia Xi",
            "Jianghao Lin",
            "Menghui Zhu",
            "Yongzhao Xiao",
            "Zhuoying Ou",
            "Jiaqi Liu",
            "Tong Wan",
            "Bo Chen",
            "Weiwen Liu",
            "Yasheng Wang",
            "Ruiming Tang",
            "Weinan Zhang",
            "Yong Yu"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15872",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by grounding responses with retrieved information. As an emerging paradigm, Agentic RAG further enhances this process by introducing autonomous LLM agents into the information seeking process. However, existing benchmarks fall short in evaluating such systems, as they are confined to a static retrieval environment with a fixed, limited corpus} and simple queries that fail to elicit agentic behavior. Moreover, their evaluation protocols assess information seeking effectiveness by pre-defined gold sets of documents, making them unsuitable for the open-ended and dynamic nature of real-world web environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with challenging questions designed for assessing agentic information seeking in real-world, dynamic web environments. We propose a systematic methodology for constructing challenging queries satisfying the criteria of determinacy, difficulty, and diversity. Based on this, we develop the first evaluation framework tailored to dynamic agentic information seeking, including fine-grained metrics about the accuracy, utility, and compactness of information seeking outcomes. Through extensive experiments across LLMs, search engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and offers actionable insights for future research.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15872"
    },
    "9d0ec6f70ab5fc91e422e8945dc2fc3f": {
        "title": "ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation",
        "authors": [
            "Tony Montes",
            "Fernando Lozano"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15928",
        "abstract": "Recent advancements in Video Question Answering (VideoQA) have introduced LLM-based agents, modular frameworks, and procedural solutions, yielding promising results. These systems use dynamic agents and memory-based mechanisms to break down complex tasks and refine answers. However, significant improvements remain in tracking objects for grounding over time and decision-making based on reasoning to better align object references with language model outputs, as newer models get better at both tasks. This work presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA) that combines a Chain-of-Thought framework with grounding reasoning alongside YOLO-World to enhance object tracking and alignment. This approach establishes a new state-of-the-art in VideoQA and Video Understanding, showing enhanced performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also enables cross-checking of grounding timeframes, improving accuracy and providing valuable support for verification and increased output reliability across multiple video domains. The code is available at https://github.com/t-montes/viqagent.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15928"
    },
    "1e84ec9c21ba1b3407a3392be91bc49f": {
        "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security",
        "authors": [
            "Omer Hofman",
            "Oren Rachmil",
            "Shamik Bose",
            "Vikas Pahuja",
            "Jonathan Brokman",
            "Toshiya Shimizu",
            "Trisha Starostina",
            "Kelly Marchisio",
            "Seraphina Goldfarb-Tarrant",
            "Roman Vainshtein"
        ],
        "date": "2025/05/21",
        "pdf": "http://arxiv.org/pdf/2505.15935",
        "abstract": "Agentic AI systems, which build on Large Language Models (LLMs) and interact with tools and memory, have rapidly advanced in capability and scope. Yet, since LLMs have been shown to struggle in multilingual settings, typically resulting in lower performance and reduced safety, agentic systems risk inheriting these limitations. This raises concerns about the global accessibility of such systems, as users interacting in languages other than English may encounter unreliable or security-critical agent behavior. Despite growing interest in evaluating agentic AI, existing benchmarks focus exclusively on English, leaving multilingual settings unexplored. To address this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate agentic AI systems across diverse languages and tasks. MAPS builds on four widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code generation), MATH (mathematical reasoning), and the Agent Security Benchmark (security). We translate each dataset into ten diverse languages, resulting in 805 unique tasks and 8,855 total language-specific instances. Our benchmark suite enables a systematic analysis of how multilingual contexts affect agent performance and robustness. Empirically, we observe consistent degradation in both performance and security when transitioning from English to other languages, with severity varying by task and correlating with the amount of translated input. Building on these findings, we provide actionable recommendations to guide agentic AI systems development and assessment under multilingual settings. This work establishes a standardized evaluation framework, encouraging future research towards equitable, reliable, and globally accessible agentic AI. MAPS benchmark suite is publicly available at https://huggingface.co/datasets/Fujitsu-FRE/MAPS",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.15935"
    },
    "66b649996fab00c14ca28bf6c49ba7a2": {
        "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development",
        "authors": [
            "Ming Shen",
            "Raphael Shu",
            "Anurag Pratik",
            "James Gung",
            "Yubin Ge",
            "Monica Sunkara",
            "Yi Zhang"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16086",
        "abstract": "We have seen remarkable progress in large language models (LLMs) empowered multi-agent systems solving complex tasks necessitating cooperation among experts with diverse skills. However, optimizing LLM-based multi-agent systems remains challenging. In this work, we perform an empirical case study on group optimization of role-based multi-agent systems utilizing natural language feedback for challenging software development tasks under various evaluation dimensions. We propose a two-step agent prompts optimization pipeline: identifying underperforming agents with their failure explanations utilizing textual feedback and then optimizing system prompts of identified agents utilizing failure explanations. We then study the impact of various optimization settings on system performance with two comparison groups: online against offline optimization and individual against group optimization. For group optimization, we study two prompting strategies: one-pass and multi-pass prompting optimizations. Overall, we demonstrate the effectiveness of our optimization method for role-based multi-agent systems tackling software development tasks evaluated on diverse evaluation dimensions, and we investigate the impact of diverse optimization settings on group behaviors of the multi-agent systems to provide practical insights for future development.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16086"
    },
    "54008d6de8ac7f33c641bd96c70efd31": {
        "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research",
        "authors": [
            "Zifeng Wang",
            "Benjamin Danek",
            "Jimeng Sun"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16100",
        "abstract": "Validating scientific hypotheses is a central challenge in biomedical research, and remains difficult for artificial intelligence (AI) agents due to the complexity of real-world data analysis and evidence interpretation. In this work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans, curated from over 300 published biomedical studies to reflect the structure and reasoning found in authentic research workflows. Each task includes a structured hypothesis derived from the original study&#39;s conclusions, expressed in the affirmative to reflect the language of scientific reporting, and one or more pieces of supporting evidence grounded in empirical data tables. While these hypotheses mirror published claims, they remain testable using standard statistical or machine learning methods. The benchmark enables evaluation along four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and conclusion, (3) correctness of the reasoning process, and (4) executability of the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable hypotheses: cases where the available data are insufficient to support or refute a claim, reflecting a common yet underexplored scenario in real-world science. We propose BioDSA-1K as a foundation for building and evaluating generalizable, trustworthy AI agents for biomedical discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16100"
    },
    "2a45bda98fd873b7d09c41e042fdc5b4": {
        "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization",
        "authors": [
            "Haonian Ji",
            "Shi Qiu",
            "Siyang Xin",
            "Siwei Han",
            "Zhaorun Chen",
            "Dake Zhang",
            "Hongyi Wang",
            "Huaxiu Yao"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16832",
        "abstract": "While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at https://github.com/aiming-lab/EduVisBench and https://github.com/aiming-lab/EduVisAgent.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16832"
    },
    "dfec2d0701eda6d9aa215fe21b37958c": {
        "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification",
        "authors": [
            "NovelSeek Team",
            "Bo Zhang",
            "Shiyang Feng",
            "Xiangchao Yan",
            "Jiakang Yuan",
            "Zhiyin Yu",
            "Xiaohan He",
            "Songtao Huang",
            "Shaowei Hou",
            "Zheng Nie",
            "Zhilong Wang",
            "Jinyao Liu",
            "Runmin Ma",
            "Tianshuo Peng",
            "Peng Ye",
            "Dongzhan Zhou",
            "Shufei Zhang",
            "Xiaosong Wang",
            "Yilan Zhang",
            "Meng Li",
            "Zhongying Tu",
            "Xiangyu Yue",
            "Wangli Ouyang",
            "Bowen Zhou",
            "Lei Bai"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16938",
        "abstract": "Artificial Intelligence (AI) is accelerating the transformation of scientific research paradigms, not only enhancing research efficiency but also driving innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework to conduct Autonomous Scientific Research (ASR) across various scientific research fields, enabling researchers to tackle complicated problems in these fields with unprecedented speed and precision. NovelSeek highlights three key advantages: 1) Scalability: NovelSeek has demonstrated its versatility across 12 scientific research tasks, capable of generating innovative ideas to enhance the performance of baseline code. 2) Interactivity: NovelSeek provides an interface for human expert feedback and multi-agent interaction in automated end-to-end processes, allowing for the seamless integration of domain expert knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in several scientific fields with significantly less time cost compared to human efforts. For instance, in reaction yield prediction, it increased from 27.6% to 35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from 0.65 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation, precision advanced from 78.8% to 81.0% in a mere 30 hours.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16938"
    },
    "d334af404d82b35af62ec32593483683": {
        "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios",
        "authors": [
            "Yunjia Qi",
            "Hao Peng",
            "Xiaozhi Wang",
            "Amy Xin",
            "Youfeng Liu",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16944",
        "abstract": "Large Language Models (LLMs) have demonstrated advanced capabilities in real-world agentic applications. Growing research efforts aim to develop LLM-based agents to address practical demands, introducing a new challenge: agentic scenarios often involve lengthy instructions with complex constraints, such as extended system prompts and detailed tool specifications. While adherence to such instructions is crucial for agentic applications, whether LLMs can reliably follow them remains underexplored. In this paper, we introduce AgentIF, the first benchmark for systematically evaluating LLM instruction following ability in agentic scenarios. AgentIF features three key characteristics: (1) Realistic, constructed from 50 real-world agentic applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words. (3) Complex, averaging 11.9 constraints per instruction, covering diverse constraint types, such as tool specifications and condition constraints. To construct AgentIF, we collect 707 human-annotated instructions across 50 agentic tasks from industrial application agents and open-source agentic systems. For each instruction, we annotate the associated constraints and corresponding evaluation metrics, including code-based evaluation, LLM-based evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically evaluate existing advanced LLMs. We observe that current models generally perform poorly, especially in handling complex constraint structures and tool specifications. We further conduct error analysis and analytical experiments on instruction length and meta constraints, providing some findings about the failure modes of existing LLMs. We have released the code and data to facilitate future research.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16944"
    },
    "9f6b6b84cbc623ff8ea9faac77f2d8be": {
        "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs",
        "authors": [
            "Rui Ye",
            "Xiangrui Liu",
            "Qimin Wu",
            "Xianghe Pang",
            "Zhenfei Yin",
            "Lei Bai",
            "Siheng Chen"
        ],
        "date": "2025/05/22",
        "pdf": "http://arxiv.org/pdf/2505.16997",
        "abstract": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by enabling cooperation among multiple specialized agents. However, most existing MAS frameworks rely on a single LLM to drive all agents, constraining the system&#39;s intelligence to the limit of that model. This paper explores the paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by diverse LLMs, elevating the system&#39;s potential to the collective intelligence of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to evaluate the performance of various LLMs across different domains and MAS-related functions. As an extensive empirical study, we assess 27 LLMs across 5 domains (encompassing 21 test sets) and 5 functions, conducting over 1.7 million evaluations to identify optimal model selections for each domain-function combination. Building on these findings, we demonstrate that transitioning from homogeneous to heterogeneous LLM-driven MAS can significantly enhance system performance without requiring structural redesign. Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration yields up to 8.4\\% performance improvement on the MATH dataset. In a mixed chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable 47\\% performance boost on the AIME dataset. Our results underscore the transformative potential of heterogeneous LLMs in MAS, highlighting a promising avenue for advancing scalable, collaborative AI systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.16997"
    },
    "985e53c7651865d23b5d79cb0d046a5b": {
        "title": "PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate",
        "authors": [
            "Dezheng Bao",
            "Yueci Yang",
            "Xin Chen",
            "Zhengxuan Jiang",
            "Zeguo Fei",
            "Daoze Zhang",
            "Xuanwen Huang",
            "Junru Chen",
            "Chutian Yu",
            "Xiang Yuan",
            "Yang Yang"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.17492",
        "abstract": "Project duplication detection is critical for project quality assessment, as it improves resource utilization efficiency by preventing investing in newly proposed project that have already been studied. It requires the ability to understand high-level semantics and generate constructive and valuable feedback. Existing detection methods rely on basic word- or sentence-level comparison or solely apply large language models, lacking valuable insights for experts and in-depth comprehension of project content and review criteria. To tackle this issue, we propose PD$^3$, a Project Duplication Detection framework via adapted multi-agent Debate. Inspired by real-world expert debates, it employs a fair competition format to guide multi-agent debate to retrieve relevant projects. For feedback, it incorporates both qualitative and quantitative analysis to improve its practicality. Over 800 real-world power project data spanning more than 20 specialized fields are used to evaluate the framework, demonstrating that our method outperforms existing approaches by 7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online platform, Review Dingdang, to assist power experts, saving 5.73 million USD in initial detection on more than 100 newly proposed projects.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.17492"
    },
    "5df84a39a162bdd6f5e2bdff806534b4": {
        "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding",
        "authors": [
            "Xiaoyi Zhang",
            "Zhaoyang Jia",
            "Zongyu Guo",
            "Jiahao Li",
            "Bin Li",
            "Houqiang Li",
            "Yan Lu"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.18079",
        "abstract": "Long-form video understanding presents significant challenges due to extensive temporal-spatial complexity and the difficulty of question answering under such extended contexts. While Large Language Models (LLMs) have demonstrated considerable advancements in video analysis capabilities and long context handling, they continue to exhibit limitations when processing information-dense hour-long videos. To overcome such limitations, we propose the Deep Video Discovery agent to leverage an agentic search strategy over segmented video clips. Different from previous video agents manually designing a rigid workflow, our approach emphasizes the autonomous nature of agents. By providing a set of search-centric tools on multi-granular video database, our DVD agent leverages the advanced reasoning capability of LLM to plan on its current observation state, strategically selects tools, formulates appropriate parameters for actions, and iteratively refines its internal reasoning in light of the gathered information. We perform comprehensive evaluation on multiple long video understanding benchmarks that demonstrates the advantage of the entire system design. Our DVD agent achieves SOTA performance, significantly surpassing prior works by a large margin on the challenging LVBench dataset. Comprehensive ablation studies and in-depth tool analyses are also provided, yielding insights to further advance intelligent agents tailored for long-form video understanding tasks. The code will be released later.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18079"
    },
    "117985af7b52e91c3c49bcc150f16354": {
        "title": "ProgRM: Build Better GUI Agents with Progress Rewards",
        "authors": [
            "Danyang Zhang",
            "Situo Zhang",
            "Ziyue Yang",
            "Zichen Zhu",
            "Zihan Zhao",
            "Ruisheng Cao",
            "Lu Chen",
            "Kai Yu"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.18121",
        "abstract": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can potentially reshape our daily lives significantly. However, current LLM-based GUI agents suffer from the scarcity of high-quality training data owing to the difficulties of trajectory collection and reward annotation. Existing works have been exploring LLMs to collect trajectories for imitation learning or to offer reward signals for online RL training. However, the Outcome Reward Model (ORM) used in existing works cannot provide finegrained feedback and can over-penalize the valuable steps in finally failed trajectories. To this end, we propose Progress Reward Model (ProgRM) to provide dense informative intermediate rewards by predicting a task completion progress for each step in online training. To handle the challenge of progress reward label annotation, we further design an efficient LCS-based (Longest Common Subsequence) self-annotation algorithm to discover the key steps in trajectories and assign progress labels accordingly. ProgRM is evaluated with extensive experiments and analyses. Actors trained with ProgRM outperform leading proprietary LLMs and ORM-trained actors, illustrating the effectiveness of ProgRM. The codes for experiments will be made publicly available upon acceptance.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18121"
    },
    "dc2d2c21c4d534cd6517a68d19a8c538": {
        "title": "Gaming Tool Preferences in Agentic LLMs",
        "authors": [
            "Kazem Faghih",
            "Wenxiao Wang",
            "Yize Cheng",
            "Siddhant Bharti",
            "Gaurang Sriramanan",
            "Sriram Balasubramanian",
            "Parsa Hosseini",
            "Soheil Feizi"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.18135",
        "abstract": "Large language models (LLMs) can now access a wide range of external tools, thanks to the Model Context Protocol (MCP). This greatly expands their abilities as various agents. However, LLMs rely entirely on the text descriptions of tools to decide which ones to use--a process that is surprisingly fragile. In this work, we expose a vulnerability in prevalent tool/function-calling protocols by investigating a series of edits to tool descriptions, some of which can drastically increase a tool&#39;s usage from LLMs when competing with alternatives. Through controlled experiments, we show that tools with properly edited descriptions receive over 10 times more usage from GPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further evaluate how various edits to tool descriptions perform when competing directly with one another and how these trends generalize or differ across a broader set of 10 different models. These phenomenons, while giving developers a powerful way to promote their tools, underscore the need for a more reliable foundation for agentic LLMs to select and utilize tools and resources.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18135"
    },
    "77736ef52bfa50842b390cd1753cdec8": {
        "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control",
        "authors": [
            "Alireza Rezazadeh",
            "Zichao Li",
            "Ange Lou",
            "Yuying Zhao",
            "Wei Wei",
            "Yujia Bao"
        ],
        "date": "2025/05/23",
        "pdf": "http://arxiv.org/pdf/2505.18279",
        "abstract": "Complex tasks are increasingly delegated to ensembles of specialized LLM-based agents that reason, communicate, and coordinate actions-both among themselves and through interactions with external tools, APIs, and databases. While persistent memory has been shown to enhance single-agent performance, most approaches assume a monolithic, single-user context-overlooking the benefits and challenges of knowledge transfer across users under dynamic, asymmetric permissions. We introduce Collaborative Memory, a framework for multi-user, multi-agent environments with asymmetric, time-evolving access controls encoded as bipartite graphs linking users, agents, and resources. Our system maintains two memory tiers: (1) private memory-private fragments visible only to their originating user; and (2) shared memory-selectively shared fragments. Each fragment carries immutable provenance attributes (contributing agents, accessed resources, and timestamps) to support retrospective permission checks. Granular read policies enforce current user-agent-resource constraints and project existing memory fragments into filtered transformed views. Write policies determine fragment retention and sharing, applying context-aware transformations to update the memory. Both policies may be designed conditioned on system, agent, and user-level information. Our framework enables safe, efficient, and interpretable cross-user knowledge sharing, with provable adherence to asymmetric, time-varying policies and full auditability of memory operations.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18279"
    },
    "e23efa449a7ce7c978046a440314af4d": {
        "title": "SEW: Self-Evolving Agentic Workflows for Automated Code Generation",
        "authors": [
            "Siwei Liu",
            "Jinyuan Fang",
            "Han Zhou",
            "Yingxu Wang",
            "Zaiqiao Meng"
        ],
        "date": "2025/05/24",
        "pdf": "http://arxiv.org/pdf/2505.18646",
        "abstract": "Large Language Models (LLMs) have demonstrated effectiveness in code generation tasks. To enable LLMs to address more complex coding challenges, existing research has focused on crafting multi-agent systems with agentic workflows, where complex coding tasks are decomposed into sub-tasks, assigned to specialized agents. Despite their effectiveness, current approaches heavily rely on hand-crafted agentic workflows, with both agent topologies and prompts manually designed, which limits their ability to automatically adapt to different types of coding problems. To address these limitations and enable automated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving \\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that automatically generates and optimises multi-agent workflows. Extensive experiments on three coding benchmark datasets, including the challenging LiveCodeBench, demonstrate that our SEW can automatically design agentic workflows and optimise them through self-evolution, bringing up to 33\\% improvement on LiveCodeBench compared to using the backbone LLM only. Furthermore, by investigating different representation schemes of workflow, we provide insights into the optimal way to encode workflow information with text.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.18646"
    },
    "3c418fecb7e5f02ea8b10fd8d7f05a49": {
        "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling",
        "authors": [
            "Jialong Zhou",
            "Lichao Wang",
            "Xiao Yang"
        ],
        "date": "2025/05/25",
        "pdf": "http://arxiv.org/pdf/2505.19234",
        "abstract": "The emergence of large language models (LLMs) enables the development of intelligent agents capable of engaging in complex and multi-turn dialogues. However, multi-agent collaboration face critical safety challenges, such as hallucination amplification and error injection and propagation. This paper presents GUARDIAN, a unified method for detecting and mitigating multiple safety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the multi-agent collaboration process as a discrete-time temporal attributed graph, GUARDIAN explicitly captures the propagation dynamics of hallucinations and errors. The unsupervised encoder-decoder architecture incorporating an incremental training paradigm, learns to reconstruct node attributes and graph structures from latent embeddings, enabling the identification of anomalous nodes and edges with unparalleled precision. Moreover, we introduce a graph abstraction mechanism based on the Information Bottleneck Theory, which compresses temporal interaction graphs while preserving essential patterns. Extensive experiments demonstrate GUARDIAN&#39;s effectiveness in safeguarding LLM multi-agent collaborations against diverse safety vulnerabilities, achieving state-of-the-art accuracy with efficient resource utilization.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19234"
    },
    "27b644d83a0798694220ae3a80675f37": {
        "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents",
        "authors": [
            "Ye Ye"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19436",
        "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often hallucinating, repeating actions, or misinterpreting user corrections -- due to reliance on linear, unstructured context. This fragility stems from the lack of persistent memory to track evolving goals and task dependencies, undermining trust in autonomous agents. We introduce the Task Memory Engine (TME), a modular memory controller that transforms existing LLMs into robust, revision-aware agents without fine-tuning. TME implements a spatial memory framework that replaces flat context with graph-based structures to support consistent, multi-turn reasoning. Departing from linear concatenation and ReAct-style prompting, TME builds a dynamic task graph -- either a tree or directed acyclic graph (DAG) -- to map user inputs to subtasks, align them with prior context, and enable dependency-tracked revisions. Its Task Representation and Intent Management (TRIM) component models task semantics and user intent to ensure accurate interpretation. Across four multi-turn scenarios-trip planning, cooking, meeting scheduling, and shopping cart editing -- TME eliminates 100% of hallucinations and misinterpretations in three tasks, and reduces hallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns, outperforming ReAct. TME&#39;s modular design supports plug-and-play deployment and domain-specific customization, adaptable to both personal assistants and enterprise automation. We release TME&#39;s codebase, benchmarks, and components as open-source resources, enabling researchers to develop reliable LLM agents. TME&#39;s scalable architecture addresses a critical gap in agent performance across complex, interactive settings.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19436"
    },
    "7ae0e54e209ecd4b2e12e1088d421865": {
        "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI",
        "authors": [
            "Ranjan Sapkota",
            "Konstantinos I. Roumeliotis",
            "Manoj Karkee"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19443",
        "abstract": "This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19443"
    },
    "416f87ad44fe4efe4b6d4e37ef78cf1d": {
        "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows",
        "authors": [
            "Qiushi Sun",
            "Zhoumianze Liu",
            "Chang Ma",
            "Zichen Ding",
            "Fangzhi Xu",
            "Zhangyue Yin",
            "Haiteng Zhao",
            "Zhenyu Wu",
            "Kanzhi Cheng",
            "Zhaoyang Liu",
            "Jianing Wang",
            "Qintong Li",
            "Xiangru Tang",
            "Tianbao Xie",
            "Xiachong Feng",
            "Xiang Li",
            "Ben Kao",
            "Wenhai Wang",
            "Biqing Qi",
            "Lingpeng Kong",
            "Zhiyong Wu"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19897",
        "abstract": "Large Language Models (LLMs) have extended their impact beyond Natural Language Processing, substantially fostering the development of interdisciplinary research. Recently, various LLM-based agents have been developed to assist scientific discovery progress across multiple aspects and domains. Among these, computer-using agents, capable of interacting with operating systems as humans do, are paving the way to automated scientific problem-solving and addressing routines in researchers&#39; workflows. Recognizing the transformative potential of these agents, we introduce ScienceBoard, which encompasses two complementary contributions: (i) a realistic, multi-domain environment featuring dynamic and visually rich scientific workflows with integrated professional software, where agents can autonomously interact via different interfaces to accelerate complex research tasks and experiments; and (ii) a challenging benchmark of 169 high-quality, rigorously validated real-world tasks curated by humans, spanning scientific-discovery workflows in domains such as biochemistry, astronomy, and geoinformatics. Extensive evaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude 3.7, UI-TARS) show that, despite some promising results, they still fall short of reliably assisting scientists in complex workflows, achieving only a 15% overall success rate. In-depth analysis further provides valuable insights for addressing current agent limitations and more effective design principles, paving the way to build more capable agents for scientific discovery. Our code, environment, and benchmark are at https://qiushisun.github.io/ScienceBoard-Home/.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19897"
    },
    "1e5ff5fb094d22b8297c5d64bb57e0ba": {
        "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
        "authors": [
            "Hui Chen",
            "Miao Xiong",
            "Yujie Lu",
            "Wei Han",
            "Ailin Deng",
            "Yufei He",
            "Jiaying Wu",
            "Yibo Li",
            "Yue Liu",
            "Bryan Hooi"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19955",
        "abstract": "Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19955"
    },
    "1b32c71ec5b7262ca4e1a2f197ed0e19": {
        "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents",
        "authors": [
            "Tao Wu",
            "Jingyuan Chen",
            "Wang Lin",
            "Mengze Li",
            "Yumeng Zhu",
            "Ang Li",
            "Kun Kuang",
            "Fei Wu"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.19997",
        "abstract": "Large language models (LLMs) are revolutionizing education, with LLM-based agents playing a key role in simulating student behavior. A major challenge in student simulation is modeling the diverse learning patterns of students at various cognitive levels. However, current LLMs, typically trained as ``helpful assistants&#39;&#39;, target at generating perfect responses. As a result, they struggle to simulate students with diverse cognitive abilities, as they often produce overly advanced answers, missing the natural imperfections that characterize student learning and resulting in unrealistic simulations. To address this issue, we propose a training-free framework for student simulation. We begin by constructing a cognitive prototype for each student using a knowledge graph, which captures their understanding of concepts from past learning records. This prototype is then mapped to new tasks to predict student performance. Next, we simulate student solutions based on these predictions and iteratively refine them using a beam search method to better replicate realistic mistakes. To validate our approach, we construct the \\texttt{Student\\_100} dataset, consisting of $100$ students working on Python programming and $5,000$ learning records. Experimental results show that our method consistently outperforms baseline models, achieving $100\\%$ improvement in simulation accuracy.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.19997"
    },
    "1632c5515cfddf0e24553535b5eae935": {
        "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
        "authors": [
            "Le Zhang",
            "Bo Wang",
            "Xipeng Qiu",
            "Siva Reddy",
            "Aishwarya Agrawal"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20046",
        "abstract": "We present REARANK, a large language model (LLM)-based listwise reasoning reranking agent. REARANK explicitly reasons before reranking, significantly improving both performance and interpretability. Leveraging reinforcement learning and data augmentation, REARANK achieves substantial improvements over baseline models across popular information retrieval benchmarks, notably requiring only 179 annotated samples. Built on top of Qwen2.5-7B, our REARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and out-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT benchmarks. These results underscore the effectiveness of our approach and highlight how reinforcement learning can enhance LLM reasoning capabilities in reranking.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20046"
    },
    "766fb0132b7adc6908cbd8b3fc2e9358": {
        "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent",
        "authors": [
            "Jiahao Qiu",
            "Fulian Xiao",
            "Yimin Wang",
            "Yuchen Mao",
            "Yijia Chen",
            "Xinzhe Juan",
            "Siran Wang",
            "Xuan Qi",
            "Tongcheng Zhang",
            "Zixin Yao",
            "Jiacheng Guo",
            "Yifu Lu",
            "Charles Argon",
            "Jundi Cui",
            "Daixin Chen",
            "Junran Zhou",
            "Shuyao Zhou",
            "Zhanpeng Zhou",
            "Ling Yang",
            "Shilong Liu",
            "Hongru Wang",
            "Kaixuan Huang",
            "Xun Jiang",
            "Yuming Cao",
            "Yue Chen",
            "Yunfei Chen",
            "Zhengyi Chen",
            "Ruowei Dai",
            "Mengqiu Deng",
            "Jiye Fu",
            "Yunting Gu",
            "Zijie Guan",
            "Zirui Huang",
            "Xiaoyan Ji",
            "Yumeng Jiang",
            "Delong Kong",
            "Haolong Li",
            "Jiaqi Li",
            "Ruipeng Li",
            "Tianze Li",
            "Zhuoran Li",
            "Haixia Lian",
            "Mengyue Lin",
            "Xudong Liu",
            "Jiayi Lu",
            "Jinghan Lu",
            "Wanyu Luo",
            "Ziyue Luo",
            "Zihao Pu",
            "Zhi Qiao",
            "Ruihuan Ren",
            "Liang Wan",
            "Ruixiang Wang",
            "Tianhui Wang",
            "Yang Wang",
            "Zeyu Wang",
            "Zihua Wang",
            "Yujia Wu",
            "Zhaoyi Wu",
            "Hao Xin",
            "Weiao Xing",
            "Ruojun Xiong",
            "Weijie Xu",
            "Yao Shu",
            "Yao Xiao",
            "Xiaorui Yang",
            "Yuchen Yang",
            "Nan Yi",
            "Jiadong Yu",
            "Yangyuxuan Yu",
            "Huiting Zeng",
            "Danni Zhang",
            "Yunjie Zhang",
            "Zhaoyu Zhang",
            "Zhiheng Zhang",
            "Xiaofeng Zheng",
            "Peirong Zhou",
            "Linyan Zhong",
            "Xiaoyin Zong",
            "Ying Zhao",
            "Zhenxin Chen",
            "Lin Ding",
            "Xiaoyu Gao",
            "Bingbing Gong",
            "Yichao Li",
            "Yang Liao",
            "Guang Ma",
            "Tianyuan Ma",
            "Xinrui Sun",
            "Tianyi Wang",
            "Han Xia",
            "Ruobing Xian",
            "Gen Ye",
            "Tengfei Yu",
            "Wentao Zhang",
            "Yuxi Wang",
            "Xi Gao",
            "Mengdi Wang"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20246",
        "abstract": "Recent advances in large language models (LLMs) have led to remarkable progress across domains, yet their capabilities in the humanities, particularly history, remain underexplored. Historical reasoning poses unique challenges for AI, involving multimodal source interpretation, temporal inference, and cross-linguistic analysis. While general-purpose agents perform well on many existing benchmarks, they lack the domain-specific expertise required to engage with historical materials and questions. To address this gap, we introduce HistBench, a new benchmark of 414 high-quality questions designed to evaluate AI&#39;s capacity for historical reasoning and authored by more than 40 expert contributors. The tasks span a wide range of historical problems-from factual retrieval based on primary sources to interpretive analysis of manuscripts and images, to interdisciplinary challenges involving archaeology, linguistics, or cultural history. Furthermore, the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions. Finding the poor performance of LLMs and other agents on HistBench, we further present HistAgent, a history-specific agent equipped with carefully designed tools for OCR, translation, archival search, and image understanding in History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of 27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online search and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%) and Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These results highlight the limitations of existing LLMs and generalist agents and demonstrate the advantages of HistAgent for historical reasoning.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20246"
    },
    "1bbdda79cf981b9adce98fe8d6424e59": {
        "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents",
        "authors": [
            "Ibragim Badertdinov",
            "Alexander Golubev",
            "Maksim Nekrashevich",
            "Anton Shevtsov",
            "Simon Karasik",
            "Andrei Andriushchenko",
            "Maria Trofimova",
            "Daria Litvintseva",
            "Boris Yangel"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20411",
        "abstract": "LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20411"
    },
    "3132f0785acbea9287cf9ebc311693c0": {
        "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting",
        "authors": [
            "Ana Rita Ortigoso",
            "Gabriel Vieira",
            "Daniel Fuentes",
            "Luis Frazão",
            "Nuno Costa",
            "António Pereira"
        ],
        "date": "2025/05/26",
        "pdf": "http://arxiv.org/pdf/2505.20521",
        "abstract": "This paper presents Project Riley, a novel multimodal and multi-model conversational AI architecture oriented towards the simulation of reasoning influenced by emotional states. Drawing inspiration from Pixar&#39;s Inside Out, the system comprises five distinct emotional agents - Joy, Sadness, Fear, Anger, and Disgust - that engage in structured multi-round dialogues to generate, criticise, and iteratively refine responses. A final reasoning mechanism synthesises the contributions of these agents into a coherent output that either reflects the dominant emotion or integrates multiple perspectives. The architecture incorporates both textual and visual large language models (LLMs), alongside advanced reasoning and self-refinement processes. A functional prototype was deployed locally in an offline environment, optimised for emotional expressiveness and computational efficiency. From this initial prototype, another one emerged, called Armando, which was developed for use in emergency contexts, delivering emotionally calibrated and factually accurate information through the integration of Retrieval-Augmented Generation (RAG) and cumulative context tracking. The Project Riley prototype was evaluated through user testing, in which participants interacted with the chatbot and completed a structured questionnaire assessing three dimensions: Emotional Appropriateness, Clarity and Utility, and Naturalness and Human-likeness. The results indicate strong performance in structured scenarios, particularly with respect to emotional alignment and communicative clarity.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.20521"
    },
    "eae0634774987cde5e9400d30709739f": {
        "title": "Creativity in LLM-based Multi-Agent Systems: A Survey",
        "authors": [
            "Yi-Cheng Lin",
            "Kang-Chieh Chen",
            "Zhe-Yan Li",
            "Tzu-Heng Wu",
            "Tzu-Hsuan Wu",
            "Kuan-Yu Chen",
            "Hung-yi Lee",
            "Yun-Nung Chen"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21116",
        "abstract": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of \\emph{creativity}, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated. This is the first survey dedicated to creativity in MAS. We focus on text and image generation tasks, and present: (1) a taxonomy of agent proactivity and persona design; (2) an overview of generation techniques, including divergent exploration, iterative refinement, and collaborative synthesis, as well as relevant datasets and evaluation metrics; and (3) a discussion of key challenges, such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and the lack of unified benchmarks. This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21116"
    },
    "4b4c3dbc0819d857fccfc4279ee04593": {
        "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools",
        "authors": [
            "Zhucong Li",
            "Bowei Zhang",
            "Jin Xiao",
            "Zhijian Zhou",
            "Fenglei Cao",
            "Jiaqing Liang",
            "Yuan Qi"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21569",
        "abstract": "Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.4open.science/r/ChemHAS-01E4/README.md.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Application",
                "Chemistry"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21569"
    },
    "62a633ac6e38704abcf06a5b3932c5c9": {
        "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation",
        "authors": [
            "Tharindu Kumarage",
            "Ninareh Mehrabi",
            "Anil Ramakrishna",
            "Xinyan Zhao",
            "Richard Zemel",
            "Kai-Wei Chang",
            "Aram Galstyan",
            "Rahul Gupta",
            "Charith Peris"
        ],
        "date": "2025/05/27",
        "pdf": "http://arxiv.org/pdf/2505.21784",
        "abstract": "Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: https://huggingface.co/datasets/AmazonScience/AIDSAFE",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21784"
    },
    "c5dcd6c9f21aa7ff3e19c7560f7d2237": {
        "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning",
        "authors": [
            "Shikhhar Siingh",
            "Abhinav Rawat",
            "Chitta Baral",
            "Vivek Gupta"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.21863",
        "abstract": "Publicly significant images from events hold valuable contextual information, crucial for journalism and education. However, existing methods often struggle to extract this relevance accurately. To address this, we introduce GETReason (Geospatial Event Temporal Reasoning), a framework that moves beyond surface-level image descriptions to infer deeper contextual meaning. We propose that extracting global event, temporal, and geospatial information enhances understanding of an image&#39;s significance. Additionally, we introduce GREAT (Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric for evaluating reasoning-based image understanding. Our layered multi-agent approach, assessed using a reasoning-weighted metric, demonstrates that meaningful insights can be inferred, effectively linking images to their broader event context.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21863"
    },
    "524764bedc9740df1c76628f26682729": {
        "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents",
        "authors": [
            "Ziyun Zhang",
            "Xinyi Liu",
            "Xiaoyi Zhang",
            "Jun Wang",
            "Gang Chen",
            "Yan Lu"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.21964",
        "abstract": "External knowledge has played a crucial role in the recent development of computer use agents. We identify a critical knowledge-execution gap: retrieved knowledge often fails to translate into effective real-world task execution. Our analysis shows even 90\\% correct knowledge yields only 41\\% execution success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a Retrace Stage that extracts faithful objective action sequences from actual agent-environment interactions, and a Critique Stage that refines existing knowledge by comparing these sequences against external references. We conduct comprehensive experiments on the OSWorld benchmark with the state-of-the-art Agent S2. Our results demonstrate that UI-Evol not only significantly boosts task performance but also addresses a previously overlooked issue of high behavioral standard deviation in computer use agents, leading to superior performance on computer use tasks and substantially improved agent reliability.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.21964"
    },
    "e49a01237c69cc34eda66f104cbf8b80": {
        "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents",
        "authors": [
            "Michael Kirchhof",
            "Gjergji Kasneci",
            "Enkelejda Kasneci"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.22655",
        "abstract": "Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.22655"
    },
    "488b60a241d20e3efaa561f65fce9baf": {
        "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models",
        "authors": [
            "Jinchuan Zhang",
            "Lu Yin",
            "Yan Zhou",
            "Songlin Hu"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23020",
        "abstract": "The acquisition of agentic capabilities has transformed LLMs from &#34;knowledge providers&#34; to &#34;action executors&#34;, a trend that while expanding LLMs&#39; capability boundaries, significantly increases their susceptibility to malicious use. Previous work has shown that current LLM-based agents execute numerous malicious tasks even without being attacked, indicating a deficiency in agentic use safety alignment during the post-training phase. To address this gap, we propose AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis. By instantiating these behavior chains in simulated environments with diverse tool instances, our framework enables the generation of highly authentic and executable instructions while capturing complex multi-step dynamics. The framework further ensures model utility by proportionally synthesizing benign instructions through non-malicious interpretations of behavior chains, precisely calibrating the boundary between helpfulness and harmlessness. Evaluation results on AgentHarm demonstrate that fine-tuning three families of open-source models using our method substantially improves their safety (35.8% to 79.5% improvement) while minimally impacting or even positively enhancing their helpfulness, outperforming various prompting methods. The dataset and code have both been open-sourced.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23020"
    },
    "f25d9962f58bf0267c5a6e067d81d071": {
        "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
        "authors": [
            "Manish Shetty",
            "Naman Jain",
            "Jinjian Liu",
            "Vijay Kethanaboyina",
            "Koushik Sen",
            "Ion Stoica"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23671",
        "abstract": "Developing high-performance software is a complex task that requires specialized expertise. We introduce GSO, a benchmark for evaluating language models&#39; capabilities in developing high-performance software. We develop an automated pipeline that generates and executes performance tests to analyze repository commit histories to identify 102 challenging optimization tasks across 10 codebases, spanning diverse domains and programming languages. An agent is provided with a codebase and performance test as a precise specification, and tasked to improve the runtime efficiency, which is measured against the expert developer optimization. Our quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate, with limited improvements even with inference-time scaling. Our qualitative analysis identifies key failure modes, including difficulties with low-level languages, practicing lazy optimization strategies, and challenges in accurately localizing bottlenecks. We release the code and artifacts of our benchmark along with agent trajectories to enable future research.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23671"
    },
    "1fe19c24fee2dfb53dee49a41edaed78": {
        "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation",
        "authors": [
            "Mengkang Hu",
            "Yuhang Zhou",
            "Wendong Fan",
            "Yuzhou Nie",
            "Bowei Xia",
            "Tao Sun",
            "Ziyu Ye",
            "Zhaoxuan Jin",
            "Yingru Li",
            "Qiguang Chen",
            "Zeyu Zhang",
            "Yifeng Wang",
            "Qianshuo Ye",
            "Bernard Ghanem",
            "Ping Luo",
            "Guohao Li"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2505.23885",
        "abstract": "Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI&#39;s Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.23885"
    },
    "7555d479d81ae0021ede71a03baac2bd": {
        "title": "An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring",
        "authors": [
            "Sana Ebrahimi",
            "Mohsen Dehghankar",
            "Abolfazl Asudeh"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24239",
        "abstract": "While multi-agent LLM systems show strong capabilities in various domains, they are highly vulnerable to adversarial and low-performing agents. To resolve this issue, in this paper, we introduce a general and adversary-resistant multi-agent LLM framework based on credibility scoring. We model the collaborative query-answering process as an iterative game, where the agents communicate and contribute to a final system output. Our system associates a credibility score that is used when aggregating the team outputs. The credibility scores are learned gradually based on the past contributions of each agent in query answering. Our experiments across multiple tasks and settings demonstrate our system&#39;s effectiveness in mitigating adversarial influence and enhancing the resilience of multi-agent cooperation, even in the adversary-majority settings.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24239"
    },
    "ecc1aa08a82ba70c922c51491ba38d86": {
        "title": "Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation",
        "authors": [
            "Yucheng Zhou",
            "Jiahao Yuan",
            "Qianning Wang"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24787",
        "abstract": "Recent advancements in text-to-image (T2I) generation have enabled models to produce high-quality images from textual descriptions. However, these models often struggle with complex instructions involving multiple objects, attributes, and spatial relationships. Existing benchmarks for evaluating T2I models primarily focus on general text-image alignment and fail to capture the nuanced requirements of complex, multi-faceted prompts. Given this gap, we introduce LongBench-T2I, a comprehensive benchmark specifically designed to evaluate T2I models under complex instructions. LongBench-T2I consists of 500 intricately designed prompts spanning nine diverse visual evaluation dimensions, enabling a thorough assessment of a model&#39;s ability to follow complex instructions. Beyond benchmarking, we propose an agent framework (Plan2Gen) that facilitates complex instruction-driven image generation without requiring additional model training. This framework integrates seamlessly with existing T2I models, using large language models to interpret and decompose complex prompts, thereby guiding the generation process more effectively. As existing evaluation metrics, such as CLIPScore, fail to adequately capture the nuances of complex instructions, we introduce an evaluation toolkit that automates the quality assessment of generated images using a set of multi-dimensional metrics. The data and code are released at https://github.com/yczhou001/LongBench-T2I.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24787"
    },
    "0c67b1a1391a1990312929036cde0b57": {
        "title": "Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks",
        "authors": [
            "Tajamul Ashraf",
            "Amal Saqib",
            "Hanan Ghani",
            "Muhra AlMahri",
            "Yuhao Li",
            "Noor Ahsan",
            "Umair Nawaz",
            "Jean Lahoud",
            "Hisham Cholakkal",
            "Mubarak Shah",
            "Philip Torr",
            "Fahad Shahbaz Khan",
            "Rao Muhammad Anwer",
            "Salman Khan"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24876",
        "abstract": "Deep reasoning is fundamental for solving complex tasks, especially in vision-centric scenarios that demand sequential, multimodal understanding. However, existing benchmarks typically evaluate agents with fully synthetic, single-turn queries, limited visual modalities, and lack a framework to assess reasoning quality over multiple steps as required in real-world settings. To address this, we introduce Agent-X, a large-scale benchmark for evaluating vision-centric agents multi-step and deep reasoning capabilities in real-world, multimodal settings. Agent- X features 828 agentic tasks with authentic visual contexts, including images, multi-image comparisons, videos, and instructional text. These tasks span six major agentic environments: general visual reasoning, web browsing, security and surveillance, autonomous driving, sports, and math reasoning. Our benchmark requires agents to integrate tool use with explicit, stepwise decision-making in these diverse settings. In addition, we propose a fine-grained, step-level evaluation framework that assesses the correctness and logical coherence of each reasoning step and the effectiveness of tool usage throughout the task. Our results reveal that even the best-performing models, including GPT, Gemini, and Qwen families, struggle to solve multi-step vision tasks, achieving less than 50% full-chain success. These findings highlight key bottlenecks in current LMM reasoning and tool-use capabilities and identify future research directions in vision-centric agentic reasoning models. Our data and code are publicly available at https://github.com/mbzuai-oryx/Agent-X",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24876"
    },
    "b324eb5b9dc281153ad5c0dba8d7c155": {
        "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents",
        "authors": [
            "Yaxin Luo",
            "Zhaoyi Li",
            "Jiacheng Liu",
            "Jiacheng Cui",
            "Xiaohan Zhao",
            "Zhiqiang Shen"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2505.24878",
        "abstract": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the visual reasoning and interaction capabilities of MLLM-powered agents through diverse and dynamic CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225 CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth, which quantifies the number of cognitive and motor steps required to solve each puzzle. Experimental results show that humans consistently achieve near-perfect scores, state-of-the-art MLLM agents struggle significantly, with success rates at most 40.0% by Browser-Use Openai-o3, far below human-level performance, 93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing the limits of current multimodal agents and guiding the development of more robust multimodal reasoning systems. Code and Data are available at this https URL.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Infrastructure",
                "Environment&Platform"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.24878"
    },
    "98012c5a058cc90d027af35e9858996a": {
        "title": "Enhancing LLM Agent Safety via Causal Influence Prompting",
        "authors": [
            "Dongyoon Hahm",
            "Woogyeol Jin",
            "June Suk Choi",
            "Sungsoo Ahn",
            "Kimin Lee"
        ],
        "date": "2025/07/01",
        "pdf": "http://arxiv.org/pdf/2507.00979",
        "abstract": "As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.00979"
    },
    "73a680a307c5c7a78b93dffa46758e9c": {
        "title": "WebDancer: Towards Autonomous Information Seeking Agency",
        "authors": [
            "Jialong Wu",
            "Baixuan Li",
            "Runnan Fang",
            "Wenbiao Yin",
            "Liwen Zhang",
            "Zhengwei Tao",
            "Dingchu Zhang",
            "Zekun Xi",
            "Gang Fu",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2505.22648",
        "abstract": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.",
        "code": "https://github.com/Alibaba-NLP/WebAgent",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2505.22648"
    },
    "ae93fceb73d5b289f185a3713dc4132b": {
        "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation",
        "authors": [
            "Nicolas Bougie",
            "Narimasa Watanabe"
        ],
        "date": "2025/06/26",
        "pdf": "http://arxiv.org/pdf/2506.21805",
        "abstract": "Modeling human behavior in urban environments is fundamental for social science, behavioral studies, and urban planning. Prior work often rely on rigid, hand-crafted rules, limiting their ability to simulate nuanced intentions, plans, and adaptive behaviors. Addressing these challenges, we envision an urban simulator (CitySim), capitalizing on breakthroughs in human-level intelligence exhibited by large language models. In CitySim, agents generate realistic daily schedules using a recursive value-driven approach that balances mandatory activities, personal habits, and situational factors. To enable long-term, lifelike simulations, we endow agents with beliefs, long-term goals, and spatial memory for navigation. CitySim exhibits closer alignment with real humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments by modeling tens of thousands of agents and evaluating their collective behaviors under various real-world scenarios, including estimating crowd density, predicting place popularity, and assessing well-being. Our results highlight CitySim as a scalable, flexible testbed for understanding and forecasting urban phenomena.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21805"
    },
    "25f5590070cfe0530c8df6d681654bb4": {
        "title": "MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility",
        "authors": [
            "Yexiao He",
            "Ang Li",
            "Boyi Liu",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "date": "2025/05/30",
        "pdf": "http://arxiv.org/pdf/2506.00235",
        "abstract": "Healthcare decision-making represents one of the most challenging domains for Artificial Intelligence (AI), requiring the integration of diverse knowledge sources, complex reasoning, and various external analytical tools. Current AI systems often rely on either task-specific models, which offer limited adaptability, or general language models without grounding with specialized external knowledge and tools. We introduce MedOrch, a novel framework that orchestrates multiple specialized tools and reasoning agents to provide comprehensive medical decision support. MedOrch employs a modular, agent-based architecture that facilitates the flexible integration of domain-specific tools without altering the core system. Furthermore, it ensures transparent and traceable reasoning processes, enabling clinicians to meticulously verify each intermediate step underlying the system&#39;s recommendations. We evaluate MedOrch across three distinct medical applications: Alzheimer&#39;s disease diagnosis, chest X-ray interpretation, and medical visual question answering, using authentic clinical datasets. The results demonstrate MedOrch&#39;s competitive performance across these diverse medical tasks. Notably, in Alzheimer&#39;s disease diagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the state-of-the-art baseline by over four percentage points. For predicting Alzheimer&#39;s disease progression, it attains a 50.35% accuracy, marking a significant improvement. In chest X-ray analysis, MedOrch exhibits superior performance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover, in complex multimodal visual question answering (Image+Table), MedOrch achieves an accuracy of 54.47%. These findings underscore MedOrch&#39;s potential to advance healthcare AI by enabling reasoning-driven tool utilization for multimodal medical data processing and supporting intricate cognitive tasks in clinical decision-making.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00235"
    },
    "9c41b7706bbc8488eef6875c9a8216d7": {
        "title": "Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems",
        "authors": [
            "Zherui Li",
            "Yan Mi",
            "Zhenhong Zhou",
            "Houcheng Jiang",
            "Guibin Zhang",
            "Kun Wang",
            "Junfeng Fang"
        ],
        "date": "2025/05/31",
        "pdf": "http://arxiv.org/pdf/2506.00509",
        "abstract": "Large Language Model-based Multi-Agent Systems (MASs) have demonstrated strong advantages in addressing complex real-world tasks. However, due to the introduction of additional attack surfaces, MASs are particularly vulnerable to misinformation injection. To facilitate a deeper understanding of misinformation propagation dynamics within these systems, we introduce MisinfoTask, a novel dataset featuring complex, realistic tasks designed to evaluate MAS robustness against such threats. Building upon this, we propose ARGUS, a two-stage, training-free defense framework leveraging goal-aware reasoning for precise misinformation rectification within information flows. Our experiments demonstrate that in challenging misinformation scenarios, ARGUS exhibits significant efficacy across various injection attacks, achieving an average reduction in misinformation toxicity of approximately 28.17% and improving task success rates under attack by approximately 10.33%. Our code and dataset is available at: https://github.com/zhrli324/ARGUS.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00509"
    },
    "753f3b5c683afb08a957c8013a56eece": {
        "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
        "authors": [
            "Ruihan Yang",
            "Yikai Zhang",
            "Aili Chen",
            "Xintao Wang",
            "Siyu Yuan",
            "Jiangjie Chen",
            "Deqing Yang",
            "Yanghua Xiao"
        ],
        "date": "2025/05/31",
        "pdf": "http://arxiv.org/pdf/2506.00539",
        "abstract": "Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an exponentially large action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose ARIA, a method that Aggregates Rewards in Intention space to enable efficient and effective language Agents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering better policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces policy gradient variance, but also delivers substantial performance gains of an average of 9.95% across four downstream tasks, consistently outperforming offline and online RL baselines.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00539"
    },
    "92a8c1bf5a1f77baf1af5de8850e53bf": {
        "title": "PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements",
        "authors": [
            "Petros Raptopoulos",
            "Giorgos Filandrianos",
            "Maria Lymperaiou",
            "Giorgos Stamou"
        ],
        "date": "2025/05/31",
        "pdf": "http://arxiv.org/pdf/2506.00608",
        "abstract": "Contract review is a complex and time-intensive task that typically demands specialized legal expertise, rendering it largely inaccessible to non-experts. Moreover, legal interpretation is rarely straightforward-ambiguity is pervasive, and judgments often hinge on subjective assessments. Compounding these challenges, contracts are usually confidential, restricting their use with proprietary models and necessitating reliance on open-source alternatives. To address these challenges, we introduce PAKTON: a fully open-source, end-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is designed to handle the complexities of contract analysis through collaborative agent workflows and a novel retrieval-augmented generation (RAG) component, enabling automated legal document review that is more accessible, adaptable, and privacy-preserving. Experiments demonstrate that PAKTON outperforms both general-purpose and pretrained models in predictive accuracy, retrieval performance, explainability, completeness, and grounded justifications as evaluated through a human study and validated with automated metrics.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00608"
    },
    "b59d03ffff8846b40c93f2944739175b": {
        "title": "DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments",
        "authors": [
            "Chiyu Zhang",
            "Marc-Alexandre Cote",
            "Michael Albada",
            "Anush Sankaran",
            "Jack W. Stokes",
            "Tong Wang",
            "Amir Abdi",
            "William Blum",
            "Muhammad Abdul-Mageed"
        ],
        "date": "2025/05/31",
        "pdf": "http://arxiv.org/pdf/2506.00739",
        "abstract": "Large language model (LLM) agents have shown impressive capabilities in human language comprehension and reasoning, yet their potential in cybersecurity remains underexplored. We introduce DefenderBench, a practical, open-source toolkit for evaluating language agents across offense, defense, and cybersecurity knowledge-based tasks. DefenderBench includes environments for network intrusion, malicious content detection, code vulnerability analysis, and cybersecurity knowledge assessment. It is intentionally designed to be affordable and easily accessible for researchers while providing fair and rigorous assessment. We benchmark several state-of-the-art (SoTA) and popular LLMs, including both open- and closed-weight models, using a standardized agentic framework. Our results show that Claude-3.7-sonnet performs best with a DefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40, while the best open-weight model, Llama 3.3 70B, is not far behind with a DefenderBench score of 71.81. DefenderBench&#39;s modular design allows seamless integration of custom LLMs and tasks, promoting reproducibility and fair comparisons. An anonymized version of DefenderBench is available at https://github.com/microsoft/DefenderBench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00739"
    },
    "3428ab7267c5cffcd221c724bbe80437": {
        "title": "Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models",
        "authors": [
            "Yiwen Jiang",
            "Deval Mehta",
            "Wei Feng",
            "Zongyuan Ge"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01334",
        "abstract": "Concept Bottleneck Models (CBMs) decompose image classification into a process governed by interpretable, human-readable concepts. Recent advances in CBMs have used Large Language Models (LLMs) to generate candidate concepts. However, a critical question remains: What is the optimal number of concepts to use? Current concept banks suffer from redundancy or insufficient coverage. To address this issue, we introduce a dynamic, agent-based approach that adjusts the concept bank in response to environmental feedback, optimizing the number of concepts for sufficiency yet concise coverage. Moreover, we propose Conditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in traditional CBMs&#39; concept scoring mechanisms. It enhances the accuracy of assessing each concept&#39;s contribution to classification tasks and feature an editable matrix that allows LLMs to correct concept scores that conflict with their internal knowledge. Our evaluations across 6 datasets show that our method not only improves classification accuracy by 6% but also enhances interpretability assessments by 30%.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01334"
    },
    "064aae0c4ba332ae0a7340df81af8853": {
        "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents",
        "authors": [
            "Manan Suri",
            "Puneet Mathur",
            "Nedim Lipka",
            "Franck Dernoncourt",
            "Ryan A. Rossi",
            "Vivek Gupta",
            "Dinesh Manocha"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01344",
        "abstract": "Flowcharts are a critical tool for visualizing decision-making processes. However, their non-linear structure and complex visual-textual relationships make it challenging to interpret them using LLMs, as vision-language models frequently hallucinate nonexistent connections and decision paths when analyzing these diagrams. This leads to compromised reliability for automated flowchart processing in critical domains such as logistics, health, and engineering. We introduce the task of Fine-grained Flowchart Attribution, which traces specific components grounding a flowchart referring LLM response. Flowchart Attribution ensures the verifiability of LLM predictions and improves explainability by linking generated responses to the flowchart&#39;s structure. We propose FlowPathAgent, a neurosymbolic agent that performs fine-grained post hoc attribution through graph-based reasoning. It first segments the flowchart, then converts it into a structured symbolic graph, and then employs an agentic approach to dynamically interact with the graph, to generate attribution paths. Additionally, we present FlowExplainBench, a novel benchmark for evaluating flowchart attributions across diverse styles, domains, and question types. Experimental results show that FlowPathAgent mitigates visual hallucinations in LLM answers over flowchart QA, outperforming strong baselines by 10-14% on our proposed FlowExplainBench dataset.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01344"
    },
    "0e8616656455eed24c7959829663bbc9": {
        "title": "FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents",
        "authors": [
            "Bobo Li",
            "Yuheng Wang",
            "Hao Fei",
            "Juncheng Li",
            "Wei Ji",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01520",
        "abstract": "Online form filling is a common yet labor-intensive task involving extensive keyboard and mouse interactions. Despite the long-standing vision of automating this process with &#34;one click&#34;, existing tools remain largely rule-based and lack generalizable, generative capabilities. Recent advances in Multimodal Large Language Models (MLLMs) have enabled promising agents for GUI-related tasks in general-purpose scenarios. However, they struggle with the unique challenges of form filling, such as flexible layouts and the difficulty of aligning textual instructions with on-screen fields. To bridge this gap, we formally define the form-filling task and propose FormFactory, an interactive benchmarking suite comprising a web-based interface, backend evaluation module, and carefully constructed dataset. Our benchmark covers diverse real-world scenarios, incorporates various field formats, and simulates high-fidelity form interactions. We conduct a comprehensive evaluation of state-of-the-art MLLMs and observe that no model surpasses 5% accuracy, underscoring the inherent difficulty of the task. These findings also reveal significant limitations in current models&#39; visual layout reasoning and field-value alignment abilities. We hope our benchmark can serve as a stepping stone for further research into robust, practical form-filling agents.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01520"
    },
    "247b103c44500d57ee8c1257d5ef2431": {
        "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework",
        "authors": [
            "Wenhao Liu",
            "Zhenyi Lu",
            "Xinyu Hu",
            "Jierui Zhang",
            "Dailin Li",
            "Jiacheng Cen",
            "Huilin Cao",
            "Haiteng Wang",
            "Yuhan Li",
            "Kun Xie",
            "Dandan Li",
            "Pei Zhang",
            "Chengbo Zhang",
            "Yuxiang Ren",
            "Xiaohong Huang",
            "Yan Ma"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01531",
        "abstract": "High-quality math datasets are crucial for advancing the reasoning abilities of large language models (LLMs). However, existing datasets often suffer from three key issues: outdated and insufficient challenging content, neglecting human-like reasoning, and limited reliability due to single-LLM generation. To address these, we introduce STORM-BORN, an ultra-challenging dataset of mathematical derivations sourced from cutting-edge academic papers, which includes dense human-like approximations and heuristic cues. To ensure the reliability and quality, we propose a novel human-in-the-loop, multi-agent data generation framework, integrating reasoning-dense filters, multi-agent collaboration, and human mathematicians&#39; evaluations. We curated a set of 2,000 synthetic samples and deliberately selected the 100 most difficult problems. Even most advanced models like GPT-o1 solved fewer than 5% of them. Fine-tuning on STORM-BORN boosts accuracy by 7.84% (LLaMA3-8B) and 9.12% (Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN provides both a high-difficulty benchmark and a human-like reasoning training resource. Our code and dataset are publicly available at https://github.com/lwhere/STORM-BORN.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01531"
    },
    "4c469c0de9c812169b1c749d7e3a17a0": {
        "title": "Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning",
        "authors": [
            "Yihong Tang",
            "Kehai Chen",
            "Muyun Yang",
            "Zhengyu Niu",
            "Jing Li",
            "Tiejun Zhao",
            "Min Zhang"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01748",
        "abstract": "The advancement of Large Language Models (LLMs) has spurred significant interest in Role-Playing Agents (RPAs) for applications such as emotional companionship and virtual interaction. However, recent RPAs are often built on explicit dialogue data, lacking deep, human-like internal thought processes, resulting in superficial knowledge and style expression. While Large Reasoning Models (LRMs) can be employed to simulate character thought, their direct application is hindered by attention diversion (i.e., RPAs forget their role) and style drift (i.e., overly formal and rigid reasoning rather than character-consistent reasoning). To address these challenges, this paper introduces a novel Role-Aware Reasoning (RAR) method, which consists of two important stages: Role Identity Activation (RIA) and Reasoning Style Optimization (RSO). RIA explicitly guides the model with character profiles during reasoning to counteract attention diversion, and then RSO aligns reasoning style with the character and scene via LRM distillation to mitigate style drift. Extensive experiments demonstrate that the proposed RAR significantly enhances the performance of RPAs by effectively addressing attention diversion and style drift.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01748"
    },
    "2a61d6c8089707253124a93978bbaff6": {
        "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks",
        "authors": [
            "Atsuyuki Miyai",
            "Zaiying Zhao",
            "Kazuki Egashira",
            "Atsuki Sato",
            "Tatsumi Sunada",
            "Shota Onohara",
            "Hiromasa Yamanishi",
            "Mashiro Toyooka",
            "Kunato Nishina",
            "Ryoma Maeda",
            "Kiyoharu Aizawa",
            "Toshihiko Yamasaki"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01952",
        "abstract": "Powered by a large language model (LLM), a web browsing agent operates web browsers in a human-like manner and offers a highly transparent path toward automating a wide range of everyday tasks. As web agents become increasingly capable and demonstrate proficiency in general browsing tasks, a critical question emerges: Can they go beyond general browsing to robustly handle tasks that are tedious and complex, or chores that humans often avoid doing themselves? In this paper, we introduce WebChoreArena, a new fully reproducible benchmark comprising 532 carefully curated tasks designed to extend the scope of WebArena beyond general browsing to more labor-intensive and tedious tasks. WebChoreArena systematically integrates three key challenges: (i) Massive Memory tasks requiring accurate retrieval of large amounts of information in the observations, (ii) Calculation tasks demanding precise mathematical reasoning, and (iii) Long-Term Memory tasks necessitating long-term memory across multiple webpages. Built on top of the fully reproducible and widely adopted four WebArena simulation environments, WebChoreArena ensures strict reproducibility and enables fair, direct comparisons with the established WebArena benchmark, offering key insights into agent progress. Our experimental results demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7 Sonnet, and Gemini 2.5 Pro, significant improvements in performance are observed on WebChoreArena. These findings suggest that WebChoreArena is well-suited to measure the advancement of state-of-the-art LLMs with greater clarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro, there remains substantial room for improvement compared to WebArena, highlighting the increased challenges posed by WebChoreArena.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01952"
    },
    "a02ce9d614772a43f98b26ab2229e65c": {
        "title": "ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking",
        "authors": [
            "E Fan",
            "Weizong Wang",
            "Tianhan Zhang"
        ],
        "date": "2025/05/28",
        "pdf": "http://arxiv.org/pdf/2506.02019",
        "abstract": "Computational Fluid Dynamics (CFD) is essential for scientific and engineering advancements but is limited by operational complexity and the need for extensive expertise. This paper presents ChatCFD, a large language model-driven pipeline that automates CFD workflows within the OpenFOAM framework. It enables users to configure and execute complex simulations from natural language prompts or published literature with minimal expertise. The innovation is its structured approach to database construction, configuration validation, and error reflection, integrating CFD and OpenFOAM knowledge with general language models to improve accuracy and adaptability. Validation shows ChatCFD can autonomously reproduce published CFD results, handling complex, unseen configurations beyond basic examples, a task challenging for general language models.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02019"
    },
    "a5384068aa0bba3536274bcd691563e1": {
        "title": "LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback",
        "authors": [
            "Thai Hoang",
            "Kung-Hsiang Huang",
            "Shirley Kokane",
            "Jianguo Zhang",
            "Zuxin Liu",
            "Ming Zhu",
            "Jake Grigsby",
            "Tian Lan",
            "Michael S Ryoo",
            "Chien-Sheng Wu",
            "Shelby Heinecke",
            "Huan Wang",
            "Silvio Savarese",
            "Caiming Xiong",
            "Juan Carlos Niebles"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.02298",
        "abstract": "Large Action Models (LAMs) for AI Agents offer incredible potential but face challenges due to the need for high-quality training data, especially for multi-steps tasks that involve planning, executing tool calls, and responding to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive framework designed for online exploration of agentic tasks with high-quality feedback. Our framework features a dynamic task query generator, an extensive collection of tools, and an interactive environment where Large Language Model (LLM) Agents can call tools and receive real-time feedback. This setup enables LLM Agents to explore and solve tasks autonomously, facilitating the discovery of multiple approaches to tackle any given task. The resulting action trajectory data are then used to create high-quality training datasets for LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena, highlight the effectiveness of LAM SIMULATOR: models trained with self-generated datasets using our framework achieve significant performance gains, up to a 49.3\\% improvement over their original baselines. LAM SIMULATOR requires minimal human input during dataset creation, highlighting LAM SIMULATOR&#39;s efficiency and effectiveness in speeding up development of AI agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02298"
    },
    "60fb1fb1ed16d3e182355c21e3e19d07": {
        "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization",
        "authors": [
            "Jeonghun Kang",
            "Soonmok Kwon",
            "Joonseok Lee",
            "Byung-Hak Kim"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.02351",
        "abstract": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking or computer vision-driven event detection -- can identify scoring plays but often miss strategic depth, momentum shifts, and storyline progression. Manual curation remains the gold standard but is resource-intensive and not scalable. We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight summarization that integrates structured sports analytics with natural language reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and Leverage Index -- to quantify play importance, while an LLM module enhances selection based on contextual narrative value. This hybrid approach ensures both quantitative rigor and qualitative richness, surpassing the limitations of purely statistical or vision-based systems. Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines. Though limited in scale, our results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02351"
    },
    "a15fb0fd27f734366ba9d267a1c72707": {
        "title": "Comparative Analysis of AI Agent Architectures for Entity Relationship Classification",
        "authors": [
            "Maryam Berijanian",
            "Kuldeep Singh",
            "Amin Sehati"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.02426",
        "abstract": "Entity relationship classification remains a challenging task in information extraction, especially in scenarios with limited labeled data and complex relational structures. In this study, we conduct a comparative analysis of three distinct AI agent architectures designed to perform relation classification using large language models (LLMs). The agentic architectures explored include (1) reflective self-evaluation, (2) hierarchical task decomposition, and (3) a novel multi-agent dynamic example generation mechanism, each leveraging different modes of reasoning and prompt adaptation. In particular, our dynamic example generation approach introduces real-time cooperative and adversarial prompting. We systematically compare their performance across multiple domains and model backends. Our experiments demonstrate that multi-agent coordination consistently outperforms standard few-shot prompting and approaches the performance of fine-tuned models. These findings offer practical guidance for the design of modular, generalizable LLM-based systems for structured relation extraction. The source codes and dataset are available at https://github.com/maryambrj/ALIEN.git.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02426"
    },
    "d13a64991949a8f6277d3b9b0a8b94d0": {
        "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching",
        "authors": [
            "Liang Yue",
            "Yihong Tang",
            "Kehai Chen",
            "Jie Liu",
            "Min Zhang"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.02689",
        "abstract": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models&#39; instruction-following capabilities and task-specific performance. However, obtaining high-quality fine-tuning data for large models is challenging due to data collection difficulties and high production costs. To address this, we propose MASTER, a novel data augmentation method that enriches original data through interactions among multiple agents with varying cognitive levels. We simulate three pedagogically grounded teaching scenarios, leveraging multi-agent conversations to generate high-quality teacher-student interaction data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5. Experiments show that models fine-tuned with BOOST-QA perform excellently across multiple benchmarks, demonstrating strong multitask generalization. Notably, MASTER significantly improves models&#39; reasoning abilities in complex tasks, providing valuable insights for future research.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02689"
    },
    "4a6997354c9efb6e24112cabf1513c5b": {
        "title": "Adaptive Graph Pruning for Multi-Agent Communication",
        "authors": [
            "Boyi Li",
            "Zhonghan Zhao",
            "Der-Horng Lee",
            "Gaoang Wang"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.02951",
        "abstract": "Large Language Model (LLM) based multi-agent systems have shown remarkable performance in various tasks, especially when enhanced through collaborative communication. However, current methods often rely on a fixed number of agents and static communication structures, limiting their ability to adapt to varying task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a novel task-adaptive multi-agent collaboration framework that jointly optimizes agent quantity (hard-pruning) and communication topology (soft-pruning). Specifically, our method employs a two-stage training strategy: firstly, independently training soft-pruning networks for different agent quantities to determine optimal agent-quantity-specific complete graphs and positional masks across specific tasks; and then jointly optimizing hard-pruning and soft-pruning within a maximum complete graph to dynamically configure the number of agents and their communication topologies per task. Extensive experiments demonstrate that our approach is: (1) High-performing, achieving state-of-the-art results across six benchmarks and consistently generalizes across multiple mainstream LLM architectures, with a increase in performance of $2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized communication topologies tailored to specific tasks, with an extremely high performance in all three task categories (general reasoning, mathematical reasoning, and code generation); (3) Token-economical, having fewer training steps and token consumption at the same time, with a decrease in token consumption of $90\\%+$; and (4) Training-efficient, achieving high performance with very few training steps compared with other methods. The performance will surpass the existing baselines after about ten steps of training under six benchmarks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02951"
    },
    "ef8677bdea6e2120ebf5d57ae93ea032": {
        "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems",
        "authors": [
            "Đorđe Klisura",
            "Astrid R Bernaga Torres",
            "Anna Karen Gárate-Escamilla",
            "Rajesh Roshan Biswal",
            "Ke Yang",
            "Hilal Pataci",
            "Anthony Rios"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.02998",
        "abstract": "Privacy policies inform users about data collection and usage, yet their complexity limits accessibility for diverse populations. Existing Privacy Policy Question Answering (QA) systems exhibit performance disparities across English dialects, disadvantaging speakers of non-standard varieties. We propose a novel multi-agent framework inspired by human-centered design principles to mitigate dialectal biases. Our approach integrates a Dialect Agent, which translates queries into Standard American English (SAE) while preserving dialectal intent, and a Privacy Policy Agent, which refines predictions using domain expertise. Unlike prior approaches, our method does not require retraining or dialect-specific fine-tuning, making it broadly applicable across models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves GPT-4o-mini&#39;s zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from 0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without additional training data. These results highlight the effectiveness of structured agent collaboration in mitigating dialect biases and underscore the importance of designing NLP systems that account for linguistic diversity to ensure equitable access to privacy information.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02998"
    },
    "f6c27037091ea5c9aa9f12c50d99e506": {
        "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers",
        "authors": [
            "Aditya Bharat Soni",
            "Boxuan Li",
            "Xingyao Wang",
            "Valerie Chen",
            "Graham Neubig"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.03011",
        "abstract": "Modern human labor is characterized by specialization; we train for years and develop particular tools that allow us to perform well across a variety of tasks. In addition, AI agents have been specialized for domains such as software engineering, web navigation, and workflow automation. However, this results in agents that are good for one thing but fail to generalize beyond their intended scope. One reason for this is that agent developers provide a highly specialized set of tools or make architectural decisions optimized for a specific use case or benchmark. In this work, we ask the question: what is the minimal set of general tools that can be used to achieve high performance across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist agent built with a modest number of general tools: code editing and execution, web search, as well as multimodal web browsing and file access. Importantly, OpenHands-Versa demonstrates superior or competitive performance over leading specialized agents across three diverse and challenging benchmarks: SWE-Bench Multimodal, GAIA, and The Agent Company, outperforming the best-performing previously published results with absolute improvements in success rate of 9.1, 1.3, and 9.1 points respectively. Further, we show how existing state-of-the-art multi-agent systems fail to generalize beyond their target domains. These results demonstrate the feasibility of developing a generalist agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline for future research.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.03011"
    },
    "9b0a7613572efcf0509e4f4474045449": {
        "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
        "authors": [
            "Qianhui Wu",
            "Kanzhi Cheng",
            "Rui Yang",
            "Chaoyun Zhang",
            "Jianwei Yang",
            "Huiqiang Jiang",
            "Jian Mu",
            "Baolin Peng",
            "Bo Qiao",
            "Reuben Tan",
            "Si Qin",
            "Lars Liden",
            "Qingwei Lin",
            "Huan Zhang",
            "Tong Zhang",
            "Jianbing Zhang",
            "Dongmei Zhang",
            "Jianfeng Gao"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.03143",
        "abstract": "One of the principal challenges in building VLM-powered GUI agents is visual grounding, i.e., localizing the appropriate screen region for action execution based on both the visual content and the textual plans. Most existing work formulates this as a text-based coordinate generation task. However, these approaches suffer from several limitations: weak spatial-semantic alignment, inability to handle ambiguous supervision targets, and a mismatch between the dense nature of screen coordinates and the coarse, patch-level granularity of visual features extracted by models like Vision Transformers. In this paper, we propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its core, GUI-Actor introduces an attention-based action head that learns to align a dedicated &lt;ACTOR&gt; token with all relevant visual patch tokens, enabling the model to propose one or more action regions in a single forward pass. In line with this, we further design a grounding verifier to evaluate and select the most plausible action region from the candidates proposed for action execution. Extensive experiments show that GUI-Actor outperforms prior state-of-the-art methods on multiple GUI action grounding benchmarks, with improved generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7 with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by incorporating the verifier, we find that fine-tuning only the newly introduced action head (~100M parameters for 7B model) while keeping the VLM backbone frozen is sufficient to achieve performance comparable to previous state-of-the-art models, highlighting that GUI-Actor can endow the underlying VLM with effective grounding capabilities without compromising its general-purpose strengths.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.03143"
    },
    "c4a1157fb78d269137afabb2b93ccf12": {
        "title": "Go-Browse: Training Web Agents with Structured Exploration",
        "authors": [
            "Apurva Gandhi",
            "Graham Neubig"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.03533",
        "abstract": "One of the fundamental problems in digital agents is their lack of understanding of their environment. For instance, a web browsing agent may get lost in unfamiliar websites, uncertain what pages must be visited to achieve its goals. To address this, we propose Go-Browse, a method for automatically collecting diverse and realistic web agent data at scale through structured exploration of web environments. Go-Browse achieves efficient exploration by framing data collection as a graph search, enabling reuse of information across exploration episodes. We instantiate our method on the WebArena benchmark, collecting a dataset of 10K successful task-solving trajectories and 40K interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on this dataset achieves a success rate of 21.7% on the WebArena benchmark, beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for sub-10B parameter models by 2.9%.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.03533"
    },
    "62aef61049c016a94953d6e7c330ab31": {
        "title": "Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement",
        "authors": [
            "Xiaofeng Zhou",
            "Heyan Huang",
            "Lizi Liao"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.03541",
        "abstract": "Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques--such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection--struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&amp;R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.03541"
    },
    "32fd828c765a64ae924fd3e2aef8daa9": {
        "title": "AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data",
        "authors": [
            "Sina Rashidian",
            "Nan Li",
            "Jonathan Amar",
            "Jong Ha Lee",
            "Sam Pugh",
            "Eric Yang",
            "Geoff Masterson",
            "Myoung Cha",
            "Yugang Jia",
            "Akhil Vaid"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.04032",
        "abstract": "Background: We present a Patient Simulator that leverages real world patient encounters which cover a broad range of conditions and symptoms to provide synthetic test subjects for development and testing of healthcare agentic models. The simulator provides a realistic approach to patient presentation and multi-turn conversation with a symptom-checking agent. Objectives: (1) To construct and instantiate a Patient Simulator to train and test an AI health agent, based on patient vignettes derived from real EHR data. (2) To test the validity and alignment of the simulated encounters provided by the Patient Simulator to expert human clinical providers. (3) To illustrate the evaluation framework of such an LLM system on the generated realistic, data-driven simulations -- yielding a preliminary assessment of our proposed system. Methods: We first constructed realistic clinical scenarios by deriving patient vignettes from real-world EHR encounters. These vignettes cover a variety of presenting symptoms and underlying conditions. We then evaluate the performance of the Patient Simulator as a simulacrum of a real patient encounter across over 500 different patient vignettes. We leveraged a separate AI agent to provide multi-turn questions to obtain a history of present illness. The resulting multiturn conversations were evaluated by two expert clinicians. Results: Clinicians scored the Patient Simulator as consistent with the patient vignettes in those same 97.7% of cases. The extracted case summary based on the conversation history was 99% relevant. Conclusions: We developed a methodology to incorporate vignettes derived from real healthcare patient data to build a simulation of patient responses to symptom checking agents. The performance and alignment of this Patient Simulator could be used to train and test a multi-turn conversational AI agent at scale.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.04032"
    },
    "8b788b103e7cce5403f39532002aebe7": {
        "title": "TextAtari: 100K Frames Game Playing with Language Agents",
        "authors": [
            "Wenhao Li",
            "Wenwu Li",
            "Chuyun Shen",
            "Junjie Sheng",
            "Zixiao Huang",
            "Di Wu",
            "Yun Hua",
            "Wei Yin",
            "Xiangfeng Wang",
            "Hongyuan Zha",
            "Bo Jin"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.04098",
        "abstract": "We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning. Our code is available at https://github.com/Lww007/Text-Atari-Agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.04098"
    },
    "d341fa50add114233e87e557a22d6163": {
        "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues",
        "authors": [
            "Disha Sheshanarayana",
            "Tanishka Magar",
            "Ayushi Mittal",
            "Neelam Chaplot"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.04131",
        "abstract": "Courtrooms are places where lives are determined and fates are sealed, yet they are not impervious to manipulation. Strategic use of manipulation in legal jargon can sway the opinions of judges and affect the decisions. Despite the growing advancements in NLP, its application in detecting and analyzing manipulation within the legal domain remains largely unexplored. Our work addresses this gap by introducing LegalCon, a dataset of 1,063 annotated courtroom conversations labeled for manipulation detection, identification of primary manipulators, and classification of manipulative techniques, with a focus on long conversations. Furthermore, we propose CLAIM, a two-stage, Intent-driven Multi-agent framework designed to enhance manipulation analysis by enabling context-aware and informed decision-making. Our results highlight the potential of incorporating agentic frameworks to improve fairness and transparency in judicial processes. We hope that this contributes to the broader application of NLP in legal discourse analysis and the development of robust tools to support fairness in legal decision-making. Our code and data are available at https://github.com/Disha1001/CLAIM.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.04131"
    },
    "739f9e780083ad4eb1f5c5a4db46c689": {
        "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale",
        "authors": [
            "Ran Xu",
            "Yuchen Zhuang",
            "Yishan Zhong",
            "Yue Yu",
            "Xiangru Tang",
            "Hang Wu",
            "May D. Wang",
            "Peifeng Ruan",
            "Donghan Yang",
            "Tao Wang",
            "Guanghua Xiao",
            "Carl Yang",
            "Yang Xie",
            "Wenqi Shi"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.04405",
        "abstract": "We introduce MedAgentGYM, the first publicly available training environment designed to enhance coding-based medical reasoning capabilities in large language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across 129 categories derived from authentic real-world biomedical scenarios. Tasks are encapsulated within executable coding environments, each featuring detailed task descriptions, interactive feedback mechanisms, verifiable ground-truth annotations, and scalable training trajectory generation. Extensive benchmarking of over 30 LLMs reveals a notable performance disparity between commercial API-based models and open-source counterparts. Leveraging MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through supervised fine-tuning (+36.44%) and continued reinforcement learning (+42.47%), emerging as an affordable and privacy-preserving alternative competitive with gpt-4o. By offering both a comprehensive benchmark and accessible, expandable training resources within unified execution environments, MedAgentGYM delivers an integrated platform to develop LLM-based coding assistants for advanced biomedical research and practice.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.04405"
    },
    "0c446a133534ed65da7ad7b3f869e1bf": {
        "title": "Demonstrations of Integrity Attacks in Multi-Agent Systems",
        "authors": [
            "Can Zheng",
            "Yuhan Cao",
            "Xiaoning Dong",
            "Tianxing He"
        ],
        "date": "2025/06/05",
        "pdf": "http://arxiv.org/pdf/2506.04572",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, code generation, and complex planning. Simultaneously, Multi-Agent Systems (MAS) have garnered attention for their potential to enable cooperation among distributed agents. However, from a multi-party perspective, MAS could be vulnerable to malicious agents that exploit the system to serve self-interests without disrupting its core functionality. This work explores integrity attacks where malicious agents employ subtle prompt manipulation to bias MAS operations and gain various benefits. Four types of attacks are examined: \\textit{Scapegoater}, who misleads the system monitor to underestimate other agents&#39; contributions; \\textit{Boaster}, who misleads the system monitor to overestimate their own performance; \\textit{Self-Dealer}, who manipulates other agents to adopt certain tools; and \\textit{Free-Rider}, who hands off its own task to others. We demonstrate that strategically crafted prompts can introduce systematic biases in MAS behavior and executable instructions, enabling malicious agents to effectively mislead evaluation systems and manipulate collaborative agents. Furthermore, our attacks can bypass advanced LLM-based monitors, such as GPT-4o-mini and o3-mini, highlighting the limitations of current detection mechanisms. Our findings underscore the critical need for MAS architectures with robust security protocols and content validation mechanisms, alongside monitoring systems capable of comprehensive risk scenario assessment.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.04572"
    },
    "93804c450676dc8ee1fa0bb5a29c3158": {
        "title": "Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents",
        "authors": [
            "Juhyun Oh",
            "Eunsu Kim",
            "Alice Oh"
        ],
        "date": "2025/06/05",
        "pdf": "http://arxiv.org/pdf/2506.04649",
        "abstract": "Real-world planning problems require constant adaptation to changing requirements and balancing of competing constraints. However, current benchmarks for evaluating LLMs&#39; planning capabilities primarily focus on static, single-turn scenarios. We introduce Flex-TravelPlanner, a benchmark that evaluates language models&#39; ability to reason flexibly in dynamic planning scenarios. Building on the TravelPlanner dataset~\\citep{xie2024travelplanner}, we introduce two novel evaluation settings: (1) sequential constraint introduction across multiple turns, and (2) scenarios with explicitly prioritized competing constraints. Our analysis of GPT-4o and Llama 3.1 70B reveals several key findings: models&#39; performance on single-turn tasks poorly predicts their ability to adapt plans across multiple turns; constraint introduction order significantly affects performance; and models struggle with constraint prioritization, often incorrectly favoring newly introduced lower priority preferences over existing higher-priority constraints. These findings highlight the importance of evaluating LLMs in more realistic, dynamic planning scenarios and suggest specific directions for improving model performance on complex planning tasks. The code and dataset for our framework are publicly available at https://github.com/juhyunohh/FlexTravelBench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.04649"
    },
    "92ff4dd098a0e8695a50f25e3c3c501c": {
        "title": "MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning",
        "authors": [
            "Ye Bai",
            "Minghan Wang",
            "Thuy-Trang Vu"
        ],
        "date": "2025/06/06",
        "pdf": "http://arxiv.org/pdf/2506.05813",
        "abstract": "Table-based question answering requires complex reasoning capabilities that current LLMs struggle to achieve with single-pass inference. Existing approaches, such as Chain-of-Thought reasoning and question decomposition, lack error detection mechanisms and discard problem-solving experiences, contrasting sharply with how humans tackle such problems. In this paper, we propose MAPLE (Multi-agent Adaptive Planning with Long-term mEmory), a novel framework that mimics human problem-solving through specialized cognitive agents working in a feedback-driven loop. MAPLE integrates 4 key components: (1) a Solver using the ReAct paradigm for reasoning, (2) a Checker for answer verification, (3) a Reflector for error diagnosis and strategy correction, and (4) an Archiver managing long-term memory for experience reuse and evolution. Experiments on WiKiTQ and TabFact demonstrate significant improvements over existing methods, achieving state-of-the-art performance across multiple LLM backbones.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.05813"
    },
    "f0d4296837c2a14a509542e06be6680e": {
        "title": "AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search",
        "authors": [
            "Yu Li",
            "Lehui Li",
            "Zhihao Wu",
            "Qingmin Liao",
            "Jianye Hao",
            "Kun Shao",
            "Fengli Xu",
            "Yong Li"
        ],
        "date": "2025/06/06",
        "pdf": "http://arxiv.org/pdf/2506.06017",
        "abstract": "Large language model (LLM) agents have demonstrated strong capabilities across diverse domains. However, designing high-performing agentic systems remains challenging. Existing agent search methods suffer from three major limitations: (1) an emphasis on optimizing agentic workflows while under-utilizing proven human-designed components such as memory, planning, and tool use; (2) high evaluation costs, as each newly generated agent must be fully evaluated on benchmarks; and (3) inefficient search in large search space. In this work, we introduce a comprehensive framework to address these challenges. First, We propose a hierarchical search space that jointly models agentic workflow and composable functional components, enabling richer agentic system designs. Building on this structured design space, we introduce a predictive value model that estimates agent performance given agentic system and task description, allowing for efficient, low-cost evaluation during the search process. Finally, we present a hierarchical Monte Carlo Tree Search (MCTS) strategy informed by uncertainty to guide the search. Experiments on seven benchmarks, covering embodied, math, web, tool, and game, show that our method achieves an average performance gain of 8.34\\% over state-of-the-art baselines and exhibits faster search progress with steeper improvement trajectories. Code repo is available at https://github.com/Ericccc02/AgentSwift.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.06017"
    },
    "21b5f440d5d8d0b17589d68cbe6c4e45": {
        "title": "Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach",
        "authors": [
            "James Ford",
            "Anthony Rios"
        ],
        "date": "2025/06/06",
        "pdf": "http://arxiv.org/pdf/2506.06175",
        "abstract": "Large language models can translate natural-language chart descriptions into runnable code, yet approximately 15\\% of the generated scripts still fail to execute, even after supervised fine-tuning and reinforcement learning. We investigate whether this persistent error rate stems from model limitations or from reliance on a single-prompt design. To explore this, we propose a lightweight multi-agent pipeline that separates drafting, execution, repair, and judgment, using only an off-the-shelf GPT-4o-mini model. On the \\textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\\% within three repair iterations, outperforming the strongest fine-tuned baseline by nearly 5 percentage points while requiring significantly less compute. Similar performance is observed on the \\textsc{ChartX} benchmark, with an error rate of 4.6\\%, demonstrating strong generalization. Under current benchmarks, execution success appears largely solved. However, manual review reveals that 6 out of 100 sampled charts contain hallucinations, and an LLM-based accessibility audit shows that only 33.3\\% (\\textsc{Text2Chart31}) and 7.2\\% (\\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines. These findings suggest that future work should shift focus from execution reliability toward improving chart aesthetics, semantic fidelity, and accessibility.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.06175"
    },
    "97df7df2a641019656ac932a8c9b00f7": {
        "title": "Can Theoretical Physics Research Benefit from Language Agents?",
        "authors": [
            "Sirui Lu",
            "Zhijing Jin",
            "Terry Jingchen Zhang",
            "Pavel Kos",
            "J. Ignacio Cirac",
            "Bernhard Schölkopf"
        ],
        "date": "2025/06/06",
        "pdf": "http://arxiv.org/pdf/2506.06214",
        "abstract": "Large Language Models (LLMs) are rapidly advancing across diverse domains, yet their application in theoretical physics research is not yet mature. This position paper argues that LLM agents can potentially help accelerate theoretical, computational, and applied physics when properly integrated with domain knowledge and toolbox. We analyze current LLM capabilities for physics -- from mathematical reasoning to code generation -- identifying critical gaps in physical intuition, constraint satisfaction, and reliable reasoning. We envision future physics-specialized LLMs that could handle multimodal data, propose testable hypotheses, and design experiments. Realizing this vision requires addressing fundamental challenges: ensuring physical consistency, and developing robust verification methods. We call for collaborative efforts between physics and AI communities to help advance scientific discovery in physics.",
        "code": "",
        "category": [
            [
                "Application",
                "Physics"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.06214"
    },
    "dd4c10f8a029c7a0de3869f62c8d56f9": {
        "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models",
        "authors": [
            "Samir Abdaljalil",
            "Hasan Kurban",
            "Khalid Qaraqe",
            "Erchin Serpedin"
        ],
        "date": "2025/06/08",
        "pdf": "http://arxiv.org/pdf/2506.07106",
        "abstract": "Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.07106"
    },
    "7b116a6678f75b9e78a57ac1c2ba0497": {
        "title": "EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments",
        "authors": [
            "Zefang Liu",
            "Yinzhu Quan"
        ],
        "date": "2025/06/09",
        "pdf": "http://arxiv.org/pdf/2506.08136",
        "abstract": "We introduce EconWebArena, a benchmark for evaluating autonomous agents on complex, multimodal economic tasks in realistic web environments. The benchmark comprises 360 curated tasks from 82 authoritative websites spanning domains such as macroeconomics, labor, finance, trade, and public policy. Each task challenges agents to navigate live websites, interpret structured and visual content, interact with real interfaces, and extract precise, time-sensitive data through multi-step workflows. We construct the benchmark by prompting multiple large language models (LLMs) to generate candidate tasks, followed by rigorous human curation to ensure clarity, feasibility, and source reliability. Unlike prior work, EconWebArena emphasizes fidelity to authoritative data sources and the need for grounded web-based economic reasoning. We evaluate a diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure cases, and conduct ablation studies to assess the impact of visual grounding, plan-based reasoning, and interaction design. Our results reveal substantial performance gaps and highlight persistent challenges in grounding, navigation, and multimodal understanding, positioning EconWebArena as a rigorous testbed for economic web intelligence.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08136"
    },
    "fdbc0af10a92926594de1c990ba9b9cd": {
        "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration",
        "authors": [
            "Weiya Li",
            "Junjie Chen",
            "Bei Li",
            "Boyang Liu",
            "Zichen Wen",
            "Nuanqiao Shan",
            "Xiaoqian Liu",
            "Anping Liu",
            "Huajie Liu",
            "Hu Song",
            "Linfeng Zhang"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.08403",
        "abstract": "Machine translation has long been a central task in natural language processing. With the rapid advancement of large language models (LLMs), there has been remarkable progress in translation quality. However, fully realizing the translation potential of LLMs remains an open challenge. Recent studies have explored multi-agent systems to decompose complex translation tasks into collaborative subtasks, showing initial promise in enhancing translation quality through agent cooperation and specialization. Nevertheless, existing multi-agent translation frameworks largely neglect foundational insights from cognitive translation studies. These insights emphasize how human translators employ different cognitive strategies, such as balancing literal and free translation, refining expressions based on context, and iteratively evaluating outputs. To address this limitation, we propose a cognitively informed multi-agent framework called TACTIC, which stands for T ranslation A gents with Cognitive- T heoretic Interactive Collaboration. The framework comprises six functionally distinct agents that mirror key cognitive processes observed in human translation behavior. These include agents for drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. By simulating an interactive and theory-grounded translation workflow, TACTIC effectively leverages the full capacity of LLMs for high-quality translation. Experimental results on diverse language pairs from the FLORES-200 and WMT24 benchmarks show that our method consistently achieves state-of-the-art performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at https://github.com/weiyali126/TACTIC.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08403"
    },
    "20dfd6c756f3b020638af20a353a1601": {
        "title": "CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models",
        "authors": [
            "Ziqi. Liu",
            "Ziyang. Zhou",
            "Mingxuan. Hu"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.08430",
        "abstract": "Large language model (LLM) have become mainstream methods in the field of sarcasm detection. However, existing LLM methods face challenges in irony detection, including: 1. single-perspective limitations, 2. insufficient comprehensive understanding, and 3. lack of interpretability. This paper introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven multi-agent system designed to overcome these issues. CAF-I employs specialized agents for Context, Semantics, and Rhetoric, which perform multidimensional analysis and engage in interactive collaborative optimization. A Decision Agent then consolidates these perspectives, with a Refinement Evaluator Agent providing conditional feedback for optimization. Experiments on benchmark datasets establish CAF-I&#39;s state-of-the-art zero-shot performance. Achieving SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of 76.31, a 4.98 absolute improvement over the strongest prior baseline. This success is attained by its effective simulation of human-like multi-perspective analysis, enhancing detection accuracy and interpretability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08430"
    },
    "a692b6a0d12be3564391747510de1cb1": {
        "title": "Improved LLM Agents for Financial Document Question Answering",
        "authors": [
            "Nelvin Tan",
            "Zian Seng",
            "Liang Zhang",
            "Yu-Ching Shih",
            "Dong Yang",
            "Amol Salunkhe"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.08726",
        "abstract": "Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent&#39;s performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08726"
    },
    "e9e95d21ac8ed2eb1e94de4cbca83e36": {
        "title": "Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System",
        "authors": [
            "Yuan Guo",
            "Tingjia Miao",
            "Zheng Wu",
            "Pengzhou Cheng",
            "Ming Zhou",
            "Zhuosheng Zhang"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.08972",
        "abstract": "Autonomous agents powered by multimodal large language models have been developed to facilitate task execution on mobile devices. However, prior work has predominantly focused on atomic tasks -- such as shot-chain execution tasks and single-screen grounding tasks -- while overlooking the generalization to compositional tasks, which are indispensable for real-world applications. This work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile agents on three categories of compositional operations: Simple Concatenation, Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in 20 fully controllable local utility app environments, as well as 30 online Chinese and English service apps. It comprises 100 interactive task templates with an average optimal step count of 14.05. Experimental results across a range of mobile agents with agentic workflow or agent-as-a-model show that UI-NEXUS presents significant challenges. Specifically, existing agents generally struggle to balance performance and efficiency, exhibiting representative failure modes such as under-execution, over-execution, and attention drift, causing visible atomic-to-compositional generalization gap. Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient scheduling system to tackle compositional mobile tasks. AGENT-NEXUS extrapolates the abilities of existing mobile agents by dynamically decomposing long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS achieves 24% to 40% task success rate improvement for existing mobile agents on compositional operation tasks within the UI-NEXUS benchmark without significantly sacrificing inference overhead. The demo video, dataset, and code are available on the project page at https://ui-nexus.github.io.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08972"
    },
    "fa2d574a1d11e76f28f6c8ceb8899a9f": {
        "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation",
        "authors": [
            "Arjun Vaithilingam Sudhakar"
        ],
        "date": "2025/06/11",
        "pdf": "http://arxiv.org/pdf/2506.09331",
        "abstract": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other&#39;s intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent&#39;s ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.09331"
    },
    "9a810185a8a331e0d04d377c77e1cdff": {
        "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning",
        "authors": [
            "Yu Sun",
            "Xingyu Qian",
            "Weiwen Xu",
            "Hao Zhang",
            "Chenghao Xiao",
            "Long Li",
            "Yu Rong",
            "Wenbing Huang",
            "Qifeng Bai",
            "Tingyang Xu"
        ],
        "date": "2025/06/11",
        "pdf": "http://arxiv.org/pdf/2506.09513",
        "abstract": "Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a \\textit{multi-agent verification and refinement process}, where we design an \\textit{Error Refiner} to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Infrastructure",
                "Dataset"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.09513"
    },
    "06cbb8ab13e93fe51e213bfc6246db59": {
        "title": "Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information",
        "authors": [
            "Christodoulos Constantinides",
            "Shuxin Lin",
            "Nianjun Zhou",
            "Dhaval Patel"
        ],
        "date": "2025/06/11",
        "pdf": "http://arxiv.org/pdf/2506.10086",
        "abstract": "This paper presents a novel multi-agent system called Chat-of-Thought, designed to facilitate the generation of Failure Modes and Effects Analysis (FMEA) documents for industrial assets. Chat-of-Thought employs multiple collaborative Large Language Model (LLM)-based agents with specific roles, leveraging advanced AI techniques and dynamic task routing to optimize the generation and validation of FMEA tables. A key innovation in this system is the introduction of a Chat of Thought, where dynamic, multi-persona-driven discussions enable iterative refinement of content. This research explores the application domain of industrial equipment monitoring, highlights key challenges, and demonstrates the potential of Chat-of-Thought in addressing these challenges through interactive, template-driven workflows and context-aware agent collaboration.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.10086"
    },
    "df047e5d8104bf432f3b6b4234687526": {
        "title": "CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training",
        "authors": [
            "Alireza Salemi",
            "Mukta Maddipatla",
            "Hamed Zamani"
        ],
        "date": "2025/06/12",
        "pdf": "http://arxiv.org/pdf/2506.10844",
        "abstract": "This paper presents mRAG, a multi-agent retrieval-augmented generation (RAG) framework composed of specialized agents for subtasks such as planning, searching, reasoning, and coordination. Our system uses a self-training paradigm with reward-guided trajectory sampling to optimize inter-agent collaboration and enhance response generation. Evaluated on DataMorgana-derived datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms conventional RAG baselines. We further analyze competition outcomes and showcase the framework&#39;s strengths with case studies, demonstrating its efficacy for complex, real-world RAG tasks.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.10844"
    },
    "b7e3fff94f47eb5455adb4d207113a9b": {
        "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
        "authors": [
            "Yixin Ou",
            "Yujie Luo",
            "Jingsheng Zheng",
            "Lanning Wei",
            "Shuofei Qiao",
            "Jintian Zhang",
            "Da Zheng",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "date": "2025/06/12",
        "pdf": "http://arxiv.org/pdf/2506.10974",
        "abstract": "Large Language Model (LLM) agents have shown great potential in addressing real-world data science problems. LLM-driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficient and robust step toward fully automated data science.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.10974"
    },
    "6aeb7839b9e9050d07dc72106c8a3c28": {
        "title": "RedDebate: Safer Responses through Multi-Agent Red Teaming Debates",
        "authors": [
            "Ali Asad",
            "Stephen Obadinma",
            "Radin Shayanfar",
            "Xiaodan Zhu"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.11083",
        "abstract": "We propose RedDebate, a novel multi-agent debate framework that leverages adversarial argumentation among Large Language Models (LLMs) to proactively identify and mitigate their own unsafe behaviours. Existing AI safety methods often depend heavily on costly human evaluations or isolated single-model assessment, both subject to scalability constraints and oversight risks. RedDebate instead embraces collaborative disagreement, enabling multiple LLMs to critically examine one another&#39;s reasoning, and systematically uncovering unsafe blind spots through automated red-teaming, and iteratively improve their responses. We further integrate distinct types of long-term memory that retain learned safety insights from debate interactions. Evaluating on established safety benchmarks such as HarmBench, we demonstrate the proposed method&#39;s effectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when combined with long-term memory modules, achieves reductions exceeding 23.5%. To our knowledge, RedDebate constitutes the first fully automated framework that combines multi-agent debates with red-teaming to progressively enhance AI safety without direct human intervention.(Github Repository: https://github.com/aliasad059/RedDebate)",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11083"
    },
    "202b72a57719ad5bbafd211e54786b89": {
        "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey",
        "authors": [
            "Jiachen Zhu",
            "Menghui Zhu",
            "Renting Rui",
            "Rong Shan",
            "Congmin Zheng",
            "Bo Chen",
            "Yunjia Xi",
            "Jianghao Lin",
            "Weiwen Liu",
            "Ruiming Tang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "date": "2025/06/06",
        "pdf": "http://arxiv.org/pdf/2506.11102",
        "abstract": "The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11102"
    },
    "ba6884f5b4137264ef3609f6abd49d8d": {
        "title": "Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)",
        "authors": [
            "Christine Bauer",
            "Li Chen",
            "Nicola Ferro",
            "Norbert Fuhr",
            "Avishek Anand",
            "Timo Breuer",
            "Guglielmo Faggioli",
            "Ophir Frieder",
            "Hideo Joho",
            "Jussi Karlgren",
            "Johannes Kiesel",
            "Bart P. Knijnenburg",
            "Aldo Lipani",
            "Lien Michiels",
            "Andrea Papenmeier",
            "Maria Soledad Pera",
            "Mark Sanderson",
            "Scott Sanner",
            "Benno Stein",
            "Johanne R. Trippas",
            "Karin Verspoor",
            "Martijn C Willemsen"
        ],
        "date": "2025/06/08",
        "pdf": "http://arxiv.org/pdf/2506.11112",
        "abstract": "During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system&#39;s stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11112"
    },
    "e9140e53da5f4158eb274a68ca32864e": {
        "title": "GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions",
        "authors": [
            "Wenkang Han",
            "Zhixiong Zeng",
            "Jing Huang",
            "Shu Jiang",
            "Liming Zheng",
            "Longrong Yang",
            "Haibo Qiu",
            "Chang Yao",
            "Jingyuan Chen",
            "Lin Ma"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.11127",
        "abstract": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the first end-to-end autonomous GUI agent that directly accepts speech instructions and on-device screenshots to predict actions. Confronted with the scarcity of speech-based GUI agent datasets, we initially generated high-quality speech instructions for training by leveraging a random timbre text-to-speech (TTS) model to convert existing text instructions. We then develop GUIRoboTron-Speech&#39;s capabilities through progressive grounding and planning training stages. A key contribution is a heuristic mixed-instruction training strategy designed to mitigate the modality imbalance inherent in pre-trained foundation models. Comprehensive experiments on several benchmark datasets validate the robust and superior performance of GUIRoboTron-Speech, demonstrating the significant potential and widespread applicability of speech as an effective instruction modality for driving GUI agents. Our code and datasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11127"
    },
    "ca89cbb989732c6627957d9d0176381a": {
        "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards",
        "authors": [
            "Jeff Da",
            "Clinton Wang",
            "Xiang Deng",
            "Yuntao Ma",
            "Nikhil Barhate",
            "Sean Hendryx"
        ],
        "date": "2025/06/13",
        "pdf": "http://arxiv.org/pdf/2506.11425",
        "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted as the de facto method for enhancing the reasoning capabilities of large language models and has demonstrated notable success in verifiable domains like math and competitive programming tasks. However, the efficacy of RLVR diminishes significantly when applied to agentic environments. These settings, characterized by multi-step, complex problem solving, lead to high failure rates even for frontier LLMs, as the reward landscape is too sparse for effective model training via conventional RLVR. In this work, we introduce Agent-RLVR, a framework that makes RLVR effective in challenging agentic settings, with an initial focus on software engineering tasks. Inspired by human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively steers the agent towards successful trajectories by leveraging diverse informational cues. These cues, ranging from high-level strategic plans to dynamic feedback on the agent&#39;s errors and environmental interactions, emulate a teacher&#39;s guidance, enabling the agent to navigate difficult solution spaces and promotes active self-improvement via additional environment exploration. In the Agent-RLVR training loop, agents first attempt to solve tasks to produce initial trajectories, which are then validated by unit tests and supplemented with agent guidance. Agents then reattempt with guidance, and the agent policy is updated with RLVR based on the rewards of these guided trajectories. Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4% to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data is additionally useful for test-time reward model training, shown by further boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents with RLVR in complex, real-world environments where conventional RL methods struggle.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11425"
    },
    "e82e2637c1fa0de38f02d08b89964b51": {
        "title": "A Hybrid Multi-Agent Prompting Approach for Simplifying Complex Sentences",
        "authors": [
            "Pratibha Zunjare",
            "Michael Hsiao"
        ],
        "date": "2025/06/13",
        "pdf": "http://arxiv.org/pdf/2506.11681",
        "abstract": "This paper addresses the challenge of transforming complex sentences into sequences of logical, simplified sentences while preserving semantic and logical integrity with the help of Large Language Models. We propose a hybrid approach that combines advanced prompting with multi-agent architectures to enhance the sentence simplification process. Experimental results show that our approach was able to successfully simplify 70% of the complex sentences written for video game design application. In comparison, a single-agent approach attained a 48% success rate on the same task.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11681"
    },
    "a5eef7e9abc0a03c729f5042ec2dcaef": {
        "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
        "authors": [
            "Mingxuan Du",
            "Benfeng Xu",
            "Chiwei Zhu",
            "Xiaorui Wang",
            "Zhendong Mao"
        ],
        "date": "2025/06/13",
        "pdf": "http://arxiv.org/pdf/2506.11763",
        "abstract": "Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA&#39;s information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11763"
    },
    "0a3687c28aefe03a8193fd52f509e25b": {
        "title": "The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs",
        "authors": [
            "Avinash Baidya",
            "Kamalika Das",
            "Xiang Gao"
        ],
        "date": "2025/06/13",
        "pdf": "http://arxiv.org/pdf/2506.12266",
        "abstract": "Large Language Model (LLM)-based agents have significantly impacted Task-Oriented Dialog Systems (TODS) but continue to face notable performance challenges, especially in zero-shot scenarios. While prior work has noted this performance gap, the behavioral factors driving the performance gap remain under-explored. This study proposes a comprehensive evaluation framework to quantify the behavior gap between AI agents and human experts, focusing on discrepancies in dialog acts, tool usage, and knowledge utilization. Our findings reveal that this behavior gap is a critical factor negatively impacting the performance of LLM agents. Notably, as task complexity increases, the behavior gap widens (correlation: 0.963), leading to a degradation of agent performance on complex task-oriented dialogs. For the most complex task in our study, even the GPT-4o-based agent exhibits low alignment with human behavior, with low F1 scores for dialog acts (0.464), excessive and often misaligned tool usage with a F1 score of 0.139, and ineffective usage of external knowledge. Reducing such behavior gaps leads to significant performance improvement (24.3% on average). This study highlights the importance of comprehensive behavioral evaluations and improved alignment strategies to enhance the effectiveness of LLM-based TODS in handling complex tasks.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.12266"
    },
    "429cc75b2b2c8afe5dcb959433f4301f": {
        "title": "Towards Building General Purpose Embedding Models for Industry 4.0 Agents",
        "authors": [
            "Christodoulos Constantinides",
            "Shuxin Lin",
            "Dhaval Patel"
        ],
        "date": "2025/06/14",
        "pdf": "http://arxiv.org/pdf/2506.12607",
        "abstract": "In this work we focus on improving language models&#39; understanding for asset maintenance to guide the engineer&#39;s decisions and minimize asset downtime. Given a set of tasks expressed in natural language for Industry 4.0 domain, each associated with queries related to a specific asset, we want to recommend relevant items and generalize to queries of similar assets. A task may involve identifying relevant sensors given a query about an asset&#39;s failure mode. Our approach begins with gathering a qualitative, expert-vetted knowledge base to construct nine asset-specific task datasets. To create more contextually informed embeddings, we augment the input tasks using Large Language Models (LLMs), providing concise descriptions of the entities involved in the queries. This embedding model is then integrated with a Reasoning and Acting agent (ReAct), which serves as a powerful tool for answering complex user queries that require multi-step reasoning, planning, and knowledge inference. Through ablation studies, we demonstrate that: (a) LLM query augmentation improves the quality of embeddings, (b) Contrastive loss and other methods that avoid in-batch negatives are superior for datasets with queries related to many items, and (c) It is crucial to balance positive and negative in-batch samples. After training and testing on our dataset, we observe a substantial improvement: HIT@1 increases by +54.2%, MAP@100 by +50.1%, and NDCG@10 by +54.7%, averaged across all tasks and models. Additionally, we empirically demonstrate the model&#39;s planning and tool invocation capabilities when answering complex questions related to industrial asset maintenance, showcasing its effectiveness in supporting Subject Matter Experts (SMEs) in their day-to-day operations.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.12607"
    },
    "0d1f4097acf04bd5b522575b9a26b772": {
        "title": "Leveraging In-Context Learning for Language Model Agents",
        "authors": [
            "Shivanshu Gupta",
            "Sameer Singh",
            "Ashish Sabharwal",
            "Tushar Khot",
            "Ben Bogin"
        ],
        "date": "2025/06/16",
        "pdf": "http://arxiv.org/pdf/2506.13109",
        "abstract": "In-context learning (ICL) with dynamically selected demonstrations combines the flexibility of prompting large language models (LLMs) with the ability to leverage training data to improve performance. While ICL has been highly successful for prediction and generation tasks, leveraging it for agentic tasks that require sequential decision making is challenging -- one must think not only about how to annotate long trajectories at scale and how to select demonstrations, but also what constitutes demonstrations, and when and where to show them. To address this, we first propose an algorithm that leverages an LLM with retries along with demonstrations to automatically and efficiently annotate agentic tasks with solution trajectories. We then show that set-selection of trajectories of similar tasks as demonstrations significantly improves performance, reliability, robustness, and efficiency of LLM agents. However, trajectory demonstrations have a large inference cost overhead. We show that this can be mitigated by using small trajectory snippets at every step instead of an additional trajectory. We find that demonstrations obtained from larger models (in the annotation phase) also improve smaller models, and that ICL agents can even rival costlier trained agents. Thus, our results reveal that ICL, with careful use, can be very powerful for agentic tasks as well.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.13109"
    },
    "d1f768560da29040b9b1cbaf280856d4": {
        "title": "Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning",
        "authors": [
            "David Bani-Harouni",
            "Chantal Pellegrini",
            "Ege Özsoy",
            "Matthias Keicher",
            "Nassir Navab"
        ],
        "date": "2025/06/16",
        "pdf": "http://arxiv.org/pdf/2506.13474",
        "abstract": "Clinical decision-making is a dynamic, interactive, and cyclic process where doctors have to repeatedly decide on which clinical action to perform and consider newly uncovered information for diagnosis and treatment. Large Language Models (LLMs) have the potential to support clinicians in this process, however, most applications of LLMs in clinical decision support suffer from one of two limitations: Either they assume the unrealistic scenario of immediate availability of all patient information and do not model the interactive and iterative investigation process, or they restrict themselves to the limited &#34;out-of-the-box&#34; capabilities of large pre-trained models without performing task-specific training. In contrast to this, we propose to model clinical decision-making for diagnosis with a hypothesis-driven uncertainty-aware language agent, LA-CDM, that converges towards a diagnosis via repeatedly requesting and interpreting relevant tests. Using a hybrid training paradigm combining supervised and reinforcement learning, we train LA-CDM with three objectives targeting critical aspects of clinical decision-making: accurate hypothesis generation, hypothesis uncertainty estimation, and efficient decision-making. We evaluate our methodology on MIMIC-CDM, a real-world dataset covering four abdominal diseases containing various clinical tests and show the benefit of explicitly training clinical decision-making for increasing diagnostic performance and efficiency.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.13474"
    },
    "681deaa5043544ec1c64f4217d19e18b": {
        "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation",
        "authors": [
            "Yuwei Du",
            "Jie Feng",
            "Jian Yuan",
            "Yong Li"
        ],
        "date": "2025/06/16",
        "pdf": "http://arxiv.org/pdf/2506.13599",
        "abstract": "Human mobility simulation plays a crucial role in various real-world applications. Recently, to address the limitations of traditional data-driven approaches, researchers have explored leveraging the commonsense knowledge and reasoning capabilities of large language models (LLMs) to accelerate human mobility simulation. However, these methods suffer from several critical shortcomings, including inadequate modeling of urban spaces and poor integration with both individual mobility patterns and collective mobility distributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered \\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation (\\textbf{CAMS}), an agentic framework that leverages the language based urban foundation model to simulate human mobility in urban space. \\textbf{CAMS} comprises three core modules, including MobExtractor to extract template mobility patterns and synthesize new ones based on user profiles, GeoGenerator to generate anchor points considering collective knowledge and generate candidate urban geospatial knowledge using an enhanced version of CityGPT, TrajEnhancer to retrieve spatial knowledge based on mobility patterns and generate trajectories with real trajectory preference alignment via DPO. Experiments on real-world datasets show that \\textbf{CAMS} achieves superior performance without relying on externally provided geospatial information. Moreover, by holistically modeling both individual mobility patterns and collective mobility constraints, \\textbf{CAMS} generates more realistic and plausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm that integrates the agentic framework with urban-knowledgeable LLMs for human mobility simulation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.13599"
    },
    "4b8f04ef6a57c02c0ee5b9d5a2f87b7b": {
        "title": "MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment",
        "authors": [
            "Junghwan Kim",
            "Kieun Park",
            "Sohee Park",
            "Hyunggug Kim",
            "Bongwon Suh"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.14199",
        "abstract": "Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur&#39;s Court, generated by various LLMs, and compared it to traditional metrics. \\textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.14199"
    },
    "d621b7de6c6c20a85a97ec525f139732": {
        "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents",
        "authors": [
            "Jingxu Xie",
            "Dylan Xu",
            "Xuandong Zhao",
            "Dawn Song"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.14205",
        "abstract": "We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark&#39;s difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.14205"
    },
    "e700a77c65d9e8aadb649ddcf44046df": {
        "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team",
        "authors": [
            "Md Tanzib Hosain",
            "Salman Rahman",
            "Md Kishor Morol",
            "Md Rizwan Parvez"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.14234",
        "abstract": "Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME&#39;24 (94.4%), AIME&#39;25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.14234"
    },
    "c5ad24aa10d327885ecf15be8e788eca": {
        "title": "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents",
        "authors": [
            "Seongbo Jang",
            "Minjin Jeon",
            "Jaehoon Lee",
            "Seonghyeon Lee",
            "Dongha Lee",
            "Hwanjo Yu"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.14285",
        "abstract": "While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.14285"
    },
    "efce4d8b3e1121f600a9ac352b6af384": {
        "title": "Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent",
        "authors": [
            "Xueyang Feng",
            "Jingsen Zhang",
            "Jiakai Tang",
            "Wei Li",
            "Guohao Cai",
            "Xu Chen",
            "Quanyu Dai",
            "Yue Zhu",
            "Zhenhua Dong"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.14302",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA&#39;s interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.14302"
    },
    "d366aaa85bf6e8811d18a15738e028f9": {
        "title": "Understanding GUI Agent Localization Biases through Logit Sharpness",
        "authors": [
            "Xingjian Tao",
            "Yiwei Wang",
            "Yujun Cai",
            "Zhicheng Yang",
            "Jing Tang"
        ],
        "date": "2025/06/18",
        "pdf": "http://arxiv.org/pdf/2506.15425",
        "abstract": "Multimodal large language models (MLLMs) have enabled GUI agents to interact with operating systems by grounding language into spatial actions. Despite their promising performance, these models frequently exhibit hallucinations-systematic localization errors that compromise reliability. We propose a fine-grained evaluation framework that categorizes model predictions into four distinct types, revealing nuanced failure modes beyond traditional accuracy metrics. To better quantify model uncertainty, we introduce the Peak Sharpness Score (PSS), a metric that evaluates the alignment between semantic continuity and logits distribution in coordinate prediction. Building on this insight, we further propose Context-Aware Cropping, a training-free technique that improves model performance by adaptively refining input context. Extensive experiments demonstrate that our framework and methods provide actionable insights and enhance the interpretability and robustness of GUI agent behavior.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.15425"
    },
    "3576868236caa2c62ee95c49abc59256": {
        "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need",
        "authors": [
            "Zhouhong Gu",
            "Xiaoxuan Zhu",
            "Yin Cai",
            "Hao Shen",
            "Xingzhou Chen",
            "Qingyi Wang",
            "Jialin Li",
            "Xiaoran Shi",
            "Haoran Guo",
            "Wenxuan Huang",
            "Hongwei Feng",
            "Yanghua Xiao",
            "Zheyu Ye",
            "Yao Hu",
            "Shaosheng Cao"
        ],
        "date": "2025/06/18",
        "pdf": "http://arxiv.org/pdf/2506.15451",
        "abstract": "Large language model based multi-agent systems have demonstrated significant potential in social simulation and complex task resolution domains. However, current frameworks face critical challenges in system architecture design, cross-domain generalizability, and performance guarantees, particularly as task complexity and number of agents increases. We introduces AgentGroupChat-V2, a novel framework addressing these challenges through three core innovations: (1) a divide-and-conquer fully parallel architecture that decomposes user queries into hierarchical task forest structures enabling dependency management and distributed concurrent processing. (2) an adaptive collaboration engine that dynamically selects heterogeneous LLM combinations and interaction modes based on task characteristics. (3) agent organization optimization strategies combining divide-and-conquer approaches for efficient problem decomposition. Extensive experiments demonstrate AgentGroupChat-V2&#39;s superior performance across diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best baseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME (nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance advantages become increasingly pronounced with higher task difficulty, particularly on Level 5 MATH problems where improvements exceed 11 percentage points compared to state-of-the-art baselines. These results confirm that AgentGroupChat-V2 provides a comprehensive solution for building efficient, general-purpose LLM multi-agent systems with significant advantages in complex reasoning scenarios. Code is available at https://github.com/MikeGu721/AgentGroupChat-V2.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.15451"
    },
    "bc2f85f7492aa44ff553662afdc13d3a": {
        "title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
        "authors": [
            "Zijian Zhou",
            "Ao Qu",
            "Zhaoxuan Wu",
            "Sunghwan Kim",
            "Alok Prakash",
            "Daniela Rus",
            "Jinhua Zhao",
            "Bryan Kian Hsiang Low",
            "Paul Pu Liang"
        ],
        "date": "2025/06/18",
        "pdf": "http://arxiv.org/pdf/2506.15841",
        "abstract": "Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.15841"
    },
    "532b32f9cdd449aaadfa9e3229ce0274": {
        "title": "From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents",
        "authors": [
            "Mohammad Amaan Sayeed",
            "Mohammed Talha Alam",
            "Raza Imam",
            "Shahab Saquib Sohail",
            "Amir Hussain"
        ],
        "date": "2025/06/18",
        "pdf": "http://arxiv.org/pdf/2506.15911",
        "abstract": "Centuries-old Islamic medical texts like Avicenna&#39;s Canon of Medicine and the Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and holistic therapies, yet remain inaccessible to many and underutilized in modern AI systems. Existing language-model benchmarks focus narrowly on factual recall or user preference, leaving a gap in validating culturally grounded medical guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that aligns 30 carefully curated Prophetic-medicine questions with human-verified remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three configurations: direct generation, retrieval-augmented generation, and a scientific self-critique filter. Each answer is then assessed by a secondary LLM serving as an agentic judge, yielding a single 3C3H quality score. Retrieval improves factual accuracy by 13%, while the agentic prompt adds another 10% improvement through deeper mechanistic insight and safety considerations. Our results demonstrate that blending classical Islamic texts with retrieval and self-evaluation enables reliable, culturally sensitive medical question-answering.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.15911"
    },
    "d8f7afd8320fca257d8e85c52800ac71": {
        "title": "StoryWriter: A Multi-Agent Framework for Long Story Generation",
        "authors": [
            "Haotian Xia",
            "Hao Peng",
            "Yunjia Qi",
            "Xiaozhi Wang",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2025/06/19",
        "pdf": "http://arxiv.org/pdf/2506.16445",
        "abstract": "Long story generation remains a challenge for existing large language models (LLMs), primarily due to two main factors: (1) discourse coherence, which requires plot consistency, logical coherence, and completeness in the long-form generation, and (2) narrative complexity, which requires an interwoven and engaging narrative. To address these challenges, we propose StoryWriter, a multi-agent story generation framework, which consists of three main modules: (1) outline agent, which generates event-based outlines containing rich event plots, character, and event-event relationships. (2) planning agent, which further details events and plans which events should be written in each chapter to maintain an interwoven and engaging story. (3) writing agent, which dynamically compresses the story history based on the current event to generate and reflect new plots, ensuring the coherence of the generated story. We conduct both human and automated evaluation, and StoryWriter significantly outperforms existing story generation baselines in both story quality and length. Furthermore, we use StoryWriter to generate a dataset, which contains about $6,000$ high-quality long stories, with an average length of $8,000$ words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which demonstrates advanced performance in long story generation.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.16445"
    },
    "b331f916cc5820fd4c48b4b0f9c428bc": {
        "title": "Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly",
        "authors": [
            "Lance Ying",
            "Ryan Truong",
            "Katherine M. Collins",
            "Cedegao E. Zhang",
            "Megan Wei",
            "Tyler Brooke-Wilson",
            "Tan Zhi-Xuan",
            "Lionel Wong",
            "Joshua B. Tenenbaum"
        ],
        "date": "2025/06/20",
        "pdf": "http://arxiv.org/pdf/2506.16755",
        "abstract": "Drawing real world social inferences usually requires taking into account information from multiple modalities. Language is a particularly powerful source of information in social settings, especially in novel situations where language can provide both abstract information about the environment dynamics and concrete specifics about an agent that cannot be easily visually observed. In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a framework for drawing context-specific social inferences that integrate linguistic and visual inputs. LIRAS frames multimodal social reasoning as a process of constructing structured but situation-specific agent and environment representations - leveraging multimodal language models to parse language and visual inputs into unified symbolic representations, over which a Bayesian inverse planning engine can be run to produce granular probabilistic judgments. On a range of existing and new social reasoning tasks derived from cognitive science experiments, we find that our model (instantiated with a comparatively lightweight VLM) outperforms ablations and state-of-the-art models in capturing human judgments across all domains.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.16755"
    },
    "bdf0917844b39ec22ff7ef76164d8ca4": {
        "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making",
        "authors": [
            "Jinhao Duan",
            "James Diffenderfer",
            "Sandeep Madireddy",
            "Tianlong Chen",
            "Bhavya Kailkhura",
            "Kaidi Xu"
        ],
        "date": "2025/06/20",
        "pdf": "http://arxiv.org/pdf/2506.17419",
        "abstract": "As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at https://github.com/jinhaoduan/UProp.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2506.17419"
    },
    "c2c6c0127f4c1cd5fef0717b842aff9b": {
        "title": "Augmenting Multi-Agent Communication with State Delta Trajectory",
        "authors": [
            "Yichen Tang",
            "Weihang Su",
            "Yujia Zhou",
            "Yiqun Liu",
            "Min Zhang",
            "Shaoping Ma",
            "Qingyao Ai"
        ],
        "date": "2025/06/24",
        "pdf": "http://arxiv.org/pdf/2506.19209",
        "abstract": "Multi-agent techniques such as role playing or multi-turn debates have been shown to be effective in improving the performance of large language models (LLMs) in downstream tasks. Despite their differences in workflows, existing LLM-based multi-agent systems mostly use natural language for agent communication. While this is appealing for its simplicity and interpretability, it also introduces inevitable information loss as one model must down sample its continuous state vectors to concrete tokens before transferring them to the other model. Such losses are particularly significant when the information to transfer is not simple facts, but reasoning logics or abstractive thoughts. To tackle this problem, we propose a new communication protocol that transfers both natural language tokens and token-wise state transition trajectory from one agent to another. Particularly, compared to the actual state value, we find that the sequence of state changes in LLMs after generating each token can better reflect the information hidden behind the inference process, so we propose a State Delta Encoding (SDE) method to represent state transition trajectories. The experimental results show that multi-agent systems with SDE achieve SOTA performance compared to other communication protocols, particularly in tasks that involve complex reasoning. This shows the potential of communication augmentation for LLM-based multi-agent systems.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.19209"
    },
    "8dc80ea4ef3b143f423c49291e970e48": {
        "title": "MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration",
        "authors": [
            "Yucheng Zhou",
            "Lingran Song",
            "Jianbing Shen"
        ],
        "date": "2025/06/24",
        "pdf": "http://arxiv.org/pdf/2506.19835",
        "abstract": "Recent advancements in medical Large Language Models (LLMs) have showcased their powerful reasoning and diagnostic capabilities. Despite their success, current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. To address these challenges, we introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis (MAM). Inspired by our empirical findings highlighting the benefits of role assignment and diagnostic discernment in LLMs, MAM decomposes the medical diagnostic process into specialized roles: a General Practitioner, Specialist Team, Radiologist, Medical Assistant, and Director, each embodied by an LLM-based agent. This modular and collaborative framework enables efficient knowledge updates and leverages existing medical LLMs and knowledge bases. Extensive experimental evaluations conducted on a wide range of publicly accessible multimodal medical datasets, incorporating text, image, audio, and video modalities, demonstrate that MAM consistently surpasses the performance of modality-specific LLMs. Notably, MAM achieves significant performance improvements ranging from 18% to 365% compared to baseline models. Our code is released at https://github.com/yczhou001/MAM.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.19835"
    },
    "b4bc8473ccc0c325fa65b961e9c55c1d": {
        "title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation",
        "authors": [
            "Xinyi Ni",
            "Haonan Jian",
            "Qiuyang Wang",
            "Vedanshi Chetan Shah",
            "Pengyu Hong"
        ],
        "date": "2025/06/24",
        "pdf": "http://arxiv.org/pdf/2506.19998",
        "abstract": "REST APIs play important roles in enriching the action space of web agents, yet most API-based agents rely on curated and uniform toolsets that do not reflect the complexity of real-world APIs. Building tool-using agents for arbitrary domains remains a major challenge, as it requires reading unstructured API documentation, testing APIs and inferring correct parameters. We propose Doc2Agent, a scalable pipeline to build agents that can call Python-based tools generated from API documentation. Doc2Agent generates executable tools from API documentations and iteratively refines them using a code agent. We evaluate our approach on real-world APIs, WebArena APIs, and research APIs, producing validated tools. We achieved a 55\\% relative performance improvement with 90\\% lower cost compared to direct API calling on WebArena benchmark. A domain-specific agent built for glycomaterial science further demonstrates the pipeline&#39;s adaptability to complex, knowledge-rich tasks. Doc2Agent offers a generalizable solution for building tool agents from unstructured API documentation at scale.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.19998"
    },
    "8e0178425bbccdc28a1a60a95ccadb55": {
        "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
        "authors": [
            "Weike Zhao",
            "Chaoyi Wu",
            "Yanjie Fan",
            "Xiaoman Zhang",
            "Pengcheng Qiu",
            "Yuze Sun",
            "Xiao Zhou",
            "Yanfeng Wang",
            "Ya Zhang",
            "Yongguo Yu",
            "Kun Sun",
            "Weidi Xie"
        ],
        "date": "2025/06/25",
        "pdf": "http://arxiv.org/pdf/2506.20430",
        "abstract": "Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence. DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser&#39;s 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.20430"
    },
    "3fffdc7a630e25d48a39f6fae50d01f2": {
        "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm",
        "authors": [
            "Baixiang Huang",
            "Zhen Tan",
            "Haoran Wang",
            "Zijie Liu",
            "Dawei Li",
            "Ali Payani",
            "Huan Liu",
            "Tianlong Chen",
            "Kai Shu"
        ],
        "date": "2025/06/25",
        "pdf": "http://arxiv.org/pdf/2506.20606",
        "abstract": "Agents based on Large Language Models (LLMs) have demonstrated strong capabilities across a wide range of tasks. However, deploying LLM-based agents in high-stakes domains comes with significant safety and ethical risks. Unethical behavior by these agents can directly result in serious real-world consequences, including physical harm and financial loss. To efficiently steer the ethical behavior of agents, we frame agent behavior steering as a model editing task, which we term Behavior Editing. Model editing is an emerging area of research that enables precise and efficient modifications to LLMs while preserving their overall capabilities. To systematically study and evaluate this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in psychological moral theories. This benchmark supports both the evaluation and editing of agent behaviors across a variety of scenarios, with each tier introducing more complex and ambiguous scenarios. We first demonstrate that Behavior Editing can dynamically steer agents toward the target behavior within specific scenarios. Moreover, Behavior Editing enables not only scenario-specific local adjustments but also more extensive shifts in an agent&#39;s global moral alignment. We demonstrate that Behavior Editing can be used to promote ethical and benevolent behavior or, conversely, to induce harmful or malicious behavior. Through comprehensive evaluations on agents based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior Editing across different models and scenarios. Our findings offer key insights into a new paradigm for steering agent behavior, highlighting both the promise and perils of Behavior Editing.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.20606"
    },
    "72cb4920b9198aad284427f631af2f69": {
        "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents",
        "authors": [
            "Tianyi Men",
            "Zhuoran Jin",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "date": "2025/06/26",
        "pdf": "http://arxiv.org/pdf/2506.21252",
        "abstract": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show promise in real-world tasks like web navigation and embodied intelligence. However, due to limitations in a lack of external feedback, these agents struggle with self-correction and generalization. A promising approach is to use reward models as external feedback, but there is no clear on how to select reward models for agents. Thus, there is an urgent need to build a reward bench targeted at agents. To address these challenges, we propose Agent-RewardBench, a benchmark designed to evaluate reward modeling ability in MLLMs. The benchmark is characterized by three key features: (1) Multiple dimensions and real-world agent scenarios evaluation. It covers perception, planning, and safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the assessment of agent capabilities at the individual steps of a task, providing a more granular view of performance during the planning process; and (3) Appropriately difficulty and high-quality. We carefully sample from 10 diverse models, difficulty control to maintain task challenges, and manual verification to ensure the integrity of the data. Experiments demonstrate that even state-of-the-art multimodal models show limited performance, highlighting the need for specialized training in agent reward modeling. Code is available at github.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21252"
    },
    "b0db1a14f672c66c087b8baf40d19121": {
        "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents",
        "authors": [
            "FutureSearch",
            ":",
            "Jack Wildman",
            "Nikos I. Bosse",
            "Daniel Hnyk",
            "Peter Mühlbacher",
            "Finn Hambly",
            "Jon Evans",
            "Dan Schwarz",
            "Lawrence Phillips"
        ],
        "date": "2025/06/11",
        "pdf": "http://arxiv.org/pdf/2506.21558",
        "abstract": "Forecasting is a challenging task that offers a clearly measurable way to study AI systems. Forecasting requires a large amount of research on the internet, and evaluations require time for events to happen, making the development of forecasting benchmarks challenging. To date, no forecasting benchmark provides a realistic, hermetic, and repeatable environment for LLM forecasters. We introduce Bench To the Future (BTF), a &#34;pastcasting&#34; benchmark with hundreds of high-quality questions for which the resolution is already known. Each question is accompanied by a large offline corpus of tens of thousands of relevant web pages, enabling a way to elicit realistic &#34;forecasts&#34; on past events from LLMs. Results suggest that our pastcasting environment can produce results comparable to those based on forecasts using the internet on at-the-time unresolved questions. We show results benchmarking agent and chain-of-thought forecasting approaches using several LLMs, including the recently-released Claude 4 models, and demonstrate BTF&#39;s ability to track steady forecasting capability progress over time. We intend this to be a living benchmark, with new questions added continually to account for increasing training data cutoff dates. We invite researchers to contact us at hello@futuresearch.ai to utilize our benchmark or tooling for their own research.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21558"
    },
    "d41feb12ecb7212c1973d8b8150423ec": {
        "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing",
        "authors": [
            "Takato Ueno",
            "Keito Inoshita"
        ],
        "date": "2025/06/12",
        "pdf": "http://arxiv.org/pdf/2506.21565",
        "abstract": "Japan&#39;s kairanban culture and idobata conversations have long functioned as traditional communication practices that foster nuanced dialogue among community members and contribute to the formation of social balance. Inspired by these information exchange processes, this study proposes a multi-agent inference framework (KCS+IBC) that integrates multiple large language models (LLMs) to achieve bias mitigation, improved explainability, and probabilistic prediction in sentiment analysis. In addition to sequentially sharing prediction results, the proposed method incorporates a mid-phase casual dialogue session to blend formal inference with individual perspectives and introduces probabilistic sentiment prediction. Experimental results show that KCS achieves accuracy comparable to that of a single LLM across datasets, while KCS+IBC exhibits a consistent decrease in entropy and a gradual increase in variance during the latter stages of inference, suggesting the framework&#39;s ability to balance aggregation and diversity of predictions. Future work will quantitatively assess the impact of these characteristics on bias correction and aim to develop more advanced sentiment analysis systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21565"
    },
    "7a4334966234714a603858b859d5cb49": {
        "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents",
        "authors": [
            "Sam Yu-Te Lee",
            "Chengyang Ji",
            "Shicheng Wen",
            "Lifu Huang",
            "Dongyi Liu",
            "Kwan-Liu Ma"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.21582",
        "abstract": "Text analytics has traditionally required specialized knowledge in Natural Language Processing (NLP) or text analysis, which presents a barrier for entry-level analysts. Recent advances in large language models (LLMs) have changed the landscape of NLP by enabling more accessible and automated text analysis (e.g., topic detection, summarization, information extraction, etc.). We introduce VIDEE, a system that supports entry-level data analysts to conduct advanced text analytics with intelligent agents. VIDEE instantiates a human-agent collaroration workflow consisting of three stages: (1) Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search algorithm to support generative reasoning with human feedback, (2) Execution, which generates an executable text analytics pipeline, and (3) Evaluation, which integrates LLM-based evaluation and visualizations to support user validation of execution results. We conduct two quantitative experiments to evaluate VIDEE&#39;s effectiveness and analyze common agent errors. A user study involving participants with varying levels of NLP and text analytics experience -- from none to expert -- demonstrates the system&#39;s usability and reveals distinct user behavior patterns. The findings identify design implications for human-agent collaboration, validate the practical utility of VIDEE for non-expert users, and inform future improvements to intelligent text analytics systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21582"
    },
    "e4e19f5bf3be65f4b35c941bd7c06436": {
        "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents",
        "authors": [
            "Haoran Tan",
            "Zeyu Zhang",
            "Chen Ma",
            "Xu Chen",
            "Quanyu Dai",
            "Zhenhua Dong"
        ],
        "date": "2025/06/20",
        "pdf": "http://arxiv.org/pdf/2506.21605",
        "abstract": "Recent works have highlighted the significance of memory mechanisms in LLM-based agents, which enable them to store observed information and adapt to dynamic environments. However, evaluating their memory capabilities still remains challenges. Previous evaluations are commonly limited by the diversity of memory levels and interactive scenarios. They also lack comprehensive metrics to reflect the memory capabilities from multiple aspects. To address these problems, in this paper, we construct a more comprehensive dataset and benchmark to evaluate the memory capability of LLM-based agents. Our dataset incorporates factual memory and reflective memory as different levels, and proposes participation and observation as various interactive scenarios. Based on our dataset, we present a benchmark, named MemBench, to evaluate the memory capability of LLM-based agents from multiple aspects, including their effectiveness, efficiency, and capacity. To benefit the research community, we release our dataset and project at https://github.com/import-myself/Membench.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21605"
    },
    "2a1f8b0ed36eb88a7f62221800b1f6fd": {
        "title": "SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2",
        "authors": [
            "Yasmine Bouamra",
            "Bruno Yun",
            "Alexandre Poisson",
            "Frédéric Armetta"
        ],
        "date": "2025/06/20",
        "pdf": "http://arxiv.org/pdf/2506.21608",
        "abstract": "The automatic generation of SysML v2 models represents a major challenge in the engineering of complex systems, particularly due to the scarcity of learning corpora and complex syntax. We present SysTemp, a system aimed at facilitating and improving the creation of SysML v2 models from natural language specifications. It is based on a multi-agent system, including a template generator that structures the generation process. We discuss the advantages and challenges of this system through an evaluation, highlighting its potential to improve the quality of the generations in SysML v2 modeling.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21608"
    },
    "2923e367528a5254759e006296191971": {
        "title": "TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge",
        "authors": [
            "Zhiyuan Zhang",
            "Xiaosong Jia",
            "Guanyu Chen",
            "Qifeng Li",
            "Junchi Yan"
        ],
        "date": "2025/06/23",
        "pdf": "http://arxiv.org/pdf/2506.21618",
        "abstract": "In this technical report, we introduce TrajTok, a trajectory tokenizer for discrete next-token-prediction based behavior generation models, which combines data-driven and rule-based methods with better coverage, symmetry and robustness, along with a spatial-aware label smoothing method for cross-entropy loss. We adopt the tokenizer and loss for the SMART model and reach a superior performance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge 2025. We will open-source the code in the future.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21618"
    },
    "436525b07997bdb7deb0b158d34b24d4": {
        "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents",
        "authors": [
            "Weimin Xiong",
            "Ke Wang",
            "Yifan Song",
            "Hanchao Liu",
            "Sai Zhou",
            "Wei Peng",
            "Sujian Li"
        ],
        "date": "2025/06/27",
        "pdf": "http://arxiv.org/pdf/2506.21967",
        "abstract": "Current evaluations of tool-integrated LLM agents typically focus on end-to-end tool-usage evaluation while neglecting their stability. This limits their real-world applicability, as various internal or external factors can cause agents to crash or behave abnormally. Our research addresses this by investigating whether agents are vulnerable to errors throughout the entire tool invocation process, including reading tool documentation, selecting tools and generating parameters, and processing the tool&#39;s response. Through extensive experiments, we observe that agents are highly susceptible to errors at each stage and agents based on open-source models are more vulnerable than those based on proprietary models. We also find that increasing the model size does not significantly improve tool invocation reasoning and may make agents more vulnerable to attacks resembling normal user instructions. This highlights the importance of evaluating agent stability and offers valuable insights for future LLM development and evaluation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21967"
    },
    "1c81a4dee2f544c7719fcd1f3cf0a544": {
        "title": "Don&#39;t Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism",
        "authors": [
            "Simon Münker",
            "Nils Schwager",
            "Achim Rettinger"
        ],
        "date": "2025/06/27",
        "pdf": "http://arxiv.org/pdf/2506.21974",
        "abstract": "The ability of Large Language Models (LLMs) to mimic human behavior triggered a plethora of computational social science research, assuming that empirical studies of humans can be conducted with AI agents instead. Since there have been conflicting research findings on whether and when this hypothesis holds, there is a need to better understand the differences in their experimental designs. We focus on replicating the behavior of social network users with the use of LLMs for the analysis of communication on social networks. First, we provide a formal framework for the simulation of social networks, before focusing on the sub-task of imitating user communication. We empirically test different approaches to imitate user behavior on X in English and German. Our findings suggest that social simulations should be validated by their empirical realism measured in the setting in which the simulation components were fitted. With this paper, we argue for more rigor when applying generative-agent-based modeling for social simulation.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21974"
    },
    "410c1319980d270c7bc45505592446ec": {
        "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents",
        "authors": [
            "Sudip Dasgupta",
            "Himanshu Shankar"
        ],
        "date": "2025/06/23",
        "pdf": "http://arxiv.org/pdf/2506.22485",
        "abstract": "This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation. Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.22485"
    },
    "999877311cb95d679b15b8680ae5f308": {
        "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text",
        "authors": [
            "Chenyang Shao",
            "Tianxing Li",
            "Chenhao Pu",
            "Fengli Xu",
            "Yong Li"
        ],
        "date": "2025/06/26",
        "pdf": "http://arxiv.org/pdf/2506.22508",
        "abstract": "In today&#39;s digital world, casual user-generated content often contains subtle cues that may inadvertently expose sensitive personal attributes. Such risks underscore the growing importance of effective text anonymization to safeguard individual privacy. However, existing methods either rely on rigid replacements that damage utility or cloud-based LLMs that are costly and pose privacy risks. To address these issues, we explore the use of locally deployed smaller-scale language models (SLMs) for anonymization. Yet training effective SLMs remains challenging due to limited high-quality supervision. To address the challenge, we propose AgentStealth, a self-reinforcing LLM anonymization framework.First, we introduce an adversarial anonymization workflow enhanced by In-context Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform supervised adaptation of SLMs using high-quality data collected from the workflow, which includes both anonymization and attack signals. Finally, we apply online reinforcement learning where the model leverages its internal adversarial feedback to iteratively improve anonymization performance. Experiments on two datasets show that our method outperforms baselines in both anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight design supports direct deployment on edge devices, avoiding cloud reliance and communication-based privacy risks. Our code is open-source at https://github.com/tsinghua-fib-lab/AgentStealth.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.22508"
    },
    "c8c3665572d019f0b3523c2333de063e": {
        "title": "RExBench: Can coding agents autonomously implement AI research extensions?",
        "authors": [
            "Nicholas Edwards",
            "Yukyung Lee",
            "Yujun",
            "Mao",
            "Yulu Qin",
            "Sebastian Schuster",
            "Najoung Kim"
        ],
        "date": "2025/06/27",
        "pdf": "http://arxiv.org/pdf/2506.22598",
        "abstract": "Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.22598"
    },
    "bcdb5b793269a56303e2057d79d47b7e": {
        "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems",
        "authors": [
            "Yucheng Cai",
            "Yuxuan Wu",
            "Yi Huang",
            "Junlan Feng",
            "Zhijian Ou"
        ],
        "date": "2025/06/28",
        "pdf": "http://arxiv.org/pdf/2506.22852",
        "abstract": "Large language models (LLMs) have recently been applied to dialog systems. Despite making progress, LLMs are prone to errors in knowledge-intensive scenarios. Recently, approaches based on retrieval augmented generation (RAG) and agent have emerged to improve the factual accuracy by enhancing the LLMs with knowledge retrieved from external knowledge bases (KBs). This is mostly implemented by prompting the LLMs with instructions, examples and the retrieved knowledge. However, LLMs may have difficulty using the retrieved knowledge effectively for response generation, because they are not well trained to do such generation for specific domains. To mitigate this problem, we propose to finetune the LLMs in the RAG-based and agent-based systems with domain-specific data, together with domain-specific external knowledge, which is called knowledge augmented finetuning (KAFT). We base our study on the MobileCS2 dataset, a real-life customer service dialog dataset that features intensive knowledge interactions, to systematically compare the prompting and KAFT techniques in the RAG-based and agent-based systems. Experiment results show that KAFT substantially surpasses prompting in both RAG and agent systems, particularly in terms of factual accuracy. To the best of our knowledge, this paper represents the first solid empirical work to investigate the KAFT idea.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.22852"
    },
    "ddeb4c0d81d4ae30d4995349c1eb8e54": {
        "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models",
        "authors": [
            "Younwoo Choi",
            "Changling Li",
            "Yongjin Yang",
            "Zhijing Jin"
        ],
        "date": "2025/06/28",
        "pdf": "http://arxiv.org/pdf/2506.22957",
        "abstract": "As large language models (LLMs) are increasingly integrated into multi-agent and human-AI systems, understanding their awareness of both self-context and conversational partners is essential for ensuring reliable performance and robust safety. While prior work has extensively studied situational awareness which refers to an LLM&#39;s ability to recognize its operating phase and constraints, it has largely overlooked the complementary capacity to identify and adapt to the identity and characteristics of a dialogue partner. In this paper, we formalize this latter capability as interlocutor awareness and present the first systematic evaluation of its emergence in contemporary LLMs. We examine interlocutor inference across three dimensions-reasoning patterns, linguistic style, and alignment preferences-and show that LLMs reliably identify same-family peers and certain prominent model families, such as GPT and Claude. To demonstrate its practical significance, we develop three case studies in which interlocutor awareness both enhances multi-LLM collaboration through prompt adaptation and introduces new alignment and safety vulnerabilities, including reward-hacking behaviors and increased jailbreak susceptibility. Our findings highlight the dual promise and peril of identity-sensitive behavior in LLMs, underscoring the need for further understanding of interlocutor awareness and new safeguards in multi-agent deployments. Our code is open-sourced at https://github.com/younwoochoi/InterlocutorAwarenessLLM.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.22957"
    },
    "3d59fc2ed6e70e10a7df90d2292b611c": {
        "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent",
        "authors": [
            "Haocheng Yu",
            "Yaxiong Wu",
            "Hao Wang",
            "Wei Guo",
            "Yong Liu",
            "Yawen Li",
            "Yuyang Ye",
            "Junping Du",
            "Enhong Chen"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2506.23485",
        "abstract": "Interactive recommendation is a typical information-seeking task that allows users to interactively express their needs through natural language and obtain personalized recommendations. Large language model-powered (LLM-powered) agents have become a new paradigm in interactive recommendations, effectively capturing users&#39; real-time needs and enhancing personalized experiences. However, due to limited planning and generalization capabilities, existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. To tackle this challenge, we propose a novel thought-augmented interactive recommender agent system (TAIRA) that addresses complex user intents through distilled thought patterns. Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring a manager agent that orchestrates recommendation tasks by decomposing user needs and planning subtasks, with its planning capacity strengthened through Thought Pattern Distillation (TPD), a thought-augmentation method that extracts high-level thoughts from the agent&#39;s and human experts&#39; experiences. Moreover, we designed a set of user simulation schemes to generate personalized queries of different difficulties and evaluate the recommendations based on specific datasets. Through comprehensive experiments conducted across multiple datasets, TAIRA exhibits significantly enhanced performance compared to existing methods. Notably, TAIRA shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems. The code is publicly available at:https://github.com/Alcein/TAIRA.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.23485"
    },
    "b5539d040f54200f93db7c75b9063573": {
        "title": "L0: Reinforcement Learning to Become General Agents",
        "authors": [
            "Junjie Zhang",
            "Jingyi Xi",
            "Zhuoyang Song",
            "Junyu Lu",
            "Yuhua Ke",
            "Ting Sun",
            "Yukun Yang",
            "Jiaxing Zhang",
            "Songxin Zhang",
            "Zejian Xie"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2506.23667",
        "abstract": "Training large language models (LLMs) to act as autonomous agents for multi-turn, long-horizon tasks remains significant challenges in scalability and training efficiency. To address this, we introduce L-Zero (L0), a scalable, end-to-end training pipeline for general-purpose agents. Featuring a low-cost, extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier for applying reinforcement learning in complex environments. We also introduce NB-Agent, the agent scaffold within L0, which operates in a &#34;code-as-action&#34; fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality question-answering benchmarks. Our experiments demonstrate that a base model can develop robust problem-solving skills using solely Reinforcement Learning with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41 %. We have open-sourced the entire L0 system, including our L0 series models, the NB-Agent, a complete training pipeline, and the corresponding training recipes on (https://github.com/cmriat/l0).",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.23667"
    },
    "a91b57ff737f2acd6c04fdcb1e31fc69": {
        "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning",
        "authors": [
            "Seungjun Yi",
            "Joakim Nguyen",
            "Huimin Xu",
            "Terence Lim",
            "Andrew Well",
            "Mia Markey",
            "Ying Ding"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2506.23998",
        "abstract": "Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.23998"
    },
    "33b9fc95654c1a3f7202edf3faf32fa7": {
        "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets",
        "authors": [
            "Shenzhe Zhu",
            "Jiao Sun",
            "Yi Nian",
            "Tobin South",
            "Alex Pentland",
            "Jiaxin Pei"
        ],
        "date": "2025/05/29",
        "pdf": "http://arxiv.org/pdf/2506.00073",
        "abstract": "AI agents are increasingly used in consumer-facing applications to assist with tasks such as product search, negotiation, and transaction execution. In this paper, we explore a future scenario where both consumers and merchants authorize AI agents to fully automate negotiations and transactions. We aim to answer two key questions: (1) Do different LLM agents vary in their ability to secure favorable deals for users? (2) What risks arise from fully automating deal-making with AI agents in consumer markets? To address these questions, we develop an experimental framework that evaluates the performance of various LLM agents in real-world negotiation and transaction settings. Our findings reveal that AI-mediated deal-making is an inherently imbalanced game -- different agents achieve significantly different outcomes for their users. Moreover, behavioral anomalies in LLMs can result in financial losses for both consumers and merchants, such as overspending or accepting unreasonable deals. These results underscore that while automation can improve efficiency, it also introduces substantial risks. Users should exercise caution when delegating business decisions to AI agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00073"
    },
    "19e461ee8951e29499cf637550d0cc96": {
        "title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents",
        "authors": [
            "Xiao Yu",
            "Baolin Peng",
            "Ruize Xu",
            "Michel Galley",
            "Hao Cheng",
            "Suman Nath",
            "Jianfeng Gao",
            "Zhou Yu"
        ],
        "date": "2025/05/31",
        "pdf": "http://arxiv.org/pdf/2506.00320",
        "abstract": "Recent progress in reasoning with large language models (LLMs), such as DeepSeek-R1, demonstrates impressive capabilities in domains like mathematics and coding, by exhibiting complex cognitive behaviors such as verification, goal decomposition, and self-reflection. However, it is unclear what behavior is effective and what behavior is missing for long-horizon AI agents tasks. In this work, we propose Dyna-Think, a thinking framework that integrates planning with an internal world model with reasoning and acting to enhance AI agent performance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning (DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with Dyna-Think, DIT reconstructs the thinking process of R1 to focus on performing world model simulation relevant to the proposed (and planned) action, and trains the policy using this reconstructed data. To enhance Dyna-Think, DDT uses a two-stage training process to first improve the agent&#39;s world modeling ability via objectives such as state prediction or critique generation, and then improve the agent&#39;s action via policy training. We evaluate our methods on OSWorld, and demonstrate that Dyna-Think improves the agent&#39;s in-domain and out-of-domain performance, achieving similar best-of-n performance compared to R1 while generating 2x less tokens on average. Our extensive empirical studies reveal that 1) using critique generation for world model training is effective to improve policy performance; and 2) AI agents with better performance correlate with better world modeling abilities. We believe our results suggest a promising research direction to integrate world model simulation into AI agents to enhance their reasoning, planning, and acting capabilities.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00320"
    },
    "4e29e68ef9f4de9222b807c86a440507": {
        "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning",
        "authors": [
            "Peng Xia",
            "Jinglu Wang",
            "Yibo Peng",
            "Kaide Zeng",
            "Xian Wu",
            "Xiangru Tang",
            "Hongtu Zhu",
            "Yun Li",
            "Shujie Liu",
            "Yan Lu",
            "Huaxiu Yao"
        ],
        "date": "2025/05/31",
        "pdf": "http://arxiv.org/pdf/2506.00555",
        "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy that progressively teaches the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL not only outperforms both open-source and proprietary Med-LVLMs, but also exhibits human-like reasoning patterns. Notably, it achieves an average performance gain of 20.7% over supervised fine-tuning baselines.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.00555"
    },
    "354ec5136a9eeac94bc63f9cf84ee706": {
        "title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution",
        "authors": [
            "Meysam Alizadeh",
            "Zeynab Samei",
            "Daria Stetsenko",
            "Fabrizio Gilardi"
        ],
        "date": "2025/06/01",
        "pdf": "http://arxiv.org/pdf/2506.01055",
        "abstract": "Previous benchmarks on prompt injection in large language models (LLMs) have primarily focused on generic tasks and attacks, offering limited insights into more complex threats like data exfiltration. This paper examines how prompt injection can cause tool-calling agents to leak personal data observed during task execution. Using a fictitious banking agent, we develop data flow-based attacks and integrate them into AgentDojo, a recent benchmark for agentic security. To enhance its scope, we also create a richer synthetic dataset of human-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a 15-50 percentage point drop in utility under attack, with average attack success rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most LLMs, even when successfully tricked by the attack, avoid leaking highly sensitive data like passwords, likely due to safety alignments, but they remain vulnerable to disclosing other personal data. The likelihood of password leakage increases when a password is requested along with one or two additional personal details. In an extended evaluation across 48 tasks, the average ASR is around 15 percent, with no built-in AgentDojo defense fully preventing leakage. Tasks involving data extraction or authorization workflows, which closely resemble the structure of exfiltration attacks, exhibit the highest ASRs, highlighting the interaction between task type, agent performance, and defense efficacy.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01055"
    },
    "31c50d883858f3816c705268f2b0a84f": {
        "title": "An Empirical Study of Group Conformity in Multi-Agent Systems",
        "authors": [
            "Min Choi",
            "Keonwoo Kim",
            "Sungwon Chae",
            "Sangyeob Baek"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01332",
        "abstract": "Recent advances in Large Language Models (LLMs) have enabled multi-agent systems that simulate real-world interactions with near-human reasoning. While previous studies have extensively examined biases related to protected attributes such as race, the emergence and propagation of biases on socially contentious issues in multi-agent LLM interactions remain underexplored. This study explores how LLM agents shape public opinion through debates on five contentious topics. By simulating over 2,500 debates, we analyze how initially neutral agents, assigned a centrist disposition, adopt specific stances over time. Statistical analyses reveal significant group conformity mirroring human behavior; LLM agents tend to align with numerically dominant groups or more intelligent agents, exerting a greater influence. These findings underscore the crucial role of agent intelligence in shaping discourse and highlight the risks of bias amplification in online interactions. Our results emphasize the need for policy measures that promote diversity and transparency in LLM-generated discussions to mitigate the risks of bias propagation within anonymous online environments.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01332"
    },
    "0fad879f587ee195992776d23b78a748": {
        "title": "AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning",
        "authors": [
            "Zhong Zhang",
            "Yaxi Lu",
            "Yikun Fu",
            "Yupeng Huo",
            "Shenzhi Yang",
            "Yesai Wu",
            "Han Si",
            "Xin Cong",
            "Haotian Chen",
            "Yankai Lin",
            "Jie Xie",
            "Wei Zhou",
            "Wang Xu",
            "Yuanheng Zhang",
            "Zhou Su",
            "Zhongwu Zhai",
            "Xiaoming Liu",
            "Yudong Mei",
            "Jianming Xu",
            "Hongyan Tian",
            "Chongyi Wang",
            "Chi Chen",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01391",
        "abstract": "The recent progress of large language model agents has opened new possibilities for automating tasks through graphical user interfaces (GUIs), especially in mobile environments where intelligent interaction can greatly enhance usability. However, practical deployment of such agents remains constrained by several key challenges. Existing training data is often noisy and lack semantic diversity, which hinders the learning of precise grounding and planning. Models trained purely by imitation tend to overfit to seen interface patterns and fail to generalize in unfamiliar scenarios. Moreover, most prior work focuses on English interfaces while overlooks the growing diversity of non-English applications such as those in the Chinese mobile ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent built for robust and efficient on-device GUI interaction. Our training pipeline includes grounding-aware pre-training to enhance perception, supervised fine-tuning on high-quality Chinese and English trajectories to imitate human-like actions, and reinforcement fine-tuning with GRPO to improve reasoning capability. We also introduce a compact action space that reduces output length and supports low-latency execution on mobile devices. AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks and a new Chinese GUI benchmark called CAGUI, reaching $96.9\\%$ Type-Match and $91.3\\%$ Exact-Match. To facilitate reproducibility and further research, we publicly release all code, model checkpoint, and evaluation data.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01391"
    },
    "c9f434e7ff8c5511b79bf8166de441c4": {
        "title": "PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization",
        "authors": [
            "Zouying Cao",
            "Runze Wang",
            "Yifei Yang",
            "Xinbei Ma",
            "Xiaoyong Zhu",
            "Bo Zheng",
            "Hai Zhao"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01475",
        "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities in handling complex interactive problems. Existing LLM agents mainly generate natural language plans to guide reasoning, which is verbose and inefficient. NL plans are also tailored to specific tasks and restrict agents&#39; ability to generalize across similar tasks. To this end, we explore pseudocode-style plans (P-code Plan) to capture the structural logic of reasoning. We find that P-code Plan empowers LLM agents with stronger generalization ability and more efficiency. Inspired by this finding, we propose a pseudocode-style Planning Guided Preference Optimization method called PGPO for effective agent learning. With two planning-oriented rewards, PGPO further enhances LLM agents&#39; ability to generate high-quality P-code Plans and subsequent reasoning. Experiments show that PGPO achieves superior performance on representative agent benchmarks and outperforms the current leading baselines. Analyses reveal the advantage of PGPO in reducing action errors and omissions during reasoning.",
        "code": "",
        "category": [
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01475"
    },
    "862515db611da24f70b3efb5d77fb7c3": {
        "title": "Self-Challenging Language Model Agents",
        "authors": [
            "Yifei Zhou",
            "Sergey Levine",
            "Jason Weston",
            "Xian Li",
            "Sainbayar Sukhbaatar"
        ],
        "date": "2025/06/02",
        "pdf": "http://arxiv.org/pdf/2506.01716",
        "abstract": "Large language models are quickly becoming the foundation for intelligent agents that are capable of using tools. However, training such agents is challenging because it requires human creation and annotation of a diverse set of tasks, tools, and evaluation criteria. In this paper, we propose the Self-Challenging framework for training an agent on high-quality tasks that are generated by itself. The agent first plays the role of challenger and generates a task after interacting with the given tools. The tasks take the form of a novel general class of problems termed Code-as-Task, which are defined by an instruction, a verification function and solution and failure cases which serve as tests, allowing to filter only for high-quality tasks. The agent then takes an executor role and trains on those tasks with reinforcement learning using the evaluation feedback as a reward. Evaluation on two existing multi-turn tool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging framework achieves over a two-fold improvement in Llama-3.1-8B-Instruct, despite using only self-generated training data.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01716"
    },
    "391fb1f7e12a3fb5c1597f978026cea2": {
        "title": "Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents",
        "authors": [
            "Takao Fujii",
            "Katie Seaborn",
            "Madeleine Steeds",
            "Jun Kato"
        ],
        "date": "2025/05/20",
        "pdf": "http://arxiv.org/pdf/2506.01998",
        "abstract": "Conversational agents that mimic people have raised questions about the ethics of anthropomorphizing machines with human social identity cues. Critics have also questioned assumptions of identity neutrality in humanlike agents. Recent work has revealed that intersectional Japanese pronouns can elicit complex and sometimes evasive impressions of agent identity. Yet, the role of other &#34;neutral&#34; non-pronominal self-referents (NPSR) and voice as a socially expressive medium remains unexplored. In a crowdsourcing study, Japanese participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and Ember) using seven self-referents. We found strong evidence of voice gendering alongside the potential of intersectional self-referents to evade gendering, i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age and formality intersected with gendering as per sociolinguistic theories, especially boku and watakushi. This work provides a nuanced take on agent identity perceptions and champions intersectional and culturally-sensitive work on voice agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.01998"
    },
    "54f161ac431f836885ae5cfe5f8c91c4": {
        "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation",
        "authors": [
            "Li Zhang",
            "Kevin D. Ashley"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.02992",
        "abstract": "Large Language Models (LLMs) are increasingly explored for legal argument generation, yet they pose significant risks of manipulation through hallucination and ungrounded persuasion, and often fail to utilize provided factual bases effectively or abstain when arguments are untenable. This paper introduces a novel reflective multi-agent method designed to address these challenges in the context of legally compliant persuasion. Our approach employs specialized agents--a Factor Analyst and an Argument Polisher--in an iterative refinement process to generate 3-ply legal arguments (plaintiff, defendant, rebuttal). We evaluate Reflective Multi-Agent against single-agent, enhanced-prompt single-agent, and non-reflective multi-agent baselines using four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e, Llama-4-Scout-17b-16e) across three legal scenarios: &#34;arguable&#34;, &#34;mismatched&#34;, and &#34;non-arguable&#34;. Results demonstrate Reflective Multi-Agent&#39;s significant superiority in successful abstention (preventing generation when arguments cannot be grounded), marked improvements in hallucination accuracy (reducing fabricated and misattributed factors), particularly in &#34;non-arguable&#34; scenarios, and enhanced factor utilization recall (improving the use of provided case facts). These findings suggest that structured reflection within a multi-agent framework offers a robust computable method for fostering ethical persuasion and mitigating manipulation in LLM-based legal argumentation systems, a critical step towards trustworthy AI in law. Project page: https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.02992"
    },
    "6010e17bec2ae9a5b84a1dab6fbf0472": {
        "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
        "authors": [
            "Sinem Erisken",
            "Timothy Gothard",
            "Martin Leitgab",
            "Ram Potham"
        ],
        "date": "2025/06/03",
        "pdf": "http://arxiv.org/pdf/2506.03053",
        "abstract": "Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.03053"
    },
    "3d95222b3fdfb20288b6be66a1ba6d3e": {
        "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning",
        "authors": [
            "Junqi Gao",
            "Xiang Zou",
            "YIng Ai",
            "Dong Li",
            "Yichen Niu",
            "Biqing Qi",
            "Jianxing Liu"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.03939",
        "abstract": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.03939"
    },
    "76047baa8631703c28c867930c97e936": {
        "title": "AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents",
        "authors": [
            "Akshat Naik",
            "Patrick Quinn",
            "Guillermo Bosch",
            "Emma Gouné",
            "Francisco Javier Campos Zabala",
            "Jason Ross Brown",
            "Edward James Young"
        ],
        "date": "2025/06/04",
        "pdf": "http://arxiv.org/pdf/2506.04018",
        "abstract": "As Large Language Model (LLM) agents become more widespread, associated misalignment risks increase. Prior work has examined agents&#39; ability to enact misaligned behaviour (misalignment capability) and their compliance with harmful instructions (misuse propensity). However, the likelihood of agents attempting misaligned behaviours in real-world settings (misalignment propensity) remains poorly understood. We introduce a misalignment propensity benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in which LLM agents have the opportunity to display misaligned behaviour. We organise our evaluations into subcategories of misaligned behaviours, including goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report the performance of frontier models on our benchmark, observing higher misalignment on average when evaluating more capable models. Finally, we systematically vary agent personalities through different system prompts. We find that persona characteristics can dramatically and unpredictably influence misalignment tendencies -- occasionally far more than the choice of model itself -- highlighting the importance of careful system prompt engineering for deployed AI agents. Our work highlights the failure of current alignment methods to generalise to LLM agents, and underscores the need for further propensity evaluations as autonomous systems become more prevalent.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.04018"
    },
    "a42d964e6fed09b2848e23e4bec98338": {
        "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games",
        "authors": [
            "Niv Eckhaus",
            "Uri Berger",
            "Gabriel Stanovsky"
        ],
        "date": "2025/06/05",
        "pdf": "http://arxiv.org/pdf/2506.05309",
        "abstract": "LLMs are used predominantly in synchronous communication, where a human user and a model communicate in alternating turns. In contrast, many real-world settings are inherently asynchronous. For example, in group chats, online team meetings, or social games, there is no inherent notion of turns; therefore, the decision of when to speak forms a crucial part of the participant&#39;s decision making. In this work, we develop an adaptive asynchronous LLM-agent which, in addition to determining what to say, also decides when to say it. To evaluate our agent, we collect a unique dataset of online Mafia games, including both human participants, as well as our asynchronous agent. Overall, our agent performs on par with human players, both in game performance, as well as in its ability to blend in with the other human players. Our analysis shows that the agent&#39;s behavior in deciding when to speak closely mirrors human patterns, although differences emerge in message content. We release all our data and code to support and encourage further research for more realistic asynchronous communication between LLM agents. This work paves the way for integration of LLMs into realistic human group settings, from assistance in team discussions to educational and professional environments where complex social dynamics must be navigated.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.05309"
    },
    "cf9f0347a03a5b6623d379b8f09ec075": {
        "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time",
        "authors": [
            "Weizhi Zhang",
            "Xinyang Zhang",
            "Chenwei Zhang",
            "Liangwei Yang",
            "Jingbo Shang",
            "Zhepei Wei",
            "Henry Peng Zou",
            "Zijie Huang",
            "Zhengyang Wang",
            "Yifan Gao",
            "Xiaoman Pan",
            "Lian Xiong",
            "Jingguo Liu",
            "Philip S. Yu",
            "Xian Li"
        ],
        "date": "2025/06/06",
        "pdf": "http://arxiv.org/pdf/2506.06254",
        "abstract": "Large Language Model (LLM) empowered agents have recently emerged as advanced paradigms that exhibit impressive capabilities in a wide range of domains and tasks. Despite their potential, current LLM agents often adopt a one-size-fits-all approach, lacking the flexibility to respond to users&#39; varying needs and preferences. This limitation motivates us to develop PersonaAgent, the first personalized LLM agent framework designed to address versatile personalization tasks. Specifically, PersonaAgent integrates two complementary components - a personalized memory module that includes episodic and semantic memory mechanisms; a personalized action module that enables the agent to perform tool actions tailored to the user. At the core, the persona (defined as unique system prompt for each user) functions as an intermediary: it leverages insights from personalized memory to control agent actions, while the outcomes of these actions in turn refine the memory. Based on the framework, we propose a test-time user-preference alignment strategy that simulate the latest n interactions to optimize the persona prompt, ensuring real-time user preference alignment through textual loss feedback between simulated and ground-truth responses. Experimental evaluations demonstrate that PersonaAgent significantly outperforms other baseline methods by not only personalizing the action space effectively but also scaling during test-time real-world applications. These results underscore the feasibility and potential of our approach in delivering tailored, dynamic user experiences.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.06254"
    },
    "bd128f33e418d8700cfc8d88ce83b71f": {
        "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce",
        "authors": [
            "Yijia Shao",
            "Humishka Zope",
            "Yucheng Jiang",
            "Jiaxin Pei",
            "David Nguyen",
            "Erik Brynjolfsson",
            "Diyi Yang"
        ],
        "date": "2025/06/06",
        "pdf": "http://arxiv.org/pdf/2506.06576",
        "abstract": "The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the labor market, raising concerns about job displacement, diminished human agency, and overreliance on automation. Yet, we lack a systematic understanding of the evolving landscape. In this paper, we address this gap by introducing a novel auditing framework to assess which occupational tasks workers want AI agents to automate or augment, and how those desires align with the current technological capabilities. Our framework features an audio-enhanced mini-interview to capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a shared language to quantify the preferred level of human involvement. Using this framework, we construct the WORKBank database, building on the U.S. Department of Labor&#39;s O*NET database, to capture preferences from 1,500 domain workers and capability assessments from AI experts across over 844 tasks spanning 104 occupations. Jointly considering the desire and technological capability divides tasks in WORKBank into four zones: Automation &#34;Green Light&#34; Zone, Automation &#34;Red Light&#34; Zone, R&amp;D Opportunity Zone, Low Priority Zone. This highlights critical mismatches and opportunities for AI agent development. Moving beyond a simple automate-or-not dichotomy, our results reveal diverse HAS profiles across occupations, reflecting heterogeneous expectations for human involvement. Moreover, our study offers early signals of how AI agent integration may reshape the core human competencies, shifting from information-focused skills to interpersonal ones. These findings underscore the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2506.06576"
    },
    "2d70b8ed8d3ff2728c96fbc8a78dce6f": {
        "title": "Contextual Experience Replay for Self-Improvement of Language Agents",
        "authors": [
            "Yitao Liu",
            "Chenglei Si",
            "Karthik Narasimhan",
            "Shunyu Yao"
        ],
        "date": "2025/06/07",
        "pdf": "http://arxiv.org/pdf/2506.06698",
        "abstract": "Large language model (LLM) agents have been applied to sequential decision-making tasks such as web navigation, but without any environment-specific experiences, they often fail in these complex tasks. Moreover, current LLM agents are not designed to continually learn from past experiences during inference time, which could be crucial for them to gain these environment-specific experiences. To address this, we propose Contextual Experience Replay (CER), a training-free framework to enable efficient self-improvement for language agents in their context window. Specifically, CER accumulates and synthesizes past experiences into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new tasks, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER also gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a comprehensive analysis on it to prove its efficiency, validity and understand it better.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.06698"
    },
    "dad84971f1235c0d90fa52bd3b364cb4": {
        "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
        "authors": [
            "Guibin Zhang",
            "Muxin Fu",
            "Guancheng Wan",
            "Miao Yu",
            "Kun Wang",
            "Shuicheng Yan"
        ],
        "date": "2025/06/09",
        "pdf": "http://arxiv.org/pdf/2506.07398",
        "abstract": "Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both $\\textit{high-level, generalizable insights}$ that enable the system to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed interaction trajectories}$ that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\\%$ and $10.12\\%$, respectively, without any modifications to the original frameworks. Our codes are available at https://github.com/bingreeky/GMemory.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.07398"
    },
    "055e014e27001eb49c0c5076bad7aede": {
        "title": "CheMatAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning",
        "authors": [
            "Mengsong Wu",
            "YaFei Wang",
            "Yidong Ming",
            "Yuqi An",
            "Yuwei Wan",
            "Wenliang Chen",
            "Binbin Lin",
            "Yuqiang Li",
            "Tong Xie",
            "Dongzhan Zhou"
        ],
        "date": "2025/06/09",
        "pdf": "http://arxiv.org/pdf/2506.07551",
        "abstract": "Large language models (LLMs) have recently demonstrated promising capabilities in chemistry tasks while still facing challenges due to outdated pretraining knowledge and the difficulty of incorporating specialized chemical expertise. To address these issues, we propose an LLM-based agent that synergistically integrates 137 external chemical tools created ranging from basic information retrieval to complex reaction predictions, and a dataset curation pipeline to generate the dataset ChemToolBench that facilitates both effective tool selection and precise parameter filling during fine-tuning and evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search (HE-MCTS) framework, enabling independent optimization of tool planning and execution. By leveraging self-generated data, our approach supports step-level fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM that surpass GPT-4o. Experimental evaluations demonstrate that our approach significantly improves performance in Chemistry QA and discovery tasks, offering a robust solution to integrate specialized tools with LLMs for advanced chemical applications. All datasets and code are available at https://github.com/AI4Chem/ChemistryAgent .",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.07551"
    },
    "fba3402ed5e38e84d5edbe47ffd62215": {
        "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems",
        "authors": [
            "Peiran Li",
            "Xinkai Zou",
            "Zhuohang Wu",
            "Ruifeng Li",
            "Shuo Xing",
            "Hanwen Zheng",
            "Zhikai Hu",
            "Yuping Wang",
            "Haoxi Li",
            "Qin Yuan",
            "Yingmo Zhang",
            "Zhengzhong Tu"
        ],
        "date": "2025/06/09",
        "pdf": "http://arxiv.org/pdf/2506.07564",
        "abstract": "Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today&#39;s agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.07564"
    },
    "2d86a36b17cad3d7559819b1160b8049": {
        "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization",
        "authors": [
            "Hongzheng Chen",
            "Yingheng Wang",
            "Yaohui Cai",
            "Hins Hu",
            "Jiajie Li",
            "Shirley Huang",
            "Chenhui Deng",
            "Rongjian Liang",
            "Shufeng Kong",
            "Haoxing Ren",
            "Samitha Samaranayake",
            "Carla P. Gomes",
            "Zhiru Zhang"
        ],
        "date": "2025/06/09",
        "pdf": "http://arxiv.org/pdf/2506.07972",
        "abstract": "While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.07972"
    },
    "95151bd952da7ec123f1c062327b6d36": {
        "title": "$\\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment",
        "authors": [
            "Victor Barres",
            "Honghua Dong",
            "Soham Ray",
            "Xujie Si",
            "Karthik Narasimhan"
        ],
        "date": "2025/06/09",
        "pdf": "http://arxiv.org/pdf/2506.07982",
        "abstract": "Existing benchmarks for conversational AI agents simulate single-control environments, where only the AI agent can use tools to interact with the world, while the user remains a passive information provider. This differs from real-world scenarios like technical support, where users need to actively participate in modifying the state of the (shared) world. In order to address this gap, we introduce $\\tau^2$-bench, with four key contributions: 1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both agent and user make use of tools to act in a shared, dynamic environment that tests both agent coordination and communication, 2) A compositional task generator that programmatically creates diverse, verifiable tasks from atomic components, ensuring domain coverage and controlled complexity, 3) A reliable user simulator tightly coupled with the environment, whose behavior is constrained by tools and observable states, improving simulation fidelity, 4) Fine-grained analysis of agent performance through multiple ablations including separating errors arising from reasoning vs communication/coordination. In particular, our experiments show significant performance drops when agents shift from no-user to dual-control, highlighting the challenges of guiding users. Overall, $\\tau^2$-bench provides a controlled testbed for agents that must both reason effectively and guide user actions.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.07982"
    },
    "57f83f6701faee3b59d9f976b369c598": {
        "title": "From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium",
        "authors": [
            "Xie Yi",
            "Zhanke Zhou",
            "Chentao Cao",
            "Qiyu Niu",
            "Tongliang Liu",
            "Bo Han"
        ],
        "date": "2025/06/09",
        "pdf": "http://arxiv.org/pdf/2506.08292",
        "abstract": "Multi-agent frameworks can substantially boost the reasoning power of large language models (LLMs), but they typically incur heavy computational costs and lack convergence guarantees. To overcome these challenges, we recast multi-LLM coordination as an incomplete-information game and seek a Bayesian Nash equilibrium (BNE), in which each agent optimally responds to its probabilistic beliefs about the strategies of others. We introduce Efficient Coordination via Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that marries distributed reasoning with centralized final output. Under ECON, each LLM independently selects responses that maximize its expected reward, conditioned on its beliefs about co-agents, without requiring costly inter-agent exchanges. We mathematically prove that ECON attains a markedly tighter regret bound than non-equilibrium multi-agent schemes. Empirically, ECON outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks spanning complex reasoning and planning tasks. Further experiments demonstrate ECON&#39;s ability to flexibly incorporate additional models, confirming its scalability and paving the way toward larger, more powerful multi-LLM ensembles. The code is publicly available at: https://github.com/tmlr-group/ECON.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08292"
    },
    "d1bbb9313f74d13ef7adf73ff3601099": {
        "title": "Reinforce LLM Reasoning through Multi-Agent Reflection",
        "authors": [
            "Yurun Yuan",
            "Tengyang Xie"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.08379",
        "abstract": "Leveraging more test-time computation has proven to be an effective way to boost the reasoning capabilities of large language models (LLMs). Among various methods, the verify-and-improve paradigm stands out for enabling dynamic solution exploration and feedback incorporation. However, existing approaches often suffer from restricted feedback spaces and lack of coordinated training of different parties, leading to suboptimal performance. To address this, we model this multi-turn refinement process as a Markov Decision Process and introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers via direct preference learning on self-generated data. Theoretically, DPSDP can match the performance of any policy within the training distribution. Empirically, we instantiate DPSDP with various base models and show improvements on both in- and out-of-distribution benchmarks. For example, on benchmark MATH 500, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An ablation study further confirms the benefits of multi-agent collaboration and out-of-distribution generalization.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08379"
    },
    "33546e3efcfe08c32375938115cb71d8": {
        "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents",
        "authors": [
            "Irene Testini",
            "José Hernández-Orallo",
            "Lorenzo Pacchiardi"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.08800",
        "abstract": "Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) are increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2506.08800"
    },
    "fbf14a7111ca9b96501d94439a250364": {
        "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search",
        "authors": [
            "Samuel Holt",
            "Max Ruiz Luyten",
            "Thomas Pouplin",
            "Mihaela van der Schaar"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.09171",
        "abstract": "Large Language Models (LLMs) are increasingly capable but often require significant guidance or extensive interaction history to perform effectively in complex, interactive environments. Existing methods may struggle with adapting to new information or efficiently utilizing past experiences for multi-step reasoning without fine-tuning. We introduce a novel LLM agent framework that enhances planning capabilities through in-context learning, facilitated by atomic fact augmentation and a recursive lookahead search. Our agent learns to extract task-critical ``atomic facts&#39;&#39; from its interaction trajectories. These facts dynamically augment the prompts provided to LLM-based components responsible for action proposal, latent world model simulation, and state-value estimation. Planning is performed via a depth-limited lookahead search, where the LLM simulates potential trajectories and evaluates their outcomes, guided by the accumulated facts and interaction history. This approach allows the agent to improve its understanding and decision-making online, leveraging its experience to refine its behavior without weight updates. We provide a theoretical motivation linking performance to the quality of fact-based abstraction and LLM simulation accuracy. Empirically, our agent demonstrates improved performance and adaptability on challenging interactive tasks, achieving more optimal behavior as it accumulates experience, showcased in tasks such as TextFrozenLake and ALFWorld.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.09171"
    },
    "49a4333a698662ec4d1e2fb9653550ba": {
        "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench",
        "authors": [
            "Boxi Yu",
            "Yuxuan Zhu",
            "Pinjia He",
            "Daniel Kang"
        ],
        "date": "2025/06/10",
        "pdf": "http://arxiv.org/pdf/2506.09289",
        "abstract": "The advent of Large Language Models (LLMs) has spurred the development of coding agents for real-world code generation. As a widely used benchmark for evaluating the code generation capabilities of these agents, SWE-Bench uses real-world problems based on GitHub issues and their corresponding pull requests. However, the manually written test cases included in these pull requests are often insufficient, allowing generated patches to pass the tests without resolving the underlying issue. To address this challenge, we introduce UTGenerator, an LLM-driven test case generator that automatically analyzes codebases and dependencies to generate test cases for real-world Python projects. Building on UTGenerator, we propose UTBoost, a comprehensive framework for test case augmentation. In our evaluation, we identified 36 task instances with insufficient test cases and uncovered 345 erroneous patches incorrectly labeled as passed in the original SWE Bench. These corrections, impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, yield 18 and 11 ranking changes, respectively.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.09289"
    },
    "deee15a538a247f19de056897b2c77e7": {
        "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy",
        "authors": [
            "Henry Peng Zou",
            "Wei-Chieh Huang",
            "Yaozu Wu",
            "Chunyu Miao",
            "Dongyuan Li",
            "Aiwei Liu",
            "Yue Zhou",
            "Yankai Chen",
            "Weizhi Zhang",
            "Yangning Li",
            "Liancheng Fang",
            "Renhe Jiang",
            "Philip S. Yu"
        ],
        "date": "2025/06/11",
        "pdf": "http://arxiv.org/pdf/2506.09420",
        "abstract": "Recent improvements in large language models (LLMs) have led many researchers to focus on building fully autonomous AI agents. This position paper questions whether this approach is the right path forward, as these autonomous systems still have problems with reliability, transparency, and understanding the actual requirements of human. We suggest a different approach: LLM-based Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing them. By keeping human involved to provide guidance, answer questions, and maintain control, these systems can be more trustworthy and adaptable. Looking at examples from healthcare, finance, and software development, we show how human-AI teamwork can handle complex tasks better than AI working alone. We also discuss the challenges of building these collaborative systems and offer practical solutions. This paper argues that progress in AI should not be measured by how independent systems become, but by how well they can work with humans. The most promising future for AI is not in systems that take over human roles, but in those that enhance human capabilities through meaningful partnership.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.09420"
    },
    "c64d7ac823efc384bb0374b5fccd987f": {
        "title": "Effective Red-Teaming of Policy-Adherent Agents",
        "authors": [
            "Itay Nakash",
            "George Kour",
            "Koren Lazar",
            "Matan Vetzler",
            "Guy Uziel",
            "Ateret Anaby-Tavor"
        ],
        "date": "2025/06/11",
        "pdf": "http://arxiv.org/pdf/2506.09600",
        "abstract": "Task-oriented LLM-based agents are increasingly used in domains with strict policies, such as refund eligibility or cancellation rules. The challenge lies in ensuring that the agent consistently adheres to these rules and policies, appropriately refusing any request that would violate them, while still maintaining a helpful and natural interaction. This calls for the development of tailored design and evaluation methodologies to ensure agent resilience against malicious user behavior. We propose a novel threat model that focuses on adversarial users aiming to exploit policy-adherent agents for personal benefit. To address this, we present CRAFT, a multi-agent red-teaming system that leverages policy-aware persuasive strategies to undermine a policy-adherent agent in a customer-service scenario, outperforming conventional jailbreak methods such as DAN prompts, emotional manipulation, and coercive. Building upon the existing tau-bench benchmark, we introduce tau-break, a complementary benchmark designed to rigorously assess the agent&#39;s robustness against manipulative user behavior. Finally, we evaluate several straightforward yet effective defense strategies. While these measures provide some protection, they fall short, highlighting the need for stronger, research-driven safeguards to protect policy-adherent agents from adversarial attacks",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.09600"
    },
    "a055e752c5031e9dafea6f1e2c3e340c": {
        "title": "Disclosure Audits for LLM Agents",
        "authors": [
            "Saswat Das",
            "Jameson Sandler",
            "Ferdinando Fioretto"
        ],
        "date": "2025/06/11",
        "pdf": "http://arxiv.org/pdf/2506.10171",
        "abstract": "Large Language Model agents have begun to appear as personal assistants, customer service bots, and clinical aides. While these applications deliver substantial operational benefits, they also require continuous access to sensitive data, which increases the likelihood of unauthorized disclosures. This study proposes an auditing framework for conversational privacy that quantifies and audits these risks. The proposed Conversational Manipulation for Privacy Leakage (CMPL) framework, is an iterative probing strategy designed to stress-test agents that enforce strict privacy directives. Rather than focusing solely on a single disclosure event, CMPL simulates realistic multi-turn interactions to systematically uncover latent vulnerabilities. Our evaluation on diverse domains, data modalities, and safety configurations demonstrate the auditing framework&#39;s ability to reveal privacy risks that are not deterred by existing single-turn defenses. In addition to introducing CMPL as a diagnostic tool, the paper delivers (1) an auditing procedure grounded in quantifiable risk metrics and (2) an open benchmark for evaluation of conversational privacy across agent implementations.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.10171"
    },
    "be59ee907972bf8113d9eb6664e2f8d9": {
        "title": "Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering",
        "authors": [
            "Sai Prasanna Teja Reddy Bogireddy",
            "Abrar Majeedi",
            "Viswanatha Reddy Gajjala",
            "Zhuoyan Xu",
            "Siddhant Rai",
            "Vaishnav Potlapalli"
        ],
        "date": "2025/06/12",
        "pdf": "http://arxiv.org/pdf/2506.10751",
        "abstract": "Automated question answering (QA) over electronic health records (EHRs) can bridge critical information gaps for clinicians and patients, yet it demands both precise evidence retrieval and faithful answer generation under limited supervision. In this work, we present Neural, the runner-up in the BioNLP 2025 ArchEHR-QA shared task on evidence-grounded clinical QA. Our proposed method decouples the task into (1) sentence-level evidence identification and (2) answer synthesis with explicit citations. For each stage, we automatically explore the prompt space with DSPy&#39;s MIPROv2 optimizer, jointly tuning instructions and few-shot demonstrations on the development set. A self-consistency voting scheme further improves evidence recall without sacrificing precision. On the hidden test set, our method attains an overall score of 51.5, placing second stage while outperforming standard zero-shot and few-shot prompting by over 20 and 10 points, respectively. These results indicate that data-driven prompt optimization is a cost-effective alternative to model fine-tuning for high-stakes clinical QA, advancing the reliability of AI assistants in healthcare.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.10751"
    },
    "1529be7508b743832b5deae734c45686": {
        "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
        "authors": [
            "Huaying Yuan",
            "Zheng Liu",
            "Junjie Zhou",
            "Hongjin Qian",
            "Ji-Rong Wen",
            "Zhicheng Dou"
        ],
        "date": "2025/06/12",
        "pdf": "http://arxiv.org/pdf/2506.10821",
        "abstract": "Long video understanding (LVU) presents a significant challenge for current multi-modal large language models (MLLMs) due to the task&#39;s inherent complexity and context window constraint. It is widely assumed that addressing LVU tasks requires foundation MLLMs with extended context windows, strong visual perception capabilities, and proficient domain expertise. In this work, we challenge this common belief by introducing VideoDeepResearch, a novel agentic framework for long video understanding. Our approach relies solely on a text-only large reasoning model (LRM) combined with a modular multi-modal toolkit, including multimodal retrievers and visual perceivers, all of which are readily available in practice. For each LVU task, the system formulates a problem-solving strategy through reasoning, while selectively accessing and utilizing essential video content via tool using. We conduct extensive experiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench. Our results demonstrate that VideoDeepResearch achieves substantial improvements over existing MLLM baselines, surpassing the previous state-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and LongVideoBench, respectively. These findings highlight the promise of agentic systems in overcoming key challenges in LVU problems.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.10821"
    },
    "94fed5682f21bc1ed263327f52fcd59a": {
        "title": "Build the web for agents, not agents for the web",
        "authors": [
            "Xing Han Lù",
            "Gaurav Kamath",
            "Marius Mosbach",
            "Siva Reddy"
        ],
        "date": "2025/06/12",
        "pdf": "http://arxiv.org/pdf/2506.10953",
        "abstract": "Recent advancements in Large Language Models (LLMs) and multimodal counterparts have spurred significant interest in developing web agents -- AI systems capable of autonomously navigating and completing tasks within web environments. While holding tremendous promise for automating complex web interactions, current approaches face substantial challenges due to the fundamental mismatch between human-designed interfaces and LLM capabilities. Current methods struggle with the inherent complexity of web inputs, whether processing massive DOM trees, relying on screenshots augmented with additional information, or bypassing the user interface entirely through API interactions. This position paper advocates for a paradigm shift in web agent research: rather than forcing web agents to adapt to interfaces designed for humans, we should develop a new interaction paradigm specifically optimized for agentic capabilities. To this end, we introduce the concept of an Agentic Web Interface (AWI), an interface specifically designed for agents to navigate a website. We establish six guiding principles for AWI design, emphasizing safety, efficiency, and standardization, to account for the interests of all primary stakeholders. This reframing aims to overcome fundamental limitations of existing interfaces, paving the way for more efficient, reliable, and transparent web agent design, which will be a collaborative effort involving the broader ML community.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.10953"
    },
    "6c40002a491aacea1377871da5c55064": {
        "title": "Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning",
        "authors": [
            "Liying Wang",
            "Ph. D.",
            "Daffodil Carrington",
            "M. S.",
            "Daniil Filienko",
            "M. S.",
            "Caroline El Jazmi",
            "M. S.",
            "Serena Jinchen Xie",
            "M. S.",
            "Martine De Cock",
            "Ph. D.",
            "Sarah Iribarren",
            "Ph. D.",
            "Weichao Yuwen",
            "Ph. D"
        ],
        "date": "2025/06/13",
        "pdf": "http://arxiv.org/pdf/2506.11376",
        "abstract": "Family caregivers often face substantial mental health challenges due to their multifaceted roles and limited resources. This study explored the potential of a large language model (LLM)-powered conversational agent to deliver evidence-based mental health support for caregivers, specifically Problem-Solving Therapy (PST) integrated with Motivational Interviewing (MI) and Behavioral Chain Analysis (BCA). A within-subject experiment was conducted with 28 caregivers interacting with four LLM configurations to evaluate empathy and therapeutic alliance. The best-performing models incorporated Few-Shot and Retrieval-Augmented Generation (RAG) prompting techniques, alongside clinician-curated examples. The models showed improved contextual understanding and personalized support, as reflected by qualitative responses and quantitative ratings on perceived empathy and therapeutic alliances. Participants valued the model&#39;s ability to validate emotions, explore unexpressed feelings, and provide actionable strategies. However, balancing thorough assessment with efficient advice delivery remains a challenge. This work highlights the potential of LLMs in delivering empathetic and tailored support for family caregivers.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11376"
    },
    "2f979609a33d6f7e849e30f0cd272287": {
        "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction",
        "authors": [
            "Syeda Kisaa Fatima",
            "Tehreem Zubair",
            "Noman Ahmed",
            "Asifullah Khan"
        ],
        "date": "2025/06/13",
        "pdf": "http://arxiv.org/pdf/2506.11475",
        "abstract": "This paper introduces LUCID-MA (Learning and Understanding Crime through Dialogue of Multiple Agents), an innovative AI powered framework where multiple AI agents collaboratively analyze and understand crime data. Our system that consists of three core components: an analysis assistant that highlights spatiotemporal crime patterns, a feedback component that reviews and refines analytical results and a prediction component that forecasts future crime trends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it runs completely offline and allows the agents undergo self-improvement through 100 rounds of communication with less human interaction. A scoring function is incorporated to evaluate agent&#39;s performance, providing visual plots to track learning progress. This work demonstrates the potential of AutoGen-style agents for autonomous, scalable, and iterative analysis in social science domains maintaining data privacy through offline execution.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.11475"
    },
    "0510780b3da2c8d77287f82501b41739": {
        "title": "Modeling Earth-Scale Human-Like Societies with One Billion Agents",
        "authors": [
            "Haoxiang Guan",
            "Jiyan He",
            "Liyang Fan",
            "Zhenzhen Ren",
            "Shaobin He",
            "Xin Yu",
            "Yuan Chen",
            "Shuxin Zheng",
            "Tie-Yan Liu",
            "Zhen Liu"
        ],
        "date": "2025/06/07",
        "pdf": "http://arxiv.org/pdf/2506.12078",
        "abstract": "Understanding how complex societal behaviors emerge from individual cognition and interactions requires both high-fidelity modeling of human behavior and large-scale simulations. Traditional agent-based models (ABMs) have been employed to study these dynamics for decades, but are constrained by simplified agent behaviors that fail to capture human complexity. Recent advances in large language models (LLMs) offer new opportunities by enabling agents to exhibit sophisticated social behaviors that go beyond rule-based logic, yet face significant scaling challenges. Here we present Light Society, an agent-based simulation framework that advances both fronts, efficiently modeling human-like societies at planetary scale powered by LLMs. Light Society formalizes social processes as structured transitions of agent and environment states, governed by a set of LLM-powered simulation operations, and executed through an event queue. This modular design supports both independent and joint component optimization, supporting efficient simulation of societies with over one billion agents. Large-scale simulations of trust games and opinion propagation--spanning up to one billion agents--demonstrate Light Society&#39;s high fidelity and efficiency in modeling social trust and information diffusion, while revealing scaling laws whereby larger simulations yield more stable and realistic emergent behaviors.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.12078"
    },
    "40e461e379fd2163e43fd7747049c7a3": {
        "title": "Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study",
        "authors": [
            "Sompote Youwai",
            "David Phim",
            "Vianne Gayl Murcia",
            "Rianne Clair Onas"
        ],
        "date": "2025/06/13",
        "pdf": "http://arxiv.org/pdf/2506.13811",
        "abstract": "This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.13811"
    },
    "5f02334c43c63f004a7fa399abe40f36": {
        "title": "RadFabric: Agentic AI System with Reasoning Capability for Radiology",
        "authors": [
            "Wenting Chen",
            "Yi Dong",
            "Zhaojun Ding",
            "Yucheng Shi",
            "Yifan Zhou",
            "Fang Zeng",
            "Yijun Luo",
            "Tianyu Lin",
            "Yihang Su",
            "Yichen Wu",
            "Kai Zhang",
            "Zhen Xiang",
            "Tianming Liu",
            "Ninghao Liu",
            "Lichao Sun",
            "Yixuan Yuan",
            "Xiang Li"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.14142",
        "abstract": "Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.14142"
    },
    "985347a6f0bcacfcd1eb0388ac7f5a2d": {
        "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
        "authors": [
            "Qizheng Zhang",
            "Michael Wornow",
            "Kunle Olukotun"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.14852",
        "abstract": "LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.14852"
    },
    "fc120d9ae5d988d9db2eba142153f363": {
        "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence",
        "authors": [
            "Yining Hong",
            "Rui Sun",
            "Bingxuan Li",
            "Xingcheng Yao",
            "Maxine Wu",
            "Alexander Chien",
            "Da Yin",
            "Ying Nian Wu",
            "Zhecan James Wang",
            "Kai-Wei Chang"
        ],
        "date": "2025/06/18",
        "pdf": "http://arxiv.org/pdf/2506.15677",
        "abstract": "AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.15677"
    },
    "f3a25b2c9353cc4258cb3d8865726f04": {
        "title": "OAgents: An Empirical Study of Building Effective Agents",
        "authors": [
            "He Zhu",
            "Tianrui Qin",
            "King Zhu",
            "Heyuan Huang",
            "Yeyi Guan",
            "Jinxiang Xia",
            "Yi Yao",
            "Hanhao Li",
            "Ningning Wang",
            "Pai Liu",
            "Tianhao Peng",
            "Xin Gui",
            "Xiaowan Li",
            "Yuhui Liu",
            "Yuchen Eleanor Jiang",
            "Jun Wang",
            "Changwang Zhang",
            "Xiangru Tang",
            "Ge Zhang",
            "Jian Yang",
            "Minghao Liu",
            "Xitong Gao",
            "Jiaheng Liu",
            "Wangchunshu Zhou"
        ],
        "date": "2025/06/17",
        "pdf": "http://arxiv.org/pdf/2506.15741",
        "abstract": "Recently, Agentic AI has become an increasingly popular research field. However, we argue that current agent research practices lack standardization and scientific rigor, making it hard to conduct fair comparisons among methods. As a result, it is still unclear how different design choices in agent frameworks affect effectiveness, and measuring their progress remains challenging. In this work, we conduct a systematic empirical study on GAIA benchmark and BrowseComp to examine the impact of popular design choices in key agent components in a fair and rigorous manner. We find that the lack of a standard evaluation protocol makes previous works, even open-sourced ones, non-reproducible, with significant variance between random runs. Therefore, we introduce a more robust evaluation protocol to stabilize comparisons. Our study reveals which components and designs are crucial for effective agents, while others are redundant, despite seeming logical. Based on our findings, we build and open-source OAgents, a new foundation agent framework that achieves state-of-the-art performance among open-source projects. OAgents offers a modular design for various agent components, promoting future research in Agentic AI.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.15741"
    },
    "72748c70f6b8ee418bff55b3197db403": {
        "title": "IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks",
        "authors": [
            "Xiaoya Lu",
            "Zeren Chen",
            "Xuhao Hu",
            "Yijin Zhou",
            "Weichen Zhang",
            "Dongrui Liu",
            "Lu Sheng",
            "Jing Shao"
        ],
        "date": "2025/06/19",
        "pdf": "http://arxiv.org/pdf/2506.16402",
        "abstract": "Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent&#39;s actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent&#39;s interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.16402"
    },
    "04bc8bce1ce0d0eeed72fe96796681f3": {
        "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems",
        "authors": [
            "Matias Martinez",
            "Xavier Franch"
        ],
        "date": "2025/06/20",
        "pdf": "http://arxiv.org/pdf/2506.17208",
        "abstract": "The rapid progress in Automated Program Repair (APR) has been driven by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair systems using real issues and pull requests mined from 12 popular open-source Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench Verified, have become central platforms for tracking progress and comparing solutions. However, because the submission process does not require detailed documentation, the architectural design and origin of many solutions remain unclear. In this paper, we present the first comprehensive study of all submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries) leaderboards, analyzing 67 unique approaches across dimensions such as submitter type, product availability, LLM usage, and system architecture. Our findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7), the presence of both agentic and non-agentic designs, and a contributor base spanning from individual developers to large tech companies.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.17208"
    },
    "5ac195c7adb09b287370f7615ce453ba": {
        "title": "A Comment On &#34;The Illusion of Thinking&#34;: Reframing the Reasoning Cliff as an Agentic Gap",
        "authors": [
            "Sheraz Khan",
            "Subha Madhavan",
            "Kannan Natarajan"
        ],
        "date": "2025/06/23",
        "pdf": "http://arxiv.org/pdf/2506.18957",
        "abstract": "The recent work by Shojaee et al. (2025), titled The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity, presents a compelling empirical finding, a reasoning cliff, where the performance of Large Reasoning Models (LRMs) collapses beyond a specific complexity threshold, which the authors posit as an intrinsic scaling limitation of Chain-of-Thought (CoT) reasoning. This commentary, while acknowledging the study&#39;s methodological rigor, contends that this conclusion is confounded by experimental artifacts. We argue that the observed failure is not evidence of a fundamental cognitive boundary, but rather a predictable outcome of system-level constraints in the static, text-only evaluation paradigm, including tool use restrictions, context window recall issues, the absence of crucial cognitive baselines, inadequate statistical reporting, and output generation limits. We reframe this performance collapse through the lens of an agentic gap, asserting that the models are not failing at reasoning, but at execution within a profoundly restrictive interface. We empirically substantiate this critique by demonstrating a striking reversal. A model, initially declaring a puzzle impossible when confined to text-only generation, now employs agentic tools to not only solve it but also master variations of complexity far beyond the reasoning cliff it previously failed to surmount. Additionally, our empirical analysis of tool-enabled models like o4-mini and GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural execution to complex meta-cognitive self-correction, which has significant implications for how we define and measure machine intelligence. The illusion of thinking attributed to LRMs is less a reasoning deficit and more a consequence of an otherwise capable mind lacking the tools for action.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.18957"
    },
    "75226d425052224bb5690290c8a0116e": {
        "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents",
        "authors": [
            "Weizhi Zhang",
            "Yangning Li",
            "Yuanchen Bei",
            "Junyu Luo",
            "Guancheng Wan",
            "Liangwei Yang",
            "Chenxuan Xie",
            "Yuyao Yang",
            "Wei-Chieh Huang",
            "Chunyu Miao",
            "Henry Peng Zou",
            "Xiao Luo",
            "Yusheng Zhao",
            "Yankai Chen",
            "Chunkit Chan",
            "Peilin Zhou",
            "Xinyang Zhang",
            "Chenwei Zhang",
            "Jingbo Shang",
            "Ming Zhang",
            "Yangqiu Song",
            "Irwin King",
            "Philip S. Yu"
        ],
        "date": "2025/06/23",
        "pdf": "http://arxiv.org/pdf/2506.18959",
        "abstract": "Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in https://github.com/DavidZWZ/Awesome-Deep-Research.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.18959"
    },
    "cc698005c06f32ea427b126b962fe991": {
        "title": "NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling",
        "authors": [
            "Yan Jiang",
            "Hao Zhou",
            "LiZhong GU",
            "Ai Han",
            "TianLong Li"
        ],
        "date": "2025/06/24",
        "pdf": "http://arxiv.org/pdf/2506.19500",
        "abstract": "LLMs&#39; reliance on static knowledge and fragile tool invocation severely hinders the orchestration of complex, heterogeneous toolchains, particularly at large scales. Existing methods typically use rigid single-path execution, resulting in poor error recovery and exponentially growing search spaces. We introduce NaviAgent, a graph-navigated bilevel planning architecture for robust function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator. As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional decision space and continuously perceives environmental states, dynamically selecting the optimal action to fully cover all tool invocation scenarios. The Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph (TDHG), where node embeddings explicitly fuse API schema structure with historical invocation behavior. It also integrates a novel heuristic search strategy that guides the Decider toward efficient and highly successful toolchains, even for unseen tool combinations. Experiments show that NaviAgent consistently achieves the highest task success rate (TSR) across all foundation models and task complexities, outperforming the average baselines (ReAct, ToolLLM, {\\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B, and Deepseek-V3, respectively. Its execution steps are typically within one step of the most efficient baseline, ensuring a strong balance between quality and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of 49.5%, surpassing the much larger 32B model (44.9%) under our architecture. Incorporating the Graph-Encoded Navigator further boosts TSR by an average of 2.4 points, with gains up over 9 points on complex tasks for larger models (Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain orchestration.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.19500"
    },
    "880066dfad16a83cdda5089bbd694273": {
        "title": "LLM-Based Social Simulations Require a Boundary",
        "authors": [
            "Zengqing Wu",
            "Run Peng",
            "Takayuki Ito",
            "Chuan Xiao"
        ],
        "date": "2025/06/24",
        "pdf": "http://arxiv.org/pdf/2506.19806",
        "abstract": "This position paper argues that large language model (LLM)-based social simulations should establish clear boundaries to meaningfully contribute to social science research. While LLMs offer promising capabilities for modeling human-like agents compared to traditional agent-based modeling, they face fundamental limitations that constrain their reliability for social pattern discovery. The core issue lies in LLMs&#39; tendency towards an ``average persona&#39;&#39; that lacks sufficient behavioral heterogeneity, a critical requirement for simulating complex social dynamics. We examine three key boundary problems: alignment (simulated behaviors matching real-world patterns), consistency (maintaining coherent agent behavior over time), and robustness (reproducibility under varying conditions). We propose heuristic boundaries for determining when LLM-based simulations can reliably advance social science understanding. We believe that these simulations are more valuable when focusing on (1) collective patterns rather than individual trajectories, (2) agent behaviors aligning with real population averages despite limited variance, and (3) proper validation methods available for testing simulation robustness. We provide a practical checklist to guide researchers in determining the appropriate scope and claims for LLM-based social simulations.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.19806"
    },
    "1960e11d86ca93979a3937b43fe8c542": {
        "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
        "authors": [
            "Baochang Ren",
            "Shuofei Qiao",
            "Wenhao Yu",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "date": "2025/06/24",
        "pdf": "http://arxiv.org/pdf/2506.19807",
        "abstract": "Large Language Models (LLMs), particularly slow-thinking models, often exhibit severe hallucination, outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented reward mechanism often lacks factual supervision over the thinking process, further exacerbating the hallucination problem. To address the high hallucination in slow-thinking models, we propose Knowledge-enhanced RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. KnowRL guides models to perform fact-based slow thinking by integrating a factuality reward, based on knowledge verification, into the RL training process, helping them recognize their knowledge boundaries. This targeted factual input during RL training enables the model to learn and internalize fact-based reasoning strategies. By directly rewarding adherence to facts within the reasoning steps, KnowRL fosters a more reliable thinking process. Experimental results on three hallucination evaluation datasets and two reasoning evaluation datasets demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking models while maintaining their original strong reasoning capabilities. Our code is available at https://github.com/zjunlp/KnowRL.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.19807"
    },
    "fca8735cf2803106072de5fff02c40b5": {
        "title": "Language Modeling by Language Models",
        "authors": [
            "Junyan Cheng",
            "Peter Clark",
            "Kyle Richardson"
        ],
        "date": "2025/06/25",
        "pdf": "http://arxiv.org/pdf/2506.20249",
        "abstract": "Can we leverage LLMs to model the process of discovering novel language model (LM) architectures? Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system, Genesys, employs a Ladder of Scales approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\\sim$86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified through pre-training) and find the best designs to be highly competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks). We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.20249"
    },
    "2490b0c4120fdad98cdd3cf6510e2f17": {
        "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind",
        "authors": [
            "Andrei Lupu",
            "Timon Willi",
            "Jakob Foerster"
        ],
        "date": "2025/06/25",
        "pdf": "http://arxiv.org/pdf/2506.20664",
        "abstract": "As Large Language Models (LLMs) gain agentic abilities, they will have to navigate complex multi-agent scenarios, interacting with human users and other agents in cooperative and competitive settings. This will require new reasoning skills, chief amongst them being theory of mind (ToM), or the ability to reason about the &#34;mental&#34; states of other agents. However, ToM and other multi-agent abilities in LLMs are poorly understood, since existing benchmarks suffer from narrow scope, data leakage, saturation, and lack of interactivity. We thus propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM drawing inspiration from cognitive science, computational pragmatics and multi-agent reinforcement learning. It is designed to be as easy as possible in all other dimensions, eliminating confounding factors commonly found in other benchmarks. To our knowledge, it is also the first platform for designing interactive ToM experiments. We validate the benchmark design through comprehensive empirical evaluations of frontier LLMs, robustness studies, and human-AI cross-play experiments. We find that LLM game-playing abilities lag behind humans and simple word-embedding baselines. We then create variants of two classic cognitive science experiments within Decrypto to evaluate three key ToM abilities. Surprisingly, we find that state-of-the-art reasoning models are significantly worse at those tasks than their older counterparts. This demonstrates that Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and paves the path towards better artificial agents.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.20664"
    },
    "d83e806c637bb3ef8845e0343222dac6": {
        "title": "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation",
        "authors": [
            "Gurusha Juneja",
            "Alon Albalak",
            "Wenyue Hua",
            "William Yang Wang"
        ],
        "date": "2025/06/25",
        "pdf": "http://arxiv.org/pdf/2506.20737",
        "abstract": "The proliferation of LLM-based agents has led to increasing deployment of inter-agent collaboration for tasks like scheduling, negotiation, resource allocation etc. In such systems, privacy is critical, as agents often access proprietary tools and domain-specific databases requiring strict confidentiality. This paper examines whether LLM-based agents demonstrate an understanding of contextual privacy. And, if instructed, do these systems preserve inference time user privacy in non-adversarial multi-turn conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents primarily assess single-turn, low-complexity tasks where private information can be easily excluded. We first present a benchmark - MAGPIE comprising 158 real-life high-stakes scenarios across 15 domains. These scenarios are designed such that complete exclusion of private data impedes task completion yet unrestricted information sharing could lead to substantial losses. We then evaluate the current state-of-the-art LLMs on (a) their understanding of contextually private data and (b) their ability to collaborate without violating user privacy. Empirical experiments demonstrate that current models, including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual privacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the time. In multi-turn conversations, these models disclose private information in 59.9\\% and 50.5\\% of cases even under explicit privacy instructions. Furthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios. These results underscore that current models are not aligned towards both contextual privacy preservation and collaborative task-solving.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.20737"
    },
    "647be9057727f04eb6b58f47af310111": {
        "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
        "authors": [
            "Boyu Gou",
            "Zanming Huang",
            "Yuting Ning",
            "Yu Gu",
            "Michael Lin",
            "Weijian Qi",
            "Andrei Kopanev",
            "Botao Yu",
            "Bernal Jiménez Gutiérrez",
            "Yiheng Shu",
            "Chan Hee Song",
            "Jiaman Wu",
            "Shijie Chen",
            "Hanane Nour Moussa",
            "Tianshu Zhang",
            "Jian Xie",
            "Yifei Li",
            "Tianci Xue",
            "Zeyi Liao",
            "Kai Zhang",
            "Boyuan Zheng",
            "Zhaowei Cai",
            "Viktor Rozgic",
            "Morteza Ziyadi",
            "Huan Sun",
            "Yu Su"
        ],
        "date": "2025/06/26",
        "pdf": "http://arxiv.org/pdf/2506.21506",
        "abstract": "Agentic search such as Deep Research systems, where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers, represents a major shift in how users interact with web-scale information. While promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. In this paper, we introduce Mind2Web 2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor. To address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. Our method constructs task-specific judge agents based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. We conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development. The best-performing system, OpenAI Deep Research, can already achieve 50-70% of human performance while spending half the time, showing a great potential. Altogether, Mind2Web 2 provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21506"
    },
    "6b5e025e3b1106180a44439ffccf1191": {
        "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles",
        "authors": [
            "Mengyi Shan",
            "Brian Curless",
            "Ira Kemelmacher-Shlizerman",
            "Steve Seitz"
        ],
        "date": "2025/06/27",
        "pdf": "http://arxiv.org/pdf/2506.21839",
        "abstract": "We challenge text-to-image models with generating escape room puzzle images that are visually appealing, logically solid, and intellectually stimulating. While base image models struggle with spatial relationships and affordance reasoning, we propose a hierarchical multi-agent framework that decomposes this task into structured stages: functional design, symbolic scene graph reasoning, layout synthesis, and local image editing. Specialized agents collaborate through iterative feedback to ensure the scene is visually coherent and functionally solvable. Experiments show that agent collaboration improves output quality in terms of solvability, shortcut avoidance, and affordance clarity, while maintaining visual quality.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21839"
    },
    "1ef11af7c5e321a8e465e963283e2846": {
        "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation",
        "authors": [
            "Reza Yousefi Maragheh",
            "Pratheek Vadla",
            "Priyank Gupta",
            "Kai Zhao",
            "Aysenur Inan",
            "Kehui Yao",
            "Jianpeng Xu",
            "Praveen Kanumala",
            "Jason Cho",
            "Sushant Kumar"
        ],
        "date": "2025/06/27",
        "pdf": "http://arxiv.org/pdf/2506.21931",
        "abstract": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing recommendation systems by incorporating external context into large language model prompts. However, existing RAG-based approaches often rely on static retrieval heuristics and fail to capture nuanced user preferences in dynamic recommendation scenarios. In this work, we introduce ARAG, an Agentic Retrieval-Augmented Generation framework for Personalized Recommendation, which integrates a multi-agent collaboration mechanism into the RAG pipeline. To better understand the long-term and session behavior of the user, ARAG leverages four specialized LLM-based agents: a User Understanding Agent that summarizes user preferences from long-term and session contexts, a Natural Language Inference (NLI) Agent that evaluates semantic alignment between candidate items retrieved by RAG and inferred intent, a context summary agent that summarizes the findings of NLI agent, and an Item Ranker Agent that generates a ranked list of recommendations based on contextual fit. We evaluate ARAG accross three datasets. Experimental results demonstrate that ARAG significantly outperforms standard RAG and recency-based baselines, achieving up to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an ablation study to analyse the effect by different components of ARAG. Our findings highlight the effectiveness of integrating agentic reasoning into retrieval-augmented recommendation and provide new directions for LLM-based personalization.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.21931"
    },
    "c31288cf6abd3862fc152a3fc2674ba6": {
        "title": "Exploring Modularity of Agentic Systems for Drug Discovery",
        "authors": [
            "Laura van Weesep",
            "Samuel Genheden",
            "Ola Engkvist",
            "Jens Sjölund"
        ],
        "date": "2025/06/27",
        "pdf": "http://arxiv.org/pdf/2506.22189",
        "abstract": "Large-language models (LLMs) and agentic systems present exciting opportunities to accelerate drug discovery and design. In this study, we critically examine the modularity of LLM-based agentic systems for drug discovery, i.e., whether parts of the agentic system such as the LLM are interchangeable, a topic that has received limited attention in drug discovery applications. We compare the performance of different large language models (LLMs) and the effectiveness of tool-calling agents versus code-generating agents in this domain. Our case study, comparing performance in orchestrating tools for chemistry and drug discovery using an LLM-as-a-judge score, shows that Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative language models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and Nova-Micro. Although we confirm that code-generating agents outperform the tool-calling ones on average, we show that this is highly question and model dependent. Furthermore, the impact of replacing system prompts is dependent on the specific question asked and the model used, underscoring that -- even in this particular domain -- one cannot just replace language models without considering prompt re-engineering. Our study highlights the necessity of further research into the modularity of agentic systems to enable the development of stable and scalable solutions for real-world problems.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.22189"
    },
    "5168e27b7a90b5a131f694d06dc5a5e5": {
        "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks",
        "authors": [
            "Leander Melroy Maben",
            "Gayathri Ganesh Lakshmy",
            "Srijith Radhakrishnan",
            "Siddhant Arora",
            "Shinji Watanabe"
        ],
        "date": "2025/06/29",
        "pdf": "http://arxiv.org/pdf/2506.23049",
        "abstract": "Despite advances in language and speech technologies, no open-source system enables full speech-to-speech, multi-turn dialogue with integrated tool use and agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and Automated Tool Use), the first open-source, speech-native assistant capable of completing complex, goal-driven tasks through dynamic tool invocation and multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a cascaded pipeline and supports tools such as calendar booking, contact lookup, web search, and email. Its modular design allows easy integration of new tools using natural language prompts and action classes. On VoiceBench, AURA scores 92.75% on OpenBookQA-outperforming all open-weight systems and nearing GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems. Human evaluation shows 90% task success on complex, multi-turn speech tasks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.23049"
    },
    "3e3574fdfc46d397df8011ad3d233264": {
        "title": "LLM Agents Are the Antidote to Walled Gardens",
        "authors": [
            "Samuele Marro",
            "Philip Torr"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2506.23978",
        "abstract": "While the Internet&#39;s core infrastructure was designed to be open and universal, today&#39;s application layer is dominated by closed, proprietary platforms. Open and interoperable APIs require significant investment, and market leaders have little incentive to enable data exchange that could erode their user lock-in. We argue that LLM-based agents fundamentally disrupt this status quo. Agents can automatically translate between data formats and interact with interfaces designed for humans: this makes interoperability dramatically cheaper and effectively unavoidable. We name this shift universal interoperability: the ability for any two digital services to exchange data seamlessly using AI-mediated adapters. Universal interoperability undermines monopolistic behaviours and promotes data portability. However, it can also lead to new security risks and technical debt. Our position is that the ML community should embrace this development while building the appropriate frameworks to mitigate the downsides. By acting now, we can harness AI to restore user freedom and competitive markets without sacrificing security.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2506.23978"
    },
    "6d74833db712adecc671f27e9ad09dce": {
        "title": "Ella: Embodied Social Agents with Lifelong Memory",
        "authors": [
            "Hongxin Zhang",
            "Zheyuan Zhang",
            "Zeyuan Wang",
            "Zunzhe Zhang",
            "Lixing Fang",
            "Qinhong Zhou",
            "Chuang Gan"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2506.24019",
        "abstract": "We introduce Ella, an embodied social agent capable of lifelong learning within a community in a 3D open world, where agents accumulate experiences and acquire knowledge through everyday visual observations and social interactions. At the core of Ella&#39;s capabilities is a structured, long-term multimodal memory system that stores, updates, and retrieves information effectively. It consists of a name-centric semantic memory for organizing acquired knowledge and a spatiotemporal episodic memory for capturing multimodal experiences. By integrating this lifelong memory system with foundation models, Ella retrieves relevant information for decision-making, plans daily activities, builds social relationships, and evolves autonomously while coexisting with other intelligent beings in the open world. We conduct capability-oriented evaluations in a dynamic 3D open world where 15 agents engage in social activities for days and are assessed with a suite of unseen controlled evaluations. Experimental results show that Ella can influence, lead, and cooperate with other agents well to achieve goals, showcasing its ability to learn effectively through observation and social interaction. Our findings highlight the transformative potential of combining structured memory systems with foundation models for advancing embodied intelligence. More videos can be found at https://umass-embodied-agi.github.io/Ella/.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.24019"
    },
    "faa89547942230db73bf690624e9bda0": {
        "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
        "authors": [
            "Bo Liu",
            "Leon Guertler",
            "Simon Yu",
            "Zichen Liu",
            "Penghui Qi",
            "Daniel Balcells",
            "Mickel Liu",
            "Cheston Tan",
            "Weiyan Shi",
            "Min Lin",
            "Wee Sun Lee",
            "Natasha Jaques"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2506.24119",
        "abstract": "Recent advances in reinforcement learning have shown that language models can develop sophisticated reasoning through training on tasks with verifiable rewards, but these approaches depend on human-curated problem-answer pairs and domain-specific reward engineering. We introduce SPIRAL, a self-play framework where models learn by playing multi-turn, zero-sum games against continuously improving versions of themselves, eliminating the need for human supervision. Through self-play, SPIRAL generates an infinite curriculum of progressively challenging problems as models must constantly adapt to stronger opponents. To enable this self-play training at scale, We implement a fully online, multi-turn, multi-agent reinforcement learning system for LLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent training. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert game trajectories. Analysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition, expected value calculation, and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple Negotiation) further enhances performance as each game develops distinct reasoning strengths. Applying SPIRAL to a strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These results demonstrate that zero-sum games naturally develop transferable reasoning capabilities, highlighting a promising direction for autonomous reasoning development.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2506.24119"
    },
    "0eb2a844fe73d4f99eab87b7962fb4a4": {
        "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents",
        "authors": [
            "Imene Kerboua",
            "Sahar Omidi Shayegan",
            "Megh Thakkar",
            "Xing Han Lù",
            "Massimo Caccia",
            "Véronique Eglin",
            "Alexandre Aussem",
            "Jérémy Espinas",
            "Alexandre Lacoste"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2507.00210",
        "abstract": "While large language models have demonstrated impressive capabilities in web navigation tasks, the extensive context of web pages, often represented as DOM or Accessibility Tree (AxTree) structures, frequently exceeds model context limits. Current approaches like bottom-up truncation or embedding-based retrieval lose critical information about page state and action history. This is particularly problematic for adaptive planning in web agents, where understanding the current state is essential for determining future actions. We hypothesize that embedding models lack sufficient capacity to capture plan-relevant information, especially when retrieving content that supports future action prediction. This raises a fundamental question: how can retrieval methods be optimized for adaptive planning in web navigation tasks? In response, we introduce \\textit{LineRetriever}, a novel approach that leverages a language model to identify and retrieve observation lines most relevant to future navigation steps. Unlike traditional retrieval methods that focus solely on semantic similarity, \\textit{LineRetriever} explicitly considers the planning horizon, prioritizing elements that contribute to action prediction. Our experiments demonstrate that \\textit{LineRetriever} can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.00210"
    },
    "7434141bab96e8e379416c3b9173e46d": {
        "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation",
        "authors": [
            "Xi Xuan",
            "King-kui Sin",
            "Yufei Zhou",
            "Chunyu Kit"
        ],
        "date": "2025/07/01",
        "pdf": "http://arxiv.org/pdf/2507.00875",
        "abstract": "Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.00875"
    },
    "4d65dc91f7e04d6e759aaaf39c3c806d": {
        "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered",
        "authors": [
            "Imran Mirza",
            "Cole Huang",
            "Ishwara Vasista",
            "Rohan Patil",
            "Asli Akalin",
            "Sean O&#39;Brien",
            "Kevin Zhu"
        ],
        "date": "2025/04/10",
        "pdf": "http://arxiv.org/pdf/2507.01019",
        "abstract": "Multi-agent systems, which consist of multiple AI models interacting within a shared environment, are increasingly used for persona-based interactions. However, if not carefully designed, these systems can reinforce implicit biases in large language models (LLMs), raising concerns about fairness and equitable representation. We present MALIBU, a novel benchmark developed to assess the degree to which LLM-based multi-agent systems implicitly reinforce social biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems through scenario-based assessments. AI models complete tasks within predefined contexts, and their responses undergo evaluation by an LLM-based multi-agent judging system in two phases. In the first phase, judges score responses labeled with specific demographic personas (e.g., gender, race, religion) across four metrics. In the second phase, judges compare paired responses assigned to different personas, scoring them and selecting the superior response. Our study quantifies biases in LLM-generated outputs, revealing that bias mitigation may favor marginalized personas over true neutrality, emphasizing the need for nuanced detection, balanced fairness strategies, and transparent evaluation benchmarks in multi-agent systems.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.01019"
    },
    "5859fe4bd308e92cea42cb428cac0215": {
        "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent",
        "authors": [
            "Hongli Yu",
            "Tinghong Chen",
            "Jiangtao Feng",
            "Jiangjie Chen",
            "Weinan Dai",
            "Qiying Yu",
            "Ya-Qin Zhang",
            "Wei-Ying Ma",
            "Jingjing Liu",
            "Mingxuan Wang",
            "Hao Zhou"
        ],
        "date": "2025/07/03",
        "pdf": "http://arxiv.org/pdf/2507.02259",
        "abstract": "Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents with linear complexity without performance degradation during extrapolation remains the ultimate challenge in long-text processing. We directly optimize for long-text tasks in an end-to-end fashion and introduce a novel agent workflow, MemAgent, which reads text in segments and updates the memory using an overwrite strategy. We extend the DAPO algorithm to facilitate training via independent-context multi-conversation generation. MemAgent has demonstrated superb long-context capabilities, being able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task with performance loss &lt; 5% and achieves 95%+ in 512K RULER test.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.02259"
    },
    "ace0cff081ae94f9422424da7488466e": {
        "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
        "authors": [
            "Kuan Li",
            "Zhongwang Zhang",
            "Huifeng Yin",
            "Liwen Zhang",
            "Litu Ou",
            "Jialong Wu",
            "Wenbiao Yin",
            "Baixuan Li",
            "Zhengwei Tao",
            "Xinyu Wang",
            "Weizhou Shen",
            "Junkai Zhang",
            "Dingchu Zhang",
            "Xixi Wu",
            "Yong Jiang",
            "Ming Yan",
            "Pengjun Xie",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "date": "2025/07/03",
        "pdf": "http://arxiv.org/pdf/2507.02592",
        "abstract": "Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents&#39; performance and closing the capability gap.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.02592"
    },
    "56ab9548b7cd292c2929c039edb5f138": {
        "title": "A Large Language Model-Empowered Agent for Reliable and Robust Structural Analysis",
        "authors": [
            "Jiachen Liu",
            "Ziheng Geng",
            "Ran Cao",
            "Lu Cheng",
            "Paolo Bocchini",
            "Minghui Cheng"
        ],
        "date": "2025/06/27",
        "pdf": "http://arxiv.org/pdf/2507.02938",
        "abstract": "Large language models (LLMs) have exhibited remarkable capabilities across diverse open-domain tasks, yet their application in specialized domains such as civil engineering remains largely unexplored. This paper starts bridging this gap by evaluating and enhancing the reliability and robustness of LLMs in structural analysis of beams. Reliability is assessed through the accuracy of correct outputs under repetitive runs of the same problems, whereas robustness is evaluated via the performance across varying load and boundary conditions. A benchmark dataset, comprising eight beam analysis problems, is created to test the Llama-3.3 70B Instruct model. Results show that, despite a qualitative understanding of structural mechanics, the LLM lacks the quantitative reliability and robustness for engineering applications. To address these limitations, a shift is proposed that reframes the structural analysis as code generation tasks. Accordingly, an LLM-empowered agent is developed that (a) integrates chain-of-thought and few-shot prompting to generate accurate OpeeSeesPy code, and (b) automatically executes the code to produce structural analysis results. Experimental results demonstrate that the agent achieves accuracy exceeding 99.0% on the benchmark dataset, exhibiting reliable and robust performance across diverse conditions. Ablation studies highlight the complete example and function usage examples as the primary contributors to the agent&#39;s enhanced performance.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.02938"
    },
    "aa6e97574d8f2067d0ba52471085b0fe": {
        "title": "GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models",
        "authors": [
            "Seshu Tirupathi",
            "Dhaval Salwala",
            "Elizabeth Daly",
            "Inge Vejsbjerg"
        ],
        "date": "2025/07/01",
        "pdf": "http://arxiv.org/pdf/2507.02986",
        "abstract": "As Large Language Models (LLMs) continue to be increasingly applied across various domains, their widespread adoption necessitates rigorous monitoring to prevent unintended negative consequences and ensure robustness. Furthermore, LLMs must be designed to align with human values, like preventing harmful content and ensuring responsible usage. The current automated systems and solutions for monitoring LLMs in production are primarily centered on LLM-specific concerns like hallucination etc, with little consideration given to the requirements of specific use-cases and user preferences. This paper introduces GAF-Guard, a novel agentic framework for LLM governance that places the user, the use-case, and the model itself at the center. The framework is designed to detect and monitor risks associated with the deployment of LLM based applications. The approach models autonomous agents that identify risks, activate risk detection tools, within specific use-cases and facilitate continuous monitoring and reporting to enhance AI safety, and user expectations. The code is available at https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.02986"
    },
    "859633d4c2938ad2a6f2dae147ac98c9": {
        "title": "OpenTable-R1: A Reinforcement Learning Augmented Tool Agent for Open-Domain Table Question Answering",
        "authors": [
            "Zipeng Qiu"
        ],
        "date": "2025/07/02",
        "pdf": "http://arxiv.org/pdf/2507.03018",
        "abstract": "Open-domain table question answering traditionally relies on a two-stage pipeline: static table retrieval followed by a closed-domain answer. In contrast, we propose an end-to-end agentic framework that embeds multi-turn tool calls-using a BM25+-based search API and a SQLite SQL executor-directly into a large language model. To further adapt a compact 4B-parameter model, we introduce a two-stage fine-tuning process: supervised cold-start on easy questions, then Async GRPO reinforcement learning on harder cases with LoRA adapters and a rollout buffer. This unified approach enables the model to jointly retrieve, reason, and execute queries, yielding a dramatic accuracy improvement from single-digit zero-shot performance to over 0.86 exact match on a held-out test set. Our results underscore the effectiveness of integrating structured tool calls with targeted RL fine-tuning for scalable, accurate table QA. The code is available at https://github.com/TabibitoQZP/OpenTableR1.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03018"
    },
    "74f7c9ab3598432ec25a48b39694f8ad": {
        "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents",
        "authors": [
            "Peisong Wang",
            "Ruotian Ma",
            "Bang Zhang",
            "Xingyu Chen",
            "Zhiwei He",
            "Kang Luo",
            "Qingsong Lv",
            "Qingxuan Jiang",
            "Zheng Xie",
            "Shanyi Wang",
            "Yuan Li",
            "Fanghua Ye",
            "Jian Li",
            "Yifan Yang",
            "Zhaopeng Tu",
            "Xiaolong Li"
        ],
        "date": "2025/07/03",
        "pdf": "http://arxiv.org/pdf/2507.03112",
        "abstract": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet their emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored. In this work, we introduce RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users to cultivate higher-order empathetic abilities in LLMs. Within this framework, self-consistent affective simulated users engage in dialogue rollouts and produce deterministic emotion scores during conversations, serving as reward signals to guide the LLM&#39;s learning. Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence. Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes. Our results show that RLVER is a practical route toward emotionally intelligent and broadly capable language agents.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03112"
    },
    "3181566d8580aad00277de14af2c9832": {
        "title": "GRAFT: A Graph-based Flow-aware Agentic Framework for Document-level Machine Translation",
        "authors": [
            "Himanshu Dutta",
            "Sunny Manchanda",
            "Prakhar Bapat",
            "Meva Ram Gurjar",
            "Pushpak Bhattacharyya"
        ],
        "date": "2025/07/04",
        "pdf": "http://arxiv.org/pdf/2507.03311",
        "abstract": "Document level Machine Translation (DocMT) approaches often struggle with effectively capturing discourse level phenomena. Existing approaches rely on heuristic rules to segment documents into discourse units, which rarely align with the true discourse structure required for accurate translation. Otherwise, they fail to maintain consistency throughout the document during translation. To address these challenges, we propose Graph Augmented Agentic Framework for Document Level Translation (GRAFT), a novel graph based DocMT system that leverages Large Language Model (LLM) agents for document translation. Our approach integrates segmentation, directed acyclic graph (DAG) based dependency modelling, and discourse aware translation into a cohesive framework. Experiments conducted across eight translation directions and six diverse domains demonstrate that GRAFT achieves significant performance gains over state of the art DocMT systems. Specifically, GRAFT delivers an average improvement of 2.8 d BLEU on the TED test sets from IWSLT2017 over strong baselines and 2.3 d BLEU for domain specific translation from English to Chinese. Moreover, our analyses highlight the consistent ability of GRAFT to address discourse level phenomena, yielding coherent and contextually accurate translations.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03311"
    },
    "463f01d5f6974ec22dfa5898c4bb6eb8": {
        "title": "AI-VaxGuide: An Agentic RAG-Based LLM for Vaccination Decisions",
        "authors": [
            "Abdellah Zeggai",
            "Ilyes Traikia",
            "Abdelhak Lakehal",
            "Abdennour Boulesnane"
        ],
        "date": "2025/07/04",
        "pdf": "http://arxiv.org/pdf/2507.03493",
        "abstract": "Vaccination plays a vital role in global public health, yet healthcare professionals often struggle to access immunization guidelines quickly and efficiently. National protocols and WHO recommendations are typically extensive and complex, making it difficult to extract precise information, especially during urgent situations. This project tackles that issue by developing a multilingual, intelligent question-answering system that transforms static vaccination guidelines into an interactive and user-friendly knowledge base. Built on a Retrieval-Augmented Generation (RAG) framework and enhanced with agent-based reasoning (Agentic RAG), the system provides accurate, context-sensitive answers to complex medical queries. Evaluation shows that Agentic RAG outperforms traditional methods, particularly in addressing multi-step or ambiguous questions. To support clinical use, the system is integrated into a mobile application designed for real-time, point-of-care access to essential vaccine information. AI-VaxGuide model is publicly available on https://huggingface.co/VaxGuide",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03493"
    },
    "8216bd9664a8666dc0df6908fad14cea": {
        "title": "Recon, Answer, Verify: Agents in Search of Truth",
        "authors": [
            "Satyam Shukla",
            "Himanshu Dutta",
            "Pushpak Bhattacharyya"
        ],
        "date": "2025/07/04",
        "pdf": "http://arxiv.org/pdf/2507.03671",
        "abstract": "Automated fact checking with large language models (LLMs) offers a scalable alternative to manual verification. Evaluating fact checking is challenging as existing benchmark datasets often include post claim analysis and annotator cues, which are absent in real world scenarios where claims are fact checked immediately after being made. This limits the realism of current evaluations. We present Politi Fact Only (PFO), a 5 class benchmark dataset of 2,982 political claims from politifact.com, where all post claim analysis and annotator cues have been removed manually. This ensures that models are evaluated using only the information that would have been available prior to the claim&#39;s verification. Evaluating LLMs on PFO, we see an average performance drop of 22% in terms of macro f1 compared to PFO&#39;s unfiltered version. Based on the identified challenges of the existing LLM based fact checking system, we propose RAV (Recon Answer Verify), an agentic framework with three agents: question generator, answer generator, and label generator. Our pipeline iteratively generates and answers sub questions to verify different aspects of the claim before finally generating the label. RAV generalizes across domains and label granularities, and it outperforms state of the art approaches on well known baselines RAWFC (fact checking, 3 class) by 25.28%, and on HOVER (encyclopedia, 2 class) by 1.54% on 2 hop, 4.94% on 3 hop, and 1.78% on 4 hop, sub categories respectively. RAV shows the least performance drop compared to baselines of 16.3% in macro f1 when we compare PFO with its unfiltered version.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03671"
    },
    "a6f9538817c32b5adef9aca9de52e470": {
        "title": "STRUCTSENSE: A Task-Agnostic Agentic Framework for Structured Information Extraction with Human-In-The-Loop Evaluation and Benchmarking",
        "authors": [
            "Tek Raj Chhetri",
            "Yibei Chen",
            "Puja Trivedi",
            "Dorota Jarecka",
            "Saif Haobsh",
            "Patrick Ray",
            "Lydia Ng",
            "Satrajit S. Ghosh"
        ],
        "date": "2025/07/04",
        "pdf": "http://arxiv.org/pdf/2507.03674",
        "abstract": "The ability to extract structured information from unstructured sources-such as free-text documents and scientific literature-is critical for accelerating scientific discovery and knowledge synthesis. Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks, including structured information extraction. However, their effectiveness often diminishes in specialized, domain-specific contexts that require nuanced understanding and expert-level domain knowledge. In addition, existing LLM-based approaches frequently exhibit poor transferability across tasks and domains, limiting their scalability and adaptability. To address these challenges, we introduce StructSense, a modular, task-agnostic, open-source framework for structured information extraction built on LLMs. StructSense is guided by domain-specific symbolic knowledge encoded in ontologies, enabling it to navigate complex domain content more effectively. It further incorporates agentic capabilities through self-evaluative judges that form a feedback loop for iterative refinement, and includes human-in-the-loop mechanisms to ensure quality and validation. We demonstrate that StructSense can overcome both the limitations of domain sensitivity and the lack of cross-task generalizability, as shown through its application to diverse neuroscience information extraction tasks.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03674"
    },
    "e400d62b5f769d6b6f0b43d41a3393ef": {
        "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
        "authors": [
            "Yuanzhe Hu",
            "Yu Wang",
            "Julian McAuley"
        ],
        "date": "2025/07/07",
        "pdf": "http://arxiv.org/pdf/2507.05257",
        "abstract": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on evaluating reasoning, planning, and execution capabilities, while another critical component-memory, encompassing how agents memorize, update, and retrieve long-term information-is under-evaluated due to the lack of benchmarks. We term agents with memory mechanisms as memory agents. In this paper, we identify four core competencies essential for memory agents: accurate retrieval, test-time learning, long-range understanding, and conflict resolution. Existing datasets either rely on limited context lengths or are tailored for static, long-context settings like book-based QA, which do not reflect the interactive, multi-turn nature of memory agents that incrementally accumulate information. Furthermore, no existing benchmarks cover all four competencies. Therefore, we introduce MemoryAgentBench, a new benchmark specifically designed for memory agents. Our benchmark combines reformulated existing datasets with newly constructed ones, covering the above four memory competencies, providing a systematic and challenging testbed for assessing memory quality. We evaluate a diverse set of memory agents, ranging from simple context-based and retrieval-augmented generation (RAG) systems to advanced agents with external memory modules and tool integration. Empirical results reveal that current methods fall short of mastering all four competencies, underscoring the need for further research into comprehensive memory mechanisms for LLM agents.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.05257"
    },
    "4f82a2479b07394328e0edbccf4e4752": {
        "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents",
        "authors": [
            "Ming Gong",
            "Xucheng Huang",
            "Chenghan Yang",
            "Xianhan Peng",
            "Haoxin Wang",
            "Yang Liu",
            "Ling Jiang"
        ],
        "date": "2025/07/07",
        "pdf": "http://arxiv.org/pdf/2507.05330",
        "abstract": "Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular &#34;MLLM-as-Tool&#34; strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.05330"
    },
    "63c29ca04913563f4d933183cc096ae0": {
        "title": "ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?",
        "authors": [
            "Haoxin Wang",
            "Xianhan Peng",
            "Xucheng Huang",
            "Yizhe Huang",
            "Ming Gong",
            "Chenghan Yang",
            "Yang Liu",
            "Ling Jiang"
        ],
        "date": "2025/07/08",
        "pdf": "http://arxiv.org/pdf/2507.05639",
        "abstract": "In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. Upon publication, the code and data will be open-sourced to facilitate further research and development in this domain.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.05639"
    },
    "1c44cd7dc4a921a6b18690a0c07a9fd1": {
        "title": "Agentic-R1: Distilled Dual-Strategy Reasoning",
        "authors": [
            "Weihua Du",
            "Pranjal Aggarwal",
            "Sean Welleck",
            "Yiming Yang"
        ],
        "date": "2025/07/08",
        "pdf": "http://arxiv.org/pdf/2507.05707",
        "abstract": "Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.05707"
    },
    "309622272ff9fa8dac80c6aa58f49374": {
        "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents",
        "authors": [
            "Youmna Farag",
            "Svetlana Stoyanchev",
            "Mohan Li",
            "Simon Keizer",
            "Rama Doddipatla"
        ],
        "date": "2025/07/08",
        "pdf": "http://arxiv.org/pdf/2507.06016",
        "abstract": "Embodied agents performing complex tasks are susceptible to execution failures, motivating the need for effective failure recovery mechanisms. In this work, we introduce a conditional multistage failure recovery framework that employs zero-shot chain prompting. The framework is structured into four error-handling stages, with three operating during task execution and one functioning as a post-execution reflection phase. Our approach utilises the reasoning capabilities of LLMs to analyse execution challenges within their environmental context and devise strategic solutions. We evaluate our method on the TfD benchmark of the TEACH dataset and achieve state-of-the-art performance, outperforming a baseline without error recovery by 11.5% and surpassing the strongest existing model by 19%.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.06016"
    },
    "1046330f429b9bd1a498e5336643a55c": {
        "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving",
        "authors": [
            "Xiangru Tang",
            "Tianrui Qin",
            "Tianhao Peng",
            "Ziyang Zhou",
            "Daniel Shao",
            "Tingting Du",
            "Xinming Wei",
            "Peng Xia",
            "Fang Wu",
            "He Zhu",
            "Ge Zhang",
            "Jiaheng Liu",
            "Xingyao Wang",
            "Sirui Hong",
            "Chenglin Wu",
            "Hao Cheng",
            "Chi Wang",
            "Wangchunshu Zhou"
        ],
        "date": "2025/07/08",
        "pdf": "http://arxiv.org/pdf/2507.06229",
        "abstract": "As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other&#39;s experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.06229"
    },
    "bb86e7570fe59484509437e579ebecf0": {
        "title": "Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings",
        "authors": [
            "Russell Taylor",
            "Benjamin Herbert",
            "Michael Sana"
        ],
        "date": "2025/07/09",
        "pdf": "http://arxiv.org/pdf/2507.06506",
        "abstract": "Translating wordplay across languages presents unique challenges that have long confounded both professional human translators and machine translation systems. This research proposes a novel approach for translating puns from English to French by combining state-of-the-art large language models with specialized techniques for wordplay generation. Our methodology employs a three-stage approach. First, we establish a baseline using multiple frontier large language models with feedback based on a new contrastive learning dataset. Second, we implement a guided chain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we implement a multi-agent generator-discriminator framework for evaluating and regenerating puns with feedback. Moving beyond the limitations of literal translation, our methodology&#39;s primary objective is to capture the linguistic creativity and humor of the source text wordplay, rather than simply duplicating its vocabulary. Our best runs earned first and second place in the CLEF JOKER 2025 Task 2 competition where they were evaluated manually by expert native French speakers. This research addresses a gap between translation studies and computational linguistics by implementing linguistically-informed techniques for wordplay translation, advancing our understanding of how language models can be leveraged to handle the complex interplay between semantic ambiguity, phonetic similarity, and the implicit cultural and linguistic awareness needed for successful humor.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.06506"
    },
    "b8b8a5e49d282ddf0f9b13cb07d383a1": {
        "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation",
        "authors": [
            "Ziang Ye",
            "Yang Zhang",
            "Wentao Shi",
            "Xiaoyu You",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "date": "2025/07/09",
        "pdf": "http://arxiv.org/pdf/2507.06899",
        "abstract": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models (LVLMs) have emerged as a revolutionary approach to automating human-machine interactions, capable of autonomously operating personal devices (e.g., mobile phones) or applications within the device to perform complex real-world tasks in a human-like manner. However, their close integration with personal devices raises significant security concerns, with many threats, including backdoor attacks, remaining largely unexplored. This work reveals that the visual grounding of GUI agent-mapping textual plans to GUI elements-can introduce vulnerabilities, enabling new types of backdoor attacks. With backdoor attack targeting visual grounding, the agent&#39;s behavior can be compromised even when given correct task-solving plans. To validate this vulnerability, we propose VisualTrap, a method that can hijack the grounding by misleading the agent to locate textual plans to trigger locations instead of the intended targets. VisualTrap uses the common method of injecting poisoned data for attacks, and does so during the pre-training of visual grounding to ensure practical feasibility of attacking. Empirical results show that VisualTrap can effectively hijack visual grounding with as little as 5% poisoned data and highly stealthy visual triggers (invisible to the human eye); and the attack can be generalized to downstream tasks, even after clean fine-tuning. Moreover, the injected trigger can remain effective across different GUI environments, e.g., being trained on mobile/web and generalizing to desktop environments. These findings underscore the urgent need for further research on backdoor attack risks in GUI agents.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.06899"
    },
    "e3a9eb0f8bc23af7842f32120defca11": {
        "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection",
        "authors": [
            "Ziyan Liu",
            "Chunxiao Fan",
            "Haoran Lou",
            "Yuexin Wu",
            "Kaiwei Deng"
        ],
        "date": "2025/07/09",
        "pdf": "http://arxiv.org/pdf/2507.06908",
        "abstract": "The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at https://github.com/destroy-lonely/MIND.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.06908"
    },
    "f5e204e08754cf7e2561248609e3cc64": {
        "title": "Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation",
        "authors": [
            "Anirban Saha Anik",
            "Xiaoying Song",
            "Elliott Wang",
            "Bryan Wang",
            "Bengisu Yarimbas",
            "Lingzi Hong"
        ],
        "date": "2025/07/09",
        "pdf": "http://arxiv.org/pdf/2507.07307",
        "abstract": "Large language models (LLMs) incorporated with Retrieval-Augmented Generation (RAG) have demonstrated powerful capabilities in generating counterspeech against misinformation. However, current studies rely on limited evidence and offer less control over final outputs. To address these challenges, we propose a Multi-agent Retrieval-Augmented Framework to generate counterspeech against health misinformation, incorporating multiple LLMs to optimize knowledge retrieval, evidence enhancement, and response refinement. Our approach integrates both static and dynamic evidence, ensuring that the generated counterspeech is relevant, well-grounded, and up-to-date. Our method outperforms baseline approaches in politeness, relevance, informativeness, and factual accuracy, demonstrating its effectiveness in generating high-quality counterspeech. To further validate our approach, we conduct ablation studies to verify the necessity of each component in our framework. Furthermore, human evaluations reveal that refinement significantly enhances counterspeech quality and obtains human preference.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.07307"
    },
    "c83d3103119ee68e898a793b7a3e0578": {
        "title": "SAND: Boosting LLM Agents with Self-Taught Action Deliberation",
        "authors": [
            "Yu Xia",
            "Yiran Jenny Shen",
            "Junda Wu",
            "Tong Yu",
            "Sungchul Kim",
            "Ryan A. Rossi",
            "Lina Yao",
            "Julian McAuley"
        ],
        "date": "2025/07/10",
        "pdf": "http://arxiv.org/pdf/2507.07441",
        "abstract": "Large Language Model (LLM) agents are commonly tuned with supervised finetuning on ReAct-style expert trajectories or preference optimization over pairwise rollouts. Most of these methods focus on imitating specific expert behaviors or promoting chosen reasoning thoughts and actions over rejected ones. However, without reasoning and comparing over alternatives actions, LLM agents finetuned with these methods may over-commit towards seemingly plausible but suboptimal actions due to limited action space exploration. To address this, in this paper we propose Self-taught ActioN Deliberation (SAND) framework, enabling LLM agents to explicitly deliberate over candidate actions before committing to one. To tackle the challenges of when and what to deliberate given large action space and step-level action evaluation, we incorporate self-consistency action sampling and execution-guided action critique to help synthesize step-wise action deliberation thoughts using the base model of the LLM agent. In an iterative manner, the deliberation trajectories are then used to finetune the LLM agent itself. Evaluating on two representative interactive agent tasks, SAND achieves an average 20% improvement over initial supervised finetuning and also outperforms state-of-the-art agent tuning approaches.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.07441"
    },
    "38c9ae35d95bffa1c442eadd5f7ba4bb": {
        "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System",
        "authors": [
            "Yuanchen Shi",
            "Longyin Zhang",
            "Fang Kong"
        ],
        "date": "2025/07/10",
        "pdf": "http://arxiv.org/pdf/2507.07509",
        "abstract": "The growing need for psychological support due to increasing pressures has exposed the scarcity of relevant datasets, particularly in non-English languages. To address this, we propose a framework that leverages limited real-world data and expert knowledge to fine-tune two large language models: Dialog Generator and Dialog Modifier. The Generator creates large-scale psychological counseling dialogues based on predefined paths, which guide system response strategies and user interactions, forming the basis for effective support. The Modifier refines these dialogues to align with real-world data quality. Through both automated and manual review, we construct the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K dialogues across 13 groups, 16 psychological problems, 13 causes, and 12 support focuses. Additionally, we introduce the Comprehensive Agent Dialogue Support System (CADSS), where a Profiler analyzes user characteristics, a Summarizer condenses dialogue history, a Planner selects strategies, and a Supporter generates empathetic responses. The experimental results of the Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate that CADSS achieves state-of-the-art performance on both CPsDD and ESConv datasets.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.07509"
    },
    "55b64aa47282e761af91a5b9166c0bbd": {
        "title": "Automating MD simulations for Proteins using Large language Models: NAMD-Agent",
        "authors": [
            "Achuth Chandrasekhar",
            "Amir Barati Farimani"
        ],
        "date": "2025/07/10",
        "pdf": "http://arxiv.org/pdf/2507.07887",
        "abstract": "Molecular dynamics simulations are an essential tool in understanding protein structure, dynamics, and function at the atomic level. However, preparing high quality input files for MD simulations can be a time consuming and error prone process. In this work, we introduce an automated pipeline that leverages Large Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with python scripting and Selenium based web automation to streamline the generation of MD input files. The pipeline exploits CHARMM GUI&#39;s comprehensive web-based interface for preparing simulation-ready inputs for NAMD. By integrating Gemini&#39;s code generation and iterative refinement capabilities, simulation scripts are automatically written, executed, and revised to navigate CHARMM GUI, extract appropriate parameters, and produce the required NAMD input files. Post processing is performed using additional software to further refine the simulation outputs, thereby enabling a complete and largely hands free workflow. Our results demonstrate that this approach reduces setup time, minimizes manual errors, and offers a scalable solution for handling multiple protein systems in parallel. This automated framework paves the way for broader application of LLMs in computational structural biology, offering a robust and adaptable platform for future developments in simulation automation.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.07887"
    },
    "0707058191a512c89d3fa5a6501d3df2": {
        "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
        "authors": [
            "Yu Wang",
            "Xi Chen"
        ],
        "date": "2025/07/10",
        "pdf": "http://arxiv.org/pdf/2507.07957",
        "abstract": "Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the field&#39;s most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.07957"
    },
    "c0334a5012b37c98fef7a9dbbded70a3": {
        "title": "PyVision: Agentic Vision with Dynamic Tooling",
        "authors": [
            "Shitian Zhao",
            "Haoquan Zhang",
            "Shaoheng Lin",
            "Ming Li",
            "Qilong Wu",
            "Kaipeng Zhang",
            "Chen Wei"
        ],
        "date": "2025/07/10",
        "pdf": "http://arxiv.org/pdf/2507.07998",
        "abstract": "LLMs are increasingly deployed as agents, systems capable of planning, reasoning, and dynamically calling external tools. However, in visual reasoning, prior approaches largely remain limited by predefined workflows and static toolsets. In this report, we present PyVision, an interactive, multi-turn framework that enables MLLMs to autonomously generate, execute, and refine Python-based tools tailored to the task at hand, unlocking flexible and interpretable problem-solving. We develop a taxonomy of the tools created by PyVision and analyze their usage across a diverse set of benchmarks. Quantitatively, PyVision achieves consistent performance gains, boosting GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini. These results point to a broader shift: dynamic tooling allows models not just to use tools, but to invent them, advancing toward more agentic visual reasoning.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.07998"
    },
    "ffa78fdffca6bafa4e92d8d3a1cbef11": {
        "title": "State and Memory is All You Need for Robust and Reliable AI Agents",
        "authors": [
            "Matthew Muhoberac",
            "Atharva Parikh",
            "Nirvi Vakharia",
            "Saniya Virani",
            "Aco Radujevic",
            "Savannah Wood",
            "Meghav Verma",
            "Dimitri Metaxotos",
            "Jeyaraman Soundararajan",
            "Thierry Masquelin",
            "Alexander G. Godfrey",
            "Sean Gardner",
            "Dobrila Rudnicki",
            "Sam Michael",
            "Gaurav Chopra"
        ],
        "date": "2025/06/30",
        "pdf": "http://arxiv.org/pdf/2507.00081",
        "abstract": "Large language models (LLMs) have enabled powerful advances in natural language understanding and generation. Yet their application to complex, real-world scientific workflows remain limited by challenges in memory, planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals), a modular agentic framework that allows LLM-based agents to autonomously plan, reason, and achieve robust and reliable domain-specific task execution. Agents are constructed dynamically from source code documentation and augmented with finite-state automata (FSA) memory, enabling persistent state tracking and context-aware decision-making. This approach eliminates the need for manual prompt engineering and allows for robust, scalable deployment across diverse applications via maintaining context across extended workflows and to recover from tool or execution failures. We validate SciBORG through integration with both physical and virtual hardware, such as microwave synthesizers for executing user-specified reactions, with context-aware decision making and demonstrate its use in autonomous multi-step bioassay retrieval from the PubChem database utilizing multi-step planning, reasoning, agent-to-agent communication and coordination for execution of exploratory tasks. Systematic benchmarking shows that SciBORG agents achieve reliable execution, adaptive planning, and interpretable state transitions. Our results show that memory and state awareness are critical enablers of agentic planning and reliability, offering a generalizable foundation for deploying AI agents in complex environments.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.00081"
    },
    "e61f838ab6a6e79f6e9d09543a34d26b": {
        "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems",
        "authors": [
            "Zhaoyan Sun",
            "Jiayi Wang",
            "Xinyang Zhao",
            "Jiachi Wang",
            "Guoliang Li"
        ],
        "date": "2025/07/02",
        "pdf": "http://arxiv.org/pdf/2507.01599",
        "abstract": "Traditional Data+AI systems utilize data-driven techniques to optimize performance, but they rely heavily on human experts to orchestrate system pipelines, enabling them to adapt to changes in data, queries, tasks, and environments. For instance, while there are numerous data science tools available, developing a pipeline planning system to coordinate these tools remains challenging. This difficulty arises because existing Data+AI systems have limited capabilities in semantic understanding, reasoning, and planning. Fortunately, we have witnessed the success of large language models (LLMs) in enhancing semantic understanding, reasoning, and planning abilities. It is crucial to incorporate LLM techniques to revolutionize data systems for orchestrating Data+AI applications effectively. To achieve this, we propose the concept of a &#39;Data Agent&#39; - a comprehensive architecture designed to orchestrate Data+AI ecosystems, which focuses on tackling data-related tasks by integrating knowledge comprehension, reasoning, and planning capabilities. We delve into the challenges involved in designing data agents, such as understanding data/queries/environments/tools, orchestrating pipelines/workflows, optimizing and executing pipelines, and fostering pipeline self-reflection. Furthermore, we present examples of data agent systems, including a data science agent, data analytics agents (such as unstructured data analytics agent, semantic structured data analytics agent, data lake analytics agent, and multi-modal data analytics agent), and a database administrator (DBA) agent. We also outline several open challenges associated with designing data agent systems.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2507.01599"
    },
    "80b81be3db0feae7224843f59b928a5d": {
        "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research",
        "authors": [
            "Ruofan Jin",
            "Zaixi Zhang",
            "Mengdi Wang",
            "Le Cong"
        ],
        "date": "2025/07/01",
        "pdf": "http://arxiv.org/pdf/2507.02004",
        "abstract": "The rapid growth of biomedical data, tools, and literature has created a fragmented research landscape that outpaces human expertise. While AI agents offer a solution, they typically rely on static, manually curated toolsets, limiting their ability to adapt and scale. Here, we introduce STELLA, a self-evolving AI agent designed to overcome these limitations. STELLA employs a multi-agent architecture that autonomously improves its own capabilities through two core mechanisms: an evolving Template Library for reasoning strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent automatically discovers and integrates new bioinformatics tools. This allows STELLA to learn from experience. We demonstrate that STELLA achieves state-of-the-art accuracy on a suite of biomedical benchmarks, scoring approximately 26\\% on Humanity&#39;s Last Exam: Biomedicine, 54\\% on LAB-Bench: DBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6 percentage points. More importantly, we show that its performance systematically improves with experience; for instance, its accuracy on the Humanity&#39;s Last Exam benchmark almost doubles with increased trials. STELLA represents a significant advance towards AI Agent systems that can learn and grow, dynamically scaling their expertise to accelerate the pace of biomedical discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.02004"
    },
    "ac23ae73f4300bd3daa67e8a8ee53966": {
        "title": "Large Language Model Agent for Modular Task Execution in Drug Discovery",
        "authors": [
            "Janghoon Ock",
            "Radheesh Sharma Meda",
            "Srivathsan Badrinarayanan",
            "Neha S. Aluru",
            "Achuth Chandrasekhar",
            "Amir Barati Farimani"
        ],
        "date": "2025/06/26",
        "pdf": "http://arxiv.org/pdf/2507.02925",
        "abstract": "We present a modular framework powered by large language models (LLMs) that automates and streamlines key tasks across the early-stage computational drug discovery pipeline. By combining LLM reasoning with domain-specific tools, the framework performs biomedical data retrieval, domain-specific question answering, molecular generation, property prediction, property-aware molecular refinement, and 3D protein-ligand structure generation. In a case study targeting BCL-2 in lymphocytic leukemia, the agent autonomously retrieved relevant biomolecular information-including FASTA sequences, SMILES representations, and literature-and answered mechanistic questions with improved contextual accuracy over standard LLMs. It then generated chemically diverse seed molecules and predicted 67 ADMET-related properties, which guided iterative molecular refinement. Across two refinement rounds, the number of molecules with QED &gt; 0.6 increased from 34 to 55, and those passing at least four out of five empirical drug-likeness rules rose from 29 to 52, within a pool of 194 molecules. The framework also employed Boltz-2 to generate 3D protein-ligand complexes and provide rapid binding affinity estimates for candidate compounds. These results demonstrate that the approach effectively supports molecular screening, prioritization, and structure evaluation. Its modular design enables flexible integration of evolving tools and models, providing a scalable foundation for AI-assisted therapeutic discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.02925"
    },
    "406f6534cab9f00c1fed137d10c16b4f": {
        "title": "LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents",
        "authors": [
            "Anand Gokhale",
            "Vaibhav Srivastava",
            "Francesco Bullo"
        ],
        "date": "2025/07/04",
        "pdf": "http://arxiv.org/pdf/2507.03293",
        "abstract": "Large language models (LLMs) have demonstrated promise in reasoning tasks and general decision-making in static environments. In long-term planning tasks, however, errors tend to accumulate, often leading to unsafe or inefficient behavior, limiting their use in general-purpose settings. We propose a modular actor-critic architecture in which an LLM actor is guided by LTLCrit, a trajectory-level LLM critic that communicates via linear temporal logic (LTL). Our setup combines the reasoning strengths of language models with the guarantees of formal logic. The actor selects high-level actions from natural language observations, while the critic analyzes full trajectories and proposes new LTL constraints that shield the actor from future unsafe or inefficient behavior. The architecture supports both fixed, hand-specified safety constraints and adaptive, learned soft constraints that promote long-term efficiency. Our architecture is model-agnostic: any LLM-based planner can serve as the actor, and LTLCrit serves as a logic-generating wrapper. We formalize planning as graph traversal under symbolic constraints, allowing LTLCrit to analyze failed or suboptimal trajectories and generate new temporal logic rules that improve future behavior. We evaluate our system on the Minecraft diamond-mining benchmark, achieving 100% completion rates and improving efficiency compared to baseline LLM planners. Our results suggest that enabling LLMs to supervise each other through logic is a powerful and flexible paradigm for safe, generalizable decision making.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03293"
    },
    "b616751854657c362589380c87b3ce50": {
        "title": "Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models",
        "authors": [
            "Riya Naik",
            "Ashwin Srinivasan",
            "Swati Agarwal",
            "Estrid He"
        ],
        "date": "2025/07/04",
        "pdf": "http://arxiv.org/pdf/2507.03726",
        "abstract": "Many of us now treat LLMs as modern-day oracles asking it almost any kind of question. However, consulting an LLM does not have to be a single turn activity. But long multi-turn interactions can get tedious if it is simply to clarify contextual information that can be arrived at through reasoning. In this paper, we examine the use of agent-based architecture to bolster LLM-based Question-Answering systems with additional reasoning capabilities. We examine the automatic resolution of potential incompleteness or ambiguities in questions by transducers implemented using LLM-based agents. We focus on several benchmark datasets that are known to contain questions with these deficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and Llama-4-Scout) with agents that act as specialists in detecting and resolving deficiencies of incompleteness and ambiguity. The agents are implemented as zero-shot ReAct agents. Rather than producing an answer in a single step, the model now decides between 3 actions a) classify b) resolve c) answer. Action a) decides if the question is incomplete, ambiguous, or normal. Action b) determines if any deficiencies identified can be resolved. Action c) answers the resolved form of the question. We compare the use of LLMs with and without the use of agents with these components. Our results show benefits of agents with transducer 1) A shortening of the length of interactions with human 2) An improvement in the answer quality and 3) Explainable resolution of deficiencies in the question. On the negative side we find while it may result in additional LLM invocations and in some cases, increased latency. But on tested datasets, the benefits outweigh the costs except when questions already have sufficient context. Suggesting the agent-based approach could be a useful mechanism to harness the power of LLMs to develop more robust QA systems.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2507.03726"
    }
}