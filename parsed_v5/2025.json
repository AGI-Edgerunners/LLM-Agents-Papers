{
    "61b5038fc9df8e1d37fef5c460876350": {
        "title": "On Memory Construction and Retrieval for Personalized Conversational Agents",
        "authors": [
            "Zhuoshi Pan",
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Xufang Luo",
            "Hao Cheng",
            "Dongsheng Li",
            "Yuqing Yang",
            "Chin-Yew Lin",
            "H. Vicky Zhao",
            "Lili Qiu",
            "Jianfeng Gao"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05589",
        "abstract": "To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques. In this paper, we present two key findings: (1) The granularity of memory unit matters: Turn-level, session-level, and summarization-based methods each exhibit limitations in both memory retrieval accuracy and the semantic quality of the retrieved content. (2) Prompt compression methods, such as \\textit{LLMLingua-2}, can effectively serve as a denoising mechanism, enhancing memory retrieval accuracy across different granularities. Building on these insights, we propose SeCom, a method that constructs a memory bank with topical segments by introducing a conversation Segmentation model, while performing memory retrieval based on Compressed memory units. Experimental results show that SeCom outperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such as LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ],
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05589"
    },
    "281691883e4589edb720c4d85eed1799": {
        "title": "Evaluating Personality Traits in Large Language Models: Insights from Psychological Questionnaires",
        "authors": [
            "Pranav Bhandari",
            "Usman Naseem",
            "Amitava Datta",
            "Nicolas Fay",
            "Mehwish Nasim"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.05248",
        "abstract": "Psychological assessment tools have long helped humans understand behavioural patterns. While Large Language Models (LLMs) can generate content comparable to that of humans, we explore whether they exhibit personality traits. To this end, this work applies psychological tools to LLMs in diverse scenarios to generate personality profiles. Using established trait-based questionnaires such as the Big Five Inventory and by addressing the possibility of training data contamination, we examine the dimensional variability and dominance of LLMs across five core personality dimensions: Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. Our findings reveal that LLMs exhibit unique dominant traits, varying characteristics, and distinct personality profiles even within the same family of models.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05248"
    },
    "552a2c4e2a51224ee6298d6c34bf9aac": {
        "title": "CODESIM: Multi-Agent Code Generation and Problem Solving through Simulation-Driven Planning and Debugging",
        "authors": [
            "Md. Ashraful Islam",
            "Mohammed Eunus Ali",
            "Md Rizwan Parvez"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05664",
        "abstract": "Large Language Models (LLMs) have made significant strides in code generation and problem solving. Current approaches employ external tool-based iterative debuggers that use compiler or other tool-based runtime feedback to refine coarse programs generated by various methods. However, the effectiveness of these approaches heavily relies on the quality of the initial code generation, which remains an open challenge. In this paper, we introduce CodeSim, a novel multi-agent code generation framework that comprehensively addresses the stages of program synthesis-planning, coding, and debugging-through a human-like perception approach. As human verifies their understanding of any algorithms through visual simulation, CodeSim uniquely features a method of plan verification and internal debugging through the step-by-step simulation of input/output. Extensive experiments across seven challenging competitive problem-solving and program synthesis benchmarks demonstrate CodeSim&#39;s remarkable code generation capabilities. Our framework achieves new state-of-the-art (pass@1) results-(HumanEval 95.1%, MBPP 90.7%, APPS 22%, and CodeContests 29.1%). Furthermore, our method shows potential for even greater enhancement when cascaded with external debuggers. To facilitate further research and development in this area, we have open-sourced our framework in this link (https://kagnlp.github.io/codesim.github.io/).",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05664"
    },
    "c13ba9544a2a06f9994ce2dc692c444a": {
        "title": "MTPChat: A Multimodal Time-Aware Persona Dataset for Conversational Agents",
        "authors": [
            "Wanqi Yang",
            "Yanda Li",
            "Meng Fang",
            "Ling Chen"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05887",
        "abstract": "Understanding temporal dynamics is critical for conversational agents, enabling effective content analysis and informed decision-making. However, time-aware datasets, particularly for persona-grounded conversations, are still limited, which narrows their scope and diminishes their complexity. To address this gap, we introduce MTPChat, a multimodal, time-aware persona dialogue dataset that integrates linguistic, visual, and temporal elements within dialogue and persona memory. Leveraging MTPChat, we propose two time-sensitive tasks: Temporal Next Response Prediction (TNRP) and Temporal Grounding Memory Prediction (TGMP), both designed to assess a model&#39;s ability to understand implicit temporal cues and dynamic interactions. Additionally, we present an innovative framework featuring an adaptive temporal module to effectively integrate multimodal streams and capture temporal dependencies. Experimental results validate the challenges posed by MTPChat and demonstrate the effectiveness of our framework in multimodal time-sensitive scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05887"
    },
    "19879c35d36631c355da5a719ebceb80": {
        "title": "HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents",
        "authors": [
            "Mohammad Amin Abbasi",
            "Farnaz Sadat Mirnezami",
            "Hassan Naderi"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05982",
        "abstract": "This paper presents HamRaz, a novel Persian-language mental health dataset designed for Person-Centered Therapy (PCT) using Large Language Models (LLMs). Despite the growing application of LLMs in AI-driven psychological counseling, existing datasets predominantly focus on Western and East Asian contexts, overlooking cultural and linguistic nuances essential for effective Persian-language therapy. To address this gap, HamRaz combines script-based dialogues with adaptive LLM role-playing, ensuring coherent and dynamic therapy interactions. We also introduce HamRazEval, a dual evaluation framework that measures conversational quality and therapeutic effectiveness using General Dialogue Metrics and the Barrett-Lennard Relationship Inventory (BLRI). Experimental results show HamRaz outperforms conventional Script Mode and Two-Agent Mode, producing more empathetic, context-aware, and realistic therapy sessions. By releasing HamRaz, we contribute a culturally adapted, LLM-driven resource to advance AI-powered psychotherapy research in diverse communities.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05982"
    },
    "3c329c9902e3a5342ab10c8f05ce2a56": {
        "title": "Preventing Rogue Agents Improves Multi-Agent Collaboration",
        "authors": [
            "Ohav Barbi",
            "Ori Yoran",
            "Mor Geva"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05986",
        "abstract": "Multi-agent systems, where specialized agents collaborate to solve a shared task hold great potential, from increased modularity to simulating complex environments. However, they also have a major caveat -- a single agent can cause the entire system to fail. Consider a simple game where the knowledge to solve the task is distributed between agents, which share information in a communication channel. At each round, any of the agents can terminate the game and make the final prediction, even if they are uncertain about the outcome of their action. Detection of such rogue agents $\\textit{before they act}$ may prevent the system&#39;s failure. In this work, we propose to $\\textit{monitor}$ agents during action prediction and $\\textit{intervene}$ when a future error is likely to occur. To test our approach, we introduce WhoDunitEnv, a multi-agent collaboration environment that allows modular control over task complexity and communication structure. Experiments on two variants of WhoDunitEnv and the GovSim environment for resource sustainability show that our approach leads to substantial performance gains up to 17.4% and 20%, respectively. Moreover, a thorough analysis shows that our monitors successfully identify critical points of agent confusion and our interventions effectively stop agent errors from propagating.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05986"
    },
    "49063743e04babb9acce0f2448fbf35c": {
        "title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?",
        "authors": [
            "Wenzhe Li",
            "Yong Lin",
            "Mengzhou Xia",
            "Chi Jin"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.00674",
        "abstract": "Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00674"
    },
    "bf4e41ed6254617cf3cafcde348322e0": {
        "title": "Efficient Multi-Agent System Training with Data Influence-Oriented Tree Search",
        "authors": [
            "Wentao Shi",
            "Zichun Yu",
            "Fuli Feng",
            "Xiangnan He",
            "Chenyan Xiong"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.00955",
        "abstract": "Monte Carlo Tree Search (MCTS) based methods provide promising approaches for generating synthetic data to enhance the self-training of Large Language Model (LLM) based multi-agent systems (MAS). These methods leverage Q-values to estimate individual agent contributions. However, relying solely on Q-values to identify informative data may misalign with the data synthesis objective, as the focus should be on selecting data that best enhances model training. To address this discrepancy, we propose Data Influence-oriented Tree Search (DITS), a novel framework that incorporates influence scores to guide both tree search and data selection. By leveraging influence scores, we effectively identify the most impactful data for system improvement, thereby enhancing model performance. Furthermore, we derive influence score estimation methods tailored for non-differentiable metrics, significantly reducing computational overhead by utilizing inference computations. Extensive experiments on eight multi-agent datasets demonstrate the robustness and effectiveness of the proposed methods. Notably, our findings reveal that allocating more inference resources to estimate influence scores, rather than Q-values, during data synthesis can more effectively and efficiently enhance model training.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00955"
    },
    "d6e36d1157819c499d4276bf95a792d0": {
        "title": "PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.00988",
        "abstract": "Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00988"
    },
    "d51ef18416d3cba8e2e4d488e3f63a01": {
        "title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.00989",
        "abstract": "Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00989"
    },
    "f59baeb0b866580954a53deab6e95535": {
        "title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models",
        "authors": [
            "Diyana Muhammed",
            "Gollam Rabby",
            "SÃ¶ren Auer"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01812",
        "abstract": "Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce SelfCheckAgent, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89% but reveals trade-offs in Factual with 30.58% and Ranking with 30.68%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent&#39;s applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01812"
    },
    "ac59740d2e4f33bd00b9ea23dc8b1137": {
        "title": "Adaptive Self-improvement LLM Agentic System for ML Library Development",
        "authors": [
            "Genghan Zhang",
            "Weixin Liang",
            "Olivia Hsu",
            "Kunle Olukotun"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.02534",
        "abstract": "ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02534"
    },
    "62c9e6521d85e0e094da3c4da01a0bfe": {
        "title": "CAMI: A Counselor Agent Supporting Motivational Interviewing through State Inference and Topic Exploration",
        "authors": [
            "Yizhe Yang",
            "Palakorn Achananuparp",
            "Heyan Huang",
            "Jing Jiang",
            "Kit Phey Leng",
            "Nicholas Gabriel Lim",
            "Cameron Tan Shi Ern",
            "Ee-peng Lim"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.02807",
        "abstract": "Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client&#39;s state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI&#39;s performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client&#39;s state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02807"
    },
    "979770a3febd45fbb5fa84d479b32601": {
        "title": "ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation",
        "authors": [
            "Qinzhuo Wu",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.02955",
        "abstract": "Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks. Furthermore, we propose ReachAgent, a two-stage framework that focuses on improving its task-completion abilities. It utilizes the page reaching and page operation subtasks, along with reward-based preference GUI flows, to further enhance the agent. Experimental results show that ReachAgent significantly improves the IoU Acc and Text Acc by 7.12% and 7.69% on the step-level and 4.72% and 4.63% on the task-level compared to the SOTA agent. Our data and code will be released upon acceptance.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02955"
    },
    "3d449aa86235533946b11e039c77566c": {
        "title": "Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model",
        "authors": [
            "Hadas Ben-Atya",
            "Naama Gavrielov",
            "Zvi Badash",
            "Gili Focht",
            "Ruth Cytter-Kuint",
            "Talar Hagopian",
            "Dan Turner",
            "Moti Freiman"
        ],
        "date": "2025/02/02",
        "pdf": "http://arxiv.org/pdf/2502.01691",
        "abstract": "Reliable extraction of structured data from radiology reports using Large Language Models (LLMs) remains challenging, especially for complex, non-English texts like Hebrew. This study introduces an agent-based uncertainty-aware approach to improve the trustworthiness of LLM predictions in medical applications. We analyzed 9,683 Hebrew radiology reports from Crohn&#39;s disease patients (from 2010 to 2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining reports were automatically annotated using HSMP-BERT. Structured data extraction was performed using Llama 3.1 (Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty. An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty and was compared against three entropy-based models. Performance was evaluated using accuracy, F1 score, precision, recall, and Cohen&#39;s Kappa before and after filtering high-uncertainty cases. The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen&#39;s Kappa of 0.3006. After filtering high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to 0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates. By incorporating uncertainty-aware prompt ensembles and an agent-based decision model, this approach enhances the performance and reliability of LLMs in structured data extraction from radiology reports, offering a more interpretable and trustworthy solution for high-stakes medical applications.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01691"
    },
    "3c3db16cc3a88ba4b8542eff8dc4ea14": {
        "title": "PsyPlay: Personality-Infused Role-Playing Conversational Agents",
        "authors": [
            "Tao Yang",
            "Yuhua Zhu",
            "Xiaojun Quan",
            "Cong Liu",
            "Qifan Wang"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.03821",
        "abstract": "The current research on Role-Playing Conversational Agents (RPCAs) with Large Language Models (LLMs) primarily focuses on imitating specific speaking styles and utilizing character backgrounds, neglecting the depiction of deeper personality traits.~In this study, we introduce personality-infused role-playing for LLM agents, which encourages agents to accurately portray their designated personality traits during dialogues. We then propose PsyPlay, a dialogue generation framework that facilitates the expression of rich personalities among multiple LLM agents. Specifically, PsyPlay enables agents to assume roles with distinct personality traits and engage in discussions centered around specific topics, consistently exhibiting their designated personality traits throughout the interactions. Validation on generated dialogue data demonstrates that PsyPlay can accurately portray the intended personality traits, achieving an overall success rate of 80.31% on GPT-3.5. Notably, we observe that LLMs aligned with positive values are more successful in portraying positive personality roles compared to negative ones. Moreover, we construct a dialogue corpus for personality-infused role-playing, called PsyPlay-Bench. The corpus, which consists of 4745 instances of correctly portrayed dialogues using PsyPlay, aims to further facilitate research in personalized role-playing and dialogue personality detection.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03821"
    },
    "f4ea715917df0171b1e32754f198332c": {
        "title": "Simulating the Emergence of Differential Case Marking with Communicating Neural-Network Agents",
        "authors": [
            "Yuchen Lian",
            "Arianna Bisazza",
            "Tessa Verhoef"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04038",
        "abstract": "Differential Case Marking (DCM) refers to the phenomenon where grammatical case marking is applied selectively based on semantic, pragmatic, or other factors. The emergence of DCM has been studied in artificial language learning experiments with human participants, which were specifically aimed at disentangling the effects of learning from those of communication (Smith &amp; Culbertson, 2020). Multi-agent reinforcement learning frameworks based on neural networks have gained significant interest to simulate the emergence of human-like linguistic phenomena. In this study, we employ such a framework in which agents first acquire an artificial language before engaging in communicative interactions, enabling direct comparisons to human result. Using a very generic communication optimization algorithm and neural-network learners that have no prior experience with language or semantic preferences, our results demonstrate that learning alone does not lead to DCM, but when agents communicate, differential use of markers arises. This supports Smith and Culbertson (2020)&#39;s findings that highlight the critical role of communication in shaping DCM and showcases the potential of neural-agent models to complement experimental research on language evolution.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04038"
    },
    "7882b7249c571f63c48cc7312a64c042": {
        "title": "ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference Optimization",
        "authors": [
            "Yinjie Wang",
            "Ling Yang",
            "Guohao Li",
            "Mengdi Wang",
            "Bryon Aragam"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04306",
        "abstract": "Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04306"
    },
    "0effcb668d9d61287d8d4a3d308cea17": {
        "title": "Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives",
        "authors": [
            "Elliot Meyerson",
            "Xin Qiu"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.04358",
        "abstract": "Decomposing hard problems into subproblems often makes them easier and more efficient to solve. With large language models (LLMs) crossing critical reliability thresholds for a growing slate of capabilities, there is an increasing effort to decompose systems into sets of LLM-based agents, each of whom can be delegated sub-tasks. However, this decomposition (even when automated) is often intuitive, e.g., based on how a human might assign roles to members of a human team. How close are these role decompositions to optimal? This position paper argues that asymptotic analysis with LLM primitives is needed to reason about the efficiency of such decomposed systems, and that insights from such analysis will unlock opportunities for scaling them. By treating the LLM forward pass as the atomic unit of computational cost, one can separate out the (often opaque) inner workings of a particular LLM from the inherent efficiency of how a set of LLMs are orchestrated to solve hard problems. In other words, if we want to scale the deployment of LLMs to the limit, instead of anthropomorphizing LLMs, asymptotic analysis with LLM primitives should be used to reason about and develop more powerful decompositions of large problems into LLM agents.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04358"
    },
    "cc8782f2d8d6834d10194088c6413f05": {
        "title": "Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents",
        "authors": [
            "Chenyang Shao",
            "Xinyuan Hu",
            "Yutang Lin",
            "Fengli Xu"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04392",
        "abstract": "The rapid expansion of web content has made on-device AI assistants indispensable for helping users manage the increasing complexity of online tasks. The emergent reasoning ability in large language models offer a promising path for next-generation on-device AI agents. However, deploying full-scale Large Language Models (LLMs) on resource-limited local devices is challenging. In this paper, we propose Division-of-Thoughts (DoT), a collaborative reasoning framework leveraging the synergy between locally deployed Smaller-scale Language Models (SLMs) and cloud-based LLMs. DoT leverages a Task Decomposer to elicit the inherent planning abilities in language models to decompose user queries into smaller sub-tasks, which allows hybrid language models to fully exploit their respective strengths. Besides, DoT employs a Task Scheduler to analyze the pair-wise dependency of sub-tasks and create a dependency graph, facilitating parallel reasoning of sub-tasks and the identification of key steps. To allocate the appropriate model based on the difficulty of sub-tasks, DoT leverages a Plug-and-Play Adapter, which is an additional task head attached to the SLM that does not alter the SLM&#39;s parameters. To boost adapter&#39;s task allocation capability, we propose a self-reinforced training method that relies solely on task execution feedback. Extensive experiments on various benchmarks demonstrate that our DoT significantly reduces LLM costs while maintaining competitive reasoning accuracy. Specifically, DoT reduces the average reasoning time and API costs by 66.12% and 83.57%, while achieving comparable reasoning accuracy with the best baseline methods.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04392"
    },
    "c2895095f35c0f46f199a820e04634db": {
        "title": "Multi-Agent Reinforcement Learning with Focal Diversity Optimization",
        "authors": [
            "Selim Furkan Tekin",
            "Fatih Ilhan",
            "Tiansheng Huang",
            "Sihao Hu",
            "Zachary Yahn",
            "Ling Liu"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04492",
        "abstract": "The advancement of Large Language Models (LLMs) and their finetuning strategies has triggered the renewed interests in multi-agent reinforcement learning. In this paper, we introduce a focal diversity-optimized multi-agent reinforcement learning approach, coined as MARL-Focal, with three unique characteristics. First, we develop an agent-fusion framework for encouraging multiple LLM based agents to collaborate in producing the final inference output for each LLM query. Second, we develop a focal-diversity optimized agent selection algorithm that can choose a small subset of the available agents based on how well they can complement one another to generate the query output. Finally, we design a conflict-resolution method to detect output inconsistency among multiple agents and produce our MARL-Focal output through reward-aware and policy-adaptive inference fusion. Extensive evaluations on five benchmarks show that MARL-Focal is cost-efficient and adversarial-robust. Our multi-agent fusion model achieves performance improvement of 5.51\\% compared to the best individual LLM-agent and offers stronger robustness over the TruthfulQA benchmark. Code is available at https://github.com/sftekin/rl-focal",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04492"
    },
    "d130907042a9d1c207814e4a43bf6fbc": {
        "title": "S$^2$-MAD: Breaking the Token Barrier to Enhance Multi-Agent Debate Efficiency",
        "authors": [
            "Yuting Zeng",
            "Weizhe Huang",
            "Lei Jiang",
            "Tongxuan Liu",
            "Xitai Jin",
            "Chen Tianying Tiana",
            "Jing Li",
            "Xiaohua Xu"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.04790",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing (NLP) scenarios, but they still face challenges when handling complex arithmetic and logical reasoning tasks. While Chain-Of-Thought (CoT) reasoning, self-consistency (SC) and self-correction strategies have attempted to guide models in sequential, multi-step reasoning, Multi-agent Debate (MAD) has emerged as a viable approach for enhancing the reasoning capabilities of LLMs. By increasing both the number of agents and the frequency of debates, the performance of LLMs improves significantly. However, this strategy results in a significant increase in token costs, presenting a barrier to scalability. To address this challenge, we introduce a novel sparsification strategy designed to reduce token costs within MAD. This approach minimizes ineffective exchanges of information and unproductive discussions among agents, thereby enhancing the overall efficiency of the debate process. We conduct comparative experiments on multiple datasets across various models, demonstrating that our approach significantly reduces the token costs in MAD to a considerable extent. Specifically, compared to MAD, our approach achieves an impressive reduction of up to 94.5\\% in token costs while maintaining performance degradation below 2.0\\%.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04790"
    },
    "9d031511e3eba748343236b6affe8743": {
        "title": "nvAgent: Automated Data Visualization from Natural Language via Collaborative Agent Workflow",
        "authors": [
            "Geliang Ouyang",
            "Jingyao Chen",
            "Zhihe Nie",
            "Yi Gui",
            "Yao Wan",
            "Hongyu Zhang",
            "Dongping Chen"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.05036",
        "abstract": "Natural Language to Visualization (NL2Vis) seeks to convert natural-language descriptions into visual representations of given tables, empowering users to derive insights from large-scale data. Recent advancements in Large Language Models (LLMs) show promise in automating code generation to transform tabular data into accessible visualizations. However, they often struggle with complex queries that require reasoning across multiple tables. To address this limitation, we propose a collaborative agent workflow, termed nvAgent, for NL2Vis. Specifically, nvAgent comprises three agents: a processor agent for database processing and context filtering, a composer agent for planning visualization generation, and a validator agent for code translation and output verification. Comprehensive evaluations on the new VisEval benchmark demonstrate that nvAgent consistently surpasses state-of-the-art baselines, achieving a 7.88% improvement in single-table and a 9.23% improvement in multi-table scenarios. Qualitative analyses further highlight that nvAgent maintains nearly a 20% performance margin over previous models, underscoring its capacity to produce high-quality visual representations from complex, heterogeneous data sources.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05036"
    },
    "f526e3a89c0265bf86d6eac5672e6b5f": {
        "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment",
        "authors": [
            "Yuxing Lu",
            "Jinzhuo Wang"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06472",
        "abstract": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06472"
    },
    "57a611637f85a3f9d2d547f5f0f012f7": {
        "title": "Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training",
        "authors": [
            "Yuchen Zhuang",
            "Jingfeng Yang",
            "Haoming Jiang",
            "Xin Liu",
            "Kewei Cheng",
            "Sanket Lokegaonkar",
            "Yifan Gao",
            "Qing Ping",
            "Tianyi Liu",
            "Binxuan Huang",
            "Zheng Li",
            "Zhengyang Wang",
            "Pei Chen",
            "Ruijie Wang",
            "Rongzhi Zhang",
            "Nasser Zalmout",
            "Priyanka Nigam",
            "Bing Yin",
            "Chao Zhang"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06589",
        "abstract": "Due to the scarcity of agent-oriented pre-training data, LLM-based autonomous agents typically rely on complex prompting or extensive fine-tuning, which often fails to introduce new capabilities while preserving strong generalizability. We introduce Hephaestus-Forge, the first large-scale pre-training corpus designed to enhance the fundamental capabilities of LLM agents in API function calling, intrinsic reasoning and planning, and adapting to environmental feedback. Hephaestus-Forge comprises 103B agent-specific data encompassing 76,537 APIs, including both tool documentation to introduce knowledge of API functions and function calling trajectories to strengthen intrinsic reasoning. To explore effective training protocols, we investigate scaling laws to identify the optimal recipe in data mixing ratios. By continual pre-training on Hephaestus-Forge, Hephaestus outperforms small- to medium-scale open-source LLMs and rivals commercial LLMs on three agent benchmarks, demonstrating the effectiveness of our pre-training corpus in enhancing fundamental agentic capabilities and generalization of LLMs to new tasks or environments.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06589"
    },
    "98494fac2a6a4669636e9b32fe0936f9": {
        "title": "Multi-Agent Simulator Drives Language Models for Legal Intensive Interaction",
        "authors": [
            "Shengbin Yue",
            "Ting Huang",
            "Zheng Jia",
            "Siyuan Wang",
            "Shujun Liu",
            "Yun Song",
            "Xuanjing Huang",
            "Zhongyu Wei"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.06882",
        "abstract": "Large Language Models (LLMs) have significantly advanced legal intelligence, but the scarcity of scenario data impedes the progress toward interactive legal scenarios. This paper introduces a Multi-agent Legal Simulation Driver (MASER) to scalably generate synthetic data by simulating interactive legal scenarios. Leveraging real-legal case sources, MASER ensures the consistency of legal attributes between participants and introduces a supervisory mechanism to align participants&#39; characters and behaviors as well as addressing distractions. A Multi-stage Interactive Legal Evaluation (MILE) benchmark is further constructed to evaluate LLMs&#39; performance in dynamic legal scenarios. Extensive experiments confirm the effectiveness of our framework.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06882"
    },
    "d5c21311f0587298d8b6e1d42dabab8e": {
        "title": "Don&#39;t Just Demo, Teach Me the Principles: A Principle-Based Multi-Agent Prompting Strategy for Text Classification",
        "authors": [
            "Peipei Wei",
            "Dimitris Dimitriadis",
            "Yan Xu",
            "Mingwei Shen"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07165",
        "abstract": "We present PRINCIPLE-BASED PROMPTING, a simple but effective multi-agent prompting strategy for text classification. It first asks multiple LLM agents to independently generate candidate principles based on analysis of demonstration samples with or without labels, consolidates them into final principles via a finalizer agent, and then sends them to a classifier agent to perform downstream classification tasks. Extensive experiments on binary and multi-class classification datasets with different sizes of LLMs show that our approach not only achieves substantial performance gains (1.55% - 19.37%) over zero-shot prompting on macro-F1 score but also outperforms other strong baselines (CoT and stepback prompting). Principles generated by our approach help LLMs perform better on classification tasks than human crafted principles on two private datasets. Our multi-agent PRINCIPLE-BASED PROMPTING approach also shows on-par or better performance compared to demonstration-based few-shot prompting approaches, yet with substantially lower inference costs. Ablation studies show that label information and the multi-agent cooperative LLM framework play an important role in generating high-quality principles to facilitate downstream classification tasks.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07165"
    },
    "230d0a37dd7aec9d7bf30af185d6d90a": {
        "title": "Multi-Agent Collaboration for Multilingual Code Instruction Tuning",
        "authors": [
            "Jian Yang",
            "Wei Zhang",
            "Jiaxi Yang",
            "Yibo Miao",
            "Shanghaoran Quan",
            "Zhenhe Wu",
            "Qiyao Peng",
            "Liqun Yang",
            "Tianyu Liu",
            "Zeyu Cui",
            "Binyuan Hui",
            "Junyang Lin"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07487",
        "abstract": "Recent advancement in code understanding and generation demonstrates that code LLMs fine-tuned on a high-quality instruction dataset can gain powerful capabilities to address wide-ranging code-related tasks. However, most previous existing methods mainly view each programming language in isolation and ignore the knowledge transfer among different programming languages. To bridge the gap among different programming languages, we introduce a novel multi-agent collaboration framework to enhance multilingual instruction tuning for code LLMs, where multiple language-specific intelligent agent components with generation memory work together to transfer knowledge from one language to another efficiently and effectively. Specifically, we first generate the language-specific instruction data from the code snippets and then provide the generated data as the seed data for language-specific agents. Multiple language-specific agents discuss and collaborate to formulate a new instruction and its corresponding solution (A new programming language or existing programming language), To further encourage the cross-lingual transfer, each agent stores its generation history as memory and then summarizes its merits and faults. Finally, the high-quality multilingual instruction data is used to encourage knowledge transfer among different programming languages to train Qwen2.5-xCoder. Experimental results on multilingual programming benchmarks demonstrate the superior performance of Qwen2.5-xCoder in sharing common knowledge, highlighting its potential to reduce the cross-lingual gap.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07487"
    },
    "a25ee4a3ab77c6faf8c4a3819ed5fc28": {
        "title": "Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation",
        "authors": [
            "Mahnaz Koupaee",
            "Jake W. Vincent",
            "Saab Mansour",
            "Igor Shalyminov",
            "Han He",
            "Hwanjun Song",
            "Raphael Shu",
            "Jianfeng He",
            "Yi Nian",
            "Amy Wing-mei Wong",
            "Kyu J. Han",
            "Hang Su"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08514",
        "abstract": "Faithfulness evaluators based on large language models (LLMs) are often fooled by the fluency of the text and struggle with identifying errors in the summaries. We propose an approach to summary faithfulness evaluation in which multiple LLM-based agents are assigned initial stances (regardless of what their belief might be) and forced to come up with a reason to justify the imposed belief, thus engaging in a multi-round debate to reach an agreement. The uniformly distributed initial assignments result in a greater diversity of stances leading to more meaningful debates and ultimately more errors identified. Furthermore, by analyzing the recent faithfulness evaluation datasets, we observe that naturally, it is not always the case for a summary to be either faithful to the source document or not. We therefore introduce a new dimension, ambiguity, and a detailed taxonomy to identify such special cases. Experiments demonstrate our approach can help identify ambiguities, and have even a stronger performance on non-ambiguous summaries.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08514"
    },
    "474c1ff5a3034e7a01037edcf62d0eb5": {
        "title": "SPeCtrum: A Grounded Framework for Multidimensional Identity Representation in LLM-Based Agent",
        "authors": [
            "Keyeun Lee",
            "Seo Hyeong Kim",
            "Seolhee Lee",
            "Jinsu Eun",
            "Yena Ko",
            "Hayeon Jeon",
            "Esther Hehsun Kim",
            "Seonghye Cho",
            "Soeun Yang",
            "Eun-mee Kim",
            "Hajin Lim"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08599",
        "abstract": "Existing methods for simulating individual identities often oversimplify human complexity, which may lead to incomplete or flattened representations. To address this, we introduce SPeCtrum, a grounded framework for constructing authentic LLM agent personas by incorporating an individual&#39;s multidimensional self-concept. SPeCtrum integrates three core components: Social Identity (S), Personal Identity (P), and Personal Life Context (C), each contributing distinct yet interconnected aspects of identity. To evaluate SPeCtrum&#39;s effectiveness in identity representation, we conducted automated and human evaluations. Automated evaluations using popular drama characters showed that Personal Life Context (C)-derived from short essays on preferences and daily routines-modeled characters&#39; identities more effectively than Social Identity (S) and Personal Identity (P) alone and performed comparably to the full SPC combination. In contrast, human evaluations involving real-world individuals found that the full SPC combination provided a more comprehensive self-concept representation than C alone. Our findings suggest that while C alone may suffice for basic identity simulation, integrating S, P, and C enhances the authenticity and accuracy of real-world identity representation. Overall, SPeCtrum offers a structured approach for simulating individuals in LLM agents, enabling more personalized human-AI interactions and improving the realism of simulation-based behavioral studies.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08599"
    },
    "777d7f5ea00e3b823ca61eb6914014a7": {
        "title": "If Multi-Agent Debate is the Answer, What is the Question?",
        "authors": [
            "Hangfan Zhang",
            "Zhiyao Cui",
            "Xinrun Wang",
            "Qiaosheng Zhang",
            "Zhen Wang",
            "Dinghao Wu",
            "Shuyue Hu"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08788",
        "abstract": "Multi-agent debate (MAD) has emerged as a promising approach to enhance the factual accuracy and reasoning quality of large language models (LLMs) by engaging multiple agents in iterative discussions during inference. Despite its potential, we argue that current MAD research suffers from critical shortcomings in evaluation practices, including limited dataset overlap and inconsistent baselines, raising significant concerns about generalizability. Correspondingly, this paper presents a systematic evaluation of five representative MAD methods across nine benchmarks using four foundational models. Surprisingly, our findings reveal that MAD methods fail to reliably outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming additional inference-time computation. From our analysis, we found that model heterogeneity can significantly improve MAD frameworks. We propose Heter-MAD enabling a single LLM agent to access the output from heterogeneous foundation models, which boosts the performance of current MAD frameworks. Finally, we outline potential directions for advancing MAD, aiming to spark a broader conversation and inspire future work in this area.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08788"
    },
    "a6e83c9ede12dbf90c97e86fa5d68e1c": {
        "title": "Agentic Verification for Ambiguous Query Disambiguation",
        "authors": [
            "Youngwon Lee",
            "Seung-won Hwang",
            "Ruofan Wu",
            "Feng Yan",
            "Danmei Xu",
            "Moutasem Akkad",
            "Zhewei Yao",
            "Yuxiong He"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.10352",
        "abstract": "In this work, we tackle the challenge of disambiguating queries in retrieval-augmented generation (RAG) to diverse yet answerable interpretations. State-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse interpretations are generated by an LLM, later used as search queries to retrieve supporting passages. Such a process may introduce noise in either interpretations or retrieval, particularly in enterprise settings, where LLMs -- trained on static data -- may struggle with domain-specific disambiguations. Thus, a post-hoc verification phase is introduced to prune noises. Our distinction is to unify diversification with verification by incorporating feedback from retriever and generator early on. This joint approach improves both efficiency and robustness by reducing reliance on multiple retrieval and inference steps, which are susceptible to cascading errors. We validate the efficiency and effectiveness of our method, Verified-Diversification with Consolidation (VERDICT), on the widely adopted ASQA benchmark to achieve diverse yet verifiable interpretations. Empirical results show that VERDICT improves grounding-aware F1 score by an average of 23% over the strongest baseline across different backbone LLMs.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10352"
    },
    "d800ee6acc686566a88098700f570602": {
        "title": "Divergent Thoughts toward One Goal: LLM-based Multi-Agent Collaboration System for Electronic Design Automation",
        "authors": [
            "Haoyuan Wu",
            "Haisheng Zheng",
            "Zhuolun He",
            "Bei Yu"
        ],
        "date": "2025/02/15",
        "pdf": "http://arxiv.org/pdf/2502.10857",
        "abstract": "Recently, with the development of tool-calling capabilities in large language models (LLMs), these models have demonstrated significant potential for automating electronic design automation (EDA) flows by interacting with EDA tool APIs via EDA scripts. However, considering the limited understanding of EDA tools, LLMs face challenges in practical scenarios where diverse interfaces of EDA tools exist across different platforms. Additionally, EDA flow automation often involves intricate, long-chain tool-calling processes, increasing the likelihood of errors in intermediate steps. Any errors will lead to the instability and failure of EDA flow automation. To address these challenges, we introduce EDAid, a multi-agent collaboration system where multiple agents harboring divergent thoughts converge towards a common goal, ensuring reliable and successful EDA flow automation. Specifically, each agent is controlled by ChipLlama models, which are expert LLMs fine-tuned for EDA flow automation. Our experiments demonstrate the state-of-the-art (SOTA) performance of our ChipLlama models and validate the effectiveness of our EDAid in the automation of complex EDA flows, showcasing superior performance compared to single-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10857"
    },
    "051910306e3634c99f24831585d5497b": {
        "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?",
        "authors": [
            "Wenxuan Wang",
            "Zizhan Ma",
            "Zheng Wang",
            "Chenghan Wu",
            "Wenting Chen",
            "Xiang Li",
            "Yixuan Yuan"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.11211",
        "abstract": "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents&#39; performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11211"
    },
    "3e7779e7fe5866d84eb6ed7de18b487b": {
        "title": "&#34;Nuclear Deployed!&#34;: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents",
        "authors": [
            "Rongwu Xu",
            "Xiaojian Li",
            "Shuo Chen",
            "Wei Xu"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11355",
        "abstract": "Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent&#39;s Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11355"
    },
    "8fee1a3c3a526591ec860fe41e53fe01": {
        "title": "LLM Agents Making Agent Tools",
        "authors": [
            "Georg WÃ¶lflein",
            "Dyke Ferber",
            "Daniel Truhn",
            "Ognjen ArandjeloviÄ",
            "Jakob Nikolas Kather"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11705",
        "abstract": "Tool use has turned large language models (LLMs) into powerful agents that can perform complex multi-step tasks by dynamically utilising external software components. However, these tools must be implemented in advance by human developers, hindering the applicability of LLM agents in domains which demand large numbers of highly specialised tools, like in life sciences and medicine. Motivated by the growing trend of scientific studies accompanied by public code repositories, we propose ToolMaker, a novel agentic framework that autonomously transforms papers with code into LLM-compatible tools. Given a short task description and a repository URL, ToolMaker autonomously installs required dependencies and generates code to perform the task, using a closed-loop self-correction mechanism to iteratively diagnose and rectify errors. To evaluate our approach, we introduce a benchmark comprising 15 diverse and complex computational tasks spanning both medical and non-medical domains with over 100 unit tests to objectively assess tool correctness and robustness. ToolMaker correctly implements 80% of the tasks, substantially outperforming current state-of-the-art software engineering agents. ToolMaker therefore is a step towards fully autonomous agent-based scientific workflows.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11705"
    },
    "4126585e4c5333fe61494856b3e6e47f": {
        "title": "Can LLM Agents Maintain a Persona in Discourse?",
        "authors": [
            "Pranav Bhandari",
            "Nicolas Fay",
            "Michael Wise",
            "Amitava Datta",
            "Stephanie Meek",
            "Usman Naseem",
            "Mehwish Nasim"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11843",
        "abstract": "Large Language Models (LLMs) are widely used as conversational agents, exploiting their capabilities in various sectors such as education, law, medicine, and more. However, LLMs are often subjected to context-shifting behaviour, resulting in a lack of consistent and interpretable personality-aligned interactions. Adherence to psychological traits lacks comprehensive analysis, especially in the case of dyadic (pairwise) conversations. We examine this challenge from two viewpoints, initially using two conversation agents to generate a discourse on a certain topic with an assigned personality from the OCEAN framework (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) as High/Low for each trait. This is followed by using multiple judge agents to infer the original traits assigned to explore prediction consistency, inter-model agreement, and alignment with the assigned personality. Our findings indicate that while LLMs can be guided toward personality-driven dialogue, their ability to maintain personality traits varies significantly depending on the combination of models and discourse settings. These inconsistencies emphasise the challenges in achieving stable and interpretable personality-aligned interactions in LLMs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11843"
    },
    "b217ee7110f1139e7d6d3e6f8f401f96": {
        "title": "A-MEM: Agentic Memory for LLM Agents",
        "authors": [
            "Wujiang Xu",
            "Zujie Liang",
            "Kai Mei",
            "Hang Gao",
            "Juntao Tan",
            "Yongfeng Zhang"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12110",
        "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems&#39; fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution - as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The source code is available at https://github.com/WujiangXu/AgenticMemory.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12110"
    },
    "4628e63125c3c42557cd811300aa9a85": {
        "title": "InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context",
        "authors": [
            "Bryan L. M. de Oliveira",
            "Luana G. B. Martins",
            "Bruno BrandÃ£o",
            "Luckeciano C. Melo"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12257",
        "abstract": "While large language models excel at following explicit instructions, they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses rather than seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. The benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue through clarifying questions before providing appropriate responses. Our evaluation of both open and closed-source models reveals that while proprietary models generally perform better, all current assistants struggle with effectively gathering critical information, often requiring multiple turns to infer user intent and frequently defaulting to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models&#39; information-seeking capabilities, offering insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12257"
    },
    "62a231ee39598886fa9227024afd2f0b": {
        "title": "LM Agents for Coordinating Multi-User Information Gathering",
        "authors": [
            "Harsh Jhamtani",
            "Jacob Andreas",
            "Benjamin Van Durme"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12328",
        "abstract": "This paper introduces PeopleJoin, a benchmark for evaluating LM-mediated collaborative problem solving. Given a user request, PeopleJoin agents must identify teammates who might be able to assist, converse with these teammates to gather information, and finally compile a useful answer or summary for the original user. PeopleJoin comprises two evaluation domains: PeopleJoin-QA, focused on questions about tabular data, and PeopleJoin-DocCreation, focused on document creation tasks. The two domains are adapted from existing NLP benchmarks for database question answering and multi-document summarization; here, however, the information needed to complete these tasks is distributed across synthetic ``organizations&#39;&#39; of 2--20 users, simulating natural multi-user collaboration scenarios. We implemented several popular LM agent architectures, evaluating their accuracy and efficiency at completing tasks, and highlight new research questions that can be studied using PeopleJoin.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12328"
    },
    "5ebddc73abd7382da524ae84d744a569": {
        "title": "One Size doesn&#39;t Fit All: A Personalized Conversational Tutoring Agent for Mathematics Instruction",
        "authors": [
            "Ben Liu",
            "Jihan Zhang",
            "Fangquan Lin",
            "Xu Jia",
            "Min Peng"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12633",
        "abstract": "Large language models (LLMs) have been increasingly employed in various intelligent educational systems, simulating human tutors to facilitate effective human-machine interaction. However, previous studies often overlook the significance of recognizing and adapting to individual learner characteristics. Such adaptation is crucial for enhancing student engagement and learning efficiency, particularly in mathematics instruction, where diverse learning styles require personalized strategies to promote comprehension and enthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized \\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics instruction. PACE simulates students&#39; learning styles based on the Felder and Silverman learning style model, aligning with each student&#39;s persona. In this way, our PACE can effectively assess the personality of students, allowing to develop individualized teaching strategies that resonate with their unique learning styles. To further enhance students&#39; comprehension, PACE employs the Socratic teaching method to provide instant feedback and encourage deep thinking. By constructing personalized teaching data and training models, PACE demonstrates the ability to identify and adapt to the unique needs of each student, significantly improving the overall learning experience and outcomes. Moreover, we establish multi-aspect evaluation criteria and conduct extensive analysis to assess the performance of personalized teaching. Experimental results demonstrate the superiority of our model in personalizing the educational experience and motivating students compared to existing methods.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12633"
    },
    "5dce6608d2cc0f603531908cd183e0b6": {
        "title": "R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs",
        "authors": [
            "Sumin Jo",
            "Junseong Choi",
            "Jiho Kim",
            "Edward Choi"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12767",
        "abstract": "Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks are often rigid, struggling to adapt to KG or task changes. They also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning. To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across multiple KG-based reasoning tasks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability while reducing inference cost. However, it also leads to a higher abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning. It reduces reliance on high-capacity LLMs while ensuring trustworthy inference.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12767"
    },
    "aeb3f654e791e27e76d124ba503913cd": {
        "title": "An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation",
        "authors": [
            "Mohammad Feli",
            "Iman Azimi",
            "Pasi Liljeberg",
            "Amir M. Rahmani"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12836",
        "abstract": "Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs&#39; limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent&#39;s performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12836"
    },
    "8009cf67dd8c4184850e002591f18b70": {
        "title": "SEFL: Harnessing Large Language Model Agents to Improve Educational Feedback Systems",
        "authors": [
            "Mike Zhang",
            "Amalie Pernille Dilling",
            "LÃ©on Gondelman",
            "Niels Erik Ruan Lyngdorf",
            "Euan D. Lindsay",
            "Johannes Bjerva"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12927",
        "abstract": "Providing high-quality feedback is crucial for student success but is constrained by time, cost, and limited data availability. We introduce Synthetic Educational Feedback Loops (SEFL), a novel framework designed to deliver immediate, on-demand feedback at scale without relying on extensive, real-world student data. In SEFL, two large language models (LLMs) operate in teacher--student roles to simulate assignment completion and formative feedback, generating abundant synthetic pairs of student work and corresponding critiques. We then fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher--&gt;student feedback loop for diverse assignments. Through both LLM-as-a-judge and human evaluations, we demonstrate that SEFL-tuned models outperform their non-tuned counterparts in feedback quality, clarity, and timeliness. These findings reveal SEFL&#39;s potential to transform feedback processes for higher education and beyond, offering an ethical and scalable alternative to conventional manual feedback cycles.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12927"
    },
    "04b64a4d266e9bdf14c25aee46d25a8e": {
        "title": "AEIA-MN: Evaluating the Robustness of Multimodal LLM-Powered Mobile Agents Against Active Environmental Injection Attacks",
        "authors": [
            "Yurun Chen",
            "Xueyu Hu",
            "Keting Yin",
            "Juncheng Li",
            "Shengyu Zhang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13053",
        "abstract": "As researchers continuously optimize AI agents to perform tasks more effectively within operating systems, they often neglect to address the critical need for enabling these agents to identify &#34;impostors&#34; within the system. Through an analysis of the agents&#39; operating environment, we identified a potential threat: attackers can disguise their attack methods as environmental elements, injecting active disturbances into the agents&#39; execution process, thereby disrupting their decision-making. We define this type of attack as Active Environment Injection Attack (AEIA). Based on this, we propose AEIA-MN, an active environment injection attack scheme that exploits interaction vulnerabilities in the mobile operating system to evaluate the robustness of MLLM-based agents against such threats. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% in the AndroidWorld benchmark.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13053"
    },
    "da2942b9b05b8b3b487fb9190fb379a6": {
        "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors",
        "authors": [
            "Jian Wang",
            "Yinpei Dai",
            "Yichi Zhang",
            "Ziqiao Ma",
            "Wenjie Li",
            "Joyce Chai"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13311",
        "abstract": "Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized guidance in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students toward completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student&#39;s knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents holistically using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our results and findings can be extended beyond coding, providing valuable insights into advancing tutoring agents for a variety of tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ],
            [
                "Application",
                "Software Engineering"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13311"
    },
    "67b2128ecda1f7a56e0289cbd4509b50": {
        "title": "DataSciBench: An LLM Agent Benchmark for Data Science",
        "authors": [
            "Dan Zhang",
            "Sining Zhoubian",
            "Min Cai",
            "Fengzu Li",
            "Lekang Yang",
            "Wei Wang",
            "Tianjiao Dong",
            "Ziniu Hu",
            "Jie Tang",
            "Yisong Yue"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13897",
        "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13897"
    },
    "8ff3660d187b851f21a4873ff43ab836": {
        "title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Xiao Wang",
            "Yin Fang",
            "Haolin Liu",
            "Yifan Yang",
            "Fangyuan Chen",
            "Zhixing Song",
            "Dengyu Wang",
            "Minjia Zhang",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13957",
        "abstract": "Retrieval-augmented generation (RAG) has shown great potential for knowledge-intensive tasks, but its traditional architectures rely on static retrieval, limiting their effectiveness for complex questions that require sequential information-seeking. While agentic reasoning and search offer a more adaptive approach, most existing methods depend heavily on prompt engineering. In this work, we introduce RAG-Gym, a unified optimization framework that enhances information-seeking agents through fine-grained process supervision at each search step. We also propose ReSearch, a novel agent architecture that synergizes answer reasoning and search query generation within the RAG-Gym framework. Experiments on four challenging datasets show that RAG-Gym improves performance by up to 25.6\\% across various agent architectures, with ReSearch consistently outperforming existing baselines. Further analysis highlights the effectiveness of advanced LLMs as process reward judges and the transferability of trained reward models as verifiers for different LLMs. Additionally, we examine the scaling properties of training and inference in agentic RAG. The project homepage is available at https://rag-gym.github.io/.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13957"
    },
    "a7e1c6ae709b2d76c85e0c521a2b1fbc": {
        "title": "LIDDIA: Language-based Intelligent Drug Discovery Agent",
        "authors": [
            "Reza Averly",
            "Frazier N. Baker",
            "Xia Ning"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13959",
        "abstract": "Drug discovery is a long, expensive, and complex process, relying heavily on human medicinal chemists, who can spend years searching the vast space of potential therapies. Recent advances in artificial intelligence for chemistry have sought to expedite individual drug discovery tasks; however, there remains a critical need for an intelligent agent that can navigate the drug discovery process. Towards this end, we introduce LIDDiA, an autonomous agent capable of intelligently navigating the drug discovery process in silico. By leveraging the reasoning capabilities of large language models, LIDDiA serves as a low-cost and highly-adaptable tool for autonomous drug discovery. We comprehensively examine LIDDiA, demonstrating that (1) it can generate molecules meeting key pharmaceutical criteria on over 70% of 30 clinically relevant targets, (2) it intelligently balances exploration and exploitation in the chemical space, and (3) it can identify promising novel drug candidates on EGFR, a critical target for cancers.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13959"
    },
    "386e8830f028c8f68c61db0e4c811df3": {
        "title": "UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text",
        "authors": [
            "Primoz Kocbek",
            "Leon Kopitar",
            "Zhihong Zhang",
            "Emirhan Aydin",
            "Maxim Topaz",
            "Gregor Stiglic"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.14144",
        "abstract": "This paper describes our submissions to the TREC 2024 PLABA track with the aim to simplify biomedical abstracts for a K8-level audience (13-14 years old students). We tested three approaches using OpenAI&#39;s gpt-4o and gpt-4o-mini models: baseline prompt engineering, a two-AI agent approach, and fine-tuning. Adaptations were evaluated using qualitative metrics (5-point Likert scales for simplicity, accuracy, completeness, and brevity) and quantitative readability scores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the two-agent approach and baseline prompt engineering with gpt-4o-mini models show superior qualitative performance, while fine-tuned models excelled in accuracy and completeness but were less simple. The evaluation results demonstrated that prompt engineering with gpt-4o-mini outperforms iterative improvement strategies via two-agent approach as well as fine-tuning with gpt-4o. We intend to expand our investigation of the results and explore advanced evaluations.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14144"
    },
    "1d7f1c1a670d5ef09e524fa614cd496f": {
        "title": "Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction",
        "authors": [
            "Mohammadmahdi Jafari",
            "Devin Yuncheng Hua",
            "Hao Xue",
            "Flora Salim"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14171",
        "abstract": "Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14171"
    },
    "f3bf6c6ce4a547bfaec4ce33b4d88476": {
        "title": "Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization",
        "authors": [
            "Zhitao He",
            "Zijun Liu",
            "Peng Li",
            "May Fung",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Yang Liu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14496",
        "abstract": "LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents&#39; policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14496"
    },
    "52cdf9bddd3a749978a92ea7a6021c18": {
        "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
        "authors": [
            "Deepak Nathani",
            "Lovish Madaan",
            "Nicholas Roberts",
            "Nikolay Bashlykov",
            "Ajay Menon",
            "Vincent Moens",
            "Amar Budhiraja",
            "Despoina Magka",
            "Vladislav Vorotilov",
            "Gaurav Chaurasia",
            "Dieuwke Hupkes",
            "Ricardo Silveira Cabral",
            "Tatiana Shavrina",
            "Jakob Foerster",
            "Yoram Bachrach",
            "William Yang Wang",
            "Roberta Raileanu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14499",
        "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14499"
    },
    "51c734eb714ade9075f7a1d98fea4b33": {
        "title": "CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models",
        "authors": [
            "Zhenhong Zhou",
            "Zherui Li",
            "Jie Zhang",
            "Yuanhe Zhang",
            "Kun Wang",
            "Yang Liu",
            "Qing Guo"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14529",
        "abstract": "Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14529"
    },
    "d716b75b322213af61a1b675038385ab": {
        "title": "InstructAgent: Building User Controllable Recommender via LLM Agent",
        "authors": [
            "Wujiang Xu",
            "Yunxiao Shi",
            "Zujie Liang",
            "Xuying Ning",
            "Kai Mei",
            "Kun Wang",
            "Xi Zhu",
            "Min Xu",
            "Yongfeng Zhang"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14662",
        "abstract": "Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform&#39;s recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform&#39;s benefits, which may hinder their ability to protect and capture users&#39; true interests. Second, these models are typically optimized using data from all users, which may overlook individual user&#39;s preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as $\\dataset$, along with user instructions for each record.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14662"
    },
    "bdba08a4f13bc86187c524d06a4de449": {
        "title": "I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search",
        "authors": [
            "Zujie Liang",
            "Feng Wei",
            "Wujiang Xu",
            "Lin Chen",
            "Yuxi Qian",
            "Xinhui Wu"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14693",
        "abstract": "Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process.Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node&#39;s solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier.Applied to the various ML tasks, our approach demonstrates a6\\% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14693"
    },
    "5c7caf97e434d7b064871a5b80ed88a0": {
        "title": "ALU: Agentic LLM Unlearning",
        "authors": [
            "Debdeep Sanyal",
            "Murari Mandal"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00406",
        "abstract": "Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. We present the first agentic LLM unlearning (ALU) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our ALU framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and ALU seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that ALU consistently stands out as the most robust LLM unlearning framework among current state-of-the-art methods while incurring a low constant-time cost. We further highlight ALU&#39;s superior performance compared to existing methods when evaluated at scale. Specifically, ALU is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00406"
    },
    "1d0189ce757508161729909a2c110e35": {
        "title": "MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents",
        "authors": [
            "George Fatouros",
            "Kostas Metaxas",
            "John Soldatos",
            "Manos Karathanassis"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00415",
        "abstract": "MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\\&amp;P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\\&amp;P 500 stocks during 2024 demonstrates the framework&#39;s scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00415"
    },
    "1d1ac354ff3472f17033899e0a1ca010": {
        "title": "Who&#39;s the MVP? A Game-Theoretic Evaluation Benchmark for Modular Attribution in LLM Agents",
        "authors": [
            "Yingxuan Yang",
            "Bo Huang",
            "Siyuan Qi",
            "Chao Feng",
            "Haoyi Hu",
            "Yuxuan Zhu",
            "Jinbo Hu",
            "Haoran Zhao",
            "Ziyi He",
            "Xiao Liu",
            "Zongyu Wang",
            "Lin Qiu",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "date": "2025/02/01",
        "pdf": "http://arxiv.org/pdf/2502.00510",
        "abstract": "Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce CapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in cooperative game theory&#39;s Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent&#39;s architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. Key contributions include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,500 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Game Playing"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.00510"
    },
    "423438288d35797d3481f1c75c359e77": {
        "title": "Eliciting Language Model Behaviors with Investigator Agents",
        "authors": [
            "Xiang Lisa Li",
            "Neil Chowdhury",
            "Daniel D. Johnson",
            "Tatsunori Hashimoto",
            "Percy Liang",
            "Sarah Schwettmann",
            "Jacob Steinhardt"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01236",
        "abstract": "Language models exhibit complex, diverse behaviors when prompted with free-form text, making it difficult to characterize the space of possible outputs. We study the problem of behavior elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations or harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train investigator models to map randomly-chosen target behaviors to a diverse distribution of outputs that elicit them, similar to amortized Bayesian inference. We do this through supervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe training objective to iteratively discover diverse prompting strategies. Our investigator models surface a variety of effective and human-interpretable prompts leading to jailbreaks, hallucinations, and open-ended aberrant behaviors, obtaining a 100% attack success rate on a subset of AdvBench (Harmful Behaviors) and an 85% hallucination rate.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01236"
    },
    "ca182eb5c94457e3d3864caf8ce295e7": {
        "title": "Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant",
        "authors": [
            "Gaole He",
            "Gianluca Demartini",
            "Ujwal Gadiraju"
        ],
        "date": "2025/02/03",
        "pdf": "http://arxiv.org/pdf/2502.01390",
        "abstract": "Since the explosion in popularity of ChatGPT, large language models (LLMs) have continued to impact our everyday lives. Equipped with external tools that are designed for a specific purpose (e.g., for flight booking or an alarm clock), LLM agents exercise an increasing capability to assist humans in their daily work. Although LLM agents have shown a promising blueprint as daily assistants, there is a limited understanding of how they can provide daily assistance based on planning and sequential decision making capabilities. We draw inspiration from recent work that has highlighted the value of &#39;LLM-modulo&#39; setups in conjunction with humans-in-the-loop for planning tasks. We conducted an empirical study (N = 248) of LLM agents as daily assistants in six commonly occurring tasks with different levels of risk typically associated with them (e.g., flight ticket booking and credit card payments). To ensure user agency and control over the LLM agent, we adopted LLM agents in a plan-then-execute manner, wherein the agents conducted step-wise planning and step-by-step execution in a simulation environment. We analyzed how user involvement at each stage affects their trust and collaborative team performance. Our findings demonstrate that LLM agents can be a double-edged sword -- (1) they can work well when a high-quality plan and necessary user involvement in execution are available, and (2) users can easily mistrust the LLM agents with plans that seem plausible. We synthesized key insights for using LLM agents as daily assistants to calibrate user trust and achieve better overall task outcomes. Our work has important implications for the future design of daily assistants and human-AI collaboration with LLM agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.01390"
    },
    "c2e5f782a453b942ea940d54760e2a21": {
        "title": "Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies",
        "authors": [
            "Han Zhou",
            "Xingchen Wan",
            "Ruoxi Sun",
            "Hamid Palangi",
            "Shariq Iqbal",
            "Ivan VuliÄ",
            "Anna Korhonen",
            "Sercan Ã. ArÄ±k"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.02533",
        "abstract": "Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design. Based on the insights, we propose Multi-Agent System Search (MASS), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that MASS-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the MASS-found systems, we finally propose design principles behind building effective multi-agent systems.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.02533"
    },
    "3d2598771a524886d8dc2e962511838c": {
        "title": "SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex Reasoning over Knowledge Graphs",
        "authors": [
            "Ben Liu",
            "Jihai Zhang",
            "Fangquan Lin",
            "Cheng Yang",
            "Min Peng",
            "Wotao Yin"
        ],
        "date": "2025/02/05",
        "pdf": "http://arxiv.org/pdf/2502.03283",
        "abstract": "Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM&#39;s inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03283"
    },
    "fdd09538eac934057c78a0fd7a49744e": {
        "title": "Enhancing Online Learning Efficiency Through Heterogeneous Resource Integration with a Multi-Agent RAG System",
        "authors": [
            "Devansh Srivastav",
            "Hasan Md Tusfiqur Alam",
            "Afsaneh Asaei",
            "Mahmoud Fazeli",
            "Tanisha Sharma",
            "Daniel Sonntag"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.03948",
        "abstract": "Efficient online learning requires seamless access to diverse resources such as videos, code repositories, documentation, and general web content. This poster paper introduces early-stage work on a Multi-Agent Retrieval-Augmented Generation (RAG) System designed to enhance learning efficiency by integrating these heterogeneous resources. Using specialized agents tailored for specific resource types (e.g., YouTube tutorials, GitHub repositories, documentation websites, and search engines), the system automates the retrieval and synthesis of relevant information. By streamlining the process of finding and combining knowledge, this approach reduces manual effort and enhances the learning experience. A preliminary user study confirmed the system&#39;s strong usability and moderate-high utility, demonstrating its potential to improve the efficiency of knowledge acquisition.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.03948"
    },
    "abe134e8f38cffcbf25cd687a6a0ff5c": {
        "title": "Multi-agent Architecture Search via Agentic Supernet",
        "authors": [
            "Guibin Zhang",
            "Luyang Niu",
            "Junfeng Fang",
            "Kun Wang",
            "Lei Bai",
            "Xiang Wang"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.04180",
        "abstract": "Large Language Model (LLM)-empowered multi-agent systems extend the cognitive boundaries of individual agents through disciplined collaboration and interaction, while constructing these systems often requires labor-intensive manual designs. Despite the availability of methods to automate the design of agentic workflows, they typically seek to identify a static, complex, one-size-fits-all system, which, however, fails to dynamically allocate inference resources based on the difficulty and domain of each query. To address this challenge, we shift away from the pursuit of a monolithic agentic system, instead optimizing the \\textbf{agentic supernet}, a probabilistic and continuous distribution of agentic architectures. We introduce MaAS, an automated framework that samples query-dependent agentic systems from the supernet, delivering high-quality solutions and tailored resource allocation (\\textit{e.g.}, LLM calls, tool calls, token cost). Comprehensive evaluation across six benchmarks demonstrates that MaAS \\textbf{(I)} requires only $6\\sim45\\%$ of the inference costs of existing handcrafted or automated multi-agent systems, \\textbf{(II)} surpasses them by $0.54\\%\\sim11.82\\%$, and \\textbf{(III)} enjoys superior cross-dataset and cross-LLM-backbone transferability.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04180"
    },
    "7b5586a1f6eadb8c20bc07706cd4df4a": {
        "title": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research",
        "authors": [
            "Junde Wu",
            "Jiayuan Zhu",
            "Yuyuan Liu"
        ],
        "date": "2025/02/07",
        "pdf": "http://arxiv.org/pdf/2502.04644",
        "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Unlike conventional LLM-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation (RAG) systems and closed-source LLMs. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving. The code is at: https://github.com/theworldofagents/Agentic-Reasoning.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.04644"
    },
    "6da1ea26c443026424681e04a6652c0f": {
        "title": "Robotouille: An Asynchronous Planning Benchmark for LLM Agents",
        "authors": [
            "Gonzalo Gonzalez-Pumariega",
            "Leong Su Yean",
            "Neha Sunkara",
            "Sanjiban Choudhury"
        ],
        "date": "2025/02/06",
        "pdf": "http://arxiv.org/pdf/2502.05227",
        "abstract": "Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce Robotouille, a challenging benchmark environment designed to test LLM agents&#39; ability to handle long-horizon asynchronous scenarios. Our synchronous and asynchronous datasets capture increasingly complex planning challenges that go beyond existing benchmarks, requiring agents to manage overlapping tasks and interruptions. Our results show that ReAct (gpt4-o) achieves 47% on synchronous tasks but only 11% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available at https://github.com/portal-cornell/robotouille.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05227"
    },
    "6705ef475ab018d54e7b0a56a51daf0a": {
        "title": "Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews",
        "authors": [
            "Izunna Okpala",
            "Ashkan Golgoon",
            "Arjun Ravi Kannan"
        ],
        "date": "2025/02/08",
        "pdf": "http://arxiv.org/pdf/2502.05439",
        "abstract": "The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a manager and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection, hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a manager along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05439"
    },
    "71f0307a5b3c4c2ec46c52e57025f0c3": {
        "title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents",
        "authors": [
            "Jiabin Tang",
            "Tianyu Fan",
            "Chao Huang"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.05957",
        "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent&#39;s effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent&#39;s Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.05957"
    },
    "58977408842677b889fa9d2e02d0b7b9": {
        "title": "Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning",
        "authors": [
            "Bidipta Sarkar",
            "Warren Xia",
            "C. Karen Liu",
            "Dorsa Sadigh"
        ],
        "date": "2025/02/09",
        "pdf": "http://arxiv.org/pdf/2502.06060",
        "abstract": "Communicating in natural language is a powerful tool in multi-agent settings, as it enables independent agents to share information in partially observable settings and allows zero-shot coordination with humans. However, most prior works are limited as they either rely on training with large amounts of human demonstrations or lack the ability to generate natural and useful communication strategies. In this work, we train language models to have productive discussions about their environment in natural language without any human demonstrations. We decompose the communication problem into listening and speaking. Our key idea is to leverage the agent&#39;s goal to predict useful information about the world as a dense reward signal that guides communication. Specifically, we improve a model&#39;s listening skills by training them to predict information about the environment based on discussions, and we simultaneously improve a model&#39;s speaking skills with multi-agent reinforcement learning by rewarding messages based on their influence on other agents. To investigate the role and necessity of communication in complex social settings, we study an embodied social deduction game based on Among Us, where the key question to answer is the identity of an adversarial imposter. We analyze emergent behaviors due to our technique, such as accusing suspects and providing evidence, and find that it enables strong discussions, doubling the win rates compared to standard RL. We release our code and models at https://socialdeductionllm.github.io/",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06060"
    },
    "315df09f94bc364e0f21a8c65af0071b": {
        "title": "SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering",
        "authors": [
            "Xuehang Guo",
            "Xingyao Wang",
            "Yangyi Chen",
            "Sha Li",
            "Chi Han",
            "Manling Li",
            "Heng Ji"
        ],
        "date": "2025/02/10",
        "pdf": "http://arxiv.org/pdf/2502.06994",
        "abstract": "Software engineering (SE) is increasingly collaborative, with developers working together on shared complex codebases. Effective collaboration in shared environments requires participants -- whether humans or AI agents -- to stay on the same page as their environment evolves. When a collaborator&#39;s understanding diverges from the current state -- what we term the out-of-sync challenge -- the collaborator&#39;s actions may fail, leading to integration issues. In this work, we introduce SyncMind, a framework that systematically defines the out-of-sync problem faced by large language model (LLM) agents in collaborative software engineering (CSE). Based on SyncMind, we create SyncBench, a benchmark featuring 24,332 instances of agent out-of-sync scenarios in real-world CSE derived from 21 popular GitHub repositories with executable verification tests. Experiments on SyncBench uncover critical insights into existing LLM agents&#39; capabilities and limitations. Besides substantial performance gaps among agents (from Llama-3.1 agent &lt;= 3.33% to Claude-3.5-Sonnet &gt;= 28.18%), their consistently low collaboration willingness (&lt;= 4.86%) suggests fundamental limitations of existing LLM in CSE. However, when collaboration occurs, it positively correlates with out-of-sync recovery success. Minimal performance differences in agents&#39; resource-aware out-of-sync recoveries further reveal their significant lack of resource awareness and adaptability, shedding light on future resource-efficient collaborative systems. Code and data are openly available on our project website: https://xhguo7.github.io/SyncMind/.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.06994"
    },
    "9aad7e351bc7be72d4e16483798e2e0f": {
        "title": "EvoFlow: Evolving Diverse Agentic Workflows On The Fly",
        "authors": [
            "Guibin Zhang",
            "Kaijie Chen",
            "Guancheng Wan",
            "Heng Chang",
            "Hong Cheng",
            "Kun Wang",
            "Shuyue Hu",
            "Lei Bai"
        ],
        "date": "2025/02/11",
        "pdf": "http://arxiv.org/pdf/2502.07373",
        "abstract": "The past two years have witnessed the evolution of large language model (LLM)-based multi-agent systems from labor-intensive manual design to partial automation (\\textit{e.g.}, prompt engineering, communication topology) and eventually to fully automated design. However, existing agentic automation pipelines often lack LLM heterogeneity and focus on single-objective performance optimization, limiting their potential to combine weaker models for more customized and cost-effective solutions. To address this challenge, we propose EvoFlow, a niching evolutionary algorithm-based framework to automatically search a population of heterogeneous and complexity-adaptive agentic workflows, rather than a single homogeneous, complex workflow. Technically, EvoFlow performs \\textit{(1) tag-based retrieval} to extract parent workflows from an agentic population, evolves new workflows through \\textit{(2) crossover} and \\textit{(3) mutation}, and employs \\textit{(4) niching-based selection} to maintain population diversity and quality. Extensive evaluations across seven benchmarks demonstrate that EvoFlow is: \\textbf{(I) diverse}, evolving a population of workflows ranging from simple I/O tasks to complex multi-turn interactions; \\textbf{(II) high-performing}, outperforming previous handcrafted and automated workflows by $1.23\\%\\sim29.86\\%$; \\textbf{(III) economical}, surpassing powerful \\llmname{o1-preview} at $12.4\\%$ of its inference cost using weaker open-source models.",
        "code": "",
        "category": [
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.07373"
    },
    "f3089465cbbb57b6f59153af036fecb1": {
        "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model",
        "authors": [
            "Emre Can Acikgoz",
            "Jeremiah Greer",
            "Akul Datta",
            "Ze Yang",
            "William Zeng",
            "Oussama Elachqar",
            "Emmanouil Koukoumidis",
            "Dilek Hakkani-TÃ¼r",
            "Gokhan Tur"
        ],
        "date": "2025/02/12",
        "pdf": "http://arxiv.org/pdf/2502.08820",
        "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08820"
    },
    "1b26f138e797bc52615ba6d476544dfb": {
        "title": "PathFinder: A Multi-Modal Multi-Agent System for Medical Diagnostic Decision-Making Applied to Histopathology",
        "authors": [
            "Fatemeh Ghezloo",
            "Mehmet Saygin Seyfioglu",
            "Rustin Soraki",
            "Wisdom O. Ikezogwo",
            "Beibin Li",
            "Tejoram Vivekanandan",
            "Joann G. Elmore",
            "Ranjay Krishna",
            "Linda Shapiro"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.08916",
        "abstract": "Diagnosing diseases through histopathology whole slide images (WSIs) is fundamental in modern pathology but is challenged by the gigapixel scale and complexity of WSIs. Trained histopathologists overcome this challenge by navigating the WSI, looking for relevant patches, taking notes, and compiling them to produce a final holistic diagnostic. Traditional AI approaches, such as multiple instance learning and transformer-based models, fail short of such a holistic, iterative, multi-scale diagnostic procedure, limiting their adoption in the real-world. We introduce PathFinder, a multi-modal, multi-agent framework that emulates the decision-making process of expert pathologists. PathFinder integrates four AI agents, the Triage Agent, Navigation Agent, Description Agent, and Diagnosis Agent, that collaboratively navigate WSIs, gather evidence, and provide comprehensive diagnoses with natural language explanations. The Triage Agent classifies the WSI as benign or risky; if risky, the Navigation and Description Agents iteratively focus on significant regions, generating importance maps and descriptive insights of sampled patches. Finally, the Diagnosis Agent synthesizes the findings to determine the patient&#39;s diagnostic classification. Our Experiments show that PathFinder outperforms state-of-the-art methods in skin melanoma diagnosis by 8% while offering inherent explainability through natural language descriptions of diagnostically relevant patches. Qualitative analysis by pathologists shows that the Description Agent&#39;s outputs are of high quality and comparable to GPT-4o. PathFinder is also the first AI-based system to surpass the average performance of pathologists in this challenging melanoma classification task by 9%, setting a new record for efficient, accurate, and interpretable AI-assisted diagnostics in pathology. Data, code and models available at https://pathfinder-dx.github.io/",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.08916"
    },
    "c498e01526b6a1bd27683f246148d1cf": {
        "title": "Mind the Gaps: Logical English, Prolog, and Multi-agent Systems for Autonomous Vehicles",
        "authors": [
            "Galileo Sartor",
            "Adam Wyner",
            "Giuseppe Contissa"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09216",
        "abstract": "In this paper, we present a modular system for representing and reasoning with legal aspects of traffic rules for autonomous vehicles. We focus on a subset of the United Kingdom&#39;s Highway Code (HC) related to junctions. As human drivers and automated vehicles (AVs) will interact on the roads, especially in urban environments, we claim that an accessible, unitary, high-level computational model should exist and be applicable to both users. Autonomous vehicles introduce a shift in liability that should not bring disadvantages or increased burden on human drivers. We develop a system &#34;in silico&#34; of the model. The proposed system is built of three main components: a natural language interface, using Logical English, which encodes the rules; an internal representation of the rules in Prolog; and an multi-agent-based simulation environment, built in NetLogo. The three components interact: Logical English is translated into and out of Prolog (along with some support code); Prolog and NetLogo interface via predicates. Such a modular approach enables the different components to carry different &#34;burdens&#34; in the overall system; it also allows swapping of modules. Given NetLogo, we can visualize the effect of the modeled rules as well as validate the system with a simple dynamic running scenario. Designated agents monitor the behaviour of the vehicles for compliance and record potential violations where they occur. The information on potential violations is then utilized by Validators, to determine whether the violation is punishable, differentiating between exceptions and cases.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09216"
    },
    "6a7e5e4132fc422294461d5575b527c7": {
        "title": "Reliable Conversational Agents under ASP Control that Understand Natural Language",
        "authors": [
            "Yankai Zeng"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09237",
        "abstract": "Efforts have been made to make machines converse like humans in the past few decades. The recent techniques of Large Language Models (LLMs) make it possible to have human-like conversations with machines, but LLM&#39;s flaws of lacking understanding and reliability are well documented. We believe that the best way to eliminate this problem is to use LLMs only as parsers to translate text to knowledge and vice versa and carry out the conversation by reasoning over this knowledge using the answer set programming. I have been developing a framework based on LLMs and ASP to realize reliable chatbots that &#34;understand&#34; human conversation. This framework has been used to develop task-specific chatbots as well as socialbots. My future research is focused on making these chatbots scalable and trainable.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09237"
    },
    "fc84925a30798aa4c28a8e1c8de68052": {
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "authors": [
            "Daniel Jarrett",
            "Miruna PÃ®slar",
            "Michiel A. Bakker",
            "Michael Henry Tessler",
            "Raphael KÃ¶ster",
            "Jan Balaguer",
            "Romuald Elie",
            "Christopher Summerfield",
            "Andrea Tacchetti"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09369",
        "abstract": "Consider the process of collective decision-making, in which a group of individuals interactively select a preferred outcome from among a universe of alternatives. In this context, &#34;representation&#34; is the activity of making an individual&#39;s preferences present in the process via participation by a proxy agent -- i.e. their &#34;representative&#34;. To this end, learned models of human behavior have the potential to fill this role, with practical implications for multi-agent scenario studies and mechanism design. In this work, we investigate the possibility of training \\textit{language agents} to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. First, we formalize the setting of \\textit{collective decision-making} -- as the episodic process of interaction between a group of agents and a decision mechanism. On this basis, we then formalize the problem of \\textit{digital representation} -- as the simulation of an agent&#39;s behavior to yield equivalent outcomes from the mechanism. Finally, we conduct an empirical case study in the setting of \\textit{consensus-finding} among diverse humans, and demonstrate the feasibility of fine-tuning large language models to act as digital representatives.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09369"
    },
    "dba7339fc3fd44954118e551b06df63e": {
        "title": "EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents",
        "authors": [
            "Rui Yang",
            "Hanyang Chen",
            "Junyu Zhang",
            "Mark Zhao",
            "Cheng Qian",
            "Kangrui Wang",
            "Qineng Wang",
            "Teja Venkat Koripella",
            "Marziyeh Movahedi",
            "Manling Li",
            "Heng Ji",
            "Huan Zhang",
            "Tong Zhang"
        ],
        "date": "2025/02/13",
        "pdf": "http://arxiv.org/pdf/2502.09560",
        "abstract": "Leveraging Multi-modal Large Language Models (MLLMs) to create embodied agents offers a promising avenue for tackling real-world tasks. While language-centric embodied agents have garnered substantial attention, MLLM-based embodied agents remain underexplored due to the lack of comprehensive evaluation frameworks. To bridge this gap, we introduce EmbodiedBench, an extensive benchmark designed to evaluate vision-driven embodied agents. EmbodiedBench features: (1) a diverse set of 1,128 testing tasks across four environments, ranging from high-level semantic tasks (e.g., household) to low-level tasks involving atomic actions (e.g., navigation and manipulation); and (2) six meticulously curated subsets evaluating essential agent capabilities like commonsense reasoning, complex instruction understanding, spatial awareness, visual perception, and long-term planning. Through extensive experiments, we evaluated 13 leading proprietary and open-source MLLMs within EmbodiedBench. Our findings reveal that: MLLMs excel at high-level tasks but struggle with low-level manipulation, with the best model, GPT-4o, scoring only 28.9% on average. EmbodiedBench provides a multifaceted standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance MLLM-based embodied agents. Our code is available at https://embodiedbench.github.io.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09560"
    },
    "67dff052fc4f00bb9be082ebd2f58b3f": {
        "title": "Automated Hypothesis Validation with Agentic Sequential Falsifications",
        "authors": [
            "Kexin Huang",
            "Ying Jin",
            "Ryan Li",
            "Michael Y. Li",
            "Emmanuel CandÃ¨s",
            "Jure Leskovec"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.09858",
        "abstract": "Hypotheses are central to information acquisition, decision-making, and discovery. However, many real-world hypotheses are abstract, high-level statements that are difficult to validate directly. This challenge is further intensified by the rise of hypothesis generation from Large Language Models (LLMs), which are prone to hallucination and produce hypotheses in volumes that make manual validation impractical. Here we propose Popper, an agentic framework for rigorous automated validation of free-form hypotheses. Guided by Karl Popper&#39;s principle of falsification, Popper validates a hypothesis using LLM agents that design and execute falsification experiments targeting its measurable implications. A novel sequential testing framework ensures strict Type-I error control while actively gathering evidence from diverse observations, whether drawn from existing data or newly conducted procedures. We demonstrate Popper on six domains including biology, economics, and sociology. Popper delivers robust error control, high power, and scalability. Furthermore, compared to human scientists, Popper achieved comparable performance in validating complex biological hypotheses while reducing time by 10 folds, providing a scalable, rigorous solution for hypothesis validation.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ],
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09858"
    },
    "f47e1ae0afc0e200a076140e674e8b3b": {
        "title": "Position: Stop Acting Like Language Model Agents Are Normal Agents",
        "authors": [
            "Elija Perrier",
            "Michael Timothy Bennett"
        ],
        "date": "2025/02/04",
        "pdf": "http://arxiv.org/pdf/2502.10420",
        "abstract": "Language Model Agents (LMAs) are increasingly treated as capable of autonomously navigating interactions with humans and tools. Their design and deployment tends to presume they are normal agents capable of sustaining coherent goals, adapting across contexts and acting with a measure of intentionality. These assumptions are critical to prospective use cases in industrial, social and governmental settings. But LMAs are not normal agents. They inherit the structural problems of the large language models (LLMs) around which they are built: hallucinations, jailbreaking, misalignment and unpredictability. In this Position paper we argue LMAs should not be treated as normal agents, because doing so leads to problems that undermine their utility and trustworthiness. We enumerate pathologies of agency intrinsic to LMAs. Despite scaffolding such as external memory and tools, they remain ontologically stateless, stochastic, semantically sensitive, and linguistically intermediated. These pathologies destabilise the ontological properties of LMAs including identifiability, continuity, persistence and and consistency, problematising their claim to agency. In response, we argue LMA ontological properties should be measured before, during and after deployment so that the negative effects of pathologies can be mitigated.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10420"
    },
    "30f0457199ead9ca5893677c4b505f2a": {
        "title": "SCALE: Towards Collaborative Content Analysis in Social Science with Large Language Model Agents and Human Intervention",
        "authors": [
            "Chengshuai Zhao",
            "Zhen Tan",
            "Chau-Wai Wong",
            "Xinyan Zhao",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.10937",
        "abstract": "Content analysis breaks down complex and unstructured texts into theory-informed numerical categories. Particularly, in social science, this process usually relies on multiple rounds of manual annotation, domain expert discussion, and rule-based refinement. In this paper, we introduce SCALE, a novel multi-agent framework that effectively $\\underline{\\textbf{S}}$imulates $\\underline{\\textbf{C}}$ontent $\\underline{\\textbf{A}}$nalysis via $\\underline{\\textbf{L}}$arge language model (LLM) ag$\\underline{\\textbf{E}}$nts. SCALE imitates key phases of content analysis, including text coding, collaborative discussion, and dynamic codebook evolution, capturing the reflective depth and adaptive discussions of human researchers. Furthermore, by integrating diverse modes of human intervention, SCALE is augmented with expert input to further enhance its performance. Extensive evaluations on real-world datasets demonstrate that SCALE achieves human-approximated performance across various complex content analysis tasks, offering an innovative potential for future social science research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.10937"
    },
    "844230bbee12afb73e6bb9f3b8904a23": {
        "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
        "authors": [
            "Pan Lu",
            "Bowen Chen",
            "Sheng Liu",
            "Rahul Thapa",
            "Joseph Boen",
            "James Zou"
        ],
        "date": "2025/02/16",
        "pdf": "http://arxiv.org/pdf/2502.11271",
        "abstract": "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools&#39; generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11271"
    },
    "82ef266d2dae8ac235159ac655f5984e": {
        "title": "SMART: Self-Aware Agent for Tool Overuse Mitigation",
        "authors": [
            "Cheng Qian",
            "Emre Can Acikgoz",
            "Hongru Wang",
            "Xiusi Chen",
            "Avirup Sil",
            "Dilek Hakkani-TÃ¼r",
            "Gokhan Tur",
            "Heng Ji"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11435",
        "abstract": "Current Large Language Model (LLM) agents demonstrate strong reasoning and tool use capabilities, but often lack self-awareness, failing to balance these approaches effectively. This imbalance leads to Tool Overuse, where models unnecessarily rely on external tools for tasks solvable with parametric knowledge, increasing computational overhead. Inspired by human metacognition, we introduce SMART (Strategic Model-Aware Reasoning with Tools), a paradigm that enhances an agent&#39;s self-awareness to optimize task handling and reduce tool overuse. To support this paradigm, we introduce SMART-ER, a dataset spanning three domains, where reasoning alternates between parametric knowledge and tool-dependent steps, with each step enriched by rationales explaining when tools are necessary. Through supervised training, we develop SMARTAgent, a family of models that dynamically balance parametric knowledge and tool use. Evaluations show that SMARTAgent reduces tool use by 24% while improving performance by over 37%, enabling 7B-scale models to match its 70B counterpart and GPT-4o. Additionally, SMARTAgent generalizes to out-of-distribution test data like GSM8K and MINTQA, maintaining accuracy with just one-fifth the tool calls. These highlight the potential of strategic tool use to enhance reasoning, mitigate overuse, and bridge the gap between model size and performance, advancing intelligent and resource-efficient agent designs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11435"
    },
    "11e616912a082b8226ecb99dad9fee51": {
        "title": "Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning",
        "authors": [
            "Peiying Yu",
            "Guoxin Chen",
            "Jingjing Wang"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11799",
        "abstract": "Despite the remarkable capabilities of large language models (LLMs) in various reasoning tasks, they still struggle with table reasoning tasks, particularly in maintaining consistency throughout multi-step reasoning processes. While existing approaches have explored various decomposition strategies, they often lack effective mechanisms to identify and correct errors in intermediate reasoning steps, leading to cascading error propagation. To address these issues, we propose Table-Critic, a novel multi-agent framework that facilitates collaborative criticism and iterative refinement of the reasoning process until convergence to correct solutions. Our framework consists of four specialized agents: a Judge for error identification, a Critic for comprehensive critiques, a Refiner for process improvement, and a Curator for pattern distillation. To effectively deal with diverse and unpredictable error types, we introduce a self-evolving template tree that systematically accumulates critique knowledge through experience-driven learning and guides future reflections. Extensive experiments have demonstrated that Table-Critic achieves substantial improvements over existing methods, achieving superior accuracy and error correction rates while maintaining computational efficiency and lower solution degradation rate.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11799"
    },
    "709740a97b1400008d356a32d40fe33b": {
        "title": "Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration",
        "authors": [
            "Shao Zhang",
            "Xihuai Wang",
            "Wenhao Zhang",
            "Chaoran Li",
            "Junru Song",
            "Tingyu Li",
            "Lin Qiu",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Wen Yao",
            "Weinan Zhang",
            "Xinbing Wang",
            "Ying Wen"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.11882",
        "abstract": "Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent&#39;s System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent&#39;s System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.11882"
    },
    "974a9b5b1ac39b4038a36b846be9ba99": {
        "title": "A Study on Leveraging Search and Self-Feedback for Agent Reasoning",
        "authors": [
            "Karthikeyan K",
            "Michelle Yuan",
            "Elman Mansimov",
            "Katerina Margatina",
            "Anurag Pratik",
            "Daniele Bonadiman",
            "Monica Sunkara",
            "Yi Zhang",
            "Yassine Benajiba"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12094",
        "abstract": "Recent works have demonstrated that incorporating search during inference can significantly improve reasoning capabilities of language agents. Some approaches may make use of the ground truth or rely on model&#39;s own generated feedback. The search algorithm uses this feedback to then produce values that will update its criterion for exploring and exploiting various reasoning paths. In this study, we investigate how search and model&#39;s self-feedback can be leveraged for reasoning tasks. First, we explore differences in ground-truth feedback and self-feedback during search for math reasoning. Second, we observe limitations in applying search techniques to more complex tasks like tool-calling and design domain-specific approaches to address these gaps. Our experiments reveal challenges related to generalization when solely relying on self-feedback during search. For search to work effectively, either access to the ground-truth is needed or feedback mechanisms need to be carefully designed for the specific task.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12094"
    },
    "d3a80535890b30f6cd75f002dc747f2e": {
        "title": "HARBOR: Exploring Persona Dynamics in Multi-Agent Competition",
        "authors": [
            "Kenan Jiang",
            "Li Xiong",
            "Fei Liu"
        ],
        "date": "2025/02/17",
        "pdf": "http://arxiv.org/pdf/2502.12149",
        "abstract": "We investigate factors contributing to LLM agents&#39; success in competitive multi-agent environments, using auctions as a testbed where agents bid to maximize profit. The agents are equipped with bidding domain knowledge, distinct personas that reflect item preferences, and a memory of auction history. Our work extends the classic auction scenario by creating a realistic environment where multiple agents bid on houses, weighing aspects such as size, location, and budget to secure the most desirable homes at the lowest prices. Particularly, we investigate three key questions: (a) How does a persona influence an agent&#39;s behavior in a competitive setting? (b) Can an agent effectively profile its competitors&#39; behavior during auctions? (c) How can persona profiling be leveraged to create an advantage using strategies such as theory of mind? Through a series of experiments, we analyze the behaviors of LLM agents and shed light on new findings. Our testbed, called HARBOR, offers a valuable platform for deepening our understanding of multi-agent workflows in competitive environments.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12149"
    },
    "2b96f0e93540445ce1ad323c790e645a": {
        "title": "UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design",
        "authors": [
            "Yuxuan Lu",
            "Bingsheng Yao",
            "Hansu Gu",
            "Jing Huang",
            "Jessie Wang",
            "Laurence Li",
            "Jiri Gesi",
            "Qi He",
            "Toby Jia-Jun Li",
            "Dakuo Wang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.12561",
        "abstract": "Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.12561"
    },
    "4858bc6a428a55b9018b0d9d36998f50": {
        "title": "You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations",
        "authors": [
            "Frederic Kirstein",
            "Muneeb Khan",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13001",
        "abstract": "Meeting summarization suffers from limited high-quality data, mainly due to privacy restrictions and expensive collection processes. We address this gap with FAME, a dataset of 500 meetings in English and 300 in German produced by MIMIC, our new multi-agent meeting synthesis framework that generates meeting transcripts on a given knowledge source by defining psychologically grounded participant profiles, outlining the conversation, and orchestrating a large language model (LLM) debate. A modular post-processing step refines these outputs, mitigating potential repetitiveness and overly formal tones, ensuring coherent, credible dialogues at scale. We also propose a psychologically grounded evaluation framework assessing naturalness, social behavior authenticity, and transcript difficulties. Human assessments show that FAME approximates real-meeting spontaneity (4.5/5 in naturalness), preserves speaker-centric challenges (3/5 in spoken language), and introduces richer information-oriented difficulty (4/5 in difficulty). These findings highlight that FAME is a good and scalable proxy for real-world meeting conditions. It enables new test scenarios for meeting summarization research and other conversation-centric applications in tasks requiring conversation data or simulating social scenarios under behavioral constraints.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13001"
    },
    "0257d95b6b6e7237f9c4efdbca38552b": {
        "title": "Towards a Design Guideline for RPA Evaluation: A Survey of Large Language Model-Based Role-Playing Agents",
        "authors": [
            "Chaoran Chen",
            "Bingsheng Yao",
            "Ruishi Zou",
            "Wenyue Hua",
            "Weimin Lyu",
            "Toby Jia-Jun Li",
            "Dakuo Wang"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13012",
        "abstract": "Role-Playing Agent (RPA) is an increasingly popular type of LLM Agent that simulates human-like behaviors in a variety of tasks. However, evaluating RPAs is challenging due to diverse task requirements and agent designs. This paper proposes an evidence-based, actionable, and generalizable evaluation design guideline for LLM-based RPA by systematically reviewing 1,676 papers published between Jan. 2021 and Dec. 2024. Our analysis identifies six agent attributes, seven task attributes, and seven evaluation metrics from existing literature. Based on these findings, we present an RPA evaluation design guideline to help researchers develop more systematic and consistent evaluation methods.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13012"
    },
    "b457fdcfd3dc55e76ef3b3ba43a83bc3": {
        "title": "Agentic Deep Graph Reasoning Yields Self-Organizing Knowledge Networks",
        "authors": [
            "Markus J. Buehler"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13025",
        "abstract": "We present an agentic, autonomous graph expansion framework that iteratively structures and refines knowledge in situ. Unlike conventional knowledge graph construction methods relying on static extraction or single-pass learning, our approach couples a reasoning-native large language model with a continually updated graph representation. At each step, the system actively generates new concepts and relationships, merges them into a global graph, and formulates subsequent prompts based on its evolving structure. Through this feedback-driven loop, the model organizes information into a scale-free network characterized by hub formation, stable modularity, and bridging nodes that link disparate knowledge clusters. Over hundreds of iterations, new nodes and edges continue to appear without saturating, while centrality measures and shortest path distributions evolve to yield increasingly distributed connectivity. Our analysis reveals emergent patterns, such as the rise of highly connected &#39;hub&#39; concepts and the shifting influence of &#39;bridge&#39; nodes, indicating that agentic, self-reinforcing graph construction can yield open-ended, coherent knowledge structures. Applied to materials design problems, we present compositional reasoning experiments by extracting node-specific and synergy-level principles to foster genuinely novel knowledge synthesis, yielding cross-domain ideas that transcend rote summarization and strengthen the framework&#39;s potential for open-ended scientific discovery. We discuss other applications in scientific discovery and outline future directions for enhancing scalability and interpretability.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13025"
    },
    "0d91bbdcb48cfdec3f34a78e2d5b1ab4": {
        "title": "Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions",
        "authors": [
            "Taedong Yun",
            "Eric Yang",
            "Mustafa Safdari",
            "Jong Ha Lee",
            "Vaishnavi Vinod Kumar",
            "S. Sara Mahdavi",
            "Jonathan Amar",
            "Derek Peyton",
            "Reut Aharony",
            "Andreas Michaelides",
            "Logan Schneider",
            "Isaac Galatzer-Levy",
            "Yugang Jia",
            "John Canny",
            "Arthur Gretton",
            "Maja MatariÄ"
        ],
        "date": "2025/02/18",
        "pdf": "http://arxiv.org/pdf/2502.13135",
        "abstract": "We present an end-to-end framework for generating synthetic users for evaluating interactive agents designed to encourage positive behavior changes, such as in health and lifestyle coaching. The synthetic users are grounded in health and lifestyle conditions, specifically sleep and diabetes management in this study, to ensure realistic interactions with the health coaching agent. Synthetic users are created in two stages: first, structured data are generated grounded in real-world health and lifestyle factors in addition to basic demographics and behavioral attributes; second, full profiles of the synthetic users are developed conditioned on the structured data. Interactions between synthetic users and the coaching agent are simulated using generative agent-based models such as Concordia, or directly by prompting a language model. Using two independently-developed agents for sleep and diabetes coaching as case studies, the validity of this framework is demonstrated by analyzing the coaching agent&#39;s understanding of the synthetic users&#39; needs and challenges. Finally, through multiple blinded evaluations of user-coach interactions by human experts, we demonstrate that our synthetic users with health and behavioral attributes more accurately portray real human users with the same attributes, compared to generic synthetic users not grounded in such attributes. The proposed framework lays the foundation for efficient development of conversational agents through extensive, realistic, and grounded simulated interactions.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13135"
    },
    "f9514f2310d03555870205d78411a23a": {
        "title": "An LLM-based Agent for Reliable Docker Environment Configuration",
        "authors": [
            "Ruida Hu",
            "Chao Peng",
            "Xinchen Wang",
            "Cuiyun Gao"
        ],
        "date": "2025/02/19",
        "pdf": "http://arxiv.org/pdf/2502.13681",
        "abstract": "Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment &#34;pollution&#34; from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%.",
        "code": "",
        "category": [
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.13681"
    },
    "3563a557a7d757a94cdabed8117ca752": {
        "title": "STeCa: Step-level Trajectory Calibration for LLM Agent Learning",
        "authors": [
            "Hanlin Wang",
            "Jian Wang",
            "Chak Tou Leong",
            "Wenjie Li"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14276",
        "abstract": "Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14276"
    },
    "a13576fb7b6c8f02b32a8aa7a0ea6c60": {
        "title": "Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems",
        "authors": [
            "Bingyu Yan",
            "Xiaoming Zhang",
            "Litian Zhang",
            "Lian Zhang",
            "Ziyi Zhou",
            "Dezhuang Miao",
            "Chaozhuo Li"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14321",
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable capabilities in reasoning, planning, and decision-making. Building upon these strengths, researchers have begun incorporating LLMs into multi-agent systems (MAS), where agents collaborate or compete through natural language interactions to tackle tasks beyond the scope of single-agent setups. In this survey, we present a communication-centric perspective on LLM-based multi-agent systems, examining key system-level features such as architecture design and communication goals, as well as internal mechanisms like communication strategies, paradigms, objects and content. We illustrate how these communication elements interplay to enable collective intelligence and flexible collaboration. Furthermore, we discuss prominent challenges, including scalability, security, and multimodal integration, and propose directions for future work to advance research in this emerging domain. Ultimately, this survey serves as a catalyst for further innovation, fostering more robust, scalable, and intelligent multi-agent systems across diverse application domains.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14321"
    },
    "65c80808ab2692e24531b9ec34ebe56d": {
        "title": "Optimizing Model Selection for Compound AI Systems",
        "authors": [
            "Lingjiao Chen",
            "Jared Quincy Davis",
            "Boris Hanin",
            "Peter Bailis",
            "Matei Zaharia",
            "James Zou",
            "Ion Stoica"
        ],
        "date": "2025/02/20",
        "pdf": "http://arxiv.org/pdf/2502.14815",
        "abstract": "Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules.",
        "code": "",
        "category": [
            [
                "Others",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2502.14815"
    },
    "a5653a5ea070143a12a1f50be4a977b7": {
        "title": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents",
        "authors": [
            "Jingoo Lee",
            "Kyungho Lim",
            "Young-Chul Jung",
            "Byung-Hoon Kim"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01594",
        "abstract": "Recent advances in large language models (LLMs) have accelerated the development of conversational agents capable of generating human-like responses. Since psychiatric assessments typically involve complex conversational interactions between psychiatrists and patients, there is growing interest in developing LLM-based psychiatric assessment conversational agents (PACAs) that aim to simulate the role of psychiatrists in clinical evaluations. However, standardized methods for benchmarking the clinical appropriateness of PACAs&#39; interaction with patients still remain underexplored. Here, we propose PSYCHE, a novel framework designed to enable the 1) clinically relevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation of PACAs. This is achieved by simulating psychiatric patients based on a multi-faceted psychiatric construct that defines the simulated patients&#39; profiles, histories, and behaviors, which PACAs are expected to assess. We validate the effectiveness of PSYCHE through a study with 10 board-certified psychiatrists, supported by an in-depth analysis of the simulated patient utterances.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01594"
    },
    "e15c9128ad8d94e1ec27e0a2a5bd996b": {
        "title": "LatteReview: A Multi-Agent Framework for Systematic Review Automation Using Large Language Models",
        "authors": [
            "Pouria Rouzrokh",
            "Moein Shariatnia"
        ],
        "date": "2025/01/05",
        "pdf": "http://arxiv.org/pdf/2501.05468",
        "abstract": "Systematic literature reviews and meta-analyses are essential for synthesizing research insights, but they remain time-intensive and labor-intensive due to the iterative processes of screening, evaluation, and data extraction. This paper introduces and evaluates LatteReview, a Python-based framework that leverages large language models (LLMs) and multi-agent systems to automate key elements of the systematic review process. Designed to streamline workflows while maintaining rigor, LatteReview utilizes modular agents for tasks such as title and abstract screening, relevance scoring, and structured data extraction. These agents operate within orchestrated workflows, supporting sequential and parallel review rounds, dynamic decision-making, and iterative refinement based on user feedback. LatteReview&#39;s architecture integrates LLM providers, enabling compatibility with both cloud-based and locally hosted models. The framework supports features such as Retrieval-Augmented Generation (RAG) for incorporating external context, multimodal reviews, Pydantic-based validation for structured inputs and outputs, and asynchronous programming for handling large-scale datasets. The framework is available on the GitHub repository, with detailed documentation and an installable package.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05468"
    },
    "2ebcc67f683e9d099bb4494cd6479ade": {
        "title": "Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains",
        "authors": [
            "Vighnesh Subramaniam",
            "Yilun Du",
            "Joshua B. Tenenbaum",
            "Antonio Torralba",
            "Shuang Li",
            "Igor Mordatch"
        ],
        "date": "2025/01/10",
        "pdf": "http://arxiv.org/pdf/2501.05707",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05707"
    },
    "63fe8ee82c8b5f95a514e76fb4e4a33b": {
        "title": "ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning",
        "authors": [
            "Xiangru Tang",
            "Tianyu Hu",
            "Muyang Ye",
            "Yanjun Shao",
            "Xunjian Yin",
            "Siru Ouyang",
            "Wangchunshu Zhou",
            "Pan Lu",
            "Zhuosheng Zhang",
            "Yilun Zhao",
            "Arman Cohan",
            "Mark Gerstein"
        ],
        "date": "2025/01/11",
        "pdf": "http://arxiv.org/pdf/2501.06590",
        "abstract": "Chemical reasoning usually involves complex, multi-step processes that demand precise calculations, where even minor errors can lead to cascading failures. Furthermore, large language models (LLMs) encounter difficulties handling domain-specific formulas, executing reasoning steps accurately, and integrating code effectively when tackling chemical reasoning tasks. To address these challenges, we present ChemAgent, a novel framework designed to improve the performance of LLMs through a dynamic, self-updating library. This library is developed by decomposing chemical tasks into sub-tasks and compiling these sub-tasks into a structured collection that can be referenced for future queries. Then, when presented with a new problem, ChemAgent retrieves and refines pertinent information from the library, which we call memory, facilitating effective task decomposition and the generation of solutions. Our method designs three types of memory and a library-enhanced reasoning component, enabling LLMs to improve over time through experience. Experimental results on four chemical reasoning datasets from SciBench demonstrate that ChemAgent achieves performance gains of up to 46% (GPT-4), significantly outperforming existing methods. Our findings suggest substantial potential for future applications, including tasks such as drug discovery and materials science. Our code can be found at https://github.com/gersteinlab/chemagent",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.06590"
    },
    "6963aadc233c22e271b20c0392235c22": {
        "title": "Doc-Guided Sent2Sent++: A Sent2Sent++ Agent with Doc-Guided memory for Document-level Machine Translation",
        "authors": [
            "Jiaxin Guo",
            "Yuanchang Luo",
            "Daimeng Wei",
            "Ling Zhang",
            "Zongyao Li",
            "Hengchao Shang",
            "Zhiqiang Rao",
            "Shaojun Li",
            "Jinlong Yang",
            "Zhanglin Wu",
            "Hao Yang"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.08523",
        "abstract": "The field of artificial intelligence has witnessed significant advancements in natural language processing, largely attributed to the capabilities of Large Language Models (LLMs). These models form the backbone of Agents designed to address long-context dependencies, particularly in Document-level Machine Translation (DocMT). DocMT presents unique challenges, with quality, consistency, and fluency being the key metrics for evaluation. Existing approaches, such as Doc2Doc and Doc2Sent, either omit sentences or compromise fluency. This paper introduces Doc-Guided Sent2Sent++, an Agent that employs an incremental sentence-level forced decoding strategy \\textbf{to ensure every sentence is translated while enhancing the fluency of adjacent sentences.} Our Agent leverages a Doc-Guided Memory, focusing solely on the summary and its translation, which we find to be an efficient approach to maintaining consistency. Through extensive testing across multiple languages and domains, we demonstrate that Sent2Sent++ outperforms other methods in terms of quality, consistency, and fluency. The results indicate that, our approach has achieved significant improvements in metrics such as s-COMET, d-COMET, LTCR-$1_f$, and document-level perplexity (d-ppl). The contributions of this paper include a detailed analysis of current DocMT research, the introduction of the Sent2Sent++ decoding method, the Doc-Guided Memory mechanism, and validation of its effectiveness across languages and domains.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.08523"
    },
    "6980029f3a23fdd6bfca03f5bca21072": {
        "title": "Personality Modeling for Persuasion of Misinformation using AI Agent",
        "authors": [
            "Qianmin Lou",
            "Wentao Xu"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.08985",
        "abstract": "The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.08985"
    },
    "65f6d238b1eb6890b606f14a2881ac8d": {
        "title": "AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral Therapy in Psychological Counseling",
        "authors": [
            "Ancheng Xu",
            "Di Yang",
            "Renhao Li",
            "Jingwei Zhu",
            "Minghuan Tan",
            "Min Yang",
            "Wanxin Qiu",
            "Mingchen Ma",
            "Haihong Wu",
            "Bingyu Li",
            "Feng Sha",
            "Chengming Li",
            "Xiping Hu",
            "Qiang Qu",
            "Derek F. Wong",
            "Ruifeng Xu"
        ],
        "date": "2025/01/16",
        "pdf": "http://arxiv.org/pdf/2501.09426",
        "abstract": "Traditional in-person psychological counseling remains primarily niche, often chosen by individuals with psychological issues, while online automated counseling offers a potential solution for those hesitant to seek help due to feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and widely used approach in psychological counseling. The advent of large language models (LLMs) and agent technology enables automatic CBT diagnosis and treatment. However, current LLM-based CBT systems use agents with a fixed structure, limiting their self-optimization capabilities, or providing hollow, unhelpful suggestions due to redundant response patterns. In this work, we utilize Quora-like and YiXinLi single-round consultation models to build a general agent framework that generates high-quality responses for single-turn psychological consultation scenarios. We use a bilingual dataset to evaluate the quality of single-response consultations generated by each framework. Then, we incorporate dynamic routing and supervisory mechanisms inspired by real psychological counseling to construct a CBT-oriented autonomous multi-agent framework, demonstrating its general applicability. Experimental results indicate that AutoCBT can provide higher-quality automated psychological counseling services.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09426"
    },
    "3eb3d74a0ca144a0709f7ea8645be09d": {
        "title": "Agent-as-Judge for Factual Summarization of Long Narratives",
        "authors": [
            "Yeonseok Jeong",
            "Minsoo Kim",
            "Seung-won Hwang",
            "Byung-Hak Kim"
        ],
        "date": "2025/01/17",
        "pdf": "http://arxiv.org/pdf/2501.09993",
        "abstract": "Large Language Models (LLMs) have demonstrated near-human performance in summarization tasks based on traditional metrics such as ROUGE and BERTScore. However, these metrics do not adequately capture critical aspects of summarization quality, such as factual accuracy, particularly for long narratives (&gt;100K tokens). Recent advances, such as LLM-as-a-Judge, address the limitations of metrics based on lexical similarity but still exhibit factual inconsistencies, especially in understanding character relationships and states. In this work, we introduce NarrativeFactScore, a novel &#34;Agent-as-a-Judge&#34; framework for evaluating and refining summaries. By leveraging a Character Knowledge Graph (CKG) extracted from input and generated summaries, NarrativeFactScore assesses the factual consistency and provides actionable guidance for refinement, such as identifying missing or erroneous facts. We demonstrate the effectiveness of NarrativeFactScore through a detailed workflow illustration and extensive validation on widely adopted benchmarks, achieving superior performance compared to competitive methods. Our results highlight the potential of agent-driven evaluation systems to improve the factual reliability of LLM-generated summaries.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09993"
    },
    "49f35b96e2d2abc2615029be99395cd7": {
        "title": "IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems",
        "authors": [
            "Elad Levi",
            "Ilan Kadar"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.11067",
        "abstract": "Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11067"
    },
    "08b501dfcdada77f8db89771fbd74895": {
        "title": "Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks",
        "authors": [
            "Zhenhailong Wang",
            "Haiyang Xu",
            "Junyang Wang",
            "Xi Zhang",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Heng Ji"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.11733",
        "abstract": "Smartphones have become indispensable in modern life, yet navigating complex tasks on mobile devices often remains frustrating. Recent advancements in large multimodal model (LMM)-based mobile agents have demonstrated the ability to perceive and act in mobile environments. However, current approaches face significant limitations: they fall short in addressing real-world human needs, struggle with reasoning-intensive and long-horizon tasks, and lack mechanisms to learn and improve from prior experiences. To overcome these challenges, we introduce Mobile-Agent-E, a hierarchical multi-agent framework capable of self-evolution through past experience. By hierarchical, we mean an explicit separation of high-level planning and low-level action execution. The framework comprises a Manager, responsible for devising overall plans by breaking down complex tasks into subgoals, and four subordinate agents--Perceptor, Operator, Action Reflector, and Notetaker--which handle fine-grained visual perception, immediate action execution, error verification, and information aggregation, respectively. Mobile-Agent-E also features a novel self-evolution module which maintains a persistent long-term memory comprising Tips and Shortcuts. Tips are general guidance and lessons learned from prior tasks on how to effectively interact with the environment. Shortcuts are reusable, executable sequences of atomic operations tailored for specific subroutines. The inclusion of Tips and Shortcuts facilitates continuous refinement in performance and efficiency. Alongside this framework, we introduce Mobile-Eval-E, a new benchmark featuring complex mobile tasks requiring long-horizon, multi-app interactions. Empirical results show that Mobile-Agent-E achieves a 22% absolute improvement over previous state-of-the-art approaches across three foundation model backbones. Project page: https://x-plug.github.io/MobileAgent.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11733"
    },
    "6dbbf1cb2add3b300212f60478a59d68": {
        "title": "FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces",
        "authors": [
            "Zhenran Xu",
            "Longyue Wang",
            "Jifang Wang",
            "Zhouyi Li",
            "Senbao Shi",
            "Xue Yang",
            "Yiyu Wang",
            "Baotian Hu",
            "Jun Yu",
            "Min Zhang"
        ],
        "date": "2025/01/22",
        "pdf": "http://arxiv.org/pdf/2501.12909",
        "abstract": "Virtual film production requires intricate decision-making processes, including scriptwriting, virtual cinematography, and precise actor positioning and actions. Motivated by recent advances in automated decision-making with language agent-based societies, this paper introduces FilmAgent, a novel LLM-based multi-agent collaborative framework for end-to-end film automation in our constructed 3D virtual spaces. FilmAgent simulates various crew roles, including directors, screenwriters, actors, and cinematographers, and covers key stages of a film production workflow: (1) idea development transforms brainstormed ideas into structured story outlines; (2) scriptwriting elaborates on dialogue and character actions for each scene; (3) cinematography determines the camera setups for each shot. A team of agents collaborates through iterative feedback and revisions, thereby verifying intermediate scripts and reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key aspects. Human evaluation shows that FilmAgent outperforms all baselines across all aspects and scores 3.98 out of 5 on average, showing the feasibility of multi-agent collaboration in filmmaking. Further analysis reveals that FilmAgent, despite using the less advanced GPT-4o model, surpasses the single-agent o1, showing the advantage of a well-coordinated multi-agent system. Lastly, we discuss the complementary strengths and weaknesses of OpenAI&#39;s text-to-video model Sora and our FilmAgent in filmmaking.",
        "code": "",
        "category": [
            [
                "Application",
                "Art"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12909"
    },
    "6ac0e6879afe3f2a6a53d4d966db8877": {
        "title": "Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents",
        "authors": [
            "Shrinidhi Kumbhar",
            "Venkatesh Mishra",
            "Kevin Coutinho",
            "Divij Handa",
            "Ashif Iquebal",
            "Chitta Baral"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.13299",
        "abstract": "Materials discovery and design are essential for advancing technology across various industries by enabling the development of application-specific materials. Recent research has leveraged Large Language Models (LLMs) to accelerate this process. We explore the potential of LLMs to generate viable hypotheses that, once validated, can expedite materials discovery. Collaborating with materials science experts, we curated a novel dataset from recent journal publications, featuring real-world goals, constraints, and methods for designing real-world applications. Using this dataset, we test LLM-based agents that generate hypotheses for achieving given goals under specific constraints. To assess the relevance and quality of these hypotheses, we propose a novel scalable evaluation metric that emulates the process a materials scientist would use to evaluate a hypothesis critically. Our curated dataset, proposed method, and evaluation framework aim to advance future research in accelerating materials discovery and design with LLMs.",
        "code": "",
        "category": [
            [
                "Application",
                "Chemistry"
            ],
            [
                "Application",
                "Physics"
            ],
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13299"
    },
    "388d238c303993b6eb0a201766c76829": {
        "title": "Self-Explanation in Social AI Agents",
        "authors": [
            "Rhea Basappa",
            "Mustafa Tekman",
            "Hong Lu",
            "Benjamin Faught",
            "Sandeep Kakar",
            "Ashok K. Goel"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.13945",
        "abstract": "Social AI agents interact with members of a community, thereby changing the behavior of the community. For example, in online learning, an AI social assistant may connect learners and thereby enhance social interaction. These social AI assistants too need to explain themselves in order to enhance transparency and trust with the learners. We present a method of self-explanation that uses introspection over a self-model of an AI social assistant. The self-model is captured as a functional model that specifies how the methods of the agent use knowledge to achieve its tasks. The process of generating self-explanations uses Chain of Thought to reflect on the self-model and ChatGPT to provide explanations about its functioning. We evaluate the self-explanation of the AI social assistant for completeness and correctness. We also report on its deployment in a live class.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13945"
    },
    "c0ad4257855e22352665edac9454a908": {
        "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
        "authors": [
            "Diego Gosmar",
            "Deborah A. Dahl"
        ],
        "date": "2025/01/19",
        "pdf": "http://arxiv.org/pdf/2501.13946",
        "abstract": "Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI systems and their reliability. This study investigates how orchestrating multiple specialized Artificial Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations, into a front-end agent. The outputs are then systematically reviewed and refined by second- and third-level agents, each employing distinct large language models and tailored strategies to detect unverified claims, incorporate explicit disclaimers, and clarify speculative content. Additionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed to evaluate hallucination score levels. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed assessments and ensuring accurate quantification of shifts in hallucination-related behaviors. A core component of this investigation is the use of the OVON (Open Voice Network) framework, which relies on universal NLP-based interfaces to transfer contextual information among agents. Through structured JSON messages, each agent communicates its assessment of the hallucination likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to refine the text without losing context. The results demonstrate that employing multiple specialized agents capable of interoperating with each other through NLP-based agentic frameworks can yield promising outcomes in hallucination mitigation, ultimately bolstering trust within the AI community.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13946"
    },
    "5f82858e162b2c670172c9a8e2d121a2": {
        "title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
        "authors": [
            "Preston Rasmussen",
            "Pavlo Paliychuk",
            "Travis Beauvais",
            "Jack Ryan",
            "Daniel Chalef"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.13956",
        "abstract": "We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti -- a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance (94.8% vs 93.4%). Beyond DMR, Zep&#39;s capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to 18.5% while simultaneously reducing response latency by 90% compared to baseline implementations. These results are particularly pronounced in enterprise-critical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep&#39;s effectiveness for deployment in real-world applications.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Memory Mechanism"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13956"
    },
    "884bfd6c802d7881215907e5e22a1a50": {
        "title": "Communicating Activations Between Language Model Agents",
        "authors": [
            "Vignav Ramesh",
            "Kenneth Li"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.14082",
        "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate via activations; concretely, we pause an LM $\\textit{B}$&#39;s computation at an intermediate layer, combine its current activation with another LM $\\textit{A}$&#39;s intermediate activation via some function $\\textit{f}$, then pass $\\textit{f}$&#39;s output into the next layer of $\\textit{B}$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks with zero additional parameters and data, and saves a substantial amount of compute over natural language communication. We test our method with various functional forms $\\textit{f}$ on two experimental setups--multi-player coordination games and reasoning benchmarks--and find that it achieves up to $27.0\\%$ improvement over natural language communication across datasets with $&lt;$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative &#34;language&#34; for communication between LMs.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14082"
    },
    "16b1588c25fd0f3fdd2663b75443cc23": {
        "title": "Multi-agent KTO: Reinforcing Strategic Interactions of Large Language Model in Language Game",
        "authors": [
            "Rong Ye",
            "Yongxin Zhang",
            "Yikai Zhang",
            "Haoyu Kuang",
            "Zhongyu Wei",
            "Peng Sun"
        ],
        "date": "2025/01/24",
        "pdf": "http://arxiv.org/pdf/2501.14225",
        "abstract": "Achieving Artificial General Intelligence (AGI) requires AI agents that can not only make stratigic decisions but also engage in flexible and meaningful communication. Inspired by Wittgenstein&#39;s language game theory in Philosophical Investigations, we propose that language agents can learn through in-context interaction rather than traditional multi-stage frameworks that separate decision-making from language expression. Using Werewolf, a social deduction game that tests language understanding, strategic interaction, and adaptability, we develop the Multi-agent Kahneman &amp; Tversky&#39;s Optimization (MaKTO). MaKTO engages diverse models in extensive gameplay to generate unpaired desirable and unacceptable responses, then employs KTO to refine the model&#39;s decision-making process. In 9-player Werewolf games, MaKTO achieves a 61% average win rate across various models, outperforming GPT-4o and two-stage RL agents by relative improvements of 23.0% and 10.9%, respectively. Notably, MaKTO also demonstrates human-like performance, winning 60% against expert players and showing only 49% detectability in Turing-style blind tests. These results showcase MaKTO&#39;s superior decision-making, strategic adaptation, and natural language generation in complex social deduction games.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Game Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14225"
    },
    "669c9a4b755d8188c634d5905b8db9c1": {
        "title": "Unmasking Conversational Bias in AI Multiagent Systems",
        "authors": [
            "Erica Coppolillo",
            "Giuseppe Manco",
            "Luca Maria Aiello"
        ],
        "date": "2025/01/24",
        "pdf": "http://arxiv.org/pdf/2501.14844",
        "abstract": "Detecting biases in the outputs produced by generative models is essential to reduce the potential risks associated with their application in critical settings. However, the majority of existing methodologies for identifying biases in generated text consider the models in isolation and neglect their contextual applications. Specifically, the biases that may arise in multi-agent systems involving generative models remain under-researched. To address this gap, we present a framework designed to quantify biases within multi-agent systems of conversational Large Language Models (LLMs). Our approach involves simulating small echo chambers, where pairs of LLMs, initialized with aligned perspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe significant shifts in the stance expressed in the generated messages, particularly within echo chambers where all agents initially express conservative viewpoints, in line with the well-documented political bias of many LLMs toward liberal positions. Crucially, the bias observed in the echo-chamber experiment remains undetected by current state-of-the-art bias detection methods that rely on questionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias detection and mitigation for AI multi-agent systems. The code to perform the experiments is publicly available at https://anonymous.4open.science/r/LLMsConversationalBias-7725.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.14844"
    },
    "595bb55d6de8444d8ea62370b7df247e": {
        "title": "Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning",
        "authors": [
            "Yiqun Chen",
            "Lingyong Yan",
            "Weiwei Sun",
            "Xinyu Ma",
            "Yi Zhang",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Yiming Yang",
            "Jiaxin Mao"
        ],
        "date": "2025/01/25",
        "pdf": "http://arxiv.org/pdf/2501.15228",
        "abstract": "Retrieval-augmented generation (RAG) is extensively utilized to incorporate external, current knowledge into large language models, thereby minimizing hallucinations. A standard RAG pipeline may comprise several components, such as query rewriting, document retrieval, document filtering, and answer generation. However, these components are typically optimized separately through supervised fine-tuning, which can lead to misalignments between the objectives of individual modules and the overarching aim of generating accurate answers in question-answering (QA) tasks. Although recent efforts have explored reinforcement learning (RL) to optimize specific RAG components, these approaches often focus on overly simplistic pipelines with only two components or do not adequately address the complex interdependencies and collaborative interactions among the modules. To overcome these challenges, we propose treating the RAG pipeline as a multi-agent cooperative task, with each component regarded as an RL agent. Specifically, we present MMOA-RAG, a Multi-Module joint Optimization Algorithm for RAG, which employs multi-agent reinforcement learning to harmonize all agents&#39; goals towards a unified reward, such as the F1 score of the final answer. Experiments conducted on various QA datasets demonstrate that MMOA-RAG improves the overall pipeline performance and outperforms existing baselines. Furthermore, comprehensive ablation studies validate the contributions of individual components and the adaptability of MMOA-RAG across different RAG components and datasets. The code of MMOA-RAG is on https://github.com/chenyiqun/MMOA-RAG.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15228"
    },
    "953ba69bdd56a78f559bae8a78959bd4": {
        "title": "Are Human Interactions Replicable by Generative Agents? A Case Study on Pronoun Usage in Hierarchical Interactions",
        "authors": [
            "Naihao Deng",
            "Rada Mihalcea"
        ],
        "date": "2025/01/25",
        "pdf": "http://arxiv.org/pdf/2501.15283",
        "abstract": "As Large Language Models (LLMs) advance in their capabilities, researchers have increasingly employed them for social simulation. In this paper, we investigate whether interactions among LLM agents resemble those of humans. Specifically, we focus on the pronoun usage difference between leaders and non-leaders, examining whether the simulation would lead to human-like pronoun usage patterns during the LLMs&#39; interactions. Our evaluation reveals the significant discrepancies between LLM-based simulations and human pronoun usage, with prompt-based or specialized agents failing to demonstrate human-like pronoun usage patterns. In addition, we reveal that even if LLMs understand the human pronoun usage patterns, they fail to demonstrate them in the actual interaction process. Our study highlights the limitations of social simulations based on LLM agents, urging caution in using such social simulation in practitioners&#39; decision-making process.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15283"
    },
    "46fe4f04f470a7053cc835b5c47b45e6": {
        "title": "Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection",
        "authors": [
            "Bo Yang",
            "Jiaxian Guo",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "date": "2025/01/26",
        "pdf": "http://arxiv.org/pdf/2501.15355",
        "abstract": "Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent&#39;s perception of its counterpart&#39;s mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts&#39; inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the \\textit{second-order} ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart&#39;s behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15355"
    },
    "fe12b9738b6d527d4e4380a84b6e1b96": {
        "title": "MADP: Multi-Agent Deductive Planning for Enhanced Cognitive-Behavioral Mental Health Question Answer",
        "authors": [
            "Qi Chen",
            "Dexi Liu"
        ],
        "date": "2025/01/27",
        "pdf": "http://arxiv.org/pdf/2501.15826",
        "abstract": "The Mental Health Question Answer (MHQA) task requires the seeker and supporter to complete the support process in one-turn dialogue. Given the richness of help-seeker posts, supporters must thoroughly understand the content and provide logical, comprehensive, and well-structured responses. Previous works in MHQA mostly focus on single-agent approaches based on the cognitive element of Cognitive Behavioral Therapy (CBT), but they overlook the interactions among various CBT elements, such as emotion and cognition. This limitation hinders the models&#39; ability to thoroughly understand the distress of help-seekers. To address this, we propose a framework named Multi-Agent Deductive Planning (MADP), which is based on the interactions between the various psychological elements of CBT. This method guides Large Language Models (LLMs) to achieve a deeper understanding of the seeker&#39;s context and provide more personalized assistance based on individual circumstances. Furthermore, we construct a new dataset based on the MADP framework and use it to fine-tune LLMs, resulting in a specialized model named MADP-LLM. We conduct extensive experiments, including comparisons with multiple LLMs, human evaluations, and automatic evaluations, to validate the effectiveness of the MADP framework and MADP-LLM.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.15826"
    },
    "89f171e17eb49ccf57191b08a3a19db2": {
        "title": "Actions Speak Louder than Words: Agent Decisions Reveal Implicit Biases in Language Models",
        "authors": [
            "Yuxuan Li",
            "Hirokazu Shirado",
            "Sauvik Das"
        ],
        "date": "2025/01/29",
        "pdf": "http://arxiv.org/pdf/2501.17420",
        "abstract": "While advances in fairness and alignment have helped mitigate overt biases exhibited by large language models (LLMs) when explicitly prompted, we hypothesize that these models may still exhibit implicit biases when simulating human behavior. To test this hypothesis, we propose a technique to systematically uncover such biases across a broad range of sociodemographic categories by assessing decision-making disparities among agents with LLM-generated, sociodemographically-informed personas. Using our technique, we tested six LLMs across three sociodemographic groups and four decision-making scenarios. Our results show that state-of-the-art LLMs exhibit significant sociodemographic disparities in nearly all simulations, with more advanced models exhibiting greater implicit biases despite reducing explicit biases. Furthermore, when comparing our findings to real-world disparities reported in empirical studies, we find that the biases we uncovered are directionally aligned but markedly amplified. This directional alignment highlights the utility of our technique in uncovering systematic biases in LLMs rather than random variations; moreover, the presence and amplification of implicit biases emphasizes the need for novel strategies to address these biases.",
        "code": "",
        "category": [
            [
                "Stability",
                "Bias"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.17420"
    },
    "37aaa3a77aacdb7837741416f3de3591": {
        "title": "Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models",
        "authors": [
            "Manish Sanwal"
        ],
        "date": "2025/01/29",
        "pdf": "http://arxiv.org/pdf/2501.18645",
        "abstract": "Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to provide step-by-step rationales, improving performance on complex tasks. Despite its benefits, vanilla CoT often fails to fully verify intermediate inferences and can produce misleading explanations. In this work, we propose Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that systematically segments the reasoning process into multiple layers, each subjected to external checks and optional user feedback. We expand on the key concepts, present three scenarios -- medical triage, financial risk assessment, and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT in terms of transparency, correctness, and user engagement. By integrating references from recent arXiv papers on interactive explainability, multi-agent frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves the way for more reliable and grounded explanations in high-stakes domains.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.18645"
    },
    "5e77a1660e986fc9c52ea6f640afef7c": {
        "title": "KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search",
        "authors": [
            "Haoran Luo",
            "Haihong E",
            "Yikai Guo",
            "Qika Lin",
            "Xiaobao Wu",
            "Xinyu Mu",
            "Wenhao Liu",
            "Meina Song",
            "Yifan Zhu",
            "Luu Anh Tuan"
        ],
        "date": "2025/01/31",
        "pdf": "http://arxiv.org/pdf/2501.18922",
        "abstract": "Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration&#39;s performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model&#39;s GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.18922"
    },
    "83b6080f352a88b9b83b2f83a876dcc9": {
        "title": "Harnessing Multi-Agent LLMs for Complex Engineering Problem-Solving: A Framework for Senior Design Projects",
        "authors": [
            "Abdullah Mushtaq",
            "Muhammad Rafay Naeem",
            "Ibrahim Ghaznavi",
            "Muhammad Imran Taj",
            "Imran Hashmi",
            "Junaid Qadir"
        ],
        "date": "2025/01/02",
        "pdf": "http://arxiv.org/pdf/2501.01205",
        "abstract": "Multi-Agent Large Language Models (LLMs) are gaining significant attention for their ability to harness collective intelligence in complex problem-solving, decision-making, and planning tasks. This aligns with the concept of the wisdom of crowds, where diverse agents contribute collectively to generating effective solutions, making it particularly suitable for educational settings. Senior design projects, also known as capstone or final year projects, are pivotal in engineering education as they integrate theoretical knowledge with practical application, fostering critical thinking, teamwork, and real-world problem-solving skills. In this paper, we explore the use of Multi-Agent LLMs in supporting these senior design projects undertaken by engineering students, which often involve multidisciplinary considerations and conflicting objectives, such as optimizing technical performance while addressing ethical, social, and environmental concerns. We propose a framework where distinct LLM agents represent different expert perspectives, such as problem formulation agents, system complexity agents, societal and ethical agents, or project managers, thus facilitating a holistic problem-solving approach. This implementation leverages standard multi-agent system (MAS) concepts such as coordination, cooperation, and negotiation, incorporating prompt engineering to develop diverse personas for each agent. These agents engage in rich, collaborative dialogues to simulate human engineering teams, guided by principles from swarm AI to efficiently balance individual contributions towards a unified solution. We adapt these techniques to create a collaboration structure for LLM agents, encouraging interdisciplinary reasoning and negotiation similar to real-world senior design projects. To assess the efficacy of this framework, we collected six proposals of engineering and computer science of...",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01205"
    },
    "0becf291bbb8de204c69d5f70cbe9794": {
        "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
        "authors": [
            "Dayuan Fu",
            "Keqing He",
            "Yejie Wang",
            "Wentao Hong",
            "Zhuoma Gongque",
            "Weihao Zeng",
            "Wei Wang",
            "Jingang Wang",
            "Xunliang Cai",
            "Weiran Xu"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01702",
        "abstract": "Large Language Model (LLM) based agents have proved their ability to perform complex tasks like humans. However, there is still a large gap between open-sourced LLMs and commercial models like the GPT series. In this paper, we focus on improving the agent generalization capabilities of LLMs via instruction tuning. We first observe that the existing agent training corpus exhibits satisfactory results on held-in evaluation sets but fails to generalize to held-out sets. These agent-tuning works face severe formatting errors and are frequently stuck in the same mistake for a long while. We analyze that the poor generalization ability comes from overfitting to several manual agent environments and a lack of adaptation to new situations. They struggle with the wrong action steps and can not learn from the experience but just memorize existing observation-action relations. Inspired by the insight, we propose a novel AgentRefine framework for agent-tuning. The core idea is to enable the model to learn to correct its mistakes via observation in the trajectory. Specifically, we propose an agent synthesis framework to encompass a diverse array of environments and tasks and prompt a strong LLM to refine its error action according to the environment feedback. AgentRefine significantly outperforms state-of-the-art agent-tuning work in terms of generalization ability on diverse agent tasks. It also has better robustness facing perturbation and can generate diversified thought in inference. Our findings establish the correlation between agent generalization and self-refinement and provide a new paradigm for future research.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01702"
    },
    "319d39b7410b1699ca230cfc0076a29f": {
        "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
        "authors": [
            "Aobo Kong",
            "Wentao Ma",
            "Shiwan Zhao",
            "Yongbin Li",
            "Yuchuan Wu",
            "Ke Wang",
            "Xiaoqian Liu",
            "Qicheng Li",
            "Yong Qin",
            "Fei Huang"
        ],
        "date": "2025/01/03",
        "pdf": "http://arxiv.org/pdf/2501.01821",
        "abstract": "Social agents powered by large language models (LLMs) can simulate human social behaviors but fall short in handling complex goal-oriented social dialogues. Direct Preference Optimization (DPO) has proven effective in aligning LLM behavior with human preferences across a variety of agent tasks. Existing DPO-based approaches for multi-turn interactions are divided into turn-level and session-level methods. The turn-level method is overly fine-grained, focusing exclusively on individual turns, while session-level methods are too coarse-grained, often introducing training noise. To address these limitations, we propose Segment-Level Direct Preference Optimization (SDPO), which focuses on specific key segments within interactions to optimize multi-turn agent behavior while minimizing training noise. Evaluations on the SOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform both existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring SDPO&#39;s potential to advance the social intelligence of LLM-based agents. We release our code and data at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
        "code": "",
        "category": [
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.01821"
    },
    "578096cec3465cb082c34dec69c9d8f9": {
        "title": "PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides",
        "authors": [
            "Hao Zheng",
            "Xinyan Guan",
            "Hao Kong",
            "Jia Zheng",
            "Weixiang Zhou",
            "Hongyu Lin",
            "Yaojie Lu",
            "Ben He",
            "Xianpei Han",
            "Le Sun"
        ],
        "date": "2025/01/07",
        "pdf": "http://arxiv.org/pdf/2501.03936",
        "abstract": "Automatically generating presentations from documents is a challenging task that requires accommodating content quality, visual appeal, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, overlooking visual appeal and structural coherence, which limits their practical applicability. To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows. PPTAgent first analyzes reference presentations to extract slide-level functional types and content schemas, then drafts an outline and iteratively generates editing actions based on selected reference slides to create new slides. To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Results demonstrate that PPTAgent significantly outperforms existing automatic presentation generation methods across all three dimensions.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.03936"
    },
    "b8adfebfe98f0ced2e2d099d89c81412": {
        "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
        "authors": [
            "Samuel Schmidgall",
            "Yusheng Su",
            "Ze Wang",
            "Ximeng Sun",
            "Jialian Wu",
            "Xiaodong Yu",
            "Jiang Liu",
            "Zicheng Liu",
            "Emad Barsoum"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.04227",
        "abstract": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.04227"
    },
    "2d7c164824c91765d8b336c520d7f4d4": {
        "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
        "authors": [
            "Yuhang Liu",
            "Pengxiang Li",
            "Zishu Wei",
            "Congkai Xie",
            "Xueyu Hu",
            "Xinchen Xu",
            "Shengyu Zhang",
            "Xiaotian Han",
            "Hongxia Yang",
            "Fei Wu"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.04575",
        "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language models (MLLMs), have shown great potential for task automation on computing devices such as computers and mobile phones. However, existing agents face challenges in multi-step reasoning and reliance on textual annotations, limiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based GUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1 enhances fundamental skills such as GUI understanding and grounding, while Stage 2 integrates hierarchical reasoning and expectation-reflection reasoning skills using synthesized data to enable native reasoning abilities of the agents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI benchmarks, highlighting the impact of native reasoning skills in enhancing GUI interaction for automation tasks. Resources are available at \\url{https://github.com/Reallm-Labs/InfiGUIAgent}.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ],
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.04575"
    },
    "ae6eb6ca01d94c79165bd75a33a7b227": {
        "title": "Search-o1: Agentic Search-Enhanced Large Reasoning Models",
        "authors": [
            "Xiaoxi Li",
            "Guanting Dong",
            "Jiajie Jin",
            "Yuyao Zhang",
            "Yujia Zhou",
            "Yutao Zhu",
            "Peitian Zhang",
            "Zhicheng Dou"
        ],
        "date": "2025/01/09",
        "pdf": "http://arxiv.org/pdf/2501.05366",
        "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have demonstrated impressive long stepwise reasoning capabilities through large-scale reinforcement learning. However, their extended reasoning processes often suffer from knowledge insufficiency, leading to frequent uncertainties and potential errors. To address this limitation, we introduce \\textbf{Search-o1}, a framework that enhances LRMs with an agentic retrieval-augmented generation (RAG) mechanism and a Reason-in-Documents module for refining retrieved documents. Search-o1 integrates an agentic search workflow into the reasoning process, enabling dynamic retrieval of external knowledge when LRMs encounter uncertain knowledge points. Additionally, due to the verbose nature of retrieved documents, we design a separate Reason-in-Documents module to deeply analyze the retrieved information before injecting it into the reasoning chain, minimizing noise and preserving coherent reasoning flow. Extensive experiments on complex reasoning tasks in science, mathematics, and coding, as well as six open-domain QA benchmarks, demonstrate the strong performance of Search-o1. This approach enhances the trustworthiness and applicability of LRMs in complex reasoning tasks, paving the way for more reliable and versatile intelligent systems. The code is available at \\url{https://github.com/sunnynexus/Search-o1}.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Search"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.05366"
    },
    "1f869839ffaecbb783aa788d0ed5ebdc": {
        "title": "LLMs Model Non-WEIRD Populations: Experiments with Synthetic Cultural Agents",
        "authors": [
            "Augusto Gonzalez-Bonorino",
            "Monica Capra",
            "Emilio Pantoja"
        ],
        "date": "2025/01/12",
        "pdf": "http://arxiv.org/pdf/2501.06834",
        "abstract": "Despite its importance, studying economic behavior across diverse, non-WEIRD (Western, Educated, Industrialized, Rich, and Democratic) populations presents significant challenges. We address this issue by introducing a novel methodology that uses Large Language Models (LLMs) to create synthetic cultural agents (SCAs) representing these populations. We subject these SCAs to classic behavioral experiments, including the dictator and ultimatum games. Our results demonstrate substantial cross-cultural variability in experimental behavior. Notably, for populations with available data, SCAs&#39; behaviors qualitatively resemble those of real human subjects. For unstudied populations, our method can generate novel, testable hypotheses about economic behavior. By integrating AI into experimental economics, this approach offers an effective and ethical method to pilot experiments and refine protocols for hard-to-reach populations. Our study provides a new tool for cross-cultural economic studies and demonstrates how LLMs can help experimental behavioral research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Simulation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.06834"
    },
    "7b9dd6fc37e1a20f64f4c5f22ae6aa6a": {
        "title": "Talk to Right Specialists: Routing and Planning in Multi-agent System for Question Answering",
        "authors": [
            "Feijie Wu",
            "Zitao Li",
            "Fei Wei",
            "Yaliang Li",
            "Bolin Ding",
            "Jing Gao"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.07813",
        "abstract": "Leveraging large language models (LLMs), an agent can utilize retrieval-augmented generation (RAG) techniques to integrate external knowledge and increase the reliability of its responses. Current RAG-based agents integrate single, domain-specific knowledge sources, limiting their ability and leading to hallucinated or inaccurate responses when addressing cross-domain queries. Integrating multiple knowledge bases into a unified RAG-based agent raises significant challenges, including increased retrieval overhead and data sovereignty when sensitive data is involved. In this work, we propose RopMura, a novel multi-agent system that addresses these limitations by incorporating highly efficient routing and planning mechanisms. RopMura features two key components: a router that intelligently selects the most relevant agents based on knowledge boundaries and a planner that decomposes complex multi-hop queries into manageable steps, allowing for coordinating cross-domain responses. Experimental results demonstrate that RopMura effectively handles both single-hop and multi-hop queries, with the routing mechanism enabling precise answers for single-hop queries and the combined routing and planning mechanisms achieving accurate, multi-step resolutions for complex queries.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Planning"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.07813"
    },
    "063b44796c8c8be3d6c9b02aee88adf6": {
        "title": "Agent-Centric Projection of Prompting Techniques and Implications for Synthetic Training Data for Large Language Models",
        "authors": [
            "Dhruv Dhamani",
            "Mary Lou Maher"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.07815",
        "abstract": "Recent advances in prompting techniques and multi-agent systems for Large Language Models (LLMs) have produced increasingly complex approaches. However, we lack a framework for characterizing and comparing prompting techniques or understanding their relationship to multi-agent LLM systems. This position paper introduces and explains the concepts of linear contexts (a single, continuous sequence of interactions) and non-linear contexts (branching or multi-path) in LLM systems. These concepts enable the development of an agent-centric projection of prompting techniques, a framework that can reveal deep connections between prompting strategies and multi-agent systems. We propose three conjectures based on this framework: (1) results from non-linear prompting techniques can predict outcomes in equivalent multi-agent systems, (2) multi-agent system architectures can be replicated through single-LLM prompting techniques that simulate equivalent interaction patterns, and (3) these equivalences suggest novel approaches for generating synthetic training data. We argue that this perspective enables systematic cross-pollination of research findings between prompting and multi-agent domains, while providing new directions for improving both the design and training of future LLM systems.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Dataset"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.07815"
    },
    "5d735467a3ca05b8eaad8dd80a19c261": {
        "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
        "authors": [
            "Aditi Singh",
            "Abul Ehtesham",
            "Saket Kumar",
            "Tala Talaei Khoei"
        ],
        "date": "2025/01/15",
        "pdf": "http://arxiv.org/pdf/2501.09136",
        "abstract": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management. Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications. This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.",
        "code": "",
        "category": [
            [
                "Survey",
                null
            ]
        ],
        "url": "https://arxiv.org/abs/2501.09136"
    },
    "5613ddd27fb336ab0265da940cb4c896": {
        "title": "PlotEdit: Natural Language-Driven Accessible Chart Editing in PDFs via Multimodal LLM Agents",
        "authors": [
            "Kanika Goswami",
            "Puneet Mathur",
            "Ryan Rossi",
            "Franck Dernoncourt"
        ],
        "date": "2025/01/20",
        "pdf": "http://arxiv.org/pdf/2501.11233",
        "abstract": "Chart visualizations, while essential for data interpretation and communication, are predominantly accessible only as images in PDFs, lacking source data tables and stylistic information. To enable effective editing of charts in PDFs or digital scans, we present PlotEdit, a novel multi-agent framework for natural language-driven end-to-end chart image editing via self-reflective LLM agents. PlotEdit orchestrates five LLM agents: (1) Chart2Table for data table extraction, (2) Chart2Vision for style attribute identification, (3) Chart2Code for retrieving rendering code, (4) Instruction Decomposition Agent for parsing user requests into executable steps, and (5) Multimodal Editing Agent for implementing nuanced chart component modifications - all coordinated through multimodal feedback to maintain visual fidelity. PlotEdit outperforms existing baselines on the ChartCraft dataset across style, layout, format, and data-centric edits, enhancing accessibility for visually challenged users and improving novice productivity.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11233"
    },
    "2d5682f900e4853ed0ca1e1328887a98": {
        "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
        "authors": [
            "Zhili Cheng",
            "Yuge Tu",
            "Ran Li",
            "Shiqi Dai",
            "Jinyi Hu",
            "Shengding Hu",
            "Jiahao Li",
            "Yang Shi",
            "Tianyu Yu",
            "Weize Chen",
            "Lei Shi",
            "Maosong Sun"
        ],
        "date": "2025/01/21",
        "pdf": "http://arxiv.org/pdf/2501.11858",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.11858"
    },
    "09312c290520cd1bcdfd197df5ef21be": {
        "title": "UI-TARS: Pioneering Automated GUI Interaction with Native Agents",
        "authors": [
            "Yujia Qin",
            "Yining Ye",
            "Junjie Fang",
            "Haoming Wang",
            "Shihao Liang",
            "Shizuo Tian",
            "Junda Zhang",
            "Jiahao Li",
            "Yunxin Li",
            "Shijue Huang",
            "Wanjun Zhong",
            "Kuanye Li",
            "Jiale Yang",
            "Yu Miao",
            "Woyu Lin",
            "Longxiang Liu",
            "Xu Jiang",
            "Qianli Ma",
            "Jingyu Li",
            "Xiaojun Xiao",
            "Kai Cai",
            "Chuang Li",
            "Yaowei Zheng",
            "Chaolin Jin",
            "Chen Li",
            "Xiao Zhou",
            "Minchao Wang",
            "Haoli Chen",
            "Zhaojian Li",
            "Haihua Yang",
            "Haifeng Liu",
            "Feng Lin",
            "Tao Peng",
            "Xin Liu",
            "Guang Shi"
        ],
        "date": "2025/01/21",
        "pdf": "http://arxiv.org/pdf/2501.12326",
        "abstract": "This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12326"
    },
    "16bf32d84a47590d45c85592a76488fb": {
        "title": "FinSphere: A Conversational Stock Analysis Agent Equipped with Quantitative Tools based on Real-Time Database",
        "authors": [
            "Shijie Han",
            "Changhai Zhou",
            "Yiqing Shen",
            "Tianning Sun",
            "Yuhua Zhou",
            "Xiaoxia Wang",
            "Zhixiao Yang",
            "Jingshu Zhang",
            "Hongguang Li"
        ],
        "date": "2025/01/08",
        "pdf": "http://arxiv.org/pdf/2501.12399",
        "abstract": "Current financial Large Language Models (LLMs) struggle with two critical limitations: a lack of depth in stock analysis, which impedes their ability to generate professional-grade insights, and the absence of objective evaluation metrics to assess the quality of stock analysis reports. To address these challenges, this paper introduces FinSphere, a conversational stock analysis agent, along with three major contributions: (1) Stocksis, a dataset curated by industry experts to enhance LLMs&#39; stock analysis capabilities, (2) AnalyScore, a systematic evaluation framework for assessing stock analysis quality, and (3) FinSphere, an AI agent that can generate high-quality stock analysis reports in response to user queries. Experiments demonstrate that FinSphere achieves superior performance compared to both general and domain-specific LLMs, as well as existing agent-based systems, even when they are enhanced with real-time data access and few-shot guidance. The integrated framework, which combines real-time data feeds, quantitative tools, and an instruction-tuned LLM, yields substantial improvements in both analytical quality and practical applicability for real-world stock analysis.",
        "code": "",
        "category": [
            [
                "Application",
                "Finance"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.12399"
    },
    "47477179b28782d60edc21bdd95ea6ea": {
        "title": "AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback",
        "authors": [
            "Joshua Park",
            "Yongfeng Zhang"
        ],
        "date": "2025/01/23",
        "pdf": "http://arxiv.org/pdf/2501.13333",
        "abstract": "Multi-agent systems must decide which agent is the most appropriate for a given task. We propose a novel architecture for recommending which LLM agent out of many should perform a task given a natural language prompt by extending the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a top-1 accuracy of 92.2% with each classification taking less than 300 milliseconds. In contrast to traditional classification methods, our architecture is computationally cheap, adaptive to new classes, interpretable, and controllable with arbitrary metrics through reinforcement learning. By encoding natural language prompts into sentence embeddings, our model captures the semantic content relevant to recommending an agent. The distance between sentence embeddings that belong to the same agent is then minimized through fine-tuning and aligned to human values through reinforcement learning from human feedback. This allows the classification of natural language prompts based on their nearest neighbors by measuring the cosine similarity between embeddings. This work is made possible through the generation of a synthetic dataset for agent recommendation, which we have open-sourced to the public along with the code for AgentRec recommendation system at https://github.com/joshprk/agentrec.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "Feedback&Reflection"
            ],
            [
                "Interaction",
                "Role Playing"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.13333"
    },
    "63c1f5c83cd2e780c8ee5eca3cc188be": {
        "title": "Developing Enhanced Conversational Agents for Social Virtual Worlds",
        "authors": [
            "D. Griol",
            "A. Sanchis",
            "J. M. Molina",
            "Z. Callejas"
        ],
        "date": "2025/01/14",
        "pdf": "http://arxiv.org/pdf/2501.16341",
        "abstract": "In this paper, we present a methodology for the development of embodied conversational agents for social virtual worlds. The agents provide multimodal communication with their users in which speech interaction is included. Our proposal combines different techniques related to Artificial Intelligence, Natural Language Processing, Affective Computing, and User Modeling. Firstly, the developed conversational agents. A statistical methodology has been developed to model the system conversational behavior, which is learned from an initial corpus and improved with the knowledge acquired from the successive interactions. In addition, the selection of the next system response is adapted considering information stored into users profiles and also the emotional contents detected in the users utterances. Our proposal has been evaluated with the successful development of an embodied conversational agent which has been placed in the Second Life social virtual world. The avatar includes the different models and interacts with the users who inhabit the virtual world in order to provide academic information. The experimental results show that the agents conversational behavior adapts successfully to the specific characteristics of users interacting in such environments.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.16341"
    },
    "45a66881a718ec4b28663d50c69f62df": {
        "title": "CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation",
        "authors": [
            "Faria Huq",
            "Zora Zhiruo Wang",
            "Frank F. Xu",
            "Tianyue Ou",
            "Shuyan Zhou",
            "Jeffrey P. Bigham",
            "Graham Neubig"
        ],
        "date": "2025/01/28",
        "pdf": "http://arxiv.org/pdf/2501.16609",
        "abstract": "While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent&#39;s capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html",
        "code": "",
        "category": [
            [
                "Interaction",
                "Human-Agent Interaction"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.16609"
    },
    "8b5a90b00bf3332dc22a161e86b46376": {
        "title": "Context is Key for Agent Security",
        "authors": [
            "Lillian Tsai",
            "Eugene Bagdasarian"
        ],
        "date": "2025/01/28",
        "pdf": "http://arxiv.org/pdf/2501.17070",
        "abstract": "Judging the safety of an action, whether taken by a human or a system, must take into account the context in which the action takes place. For example, deleting an email from a user&#39;s mailbox may or may not be appropriate depending on the email&#39;s content, the user&#39;s goals, or even available space. Systems today that make these judgements -- providing security against harmful or inappropriate actions -- rely on manually-crafted policies or user confirmation for each relevant context. With the upcoming deployment of systems like generalist agents, we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual security for agents (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.",
        "code": "",
        "category": [
            [
                "Stability",
                "Safety"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.17070"
    },
    "7c0c41b3a914bc731bc0f01e3bd65515": {
        "title": "Enabling Autonomic Microservice Management through Self-Learning Agents",
        "authors": [
            "Fenglin Yu",
            "Fangkai Yang",
            "Xiaoting Qin",
            "Zhiyang Zhang",
            "Jue Zhang",
            "Qingwei Lin",
            "Hongyu Zhang",
            "Yingnong Dang",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "date": "2025/01/31",
        "pdf": "http://arxiv.org/pdf/2501.19056",
        "abstract": "The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2501.19056"
    },
    "177773166ac1ce4eec643e253a85c0cc": {
        "title": "The Ann Arbor Architecture for Agent-Oriented Programming",
        "authors": [
            "Wei Dong"
        ],
        "date": "2025/02/14",
        "pdf": "http://arxiv.org/pdf/2502.09903",
        "abstract": "In this paper, we reexamine prompt engineering for large language models through the lens of automata theory. We argue that language models function as automata and, like all automata, should be programmed in the languages they accept, a unified collection of all natural and formal languages. Therefore, traditional software engineering practices--conditioned on the clear separation of programming languages and natural languages--must be rethought. We introduce the Ann Arbor Architecture, a conceptual framework for agent-oriented programming of language models, as a higher-level abstraction over raw token generation, and provide a new perspective on in-context learning. Based on this framework, we present the design of our agent platform Postline, and report on our initial experiments in agent training.",
        "code": "https://github.com/aaalgo/postline_0.1",
        "category": [
            [
                "Infrastructure",
                "Environment&Platform"
            ],
            [
                "Application",
                "Software Engineering"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.09903"
    },
    "6f9c1ac089493ef3ed381bcddd216a44": {
        "title": "The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems",
        "authors": [
            "Zengqing Wu",
            "Takayuki Ito"
        ],
        "date": "2025/02/23",
        "pdf": "http://arxiv.org/pdf/2502.16565",
        "abstract": "Consensus formation is pivotal in multi-agent systems (MAS), balancing collective coherence with individual diversity. Conventional LLM-based MAS primarily rely on explicit coordination, e.g., prompts or voting, risking premature homogenization. We argue that implicit consensus, where agents exchange information yet independently form decisions via in-context learning, can be more effective in dynamic environments that require long-horizon adaptability. By retaining partial diversity, systems can better explore novel strategies and cope with external shocks. We formalize a consensus-diversity tradeoff, showing conditions where implicit methods outperform explicit ones. Experiments on three scenarios -- Dynamic Disaster Response, Information Spread and Manipulation, and Dynamic Public-Goods Provision -- confirm partial deviation from group norms boosts exploration, robustness, and performance. We highlight emergent coordination via in-context learning, underscoring the value of preserving diversity for resilient decision-making.",
        "code": "https://github.com/wuzengqing001225/ConsensusDiversityTradeoffMAS",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.16565"
    },
    "99419075ed21de8e2af08ee4afa13062": {
        "title": "MobileSteward: Integrating Multiple App-Oriented Agents with Self-Evolution to Automate Cross-App Instructions",
        "authors": [
            "Yuxuan Liu",
            "Hongda Sun",
            "Wei Liu",
            "Jian Luan",
            "Bo Du",
            "Rui Yan"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.16796",
        "abstract": "Mobile phone agents can assist people in automating daily tasks on their phones, which have emerged as a pivotal research spotlight. However, existing procedure-oriented agents struggle with cross-app instructions, due to the following challenges: (1) complex task relationships, (2) diverse app environment, and (3) error propagation and information loss in multi-step execution. Drawing inspiration from object-oriented programming principles, we recognize that object-oriented solutions is more suitable for cross-app instruction. To address these challenges, we propose a self-evolving multi-agent framework named MobileSteward, which integrates multiple app-oriented StaffAgents coordinated by a centralized StewardAgent. We design three specialized modules in MobileSteward: (1) Dynamic Recruitment generates a scheduling graph guided by information flow to explicitly associate tasks among apps. (2) Assigned Execution assigns the task to app-oriented StaffAgents, each equipped with app-specialized expertise to address the diversity between apps. (3) Adjusted Evaluation conducts evaluation to provide reflection tips or deliver key information, which alleviates error propagation and information loss during multi-step execution. To continuously improve the performance of MobileSteward, we develop a Memory-based Self-evolution mechanism, which summarizes the experience from successful execution, to improve the performance of MobileSteward. We establish the first English Cross-APP Benchmark (CAPBench) in the real-world environment to evaluate the agents&#39; capabilities of solving complex cross-app instructions. Experimental results demonstrate that MobileSteward achieves the best performance compared to both single-agent and multi-agent frameworks, highlighting the superiority of MobileSteward in better handling user instructions with diverse complexity.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.16796"
    },
    "9c6712e928529179d418e2f944f501a6": {
        "title": "Mobile-Agent-V: Learning Mobile Device Operation Through Video-Guided Multi-Agent Collaboration",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Xi Zhang",
            "Ming Yan",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17110",
        "abstract": "The rapid increase in mobile device usage necessitates improved automation for seamless task management. However, many AI-driven frameworks struggle due to insufficient operational knowledge. Manually written knowledge helps but is labor-intensive and inefficient. To address these challenges, we introduce Mobile-Agent-V, a framework that leverages video guidance to provide rich and cost-effective operational knowledge for mobile automation. Mobile-Agent-V enhances task execution capabilities by leveraging video inputs without requiring specialized sampling or preprocessing. Mobile-Agent-V integrates a sliding window strategy and incorporates a video agent and deep-reflection agent to ensure that actions align with user instructions. Through this innovative approach, users can record task processes with guidance, enabling the system to autonomously learn and execute tasks efficiently. Experimental results show that Mobile-Agent-V achieves a 30% performance improvement compared to existing frameworks. The code will be open-sourced at https://github.com/X-PLUG/MobileAgent.",
        "code": "https://github.com/X-PLUG/MobileAgent",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ],
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17110"
    },
    "3331d194b7f2bf339cf564c4b2e98a9e": {
        "title": "Stay Focused: Problem Drift in Multi-Agent Debate",
        "authors": [
            "Jonas Becker",
            "Lars Benedikt Kaesberg",
            "Andreas Stephan",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19559",
        "abstract": "Multi-agent debate - multiple instances of large language models discussing problems in turn-based interaction - has shown promise for solving knowledge and reasoning tasks. However, these methods show limitations, particularly when scaling them to longer reasoning chains. In this study, we unveil a new issue of multi-agent debate: discussions drift away from the initial problem over multiple turns. We define this phenomenon as problem drift and quantify its presence across ten tasks (i.e., three generative, three knowledge, three reasoning, and one instruction-following task). To identify the reasons for this issue, we perform a human study with eight experts on discussions suffering from problem drift, who find the most common issues are a lack of progress (35% of cases), low-quality feedback (26% of cases), and a lack of clarity (25% of cases). To systematically address the issue of problem drift, we propose DRIFTJudge, a method based on LLM-as-a-judge, to detect problem drift at test-time. We further propose DRIFTPolicy, a method to mitigate 31% of problem drift cases. Our study can be seen as a first step to understanding a key limitation of multi-agent debate, highlighting pathways for improving their effectiveness in the future.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19559"
    },
    "cf78b1c292d49a864a8fed69270d38b7": {
        "title": "Winning Big with Small Models: Knowledge Distillation vs. Self-Training for Reducing Hallucination in QA Agents",
        "authors": [
            "Ashley Lewis",
            "Michael White",
            "Jing Liu",
            "Toshiaki Koike-Akino",
            "Kieran Parsons",
            "Ye Wang"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19545",
        "abstract": "The deployment of Large Language Models (LLMs) in customer support is constrained by hallucination-generating false information-and the high cost of proprietary models. To address these challenges, we propose a retrieval-augmented question-answering (QA) pipeline and explore how to balance human input and automation. Using a dataset of questions about a Samsung Smart TV user manual, we demonstrate that synthetic data generated by LLMs outperforms crowdsourced data in reducing hallucination in finetuned models. We also compare self-training (fine-tuning models on their own outputs) and knowledge distillation (fine-tuning on stronger models&#39; outputs, e.g., GPT-4o), and find that self-training achieves comparable hallucination reduction. We conjecture that this surprising finding can be attributed to increased exposure bias issues in the knowledge distillation case and support this conjecture with post hoc analysis. We also improve robustness to unanswerable questions and retrieval failures with contextualized &#34;I don&#39;t know&#34; responses. These findings show that scalable, cost-efficient QA systems can be built using synthetic data and self-training with open-source models, reducing reliance on proprietary tools or costly human annotations.",
        "code": "",
        "category": [
            [
                "Stability",
                "Hallucination"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19545"
    },
    "d1c5e4d780996dc488d652dbb43e3a14": {
        "title": "Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis",
        "authors": [
            "Jeffrey Yang Fan Chiang",
            "Seungjae Lee",
            "Jia-Bin Huang",
            "Furong Huang",
            "Yizheng Chen"
        ],
        "date": "2025/02/27",
        "pdf": "http://arxiv.org/pdf/2502.20383",
        "abstract": "Recent advancements in Web AI agents have demonstrated remarkable capabilities in addressing complex web navigation tasks. However, emerging research shows that these agents exhibit greater vulnerability compared to standalone Large Language Models (LLMs), despite both being built upon the same safety-aligned models. This discrepancy is particularly concerning given the greater flexibility of Web AI Agent compared to standalone LLMs, which may expose them to a wider range of adversarial user inputs. To build a scaffold that addresses these concerns, this study investigates the underlying factors that contribute to the increased vulnerability of Web AI agents. Notably, this disparity stems from the multifaceted differences between Web AI agents and standalone LLMs, as well as the complex signals - nuances that simple evaluation metrics, such as success rate, often fail to capture. To tackle these challenges, we propose a component-level analysis and a more granular, systematic evaluation framework. Through this fine-grained investigation, we identify three critical factors that amplify the vulnerability of Web AI agents; (1) embedding user goals into the system prompt, (2) multi-step action generation, and (3) observational capabilities. Our findings highlights the pressing need to enhance security and robustness in AI agent design and provide actionable insights for targeted defense strategies.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Tool Usage"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.20383"
    },
    "653218e7695b65021161f0517044ac67": {
        "title": "M^3Builder: A Multi-Agent System for Automated Machine Learning in Medical Imaging",
        "authors": [
            "Jinghao Feng",
            "Qiaoyu Zheng",
            "Chaoyi Wu",
            "Ziheng Zhao",
            "Ya Zhang",
            "Yanfeng Wang",
            "Weidi Xie"
        ],
        "date": "2025/02/27",
        "pdf": "http://arxiv.org/pdf/2502.20301",
        "abstract": "Agentic AI systems have gained significant attention for their ability to autonomously perform complex tasks. However, their reliance on well-prepared tools limits their applicability in the medical domain, which requires to train specialized models. In this paper, we make three contributions: (i) We present M3Builder, a novel multi-agent system designed to automate machine learning (ML) in medical imaging. At its core, M3Builder employs four specialized agents that collaborate to tackle complex, multi-step medical ML workflows, from automated data processing and environment configuration to self-contained auto debugging and model training. These agents operate within a medical imaging ML workspace, a structured environment designed to provide agents with free-text descriptions of datasets, training codes, and interaction tools, enabling seamless communication and task execution. (ii) To evaluate progress in automated medical imaging ML, we propose M3Bench, a benchmark comprising four general tasks on 14 training datasets, across five anatomies and three imaging modalities, covering both 2D and 3D data. (iii) We experiment with seven state-of-the-art large language models serving as agent cores for our system, such as Claude series, GPT-4o, and DeepSeek-V3. Compared to existing ML agentic designs, M3Builder shows superior performance on completing ML tasks in medical imaging, achieving a 94.29% success rate using Claude-3.7-Sonnet as the agent core, showing huge potential towards fully automated machine learning in medical imaging.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.20301"
    },
    "33eceb2cc2ba664c62485d4f0ca022af": {
        "title": "Voting or Consensus? Decision-Making in Multi-Agent Debate",
        "authors": [
            "Lars Benedikt Kaesberg",
            "Jonas Becker",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19130",
        "abstract": "Much of the success of multi-agent debates depends on carefully choosing the right parameters. Among them, the decision-making protocol stands out. Systematic comparison of decision protocols is difficult because studies alter multiple discussion parameters beyond the protocol. So far, it has been largely unknown how decision-making addresses the challenges of different tasks. This work systematically evaluates the impact of seven decision protocols (e.g., majority voting, unanimity consensus). We change only one variable at a time (i.e., decision protocol) to analyze how different methods affect the collaboration between agents and test different protocols on knowledge (MMLU, MMLU-Pro, GPQA) and reasoning datasets (StrategyQA, MuSR, SQuAD 2.0). Our results show that voting protocols improve performance by 13.2% in reasoning tasks and consensus protocols by 2.8% in knowledge tasks over the other decision protocol. Increasing the number of agents improves performance, while more discussion rounds before voting reduces it. To improve decision-making by increasing answer diversity, we propose two new methods, All-Agents Drafting (AAD) and Collective Improvement (CI). Our methods improve task performance by up to 3.3% with AAD and up to 7.4% with CI. This work demonstrates the importance of decision-making in multi-agent debates beyond scaling.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19130"
    },
    "f5de8450ff665a0dd1c99cb1221a1488": {
        "title": "Agentic Reward Modeling: Integrating Human Preferences with Verifiable Correctness Signals for Reliable Reward Systems",
        "authors": [
            "Hao Peng",
            "Yunjia Qi",
            "Xiaozhi Wang",
            "Zijun Yao",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19328",
        "abstract": "Reward models (RMs) are crucial for the training and inference-time scaling up of large language models (LLMs). However, existing reward models primarily focus on human preferences, neglecting verifiable correctness signals which have shown strong potential in training LLMs. In this paper, we propose agentic reward modeling, a reward system that combines reward models with verifiable correctness signals from different aspects to provide reliable rewards. We empirically implement a reward agent, named RewardAgent, that combines human preference rewards with two verifiable signals: factuality and instruction following, to provide more reliable rewards. We conduct comprehensive experiments on existing reward model benchmarks and inference time best-of-n searches on real-world downstream tasks. RewardAgent significantly outperforms vanilla reward models, demonstrating its effectiveness. We further construct training preference pairs using RewardAgent and train an LLM with the DPO objective, achieving superior performance on various NLP benchmarks compared to conventional reward models. Our codes are publicly released to facilitate further research (https://github.com/THU-KEG/Agentic-Reward-Modeling).",
        "code": "https://github.com/THU-KEG/Agentic-Reward-Modeling",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Training",
                "DPO"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19328"
    },
    "3fe124c386a094c51407860228416e77": {
        "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis",
        "authors": [
            "Daniel Rose",
            "Chia-Chien Hung",
            "Marco Lepri",
            "Israa Alqassem",
            "Kiril Gashteovski",
            "Carolin Lawrence"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19175",
        "abstract": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19175"
    },
    "bad7c9860f3358a986e817507d081e4b": {
        "title": "Enhancing Text Classification with a Novel Multi-Agent Collaboration Framework Leveraging BERT",
        "authors": [
            "Hediyeh Baban",
            "Sai A Pidapar",
            "Aashutosh Nema",
            "Sichen Lu"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18653",
        "abstract": "We introduce a novel multi-agent collaboration framework designed to enhance the accuracy and robustness of text classification models. Leveraging BERT as the primary classifier, our framework dynamically escalates low-confidence predictions to a specialized multi-agent system comprising Lexical, Contextual, Logic, Consensus, and Explainability agents. This collaborative approach allows for comprehensive analysis and consensus-driven decision-making, significantly improving classification performance across diverse text classification tasks. Empirical evaluations on benchmark datasets demonstrate that our framework achieves a 5.5% increase in accuracy compared to standard BERT-based classifiers, underscoring its effectiveness and academic novelty in advancing multi-agent systems within natural language processing.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18653"
    },
    "6c11b5a1ee6175776c6a68083bedff85": {
        "title": "TheoremExplainAgent: Towards Multimodal Explanations for LLM Theorem Understanding",
        "authors": [
            "Max Ku",
            "Thomas Chong",
            "Jonathan Leung",
            "Krish Shah",
            "Alvin Yu",
            "Wenhu Chen"
        ],
        "date": "2025/02/26",
        "pdf": "http://arxiv.org/pdf/2502.19400",
        "abstract": "Understanding domain-specific theorems often requires more than just text-based reasoning; effective communication through structured visual explanations is crucial for deeper comprehension. While large language models (LLMs) demonstrate strong performance in text-based theorem reasoning, their ability to generate coherent and pedagogically meaningful visual explanations remains an open challenge. In this work, we introduce TheoremExplainAgent, an agentic approach for generating long-form theorem explanation videos (over 5 minutes) using Manim animations. To systematically evaluate multimodal theorem explanations, we propose TheoremExplainBench, a benchmark covering 240 theorems across multiple STEM disciplines, along with 5 automated evaluation metrics. Our results reveal that agentic planning is essential for generating detailed long-form videos, and the o3-mini agent achieves a success rate of 93.8% and an overall score of 0.77. However, our quantitative and qualitative studies show that most of the videos produced exhibit minor issues with visual element layout. Furthermore, multimodal explanations expose deeper reasoning flaws that text-based explanations fail to reveal, highlighting the importance of multimodal explanations.",
        "code": "",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ],
            [
                "Scaling",
                "Single-Agent Framework"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.19400"
    },
    "929d8ef67b10589b81a331f50b04b5b8": {
        "title": "A Cooperative Multi-Agent Framework for Zero-Shot Named Entity Recognition",
        "authors": [
            "Zihan Wang",
            "Ziqi Zhao",
            "Yougang Lyu",
            "Zhumin Chen",
            "Maarten de Rijke",
            "Zhaochun Ren"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18702",
        "abstract": "Zero-shot named entity recognition (NER) aims to develop entity recognition systems from unannotated text corpora. This task presents substantial challenges due to minimal human intervention. Recent work has adapted large language models (LLMs) for zero-shot NER by crafting specialized prompt templates. It advances model self-learning abilities by incorporating self-annotated demonstrations. However, two important challenges persist: (i) Correlations between contexts surrounding entities are overlooked, leading to wrong type predictions or entity omissions. (ii) The indiscriminate use of task demonstrations, retrieved through shallow similarity-based strategies, severely misleads LLMs during inference. In this paper, we introduce the cooperative multi-agent system (CMAS), a novel framework for zero-shot NER that uses the collective intelligence of multiple agents to address the challenges outlined above. CMAS has four main agents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor, (iii) a demonstration discriminator, and (iv) an overall predictor. To explicitly capture correlations between contexts surrounding entities, CMAS reformulates NER into two subtasks: recognizing named entities and identifying entity type-related features within the target sentence. To enable controllable utilization of demonstrations, a demonstration discriminator is established to incorporate the self-reflection mechanism, automatically evaluating helpfulness scores for the target sentence. Experimental results show that CMAS significantly improves zero-shot NER performance across six benchmarks, including both domain-specific and general-domain scenarios. Furthermore, CMAS demonstrates its effectiveness in few-shot settings and with various LLM backbones.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18702"
    },
    "b7d328538c325c921ce66febff2a3c75": {
        "title": "Scaffolding Empathy: Training Counselors with Simulated Patients and Utterance-level Performance Visualizations",
        "authors": [
            "Ian Steenstra",
            "Farnaz Nouraei",
            "Timothy W. Bickmore"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18673",
        "abstract": "Learning therapeutic counseling involves significant role-play experience with mock patients, with current manual training methods providing only intermittent granular feedback. We seek to accelerate and optimize counselor training by providing frequent, detailed feedback to trainees as they interact with a simulated patient. Our first application domain involves training motivational interviewing skills for counselors. Motivational interviewing is a collaborative counseling style in which patients are guided to talk about changing their behavior, with empathetic counseling an essential ingredient. We developed and evaluated an LLM-powered training system that features a simulated patient and visualizations of turn-by-turn performance feedback tailored to the needs of counselors learning motivational interviewing. We conducted an evaluation study with professional and student counselors, demonstrating high usability and satisfaction with the system. We present design implications for the development of automated systems that train users in counseling skills and their generalizability to other types of social skills training.",
        "code": "",
        "category": [
            [
                "Application",
                "Medicine"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18673"
    },
    "69e70ffee7c91c8f86a98df96a2933e8": {
        "title": "AgentRM: Enhancing Agent Generalization with Reward Modeling",
        "authors": [
            "Yu Xia",
            "Jingru Fan",
            "Weize Chen",
            "Siyu Yan",
            "Xin Cong",
            "Zhong Zhang",
            "Yaxi Lu",
            "Yankai Lin",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18407",
        "abstract": "Existing LLM-based agents have achieved strong performance on held-in tasks, but their generalizability to unseen tasks remains poor. Hence, some recent work focus on fine-tuning the policy model with more diverse tasks to improve the generalizability. In this work, we find that finetuning a reward model to guide the policy model is more robust than directly finetuning the policy model. Based on this finding, we propose AgentRM, a generalizable reward model, to guide the policy model for effective test-time search. We comprehensively investigate three approaches to construct the reward model, including explicit reward modeling, implicit reward modeling and LLM-as-a-judge. We then use AgentRM to guide the answer generation with Best-of-N sampling and step-level beam search. On four types of nine agent tasks, AgentRM enhances the base policy model by $8.8$ points on average, surpassing the top general agent by $4.0$. Moreover, it demonstrates weak-to-strong generalization, yielding greater improvement of $12.6$ on LLaMA-3-70B policy model. As for the specializability, AgentRM can also boost a finetuned policy model and outperform the top specialized agent by $11.4$ on three held-in tasks. Further analysis verifies its effectiveness in test-time scaling. Codes will be released to facilitate the research in this area.",
        "code": "",
        "category": [
            [
                "Training",
                "RL"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18407"
    },
    "40b0a9827149a058e2153598bbe02542": {
        "title": "RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction",
        "authors": [
            "Jianhao Yan",
            "Yun Luo",
            "Yue Zhang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18308",
        "abstract": "In the multi-turn interaction schema, large language models (LLMs) can leverage user feedback to enhance the quality and relevance of their responses. However, evaluating an LLM&#39;s ability to incorporate user refutation feedback is crucial yet challenging. In this study, we introduce RefuteBench 2.0, which significantly extends the original RefuteBench by incorporating LLM agents as refuters and evaluators, which allows for flexible and comprehensive assessment. We design both transient and persistent refutation instructions with different validity periods. Meta-evaluation shows that the LLM-based refuter could generate more human-like refutations and the evaluators could assign scores with high correlation with humans. Experimental results of various LLMs show that current models could effectively satisfy the refutation but fail to memorize the refutation information. Interestingly, we also observe that the performance of the initial task decreases as the refutations increase. Analysis of the attention scores further shows a potential weakness of current LLMs: they struggle to retain and correctly use previous information during long context dialogues. https://github.com/ElliottYan/RefuteBench-2.0",
        "code": "https://github.com/ElliottYan/RefuteBench-2.0",
        "category": [
            [
                "Infrastructure",
                "Benchmark&Evaluation"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18308"
    },
    "31071f7714f91d731ef370175c9eec55": {
        "title": "Debt Collection Negotiations with Large Language Models: An Evaluation System and Optimizing Decision Making with Multi-Agent",
        "authors": [
            "Xiaofeng Wang",
            "Zhixin Zhang",
            "Jinguang Zheng",
            "Yiming Ai",
            "Rui Wang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18228",
        "abstract": "Debt collection negotiations (DCN) are vital for managing non-performing loans (NPLs) and reducing creditor losses. Traditional methods are labor-intensive, while large language models (LLMs) offer promising automation potential. However, prior systems lacked dynamic negotiation and real-time decision-making capabilities. This paper explores LLMs in automating DCN and proposes a novel evaluation framework with 13 metrics across 4 aspects. Our experiments reveal that LLMs tend to over-concede compared to human negotiators. To address this, we propose the Multi-Agent Debt Negotiation (MADeN) framework, incorporating planning and judging modules to improve decision rationality. We also apply post-training techniques, including DPO with rejection sampling, to optimize performance. Our studies provide valuable insights for practitioners and researchers seeking to enhance efficiency and outcomes in this domain.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18228"
    },
    "08a4ce85e4e77cacad09ecb623d02ad6": {
        "title": "LAG: LLM agents for Leaderboard Auto Generation on Demanding",
        "authors": [
            "Jian Wu",
            "Jiayu Zhang",
            "Dongyuan Li",
            "Linyi Yang",
            "Aoxiao Zhong",
            "Renhe Jiang",
            "Qingsong Wen",
            "Yue Zhang"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18209",
        "abstract": "This paper introduces Leaderboard Auto Generation (LAG), a novel and well-organized framework for automatic generation of leaderboards on a given research topic in rapidly evolving fields like Artificial Intelligence (AI). Faced with a large number of AI papers updated daily, it becomes difficult for researchers to track every paper&#39;s proposed methods, experimental results, and settings, prompting the need for efficient automatic leaderboard construction. While large language models (LLMs) offer promise in automating this process, challenges such as multi-document summarization, leaderboard generation, and experiment fair comparison still remain under exploration. LAG solves these challenges through a systematic approach that involves the paper collection, experiment results extraction and integration, leaderboard generation, and quality evaluation. Our contributions include a comprehensive solution to the leaderboard construction problem, a reliable evaluation method, and experimental results showing the high quality of leaderboards.",
        "code": "",
        "category": [
            [
                "Application",
                "Research"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18209"
    },
    "f4db64903e9b8877fbfba9827856f04b": {
        "title": "FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking Evaluation of Large Language Models",
        "authors": [
            "Hongzhan Lin",
            "Yang Deng",
            "Yuxuan Gu",
            "Wenxuan Zhang",
            "Jing Ma",
            "See-Kiong Ng",
            "Tat-Seng Chua"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.17924",
        "abstract": "Large Language Models (LLMs) have significantly advanced the fact-checking studies. However, existing automated fact-checking evaluation methods rely on static datasets and classification metrics, which fail to automatically evaluate the justification production and uncover the nuanced limitations of LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven framework that adaptively and dynamically assesses LLMs&#39; fact-checking capabilities. Leveraging importance sampling principles and multi-agent collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs iterative model-centric evaluations, and updates assessments based on model-specific responses. By incorporating justification production alongside verdict prediction, this framework provides a comprehensive and evolving audit of LLMs&#39; factual reasoning capabilities, to investigate their trustworthiness. Extensive experiments demonstrate that FACT-AUDIT effectively differentiates among state-of-the-art LLMs, providing valuable insights into model strengths and limitations in model-centric fact-checking analysis.",
        "code": "",
        "category": [
            [
                "Automation",
                "Automatic Evaluation"
            ],
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17924"
    },
    "710a7abc2158db6d47ede7575e9a90b1": {
        "title": "ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents",
        "authors": [
            "Qiuchen Wang",
            "Ruixue Ding",
            "Zehui Chen",
            "Weiqi Wu",
            "Shihang Wang",
            "Pengjun Xie",
            "Feng Zhao"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.18017",
        "abstract": "Understanding information from visually rich documents remains a significant challenge for traditional Retrieval-Augmented Generation (RAG) methods. Existing benchmarks predominantly focus on image-based question answering (QA), overlooking the fundamental challenges of efficient retrieval, comprehension, and reasoning within dense visual documents. To bridge this gap, we introduce ViDoSeek, a novel dataset designed to evaluate RAG performance on visually rich documents requiring complex reasoning. Based on it, we identify key limitations in current RAG approaches: (i) purely visual retrieval methods struggle to effectively integrate both textual and visual features, and (ii) previous approaches often allocate insufficient reasoning tokens, limiting their effectiveness. To address these challenges, we propose ViDoRAG, a novel multi-agent RAG framework tailored for complex reasoning across visual documents. ViDoRAG employs a Gaussian Mixture Model (GMM)-based hybrid strategy to effectively handle multi-modal retrieval. To further elicit the model&#39;s reasoning capabilities, we introduce an iterative agent workflow incorporating exploration, summarization, and reflection, providing a framework for investigating test-time scaling in RAG domains. Extensive experiments on ViDoSeek validate the effectiveness and generalization of our approach. Notably, ViDoRAG outperforms existing methods by over 10% on the competitive ViDoSeek benchmark.",
        "code": "",
        "category": [
            [
                "Technique For Enhancement",
                "RAG"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.18017"
    },
    "aeee067bd0d4eb3c4a4a866683b07066": {
        "title": "LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena",
        "authors": [
            "Tianmi Ma",
            "Jiawei Du",
            "Wenxin Huang",
            "Wenjie Wang",
            "Liang Xie",
            "Xian Zhong",
            "Joey Tianyi Zhou"
        ],
        "date": "2025/02/25",
        "pdf": "http://arxiv.org/pdf/2502.17967",
        "abstract": "Recent advancements in large language models (LLMs) have significantly improved performance in natural language processing tasks. However, their ability to generalize to dynamic, unseen tasks, particularly in numerical reasoning, remains a challenge. Existing benchmarks mainly evaluate LLMs on problems with predefined optimal solutions, which may not align with real-world scenarios where clear answers are absent. To bridge this gap, we design the Agent Trading Arena, a virtual numerical game simulating complex economic systems through zero-sum games, where agents invest in stock portfolios. Our experiments reveal that LLMs, including GPT-4o, struggle with algebraic reasoning when dealing with plain-text stock data, often focusing on local details rather than global trends. In contrast, LLMs perform significantly better with geometric reasoning when presented with visual data, such as scatter plots or K-line charts, suggesting that visual representations enhance numerical reasoning. This capability is further improved by incorporating the reflection module, which aids in the analysis and interpretation of complex data. We validate our findings on NASDAQ Stock dataset, where LLMs demonstrate stronger reasoning with visual data compared to text. Our code and data are publicly available at https://github.com/wekjsdvnm/Agent-Trading-Arena.git.",
        "code": "",
        "category": [
            [
                "Application",
                "Math"
            ],
            [
                "Application",
                "Finance"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17967"
    },
    "b72e503d6aad3525a17bee4add471676": {
        "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling",
        "authors": [
            "Bingxuan Li",
            "Yiwei Wang",
            "Jiuxiang Gu",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17651",
        "abstract": "Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement in accuracy over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.",
        "code": "",
        "category": [
            [
                "Scaling",
                "Multi-Agent System"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17651"
    },
    "2be39e1583cea697b224e3752c123671": {
        "title": "Training a Generally Curious Agent",
        "authors": [
            "Fahim Tajwar",
            "Yiding Jiang",
            "Abitha Thankaraj",
            "Sumaita Sadia Rahman",
            "J Zico Kolter",
            "Jeff Schneider",
            "Ruslan Salakhutdinov"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17543",
        "abstract": "Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present PAPRIKA, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, PAPRIKA teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with PAPRIKA can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach&#39;s primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.",
        "code": "",
        "category": [
            [
                "Training",
                "Fine tuning"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17543"
    },
    "8d04925aa85c0d1934ad4dbbb502c5ec": {
        "title": "Turning Conversations into Workflows: A Framework to Extract and Evaluate Dialog Workflows for Service AI Agents",
        "authors": [
            "Prafulla Kumar Choubey",
            "Xiangyu Peng",
            "Shilpa Bhagavath",
            "Caiming Xiong",
            "Shiva Kumar Pentyala",
            "Chien-Sheng Wu"
        ],
        "date": "2025/02/24",
        "pdf": "http://arxiv.org/pdf/2502.17321",
        "abstract": "Automated service agents require well-structured workflows to provide consistent and accurate responses to customer queries. However, these workflows are often undocumented, and their automatic extraction from conversations remains unexplored. In this work, we present a novel framework for extracting and evaluating dialog workflows from historical interactions. Our extraction process consists of two key stages: (1) a retrieval step to select relevant conversations based on key procedural elements, and (2) a structured workflow generation process using a question-answer-based chain-of-thought (QA-CoT) prompting. To comprehensively assess the quality of extracted workflows, we introduce an automated agent and customer bots simulation framework that measures their effectiveness in resolving customer issues. Extensive experiments on the ABCD and SynthABCD datasets demonstrate that our QA-CoT technique improves workflow extraction by 12.16\\% in average macro accuracy over the baseline. Moreover, our evaluation method closely aligns with human assessments, providing a reliable and scalable framework for future research.",
        "code": "",
        "category": [
            [
                "Interaction",
                "Conversation"
            ],
            [
                "Automation",
                "Workflow"
            ]
        ],
        "url": "https://arxiv.org/abs/2502.17321"
    }
}